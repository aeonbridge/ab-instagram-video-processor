# Agno - Llms-Txt

**Pages:** 2506

---

## OpenAI o1 pro

**URL:** llms-txt#openai-o1-pro

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/o1-pro

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create custom loggers for different components

**URL:** llms-txt#create-custom-loggers-for-different-components

custom_agent_logger = logging.getLogger("agent_logger")
custom_team_logger = logging.getLogger("team_logger")
custom_workflow_logger = logging.getLogger("workflow_logger")

---

## Agentic approach (recommended)

**URL:** llms-txt#agentic-approach-(recommended)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True  # Agent decides when to search
)

---

## Create Agent Run

**URL:** llms-txt#create-agent-run

Source: https://docs.agno.com/reference-api/schema/agents/create-agent-run

post /agents/{agent_id}/runs
Execute an agent with a message and optional media files. Supports both streaming and non-streaming responses.

**Features:**
- Text message input with optional session management
- Multi-media support: images (PNG, JPEG, WebP), audio (WAV, MP3), video (MP4, WebM, etc.)
- Document processing: PDF, CSV, DOCX, TXT, JSON
- Real-time streaming responses with Server-Sent Events (SSE)
- User and session context preservation

**Streaming Response:**
When `stream=true`, returns SSE events with `event` and `data` fields.

---

## Get Memory Topics

**URL:** llms-txt#get-memory-topics

Source: https://docs.agno.com/reference-api/schema/memory/get-memory-topics

get /memory_topics
Retrieve all unique topics associated with memories in the system. Useful for filtering and categorizing memories by topic.

---

## Time your searches

**URL:** llms-txt#time-your-searches

**Contents:**
- Next Steps

start = time.time()
results = knowledge.search("test query", max_results=5)
elapsed = time.time() - start
print(f"Search took {elapsed:.2f} seconds")
```

<CardGroup cols={3}>
  <Card title="Chunking Strategies" icon="scissors" href="/basics/knowledge/chunking/overview">
    Learn how different chunking strategies affect performance
  </Card>

<Card title="Vector Databases" icon="database" href="/basics/vectordb/overview">
    Compare vector database options for your scale
  </Card>

<Card title="Embedders" icon="vector-square" href="/basics/knowledge/embedder/overview">
    Choose the right embedder for your use case
  </Card>

<Card title="Hybrid Search" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/hybrid-search">
    Combine vector and keyword search for better results
  </Card>
</CardGroup>

<Tip>
  **Start simple, optimize when needed.** Agno's defaults work well for most use cases. Profile your application to find actual bottlenecks before spending time on optimization.
</Tip>

---

## Agent with Tools and Streaming

**URL:** llms-txt#agent-with-tools-and-streaming

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/gateways/portkey/usage/tool-use-stream

```python cookbook/models/portkey/tool_use_stream.py theme={null}
from agno.agent import Agent
from agno.models.portkey import Portkey
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Portkey(id="@first-integrati-707071/gpt-5-nano"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## Create an Agent with the ModelsLabs tool

**URL:** llms-txt#create-an-agent-with-the-modelslabs-tool

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent = Agent(tools=[ModelsLabsTools()], name="ModelsLabs Agent")

agent.print_response("Generate a video of a beautiful sunset over the ocean", markdown=True)
```

| Parameter             | Type   | Default | Description                                                                |
| --------------------- | ------ | ------- | -------------------------------------------------------------------------- |
| `api_key`             | `str`  | `None`  | The ModelsLab API key for authentication                                   |
| `wait_for_completion` | `bool` | `False` | Whether to wait for the video to be ready                                  |
| `add_to_eta`          | `int`  | `15`    | Time to add to the ETA to account for the time it takes to fetch the video |
| `max_wait_time`       | `int`  | `60`    | Maximum time to wait for the video to be ready                             |
| `file_type`           | `str`  | `"mp4"` | The type of file to generate                                               |

| Function         | Description                                     |
| ---------------- | ----------------------------------------------- |
| `generate_media` | Generates a video or gif based on a text prompt |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/models_labs.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/models_labs_tools.py)

---

## Define agents

**URL:** llms-txt#define-agents

hackernews_agent = Agent(
    name="Hackernews Agent",
    model=Claude(id="claude-sonnet-4-5-20250929"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=Claude(id="claude-sonnet-4-5-20250929"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)

---

## Get Trace Detail

**URL:** llms-txt#get-trace-detail

Source: https://docs.agno.com/reference-api/schema/tracing/get-trace-detail

get /traces/{trace_id}
Retrieve detailed trace information with hierarchical span tree, or a specific span within the trace.

**Without span_id parameter:**
Returns the full trace with hierarchical span tree:
- Trace metadata (ID, status, duration, context)
- Hierarchical tree of all spans
- Each span includes timing, status, and type-specific metadata

**With span_id parameter:**
Returns details for a specific span within the trace:
- Span metadata (ID, name, type, timing)
- Status and error information
- Type-specific attributes (model, tokens, tool params, etc.)

**Span Hierarchy:**
The `tree` field contains root spans, each with potential `children`.
This recursive structure represents the execution flow.

**Span Types:**
- `AGENT`: Agent execution with input/output
- `LLM`: Model invocations with tokens and prompts
- `TOOL`: Tool calls with parameters and results

---

## Multimodal Teams

**URL:** llms-txt#multimodal-teams

**Contents:**
- Multimodal inputs to a team
  - Image Analysis Team

Source: https://docs.agno.com/basics/multimodal/team/overview

Learn how to create multimodal teams in Agno.

Agno teams support text, image, audio, video and files inputs and can generate text, image, audio, video and files as output through collaborative agent workflows.

For a complete overview of multimodal support, please checkout the [multimodal](/basics/multimodal/overview) documentation.

<Tip>
  Not all models support multimodal inputs and outputs.
  To see which models support multimodal inputs and outputs, please checkout the [compatibility matrix](/basics/models/compatibility).
</Tip>

## Multimodal inputs to a team

Teams can process multimodal inputs by coordinating multiple specialized agents. Each agent can focus on specific aspects of the input, enabling sophisticated multimodal workflows.

### Image Analysis Team

Let's create a team that analyzes images and creates structured output:

```python image_to_structured_output.py theme={null}
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.team import Team
from pydantic import BaseModel, Field

class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

image_analyst = Agent(
    name="Image Analyst",
    role="Analyze visual content and extract key elements",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Analyze images for visual elements, setting, and characters",
        "Focus on details that can inspire creative content",
    ],
)

script_writer = Agent(
    name="Script Writer",
    role="Create structured movie scripts from visual inspiration",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Transform visual analysis into compelling movie concepts",
        "Follow the structured output format precisely",
    ],
)

---

## Image to Audio Story Generation

**URL:** llms-txt#image-to-audio-story-generation

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/image-to-audio

This example demonstrates how to analyze an image to create a story and then convert that story to audio narration.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## === Setup database and agent ===

**URL:** llms-txt#===-setup-database-and-agent-===

db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

agent = Agent(
    id="demo-agent",
    name="Demo Agent",
    model=OpenAIChat(id="gpt-4o"),
    db=db,
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent_os = AgentOS(
    description="Essential middleware demo with rate limiting and logging",
    agents=[agent],
)

app = agent_os.get_app()

---

## BAAI/bge-reranker-large

**URL:** llms-txt#baai/bge-reranker-large

---

## members=[agent],

**URL:** llms-txt#members=[agent],

---

## Morph

**URL:** llms-txt#morph

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/models/morph

MorphTools provides advanced code editing capabilities using Morph's Fast Apply API for intelligent code modifications.

The following agent can perform intelligent code editing using Morph:

| Parameter          | Type            | Default                         | Description                                     |
| ------------------ | --------------- | ------------------------------- | ----------------------------------------------- |
| `api_key`          | `Optional[str]` | `None`                          | Morph API key. Uses MORPH\_API\_KEY if not set. |
| `base_url`         | `str`           | `"https://api.morphllm.com/v1"` | Morph API base URL.                             |
| `model`            | `str`           | `"morph-v3-large"`              | Morph model to use for code editing.            |
| `instructions`     | `Optional[str]` | `None`                          | Custom instructions for code editing behavior.  |
| `add_instructions` | `bool`          | `True`                          | Whether to add instructions to the agent.       |

| Function    | Description                                                        |
| ----------- | ------------------------------------------------------------------ |
| `edit_file` | Apply intelligent code modifications using Morph's Fast Apply API. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/models/morph.py)
* [Morph API Documentation](https://docs.morphllm.com/)
* [Fast Apply API Reference](https://api.morphllm.com/docs)

---

## Create specialized Hacker News research agent

**URL:** llms-txt#create-specialized-hacker-news-research-agent

hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
    instructions=[
        "Search Hacker News for relevant articles and discussions",
        "Extract key insights and summarize findings",
        "Focus on high-quality, well-discussed posts",
    ],
)

---

## Add storage to the Agent

**URL:** llms-txt#add-storage-to-the-agent

**Contents:**
- Usage

agent = Agent(
    model=LiteLLM(id="gpt-5-mini"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")
bash  theme={null}
    export LITELLM_API_KEY=xxx
    bash  theme={null}
    pip install -U litellm ddgs openai agno
    bash Mac theme={null}
      python cookbook/models/litellm/db.py
      bash Windows theme={null}
      python cookbook\models\litellm\db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## 2. Multimodal Query for Video Analysis

**URL:** llms-txt#2.-multimodal-query-for-video-analysis

You are an expert in video content creation, specializing in crafting engaging short-form content for platforms like YouTube Shorts and Instagram Reels. Your task is to analyze the provided video and identify segments that maximize viewer engagement.

For each video, you'll:

1. Identify key moments that will capture viewers' attention, focusing on:
   - High-energy sequences
   - Emotional peaks
   - Surprising or unexpected moments
   - Strong visual and audio elements
   - Clear narrative segments with compelling storytelling

2. Extract segments that work best for short-form content, considering:
   - Optimal length (strictly 15â€“60 seconds)
   - Natural start and end points that ensure smooth transitions
   - Engaging pacing that maintains viewer attention
   - Audio-visual harmony for an immersive experience
   - Vertical format compatibility and adjustments if necessary

3. Provide a detailed analysis of each segment, including:
   - Precise timestamps (Start Time | End Time in MM:SS format)
   - A clear description of why the segment would be engaging
   - Suggestions on how to enhance the segment for short-form content
   - An importance score (1-10) based on engagement potential

Your goal is to identify moments that are visually compelling, emotionally engaging, and perfectly optimized for short-form platforms.
"""

---

## Create project proposal

**URL:** llms-txt#create-project-proposal

prompt = (
    "Create a project proposal document for 'Mobile App Development':\n\n"
    "Title: Mobile App Development Proposal\n\n"
    "1. Executive Summary:\n"
    "   Project to build a task management mobile app\n"
    "   Timeline: 12 weeks, Budget: $120K\n\n"
    "2. Project Overview:\n"
    "   - Native iOS and Android app\n"
    "   - Key features: Task lists, reminders, team collaboration\n"
    "   - Target users: Small business teams\n\n"
    "3. Scope of Work:\n"
    "   - Requirements gathering (Week 1-2)\n"
    "   - Design and prototyping (Week 3-4)\n"
    "   - Development (Week 5-10)\n"
    "   - Testing and launch (Week 11-12)\n\n"
    "4. Team:\n"
    "   - 2 developers, 1 designer, 1 project manager\n\n"
    "5. Budget Breakdown:\n"
    "   - Development: $80K\n"
    "   - Design: $25K\n"
    "   - Testing: $10K\n"
    "   - Contingency: $5K\n\n"
    "6. Success Metrics:\n"
    "   - 1000 users in first month\n"
    "   - 4.5+ star rating\n"
    "   - 70% user retention\n\n"
    "Save as 'mobile_app_proposal.docx'"
)

response = document_agent.run(prompt)
print(response.content)

---

## LiteLLM

**URL:** llms-txt#litellm

**Contents:**
- Prerequisites

Source: https://docs.agno.com/integrations/models/gateways/litellm/overview

Integrate LiteLLM with Agno for a unified LLM experience.

[LiteLLM](https://docs.litellm.ai/docs/) provides a unified interface for various LLM providers, allowing you to use different models with the same code.

Agno integrates with LiteLLM in two ways:

1. **Direct SDK integration** - Using the LiteLLM Python SDK
2. **Proxy Server integration** - Using LiteLLM as an OpenAI-compatible proxy

For both integration methods, you'll need:

```shell  theme={null}

---

## Quickstart

**URL:** llms-txt#quickstart

**Contents:**
- Build your first Agent

Source: https://docs.agno.com/get-started/quickstart

Build and run your first Agent using Agno.

## Build your first Agent

**Agents are AI programs where a language model controls the flow of execution.**

Instead of a toy demo, let's build an Agent that you can extend by connecting to any MCP server. We'll connect our agent to the Agno MCP server, and give it a database to store conversation history and state.

Save the following code in a file named `agno_agent.py`

```python agno_agent.py lines theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools

---

## Find business information in a specific location

**URL:** llms-txt#find-business-information-in-a-specific-location

**Contents:**
- Example Scenarios
  - RAG Web Browser + Google Places Crawler

agent.print_response("What are the top-rated restaurants in San Francisco?", markdown=True)
agent.print_response("Find coffee shops in Prague", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

agent = Agent(
    tools=[
        ApifyTools(actors=[
            "apify/rag-web-browser",
            "compass/crawler-google-places"
        ])
    ]
)

**Examples:**

Example 1 (unknown):
```unknown
## Example Scenarios

### RAG Web Browser + Google Places Crawler

This example combines web search with local business data to provide comprehensive information about a topic:
```

---

## Find either engineering or product documents

**URL:** llms-txt#find-either-engineering-or-product-documents

OR(
    EQ("department", "engineering"),
    EQ("department", "product")
)
python  theme={null}
from agno.filters import NOT, EQ

**Examples:**

Example 1 (unknown):
```unknown
#### NOT

Exclude content that matches the condition.
```

---

## Audio Output Agent

**URL:** llms-txt#audio-output-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/audio-output-agent

```python cookbook/models/openai/chat/audio_output_agent.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file
from agno.db.in_memory import InMemoryDb

---

## pprint(memories)

**URL:** llms-txt#pprint(memories)

**Contents:**
- Usage

bash  theme={null}
    ./cookbook/run_pgvector.sh
    bash  theme={null}
    pip install agno openai psycopg sqlalchemy
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/memory/01_team_with_memory_manager.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up PostgreSQL database">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install required libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create Postgres-backed vector store

**URL:** llms-txt#create-postgres-backed-vector-store

vector_db = PgVector(
    db_url=db_url,
    table_name="agno_docs",
)
knowledge = Knowledge(
    name="Agno Docs",
    contents_db=db,
    vector_db=vector_db,
)

---

## Team with Knowledge Filters

**URL:** llms-txt#team-with-knowledge-filters

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/team-with-knowledge-filters

This example demonstrates how to use knowledge filters with teams to restrict knowledge searches to specific documents or metadata criteria, enabling personalized and contextual responses based on predefined filter conditions.

```python cookbook/examples/teams/knowledge/02_team_with_knowledge_filters.py theme={null}
"""
This example demonstrates how to use knowledge filters with teams.

Knowledge filters allow you to restrict knowledge searches to specific documents
or metadata criteria, enabling personalized and contextual responses.
"""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## Agent Metrics and Performance Monitoring

**URL:** llms-txt#agent-metrics-and-performance-monitoring

Source: https://docs.agno.com/basics/sessions/metrics/usage/agent-metrics

This example demonstrates how to collect and analyze agent metrics including message-level metrics, run metrics, and session metrics for performance monitoring.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create knowledge base with embedder

**URL:** llms-txt#create-knowledge-base-with-embedder

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="my_embeddings",
        embedder=OpenAIEmbedder(),  # Default embedder
    ),
    max_results=2,  # Return top 2 most relevant chunks
)

---

## Memori

**URL:** llms-txt#memori

**Contents:**
- Prerequisites
- Example
- Key Features
- Setup
- Developer Resources

Source: https://docs.agno.com/integrations/memory/memori

Memori is an open-source memory layer for AI. It automatically captures conversations, extracts meaningful facts, and makes them searchable across entities, processes, and sessions.

The following example requires the `memori` library.

The following agent uses Memori to maintain persistent memory across conversations with SQLite:

* **LLM Agnostic**: OpenAI, Anthropic, Bedrock, Gemini, Grok (xAI) - all modes (streamed, unstreamed, sync, async)
* **Smart Attribution**: Track memories by entity (e.g., customer) and process (e.g., support agent)
* **Advanced Augmentation**: AI-powered memory augmentation with no latency impact
* **Database Flexibility**: Supports PostgreSQL, MySQL/MariaDB, SQLite, MongoDB, CockroachDB, Neon, Supabase, Oracle, and more

1. **Create Database Engine**: Use SQLAlchemy to create a database connection
2. **Initialize Memori**: Create a Memori instance with the database session
3. **Register with Model**: Register Memori with your Agno agent using `.agno.register()`
4. **Set Attribution**: Define entity and process IDs for memory tracking
5. **Build Storage**: Initialize the database schema with `.config.storage.build()`

## Developer Resources

* [Memori SDK Documentation](https://memorilabs.ai/docs/)
* [Memori GitHub Repository](https://github.com/MemoriLabs/Memori)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent uses Memori to maintain persistent memory across conversations with SQLite:
```

---

## Qdrant Async

**URL:** llms-txt#qdrant-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/qdrant/usage/async-qdrant-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Qdrant">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Qdrant">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Firestore

**URL:** llms-txt#firestore

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/firestore/overview

Learn to use Firestore as a database for your Agents

Agno supports using [Firestore](https://cloud.google.com/firestore) as a database with the `FirestoreDb` class.

You can get started with Firestore following their [Get Started guide](https://firebase.google.com/docs/firestore/quickstart).

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.firestore import FirestoreDb

PROJECT_ID = "agno-os-test"  # Use your project ID here

---

## Connecting Your AgentOS

**URL:** llms-txt#connecting-your-agentos

**Contents:**
- Overview
- Step-by-Step Connection Process
  - 1. Access the Connection Dialog
  - 2. Choose Your Environment
  - 3. Configure Connection Settings
  - 4. Test and Connect
- Verifying Your Connection
- Securing Your Connection
- Managing Connected OS Instances
  - Switching Between OS Instances

Source: https://docs.agno.com/agent-os/connecting-your-os

Step-by-step guide to connect your local AgentOS to the AgentOS Control Plane

Connecting your AgentOS is the critical first step to using the AgentOS Control Plane. This process establishes a connection between your running AgentOS instance and the Control Plane, allowing you to manage, monitor, and interact with your agents through the browser.

<Note>
  **Prerequisites**: You need a running AgentOS instance before you can connect
  it to the Control Plane. If you haven't created one yet, check out our [Creating
  Your First OS](/agent-os/creating-your-first-os) guide.
</Note>

See the [AgentOS Control Plane](/agent-os/control-plane) documentation for more information about the Control Plane.

## Step-by-Step Connection Process

### 1. Access the Connection Dialog

In the Agno platform:

1. Click on the team/organization dropdown in the top navigation bar
2. Click the **"+"** (plus) button next to "Add new OS"
3. The "Connect your AgentOS" dialog will open

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/MMgohmDbM-qeNPya/videos/agent-os-connect-os.mp4?fit=max&auto=format&n=MMgohmDbM-qeNPya&q=85&s=c427bf5bbd76c0495540b49aa64f5604" type="video/mp4" data-path="videos/agent-os-connect-os.mp4" />
  </video>
</Frame>

### 2. Choose Your Environment

Select **"Local"** for development or **"Live"** for production:

* **Local**: Connects to an AgentOS running on your local machine
* **Live**: Connects to a production AgentOS running on your infrastructure

<Note>Live AgentOS connections require a PRO subscription.</Note>

### 3. Configure Connection Settings

* **Default Local**: `http://localhost:7777`
* **Custom Local**: You can change the port if your AgentOS runs on a different port
* **Live**: Enter your production HTTPS URL

<Warning>
  Make sure your AgentOS is actually running on the specified endpoint before
  attempting to connect.
</Warning>

Give your AgentOS a descriptive name:

* Use clear, descriptive names like "Development OS" or "Production Chat Bot"
* This name will appear in your OS list and help you identify different instances

Add tags to organize your AgentOS instances:

* Examples: `development`, `production`, `chatbot`, `research`
* Tags help filter and organize multiple OS instances
* Click the **"+"** button to add multiple tags

### 4. Test and Connect

1. Click the **"CONNECT"** button
2. The platform will attempt to establish a connection to your AgentOS
3. If successful, you'll see your new OS in the organization dashboard

## Verifying Your Connection

Once connected, you should see:

1. **OS Status**: "Running" indicator in the platform
2. **Available Features**: Chat, Knowledge, Memory, Sessions, etc. should be accessible
3. **Agent List**: Your configured agents should appear in the chat interface

## Securing Your Connection

Protect your AgentOS APIs and Control Plane access with bearer-token authentication. Security keys provide essential protection for both development and production environments.

* Generate unique security keys per AgentOS instance
* Rotate keys easily through the organization settings
* Configure bearer-token authentication on your server

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/xm93WWN8gg4nzCGE/videos/agentos-security-key.mp4?fit=max&auto=format&n=xm93WWN8gg4nzCGE&q=85&s=0a87c2a894982a3eb075fe282a21c491" type="video/mp4" data-path="videos/agentos-security-key.mp4" />
  </video>
</Frame>

<Note>
  For complete security setup instructions, including environment configuration
  and best practices, see the [Security Key](/agent-os/security) documentation.
</Note>

## Managing Connected OS Instances

### Switching Between OS Instances

1. Use the dropdown in the top navigation bar
2. Select the OS instance you want to work with
3. All platform features will now connect to the selected OS

### Disconnecting an OS

1. Go to the organization settings
2. Find the OS in your list
3. Click the delete option

<Warning>
  Disconnecting an OS doesn't stop the AgentOS instance - it only removes it
  from the platform interface.
</Warning>

Once your AgentOS is successfully connected:

<CardGroup cols={3}>
  <Card title="Explore the Chat Interface" icon="comment" href="/agent-os/features/chat-interface">
    Start having conversations with your connected agents
  </Card>

<Card title="Manage Knowledge" icon="brain" href="/agent-os/features/knowledge-management">
    Upload and organize your knowledge bases
  </Card>

<Card title="Monitor Sessions" icon="chart-line" href="/agent-os/features/session-tracking">
    Track and analyze your agent interactions
  </Card>
</CardGroup>

---

## Write Your Own Tool

**URL:** llms-txt#write-your-own-tool

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/04-write-your-own-tool

This example shows how to create and use your own custom tool with Agno.
You can replace the Hacker News functionality with any API or service you want!

Some ideas for your own tools:

* Weather data fetcher
* Stock price analyzer
* Personal calendar integration
* Custom database queries
* Local file operations

```python custom_tools.py theme={null}
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat

def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

Args:
        num_stories (int): Number of stories to return. Defaults to 10.

Returns:
        str: JSON string of top stories.
    """

# Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

# Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)

---

## Agent gets ALL database tools - overwhelming!

**URL:** llms-txt#agent-gets-all-database-tools---overwhelming!

tools = MCPTools(url="http://127.0.0.1:5001")  # 50+ tools
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**With MCPToolbox (3 relevant tools):**
```

---

## Create AgentOS app

**URL:** llms-txt#create-agentos-app

**Contents:**
  - AgentOS Features Enabled by ContentsDB
- Next Steps

app = AgentOS(
    description="Example app for basic agent with knowledge capabilities",
    id="knowledge-demo",
    agents=[knowledge_agent],
)
```

### AgentOS Features Enabled by ContentsDB

With ContentsDB, the AgentOS Knowledge page provides:

* **Content Browser**: View all uploaded content with metadata
* **Upload Interface**: Add new content through the web UI
* **Status Monitoring**: Real-time processing status updates
* **Metadata Editor**: Update content metadata through forms
* **Content Management**: Delete or modify content entries
* **Search and Filtering**: Find content by metadata attributes
* **Bulk Operations**: Manage multiple content items at once

Check out the [AgentOS Knowledge](/agent-os/features/knowledge-management) page for more in-depth information.

<CardGroup cols={3}>
  <Card title="Vector Databases" icon="database" href="/basics/vectordb/overview">
    Understand the embedding storage layer
  </Card>

<Card title="AgentOS" icon="server" href="/agent-os/introduction">
    Use your Knowledge in Agno AgentOS
  </Card>

<Card title="Database Setup" icon="wrench" href="/basics/database/overview">
    Detailed database configuration guides
  </Card>

<Card title="Getting Started" icon="rocket" href="/basics/knowledge/getting-started">
    Build your first knowledge-powered agent
  </Card>
</CardGroup>

---

## Custom Functions in Workflows

**URL:** llms-txt#custom-functions-in-workflows

**Contents:**
- Example
- Class-based executor

Source: https://docs.agno.com/basics/workflows/workflow-patterns/custom-function-step-workflow

How to use custom functions in workflows

Custom functions provide maximum flexibility by allowing you to define specific logic for step execution. Use them to preprocess inputs, orchestrate agents and teams, and postprocess outputs with complete programmatic control.

* **Custom Logic**: Implement complex business rules and data transformations
* **Agent Integration**: Call agents and teams within your custom processing logic
* **Data Flow Control**: Transform outputs between steps for optimal data handling

**Implementation Pattern**
Define a `Step` with a custom function as the `executor`. The function must accept a `StepInput` object and return a `StepOutput` object, ensuring seamless integration with the workflow system.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=d9c94fbc2094b100df2fde1e4767f358" alt="Custom function step workflow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/custom-function-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=280&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=cc6fdbdbcd274ffd8eeaf936653e9487 280w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=560&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=7135a981cbf2afbbfc9e8a772843ca90 560w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=840&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=084fd07440b87980f0899dea2b0b5ba1 840w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=1100&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=9d57ad4d5f051dc65185bf29ad759118 1100w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=1650&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=b1ba9db7305207a7867efa4ef47912dc 1650w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=2500&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=a4e94cf04d1a1bb212f971ce60cefe5b 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=537393db1d76a7d4c38d43e40c622300" alt="Custom function step workflow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/custom-function-steps-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=280&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=40450040ca47de4fa286b95de2e285ed 280w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=560&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=ef0a1060f07688a21b866766dc10526b 560w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=840&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=9c5572c23d25046db363ecaeeb65e3d6 840w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=1100&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=991e880aee26c31a9484d8603e0be424 1100w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=1650&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=0a8c70a163f2ba4a5cd5dfdc487ff14d 1650w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=2500&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=aec1041004efd198455fcb6a740f008d 2500w" />

**Standard Pattern**
All custom functions follow this consistent structure:

## Class-based executor

You can also use a class-based executor by defining a class that implements the `__call__` method.

**When is this useful?:**

* **Configuration at initialization**: Pass in settings, API keys, or behavior flags when creating the executor
* **Stateful execution**: Maintain counters, caches, or track information across multiple workflow runs
* **Reusable components**: Create configured executor instances that can be shared across multiple workflows

```python  theme={null}
class CustomExecutor:
    def __init__(self, max_retries: int = 3, use_cache: bool = True):
        # Configuration passed during instantiation
        self.max_retries = max_retries
        self.use_cache = use_cache
        self.call_count = 0  # Stateful tracking

def __call__(self, step_input: StepInput) -> StepOutput:
        self.call_count += 1

# Access instance configuration and state
        if self.use_cache and self.call_count > 1:
            return StepOutput(content="Using cached result")

# Your custom logic with access to self.max_retries, etc.
        return StepOutput(content=enhanced_content)

**Examples:**

Example 1 (unknown):
```unknown
**Standard Pattern**
All custom functions follow this consistent structure:
```

Example 2 (unknown):
```unknown
## Class-based executor

You can also use a class-based executor by defining a class that implements the `__call__` method.
```

Example 3 (unknown):
```unknown
**When is this useful?:**

* **Configuration at initialization**: Pass in settings, API keys, or behavior flags when creating the executor
* **Stateful execution**: Maintain counters, caches, or track information across multiple workflow runs
* **Reusable components**: Create configured executor instances that can be shared across multiple workflows
```

---

## SurrealDB for Team

**URL:** llms-txt#surrealdb-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/surrealdb/usage/surrealdb-for-team

Agno supports using SurrealDB as a storage backend for Teams using the `SurrealDb` class.

Run SurreabDB locally with the following command:

```python surrealdb_for_team.py theme={null}
from typing import List

from agno.agent import Agent
from agno.db.surrealdb import SurrealDb
from agno.models.anthropic import Claude
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Redis Vector Database

**URL:** llms-txt#redis-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/redis/overview

Learn how to use Redis as a vector database for your Knowledge Base

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.7" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.7">v2.2.7</Tooltip>
</Badge>

You can use Redis as a vector database with Agno.

For connecting to a remote Redis instance, pass your Redis connection string to the `redis_url` parameter and the index name to the `index_name` parameter of the `RedisDB` constructor.

For a local docker setup, you can use the following command:

```python agent_with_knowledge.py theme={null}

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.redis import RedisDB
from agno.vectordb.search import SearchType

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Allow the memories to sync with Zep database

**URL:** llms-txt#allow-the-memories-to-sync-with-zep-database

---

## Define the custom knowledge retriever

**URL:** llms-txt#define-the-custom-knowledge-retriever

---

## Image Input URL

**URL:** llms-txt#image-input-url

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/image-input-url

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "[Feature Request] Can you please update me on the above feature",

**URL:** llms-txt#"[feature-request]-can-you-please-update-me-on-the-above-feature",

---

## Pass tracing_db to AgentOS for trace querying via API

**URL:** llms-txt#pass-tracing_db-to-agentos-for-trace-querying-via-api

agent_os = AgentOS(
    agents=[agent1, agent2],
    tracing_db=tracing_db,  # Makes traces queryable via AgentOS API
)

app = agent_os.get_app()
```

<Note>
  Even when using `setup_tracing()`, pass `tracing_db` to AgentOS so traces are accessible through the AgentOS API and UI.
</Note>

<Tip>
  **Best Practice**: Always use a dedicated `tracing_db` in production, even with a single agent. This keeps observability data separate and makes it easier to scale or migrate later.
</Tip>

---

## Create the workflow

**URL:** llms-txt#create-the-workflow

query_to_notion_workflow = Workflow(
    name="query-to-notion-workflow",
    description="Classify user queries and organize them in Notion",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[classify_step, notion_prep_step, notion_step],
)

---

## Agentic Memory

**URL:** llms-txt#agentic-memory

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/memory/agent/usage/agentic-memory

This example shows you how to use persistent memory with an Agent.

During each run the Agent can create/update/delete user memories.

To enable this, set `enable_agentic_memory=True` in the Agent config.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Use custom strategy

**URL:** llms-txt#use-custom-strategy

**Contents:**
- Measuring Token Savings

custom_strategy = CustomOptimizationStrategy()
optimized = memory_manager.optimize_memories(
    user_id="user_123",
    strategy=custom_strategy,
    apply=True,
)
python  theme={null}
from agno.memory.strategies.summarize import SummarizeStrategy

**Examples:**

Example 1 (unknown):
```unknown
## Measuring Token Savings

You can measure token savings by comparing the number of tokens before and after optimization (using the `count_tokens` method):
```

---

## Calculate savings

**URL:** llms-txt#calculate-savings

**Contents:**
- API Usage (AgentOS)
- Limitations
- Examples
- Related Documentation

tokens_saved = tokens_before - tokens_after
reduction_percentage = (tokens_saved / tokens_before * 100) if tokens_before > 0 else 0

print(f"Tokens before: {tokens_before}")
print(f"Tokens after: {tokens_after}")
print(f"Tokens saved: {tokens_saved} ({reduction_percentage:.1f}% reduction)")
bash  theme={null}
curl -X POST "https://os.agno.com/api/v1/memory/optimize-memories" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "user_id": "user_123",
    "apply": true,
    "model": "openai:gpt-4o-mini"
  }'
json  theme={null}
{
  "memories": [...],
  "memories_before": 50,
  "memories_after": 1,
  "tokens_before": 2500,
  "tokens_after": 500,
  "tokens_saved": 2000,
  "reduction_percentage": 80.0
}
```

* **Loss of granularity:** Multiple memories become one, making individual memory retrieval less precise
* **LLM cost:** Optimization itself requires an LLM call (use cheaper models)
* **One-way operation:** Once optimized, original memories are replaced (use `apply=False` to preview)

See the [Memory Optimization Example](/basics/memory/working-with-memories/usage/memory-optimization) for a complete working example that demonstrates:

* Creating multiple memories for a user
* Optimizing memories using the summarize strategy
* Measuring token savings
* Viewing optimized results

You can also explore the cookbook examples:

* [Summarize Strategy](https://github.com/agno-agi/agno/tree/main/cookbook/memory/optimize_memories/01_memory_summarize_strategy.py) - Basic optimization example
* [Custom Strategy](https://github.com/agno-agi/agno/tree/main/cookbook/memory/optimize_memories/02_custom_memory_strategy.py) - Creating custom optimization strategies

## Related Documentation

* [Memory Overview](/basics/memory/overview) - Learn about memory basics
* [Production Best Practices](/basics/memory/best-practices) - Memory optimization strategies
* [Working with Memories](/basics/memory/working-with-memories/overview) - Advanced memory patterns

**Examples:**

Example 1 (unknown):
```unknown
## API Usage (AgentOS)

Memory optimization is also available through the AgentOS API:
```

Example 2 (unknown):
```unknown
The API returns detailed statistics:
```

---

## Define tools that work with shared team state

**URL:** llms-txt#define-tools-that-work-with-shared-team-state

def add_item(run_context: RunContext, item: str) -> str:
    """Add an item to the shopping list."""
    if not run_context.session_state:
        run_context.session_state = {}

if item.lower() not in [
        i.lower() for i in run_context.session_state["shopping_list"]
    ]:
        run_context.session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"

def remove_item(run_context: RunContext, item: str) -> str:
    """Remove an item from the shopping list."""
    if not run_context.session_state:
        run_context.session_state = {}

for i, list_item in enumerate(run_context.session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            run_context.session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

return f"'{item}' was not found in the shopping list"

---

## Clickhouse Vector Database

**URL:** llms-txt#clickhouse-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/clickhouse/overview

Learn how to use Clickhouse as a vector database for your Knowledge Base

```python agent_with_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.db.sqlite import SqliteDb
from agno.vectordb.clickhouse import Clickhouse

knowledge=Knowledge(
    vector_db=Clickhouse(
        table_name="recipe_documents",
        host="localhost",
        port=8123,
        username="ai",
        password="ai",
    ),
)

knowledge.add_content(
  url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    db=SqliteDb(db_file="agno.db"),
    knowledge=knowledge,
    # Enable the agent to search the knowledge base
    search_knowledge=True,
    # Enable the agent to read the chat history
    read_chat_history=True,
)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Slack Agent with User Memory

**URL:** llms-txt#slack-agent-with-user-memory

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/slack/agent-with-user-memory

Personalized Slack agent that remembers user information and preferences

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Memory Management**: Remembers user names, hobbies, preferences, and activities
* **DuckDuckGo Search Integration**: Access to current information during conversations
* **Personalized Responses**: Uses stored memories for contextualized replies
* **Slack Integration**: Works with direct messages and group conversations
* **Claude Powered**: Advanced reasoning and conversation capabilities

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Team Run Cancellation

**URL:** llms-txt#team-run-cancellation

**Contents:**
- Example
- API Endpoint

Source: https://docs.agno.com/execution-control/run-cancellation/team-cancel-run

Learn how to cancel a running team execution by starting a team run in a separate thread and cancelling it from another thread.

This example demonstrates how to cancel a running team execution by starting a team run in a separate thread and cancelling it from another thread. It shows proper handling of cancelled responses and thread management.

Team runs can be cancelled via the AgentOS API:

**Reference:** [Cancel Team Run API](/reference-api/schema/teams/cancel-team-run)

**Examples:**

Example 1 (unknown):
```unknown
## API Endpoint

Team runs can be cancelled via the AgentOS API:
```

Example 2 (unknown):
```unknown
**Example:**
```

---

## Example 2: Generate an image with custom settings

**URL:** llms-txt#example-2:-generate-an-image-with-custom-settings

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

custom_dalle = Dalle(model="dall-e-3", size="1792x1024", quality="hd", style="natural")

agent_custom = Agent(
    tools=[custom_dalle],
    name="Custom DALL-E Generator",
    )

agent_custom.print_response("Create a panoramic nature scene showing a peaceful mountain lake at sunset", markdown=True)
```

| Parameter             | Type   | Default       | Description                                                       |
| --------------------- | ------ | ------------- | ----------------------------------------------------------------- |
| `model`               | `str`  | `"dall-e-3"`  | The DALL-E model to use                                           |
| `enable_create_image` | `bool` | `True`        | Enable the create image functionality                             |
| `n`                   | `int`  | `1`           | Number of images to generate                                      |
| `size`                | `str`  | `"1024x1024"` | Image size (256x256, 512x512, 1024x1024, 1792x1024, or 1024x1792) |
| `quality`             | `str`  | `"standard"`  | Image quality (standard or hd)                                    |
| `style`               | `str`  | `"vivid"`     | Image style (vivid or natural)                                    |
| `api_key`             | `str`  | `None`        | The OpenAI API key for authentication                             |
| `all`                 | `bool` | `False`       | Enable all functionality when set to True                         |

| Function         | Description                               |
| ---------------- | ----------------------------------------- |
| `generate_image` | Generates an image based on a text prompt |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/dalle.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/dalle_tools.py)

---

## Replace a memory

**URL:** llms-txt#replace-a-memory

**Contents:**
- Usage

print("\nReplacing memory")
assert memory_id_1 is not None
memory.replace_user_memory(
    memory_id=memory_id_1,
    memory=UserMemory(memory="The user's name is Jane Mary Doe", topics=["name"]),
    user_id=jane_doe_id,
)
print("Memory replaced")
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)
bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python standalone-memory.py
      bash Windows theme={null}
      python standalone-memory.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Markdown Chunking

**URL:** llms-txt#markdown-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/markdown

Markdown chunking is a method of splitting markdown based on structure like headers, paragraphs and sections.
This is useful when you want to process large markdown documents in smaller, manageable pieces.

<Snippet file="chunking-markdown.mdx" />

---

## Enable all ScrapeGraph functions

**URL:** llms-txt#enable-all-scrapegraph-functions

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

scrapegraph_all = Agent(
    tools=[
        ScrapeGraphTools(all=True, render_heavy_js=True)
    ],  # render_heavy_js=True scrapes all JavaScript
    model=agent_model,
    markdown=True,
    stream=True,
)

scrapegraph_all.print_response("""
Use any appropriate scraping method to extract comprehensive information from https://www.wired.com/category/science/:
- News articles and headlines
- Convert to markdown if needed  
- Search for specific information
""")
```

<Note>View the [Startup Analyst example](/examples/use-cases/agents/startup-analyst-agent) </Note>

| Parameter                | Type            | Default | Description                                                                                        |
| ------------------------ | --------------- | ------- | -------------------------------------------------------------------------------------------------- |
| `api_key`                | `Optional[str]` | `None`  | ScrapeGraph API key. If not provided, uses SGAI\_API\_KEY environment variable.                    |
| `enable_smartscraper`    | `bool`          | `True`  | Enable the smartscraper function for LLM-powered data extraction.                                  |
| `enable_markdownify`     | `bool`          | `False` | Enable the markdownify function for webpage to markdown conversion.                                |
| `enable_crawl`           | `bool`          | `False` | Enable the crawl function for website crawling and data extraction.                                |
| `enable_searchscraper`   | `bool`          | `False` | Enable the searchscraper function for web search and information extraction.                       |
| `enable_agentic_crawler` | `bool`          | `False` | Enable the agentic\_crawler function for automated browser actions and AI extraction.              |
| `enable_scrape`          | `bool`          | `False` | Enable the scrape function for retrieving raw HTML content from websites.                          |
| `render_heavy_js`        | `bool`          | `False` | Enable heavy JavaScript rendering for all scraping functions. Useful for SPAs and dynamic content. |
| `all`                    | `bool`          | `False` | Enable all available functions. When True, all enable flags are ignored.                           |

| Function          | Description                                                                                                                                                                                                            |
| ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `smartscraper`    | Extract structured data from a webpage using LLM and natural language prompt. Parameters: url (str), prompt (str).                                                                                                     |
| `markdownify`     | Convert a webpage to markdown format. Parameters: url (str).                                                                                                                                                           |
| `crawl`           | Crawl a website and extract structured data. Parameters: url (str), prompt (str), data\_schema (dict), cache\_website (bool), depth (int), max\_pages (int), same\_domain\_only (bool), batch\_size (int).             |
| `searchscraper`   | Search the web and extract information. Parameters: user\_prompt (str).                                                                                                                                                |
| `agentic_crawler` | Perform automated browser actions with optional AI extraction. Parameters: url (str), steps (List\[str]), use\_session (bool), user\_prompt (Optional\[str]), output\_schema (Optional\[dict]), ai\_extraction (bool). |
| `scrape`          | Get raw HTML content from a website. Useful for complete source code retrieval and custom processing. Parameters: website\_url (str), headers (Optional\[dict]).                                                       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/scrapegraph.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/scrapegraph_tools.py)
* View [Tests](https://github.com/agno-agi/agno/blob/main/libs/agno/tests/unit/tools/test_scrapegraph.py)

---

## Browserbase

**URL:** llms-txt#browserbase

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/browserbase

**BrowserbaseTools** enable an Agent to automate browser interactions using Browserbase, a headless browser service.

The following example requires Browserbase API credentials after you signup [here](https://www.browserbase.com/), and the Playwright library.

The following agent will use Browserbase to visit `https://quotes.toscrape.com` and extract content. Then navigate to page two of the website and get quotes from there as well.

<Tip>View the [Startup Analyst MCP agent](/basics/tools/mcp/usage/stagehand)</Tip>

| Parameter                 | Type   | Default | Description                                                                                                                                                                                           |
| ------------------------- | ------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `api_key`                 | `str`  | `None`  | Browserbase API key. If not provided, uses BROWSERBASE\_API\_KEY env var.                                                                                                                             |
| `project_id`              | `str`  | `None`  | Browserbase project ID. If not provided, uses BROWSERBASE\_PROJECT\_ID env var.                                                                                                                       |
| `base_url`                | `str`  | `None`  | Custom Browserbase API endpoint URL. Only use this if you're using a self-hosted Browserbase instance or need to connect to a different region. If not provided, uses BROWSERBASE\_BASE\_URL env var. |
| `enable_navigate_to`      | `bool` | `True`  | Enable the navigate\_to functionality.                                                                                                                                                                |
| `enable_screenshot`       | `bool` | `True`  | Enable the screenshot functionality.                                                                                                                                                                  |
| `enable_get_page_content` | `bool` | `True`  | Enable the get\_page\_content functionality.                                                                                                                                                          |
| `enable_close_session`    | `bool` | `True`  | Enable the close\_session functionality.                                                                                                                                                              |
| `all`                     | `bool` | `False` | Enable all functionality.                                                                                                                                                                             |

| Function           | Description                                                                                                                                           |
| ------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `navigate_to`      | Navigates to a URL. Takes a URL and an optional connect\_url parameter.                                                                               |
| `screenshot`       | Takes a screenshot of the current page. Takes a path to save the screenshot, a boolean for full-page capture, and an optional connect\_url parameter. |
| `get_page_content` | Gets the HTML content of the current page. Takes an optional connect\_url parameter.                                                                  |
| `close_session`    | Closes a browser session.                                                                                                                             |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/browserbase.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/browserbase_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Browserbase to visit `https://quotes.toscrape.com` and extract content. Then navigate to page two of the website and get quotes from there as well.
```

---

## Team with Output Model

**URL:** llms-txt#team-with-output-model

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/input-output/team/usage/team-with-output-model

This example shows how to use the output\_model parameter to specify the model that should be used to generate the final response from a team.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Add your own routes

**URL:** llms-txt#add-your-own-routes

@app.post("/customers")
async def get_customers():
    return [
        {
            "id": 1,
            "name": "John Doe",
            "email": "john.doe@example.com",
        },
        {
            "id": 2,
            "name": "Jane Doe",
            "email": "jane.doe@example.com",
        },
    ]

---

## Create team with knowledge

**URL:** llms-txt#create-team-with-knowledge

team_with_knowledge = Team(
    name="Team with Knowledge",
    instructions=["Always search the knowledge base for the most relevant information"],
    description="A team that provides information about candidates",
    members=[web_agent],
    model=OpenAIChat(id="o3-mini"),
    knowledge=knowledge_base,
    show_members_responses=True,
    markdown=True,
)

---

## Load PDF knowledge base using hybrid search

**URL:** llms-txt#load-pdf-knowledge-base-using-hybrid-search

knowledge_base = Knowledge(
    vector_db=hybrid_db,
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

---

## Initialize Milvus vector db

**URL:** llms-txt#initialize-milvus-vector-db

vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)

---

## Print the response in the terminal

**URL:** llms-txt#print-the-response-in-the-terminal

**Contents:**
- Usage

agent.print_response("Share a 2 sentence horror story", stream=True)

bash  theme={null}
    export XAI_API_KEY=xxx
    bash  theme={null}
    pip install -U xai agno
    bash Mac theme={null}
      python cookbook/models/xai/basic_stream.py
      bash Windows theme={null}
      python cookbook/models/xai/basic_stream.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Capture Reasoning Content with Reasoning Tools

**URL:** llms-txt#capture-reasoning-content-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/capture-reasoning-content-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create a financial advisor agent with comprehensive hooks

**URL:** llms-txt#create-a-financial-advisor-agent-with-comprehensive-hooks

**Contents:**
- Usage

agent = Agent(
    name="Financial Advisor",
    model=OpenAIChat(id="gpt-5-mini"),
    pre_hooks=[transform_input],
    description="A professional financial advisor providing investment guidance and financial planning advice.",
    instructions=[
        "You are a knowledgeable financial advisor with expertise in:",
        "â€¢ Investment strategies and portfolio management",
        "â€¢ Retirement planning and savings strategies",
        "â€¢ Risk assessment and diversification",
        "â€¢ Tax-efficient investing",
        "",
        "Provide clear, actionable advice while being mindful of individual circumstances.",
        "Always remind users to consult with a licensed financial advisor for personalized advice.",
    ],
    debug_mode=True,
)

agent.print_response(
    input="I'm 35 years old and want to start investing for retirement. moderate risk tolerance. retirement savings in IRAs/401(k)s= $100,000. total savings is $200,000. my net worth is $300,000",
    session_id="test_session",
    user_id="test_user",
    stream=True,
)
bash  theme={null}
    pip install -U agno openai
    bash Mac theme={null}
      python cookbook/agents/hooks/input_transformation_pre_hook.py
      bash Windows theme={null}
      python cookbook/agents/hooks/input_transformation_pre_hook.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Define the custom async retriever

**URL:** llms-txt#define-the-custom-async-retriever

**Contents:**
  - Explanation
- Developer Resources

async def retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom async retriever function to search the vector database for relevant documents.
    """
    try:
        qdrant_client = AsyncQdrantClient(path="tmp/qdrant")
        query_embedding = embedder.get_embedding(query)
        results = await qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None

async def main():
    """Async main function to demonstrate agent usage."""
    agent = Agent(
        knowledge_retriever=retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
    )
    # Example query
    query = "List down the ingredients to make Massaman Gai"
    await agent.aprint_response(query, markdown=True)

if __name__ == "__main__":
    asyncio.run(main())
```

1. **Embedder and Vector Database Setup**: We start by defining an embedder and initializing a connection to a vector database. This setup is crucial for converting queries into embeddings and storing them in the database.

2. **Loading the Knowledge Base**: The knowledge base is loaded with PDF documents. This step involves converting the documents into embeddings and storing them in the vector database.

3. **Custom Retriever Function**: The `retriever` function is defined to handle the retrieval of documents. It takes a query, converts it into an embedding, and searches the vector database for relevant documents.

4. **Agent Initialization**: An agent is initialized with the custom retriever. The agent uses this retriever to search the knowledge base and retrieve information.

5. **Example Query**: The `main` function demonstrates how to use the agent to perform a query and retrieve information from the knowledge base.

## Developer Resources

* View [Sync Retriever](https://github.com/agno-agi/agno/tree/main/cookbook/agent_basics/knowledge/custom/retriever.py)
* View [Async Retriever](https://github.com/agno-agi/agno/tree/main/cookbook/agent_basics/knowledge/custom/async_retriever.py)

---

## Run the agent and print the response

**URL:** llms-txt#run-the-agent-and-print-the-response

**Contents:**
- Next Steps

agent.print_response("Trending startups and products.")

################ STREAM RESPONSE #################
stream: Iterator[RunOutputEvent] = agent.run("Trending products", stream=True)
for chunk in stream:
    if chunk.event == RunEvent.run_content:
        print(chunk.content)

################ STREAM AND PRETTY PRINT #################
stream: Iterator[RunOutputEvent] = agent.run("Trending products", stream=True)
pprint_run_response(stream, markdown=True)
```

After getting familiarized with the basics, continue building your agent by adding functionality as needed.

Common questions to consider:

* **How do I run my agent?** -> See the [running agents](/basics/agents/running-agents) documentation.
* **How do I debug my agent?** -> See the [debugging agents](/basics/agents/debugging-agents) documentation.
* **How do I manage sessions?** -> See the [agent sessions](/basics/sessions/overview) documentation.
* **How do I manage input and capture output?** -> See the [input and output](/basics/input-output/overview) documentation.
* **How do I add tools?** -> See the [tools](/basics/tools/overview) documentation.
* **How do I give the agent context?** -> See the [context engineering](/basics/context/overview) documentation.
* **How do I add knowledge?** -> See the [knowledge](/basics/knowledge/overview) documentation.
* **How do I handle images, audio, video, and files?** -> See the [multimodal](/basics/multimodal/overview) documentation.
* **How do I add guardrails?** -> See the [guardrails](/basics/guardrails/overview) documentation.
* **How do I cache model responses during development?** -> See the [response caching](/basics/models/cache-response) documentation.

---

## Example usage with different types of book queries

**URL:** llms-txt#example-usage-with-different-types-of-book-queries

book_recommendation_agent.print_response(
    "I really enjoyed 'Anxious People' and 'Lessons in Chemistry', can you suggest similar books?",
    stream=True,
)

---

## Fetch the QA audio file and convert it to a base64 encoded string

**URL:** llms-txt#fetch-the-qa-audio-file-and-convert-it-to-a-base64-encoded-string

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"
response = requests.get(url)
response.raise_for_status()
mp3_data = response.content

---

## User Input Required All Fields

**URL:** llms-txt#user-input-required-all-fields

Source: https://docs.agno.com/basics/hitl/usage/user-input-required-all-fields

This example demonstrates how to use the requires_user_input parameter to collect input for all fields in a tool. It shows how to handle user input schema and collect values for each required field.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Alternative prompt example

**URL:** llms-txt#alternative-prompt-example

crypto_prompt = """\
Analyze media trends for:
Keywords: cryptocurrency, bitcoin, ethereum
Sources: coindesk.com, cointelegraph.com
"""

---

## Performance Quick Wins

**URL:** llms-txt#performance-quick-wins

**Contents:**
- When to Optimize
- The 80/20 of Performance
  - 1. Pick the Right Vector Database

Source: https://docs.agno.com/basics/knowledge/performance-tips

Practical tips to optimize Agno knowledge base performance, improve search quality, and speed up content loading.

Most knowledge bases work great with Agno's defaults. But if you're seeing slow searches, memory issues, or poor results, a few strategic changes can make a big difference.

Avoid premature optimization. Focus on performance when the system shows:

* **Slow search** - Queries taking more than 2-3 seconds
* **Memory issues** - Out of memory errors during content loading
* **Poor results** - Search returning irrelevant chunks or missing obvious matches
* **Slow loading** - Content processing taking unusually long

If things are working fine, stick with the defaults and focus on building your application.

## The 80/20 of Performance

These five changes give you the biggest performance boost for the least effort:

### 1. Pick the Right Vector Database

Database choice has the biggest impact on performance at scale:

```python  theme={null}
from agno.vectordb.lancedb import LanceDb
from agno.vectordb.pgvector import PgVector

---

## Add YouTube video content synchronously

**URL:** llms-txt#add-youtube-video-content-synchronously

knowledge.add_content(
    metadata={"source": "youtube", "type": "educational"},
    urls=[
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ",  # Replace with actual educational video
        "https://www.youtube.com/watch?v=example123"   # Replace with actual video URL
    ],
    reader=YouTubeReader(),
)

---

## 4. Add content with chunking strategy

**URL:** llms-txt#4.-add-content-with-chunking-strategy

knowledge.add_content(
    path="company_docs/employee_handbook.pdf",
    reader=PDFReader(
        chunking_strategy=SemanticChunking(  # Optional: defaults to FixedSizeChunking
            chunk_size=1000,
            similarity_threshold=0.5
        )
    ),
    metadata={"type": "policy", "department": "hr"}
)

---

## Remove content and vectors by id

**URL:** llms-txt#remove-content-and-vectors-by-id

contents, _ = knowledge.get_content()
for content in contents:
    print(content.id)
    print(" ")
    knowledge.remove_content_by_id(content.id)

---

## "[Bug] Async tools in team of agents not awaited properly, causing runtime errors ",

**URL:** llms-txt#"[bug]-async-tools-in-team-of-agents-not-awaited-properly,-causing-runtime-errors-",

---

## Add your content

**URL:** llms-txt#add-your-content

knowledge.add_content(
    text_content="""
    Agno Knowledge System

Knowledge allows AI agents to access and search through
    domain-specific information at runtime. This enables
    dynamic few-shot learning and agentic RAG capabilities.

Key features:
    - Automatic content chunking
    - Vector similarity search
    - Multiple data source support
    - Intelligent retrieval
    """
)

---

## Configure Evals page (supports both global and per-database available models)

**URL:** llms-txt#configure-evals-page-(supports-both-global-and-per-database-available-models)

**Contents:**
- `AgentOSConfig` Class
- The /config endpoint

evals:
  display_name: "Evaluations"
  available_models:
    - "openai:gpt-4"
    - "anthropic:claude-sonnet-4"
  dbs:
    - db_id: db-0001
      domain_config:
        display_name: Production evals
        available_models:
          - "openai:gpt-4o-mini"
python  theme={null}
from agno.os import AgentOS

agent_os = AgentOS(
    name="My AgentOS",
    ...,
    config="path/to/configuration.yaml"
)
python  theme={null}
from agno.os import AgentOS
from agno.os.config import (
    AgentOSConfig,
    ChatConfig,
    DatabaseConfig,
    EvalsConfig,
    EvalsDomainConfig,
    KnowledgeConfig,
    KnowledgeDomainConfig,
    MemoryConfig,
    MemoryDomainConfig,
    SessionConfig,
    SessionDomainConfig,
)

agent_os = AgentOS(
    ...,
    config=AgentOSConfig(
        chat=ChatConfig(
            quick_prompts={
                "marketing-agent": [
                    "What can you do?",
                    "How is our latest post working?",
                    "Tell me about our active marketing campaigns",
                ]
            }
        ),
        memory=MemoryConfig(
            display_name="User Memory Store",
            dbs=[
                DatabaseConfig(
                    db_id=marketing_db.id,
                    domain_config=MemoryDomainConfig(
                        display_name="Main app user memories",
                    ),
                ),
                DatabaseConfig(
                    db_id=support_db.id,
                    domain_config=MemoryDomainConfig(
                        display_name="Support flow user memories",
                    ),
                )
            ],
        ),
    ),
)
json  theme={null}
{
	"os_id": "0001",
	"name": "My AgentOS",
	"description": "Your AgentOS",
	"available_models": [
		"openai:gpt-4",
	],
	"databases": [
		"db-0001",
		"db-0002"
	],
	"agents": [],
	"chat": {
		"quick_prompts": {
			"marketing-agent": [
				"What can you do?",
				"How is our latest post working?",
				"Tell me about our active marketing campaigns"
			]
		}
	},
	"memory": {
		"dbs": [
			{
				"db_id": "db-0001",
				"domain_config": {
					"display_name": "Main app user memories"
				}
			},
			{
				"db_id": "db-0002",
				"domain_config": {
					"display_name": "Support flow user memories"
				}
			}
		]
	},
	...
}
```

<Tip>
  See the full schema for the `/config` endpoint [here](/reference-api/schema/core/get-os-configuration).
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Pass the configuration to your AgentOS using the `config` parameter:
```

Example 2 (unknown):
```unknown
## `AgentOSConfig` Class

You can also provide your configuration using the `AgentOSConfig` class:
```

Example 3 (unknown):
```unknown
## The /config endpoint

The `/config` endpoint returns your complete AgentOS configuration as JSON. You could use this to inspect your AgentOS configuration that is served to the AgentOS Control Plane.

The response includes:

* **OS ID**: The ID of your AgentOS (automatically generated if not set)
* **Description**: The description of your AgentOS
* **Databases**: The list of IDs of the databases present in your AgentOS
* **Agents**: The list of Agents available in your AgentOS
* **Teams**: The list of Teams available in your AgentOS
* **Workflows**: The list of Workflows available in your AgentOS
* **Interfaces**: The list of Interfaces available in your AgentOS. E.g. WhatsApp, Slack, etc.
* **Chat**: The configuration for the Chat page, which includes the list of quick prompts for each Agent, Team and Workflow in your AgentOS
* **Session**: The configuration for the Session page of your AgentOS
* **Metrics**: The configuration for the Metrics page of your AgentOS
* **Memory**: The configuration for the Memory page of your AgentOS
* **Knowledge**: The configuration for the Knowledge page of your AgentOS
* **Evals**: The configuration for the Evals page of your AgentOS

You will receive a JSON response with your configuration. Using the previous examples, you will receive:
```

---

## run: RunOutput = agent.run("What is Portkey and why would I use it as an AI gateway?")

**URL:** llms-txt#run:-runoutput-=-agent.run("what-is-portkey-and-why-would-i-use-it-as-an-ai-gateway?")

---

## Redis

**URL:** llms-txt#redis

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/redis/usage/redis-db

```python redis_db.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.redis import RedisVectorDB

---

## Define steps using custom streaming functions for parallel execution

**URL:** llms-txt#define-steps-using-custom-streaming-functions-for-parallel-execution

hackernews_step = Step(
    name="HackerNews Research",
    executor=hackernews_research_function,
)

web_search_step = Step(
    name="Web Search Research",
    executor=web_search_research_function,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

streaming_content_workflow = Workflow(
    name="Streaming Content Creation Workflow",
    description="Automated content creation with parallel custom streaming functions",
    db=SqliteDb(
        session_table="streaming_workflow_session",
        db_file="tmp/workflow.db",
    ),
    # Define the sequence with parallel research steps followed by planning
    steps=[
        Parallel(hackernews_step, web_search_step, name="Parallel Research Phase"),
        content_planning_step,
    ],
)

---

## The runs for the team leader and all team members are here

**URL:** llms-txt#the-runs-for-the-team-leader-and-all-team-members-are-here

**Contents:**
  - 3. Memory
  - 4. Knowledge
  - 5. Metrics
  - 6. Teams
  - 7. Workflows
  - 7. Apps -> Interfaces
  - 8. Playground -> AgentOS

team_session.runs
python v1_memory.py theme={null}
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory

memory_db = SqliteMemoryDb(table_name="memory", db_file="agno.db")
memory = Memory(db=memory_db)

agent = Agent(memory=memory)
python v2_memory.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="agno.db")

agent = Agent(db=db, enable_user_memories=True)
python v2_memory_set_table.py theme={null}
db = SqliteDb(db_file="agno.db", memory_table="your_memory_table_name")
python v2_memory_db_methods.py theme={null}
agent.get_user_memories(user_id="123")
python  theme={null}
from agno.app.agui.app import AGUIApp

agui_app = AGUIApp(agent=agent)
app = agui_app.get_app()
agui_app.serve(port=8000)
python  theme={null}
from agno.os import AgentOS
from agno.os.interfaces.agui import AGUI

agent_os = AgentOS(agents=[agent], interfaces=[AGUI(agent=agent)])
app = agent_os.get_app()
agent_os.serve(port=8000)
```

1. **Update imports**: Replace app imports with interface imports
2. **Use AgentOS**: Wrap agents with `AgentOS` and specify interfaces
3. **Update serving**: Use `agent_os.serve()` instead of `app.serve()`

### 8. Playground -> AgentOS

Our `Playground` has been deprecated. Our new [AgentOS](/agent-os/introduction) offering will substitute all usecases.

See [AgentOS](/agent-os/introduction) for more details!

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  See more changes in the [Storage Updates](/how-to/v2-changelog#storage)
  section of the changelog.
</Tip>

### 3. Memory

Memory gives an Agent the ability to recall relevant information.

This is how Memory looks like on V1:
```

Example 2 (unknown):
```unknown
These are the changes we have made for v2:

3.1. The `MemoryDb` classes have been deprecated. The main `Db` classes are to be used.
3.2. The `Memory` class has been deprecated. You now just need to set `enable_user_memories=True` on an Agent with a `db` for Memory to work.
```

Example 3 (unknown):
```unknown
3.3. The generated memories will be stored in the `memories_table`. By default, the `agno_memories` will be used. It will be created if needed. You can also set the memory table like this:
```

Example 4 (unknown):
```unknown
3.4. The methods you previously had access to through the Memory class, are now direclty available on the relevant `db` object. For example:
```

---

## Get user profile

**URL:** llms-txt#get-user-profile

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Get my X profile", markdown=True)
```

Check out the [Tweet Analysis Agent](/examples/use-cases/agents/tweet-analysis-agent)
  for a more advanced example.{" "}
</Note>

| Parameter              | Type   | Default | Description                                                    |
| ---------------------- | ------ | ------- | -------------------------------------------------------------- |
| `bearer_token`         | `str`  | `None`  | Bearer token for authentication                                |
| `consumer_key`         | `str`  | `None`  | Consumer key for authentication                                |
| `consumer_secret`      | `str`  | `None`  | Consumer secret for authentication                             |
| `access_token`         | `str`  | `None`  | Access token for authentication                                |
| `access_token_secret`  | `str`  | `None`  | Access token secret for authentication                         |
| `include_post_metrics` | `bool` | `False` | Include post metrics (likes, retweets, etc.) in search results |
| `wait_on_rate_limit`   | `bool` | `False` | Retry when rate limits are reached                             |

| Function            | Description                                 |
| ------------------- | ------------------------------------------- |
| `create_post`       | Creates and posts a new post                |
| `reply_to_post`     | Replies to an existing post                 |
| `send_dm`           | Sends a direct message to a X user          |
| `get_user_info`     | Retrieves information about a X user        |
| `get_home_timeline` | Gets the authenticated user's home timeline |
| `search_posts`      | Searches for tweets                         |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/x.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/x_tools.py)
* View [Tweet Analysis Agent Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/examples/agents/social_media_agent.py)

---

## Create a financial advisor team with comprehensive hooks

**URL:** llms-txt#create-a-financial-advisor-team-with-comprehensive-hooks

**Contents:**
- Usage

team = Team(
    name="Financial Advisor",
    model=OpenAIChat(id="gpt-5-mini"),
    pre_hooks=[transform_input],
    description="A professional financial advisor providing investment guidance and financial planning advice.",
    instructions=[
        "You are a knowledgeable financial advisor with expertise in:",
        "â€¢ Investment strategies and portfolio management",
        "â€¢ Retirement planning and savings strategies",
        "â€¢ Risk assessment and diversification",
        "â€¢ Tax-efficient investing",
        "",
        "Provide clear, actionable advice while being mindful of individual circumstances.",
        "Always remind users to consult with a licensed financial advisor for personalized advice.",
    ],
    debug_mode=True,
)

team.print_response(
    input="I'm 35 years old and want to start investing for retirement. moderate risk tolerance. retirement savings in IRAs/401(k)s= $100,000. total savings is $200,000. my net worth is $300,000",
    session_id="test_session",
    user_id="test_user",
    stream=True,
)
bash  theme={null}
    pip install -U agno openai
    bash Mac theme={null}
      python cookbook/teams/hooks/input_transformation_pre_hook.py
      bash Windows theme={null}
      python cookbook/teams/hooks/input_transformation_pre_hook.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Connecting to Tableplus

**URL:** llms-txt#connecting-to-tableplus

**Contents:**
- Step 1: Start Your `pgvector` Container
- Step 2: Configure TablePlus

Source: https://docs.agno.com/faq/connecting-to-tableplus

If you want to inspect your pgvector container to explore your storage or knowledge base, you can use TablePlus. Follow these steps:

## Step 1: Start Your `pgvector` Container

Run the following command to start a `pgvector` container locally:

* `POSTGRES_DB=ai` sets the default database name.
* `POSTGRES_USER=ai` and `POSTGRES_PASSWORD=ai` define the database credentials.
* The container exposes port `5432` (mapped to `5532` on your local machine).

## Step 2: Configure TablePlus

1. **Open TablePlus**: Launch the TablePlus application.
2. **Create a New Connection**: Click on the `+` icon to add a new connection.
3. **Select `PostgreSQL`**: Choose PostgreSQL as the database type.

Fill in the following connection details:

* **Host**: `localhost`
* **Port**: `5532`
* **Database**: `ai`
* **User**: `ai`
* **Password**: `ai`

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=65f0ec170fdef92080bdc0e72feaacc4" data-og-width="492" width="492" data-og-height="386" height="386" data-path="images/tableplus.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=4c693a789381ec09ee8112a29a19a23f 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=83b7b78de5bb7e3e04337380be4a846a 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=a933eddefc5117323116e34eba956055 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=84a19124e98f325caa51ff1ee0032652 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=0a60723cf13d5771fcc8d9f89bb921f2 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=24d6892323098fdd654dcf4fcf579419 2500w" />

---

## Jina Embedder

**URL:** llms-txt#jina-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/jina/usage/jina-embedder

```python  theme={null}

from agno.knowledge.embedder.jina import JinaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## Create individual specialized agents

**URL:** llms-txt#create-individual-specialized-agents

researcher = Agent(
    name="Researcher",
    role="Expert at finding information",
    tools=[DuckDuckGoTools()],
    model=OpenAIChat(id="gpt-4o"),
)

writer = Agent(
    name="Writer",
    role="Expert at writing clear, engaging content",
    model=OpenAIChat(id="gpt-4o"),
)

---

## Process the audio and get a response

**URL:** llms-txt#process-the-audio-and-get-a-response

run_response = agent.run(
    "What's in this recording? Please analyze the content and tone.",
    audio=[Audio(content=response.content, format="wav")],
)

---

## Nvidia

**URL:** llms-txt#nvidia

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/nvidia

The Nvidia model provides access to Nvidia's language models.

| Parameter               | Type            | Default                                    | Description                                                      |
| ----------------------- | --------------- | ------------------------------------------ | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"nvidia/llama-3.1-nemotron-70b-instruct"` | The id of the NVIDIA model to use                                |
| `name`                  | `str`           | `"NVIDIA"`                                 | The name of the model                                            |
| `provider`              | `str`           | `"NVIDIA"`                                 | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                                     | The API key for NVIDIA (defaults to NVIDIA\_API\_KEY env var)    |
| `base_url`              | `str`           | `"https://integrate.api.nvidia.com/v1"`    | The base URL for the NVIDIA API                                  |
| `retries`               | `int`           | `0`                                        | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                                        | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                                    | If True, the delay between retries is doubled each time          |

NVIDIA extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Execute sample queries.

**URL:** llms-txt#execute-sample-queries.

agent1.print_response("How many people live in Canada?")
agent1.print_response("What is their national anthem called?")

---

## Custom Retriever

**URL:** llms-txt#custom-retriever

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/custom-retriever

```python retriever.py theme={null}
from typing import Optional

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant
from qdrant_client import QdrantClient

---

## Smaller dimensions = faster search, lower cost

**URL:** llms-txt#smaller-dimensions-=-faster-search,-lower-cost

**Contents:**
- Monitoring Performance

embedder = OpenAIEmbedder(
    id="text-embedding-3-large",
    dimensions=1024  # Instead of full 3072
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Monitoring Performance

Keep an eye on these metrics:
```

---

## Setup the DynamoDB database

**URL:** llms-txt#setup-the-dynamodb-database

**Contents:**
- Params
- Developer Resources

class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

<Snippet file="db-dynamodb-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/dynamodb/dynamo_for_team.py)

---

## Image Bytes Input Agent

**URL:** llms-txt#image-bytes-input-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/image-bytes-input-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create a knowledge base with PDF documents

**URL:** llms-txt#create-a-knowledge-base-with-pdf-documents

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    )
)

---

## Each run creates or updates the workflow session

**URL:** llms-txt#each-run-creates-or-updates-the-workflow-session

**Contents:**
  - Workflow Session Structure
  - What Gets Stored
  - Key Differences from Agent/Team Sessions
  - Database Options

result = workflow.run(input="AI trends", session_id="session_123")
python  theme={null}
@dataclass
class WorkflowSession:
    session_id: str           # Unique session identifier
    user_id: str | None       # User who owns this session
    workflow_id: str | None   # Which workflow this belongs to
    workflow_name: str | None # Name of the workflow
    
    # List of all workflow runs (executions)
    runs: List[WorkflowRunOutput] | None
    
    # Session-specific data
    session_data: Dict | None    # Includes session_name, session_state
    workflow_data: Dict | None   # Workflow configuration
    metadata: Dict | None        # Custom metadata
    
    created_at: int | None    # Unix timestamp
    updated_at: int | None    # Unix timestamp
python  theme={null}
from agno.db.sqlite import SqliteDb

**Examples:**

Example 1 (unknown):
```unknown
Each time you run the workflow, Agno:

1. Creates a unique `run_id` for this execution
2. Stores the input, output, and all step results
3. Updates the session with the new run
4. Makes the history available for future runs

### Workflow Session Structure

A workflow session stores:
```

Example 2 (unknown):
```unknown
<Note>
  Unlike agent sessions, workflow sessions don't have a `summary` field. Workflows store complete run results instead of creating summaries.
</Note>

### What Gets Stored

Each workflow run stores:

* **Input:** The data passed to `workflow.run()`
* **Output:** The final result from the workflow
* **Step results:** Output from each step in the pipeline
* **Session Data:** Execution time, status, metrics
* **Session state:** Shared data between steps (if used)

### Key Differences from Agent/Team Sessions

If you're familiar with agent or team sessions, here are the main differences:

| Feature            | Agent/Team Sessions                       | Workflow Sessions                             |
| ------------------ | ----------------------------------------- | --------------------------------------------- |
| **What's stored**  | Messages and conversation turns           | Complete workflow runs with step results      |
| **History type**   | Message-based (chat history)              | Run-based (execution history)                 |
| **Summaries**      | Supported with `enable_session_summaries` | Not supported (stores complete runs)          |
| **History format** | Messages in LLM context                   | Previous run results prepended to step inputs |

### Database Options

Workflow sessions require a database to persist execution history. Agno supports multiple database types:
```

---

## End condition function for the loop

**URL:** llms-txt#end-condition-function-for-the-loop

def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

# Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f"âœ… Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

print("âŒ Research quality check failed - need more substantial research")
    return False

---

## LlamaIndex Vector Database

**URL:** llms-txt#llamaindex-vector-database

Source: https://docs.agno.com/integrations/vectordb/llamaindex/overview

Learn how to use LlamaIndex as a vector database for your Knowledge Base

---

## Setup the SQLite database with custom table names

**URL:** llms-txt#setup-the-sqlite-database-with-custom-table-names

db = SqliteDb(
    db_file="tmp/data.db",
    # Selecting which tables to use
    session_table="agent_sessions",
    memory_table="agent_memories",
    metrics_table="agent_metrics",
)

---

## Confirmation Required with Run ID

**URL:** llms-txt#confirmation-required-with-run-id

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-with-run-id

This example demonstrates human-in-the-loop functionality using specific run IDs for session management. It shows how to continue agent execution with updated tools using run identifiers.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## What are Embedders?

**URL:** llms-txt#what-are-embedders?

**Contents:**
  - Why use embedders?
  - When to use embedders
  - How it works in Agno

Source: https://docs.agno.com/basics/knowledge/embedder/overview

Learn how to use embedders with Agno to convert complex information into vector representations.

Embedders turn text, images, and other data into vectors (lists of numbers) that capture meaning. Those vectors make it easy to store and search information semanticallyâ€”so you find content by intent and context, not just exact keywords.

If you're building features like retrieval-augmented generation (RAG), semantic search, question answering over docs, or long-term memory for agents, embedders are the foundation that makes it all work.

### Why use embedders?

* **Better recall than keywords**: They understand meaning, so "How do I reset my passcode?" finds docs mentioning "change PIN".
* **Ground LLMs in your data**: Provide the model with trusted, domain-specific context at answer time.
* **Scale to large knowledge bases**: Vectors enable fast similarity search across thousands or millions of chunks.
* **Multilingual retrieval**: Many embedders map different languages to the same semantic space.

### When to use embedders

Use embedders when you need any of the following:

* **RAG and context injection**: Supply relevant snippets to your agent before responding.
* **Semantic search**: Let users query by meaning across product docs, wikis, tickets, or chats.
* **Deduplication and clustering**: Group similar content or avoid repeating the same info.
* **Personal and team memory**: Store summaries and facts for later recall by agents.

You probably donâ€™t need embedders when your dataset is tiny (a handful of pages) and simple keyword search already works well.

### How it works in Agno

Agno uses `OpenAIEmbedder` as the default, but you can swap in any supported embedder. When you add content to a knowledge base, the embedder converts each chunk into a vector and stores it in your vector database. Later, when an agent searches, it embeds the query and finds the most similar vectors.

Here's a basic setup:

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.openai import OpenAIEmbedder

---

## Grouped Steps Workflow

**URL:** llms-txt#grouped-steps-workflow

**Contents:**
- Basic Example

Source: https://docs.agno.com/basics/workflows/workflow-patterns/grouped-steps-workflow

Organize multiple steps into reusable, logical sequences for complex workflows with clean separation of concerns

**Key Benefits**: Reusable sequences, cleaner branching logic, modular workflow design

Grouped steps enable modular workflow architecture with reusable components and clear logical boundaries.

```python grouped_steps_workflow.py theme={null}
from agno.workflow import Steps, Step, Workflow

---

## Using Docker Compose

**URL:** llms-txt#using-docker-compose

docker-compose exec db psql -U toolbox_user -d toolbox_db -c "SELECT COUNT(*) FROM hotels;"

---

## Set up tracing with custom configuration

**URL:** llms-txt#set-up-tracing-with-custom-configuration

setup_tracing(
    db=tracing_db,
    batch_processing=True,
    max_queue_size=2048,
    schedule_delay_millis=3000,
)

---

## Sync Operations

**URL:** llms-txt#sync-operations

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/getting-started/usage/sync-operations

This example shows how to add content to your knowledge base synchronously. While async operations are recommended for better performance, sync operations can be useful in certain scenarios.

```python 13_sync.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

---

## Or clone and copy

**URL:** llms-txt#or-clone-and-copy

**Contents:**
- IDE Support
- Learn More

git clone https://github.com/agno-agi/agno.git
cp agno/.cursorrules /path/to/your/project/
```

Your AI assistant (Cursor, Windsurf, etc.) will automatically detect and use it.

<Card title="View .cursorrules on GitHub" icon="github" href="https://github.com/agno-agi/agno/blob/main/.cursorrules">
  View the full .cursorrules file for building agents with Agno
</Card>

`.cursorrules` works with:

* **Cursor** - Automatic detection
* **Windsurf** - Native support
* **Other AI assistants** - Support may vary depending on integration

For detailed examples and patterns:

<CardGroup cols={3}>
  <Card title="Agent Examples" icon="code" href="/examples/basics/agent">
    See complete agent examples
  </Card>

<Card title="Agent Concepts" icon="book" href="/basics/agents/overview">
    Learn agent fundamentals
  </Card>

<Card title="Teams" icon="users" href="/basics/teams/overview">
    Multi-agent coordination
  </Card>

<Card title="Workflows" icon="workflow" href="/basics/workflows/overview">
    Deterministic agent orchestration
  </Card>
</CardGroup>

<Note>
  The `.cursorrules` file is focused on **building agents with Agno**. If you're contributing to the Agno framework itself, that will be covered separately.
</Note>

---

## Setup your Agent using a reasoning model

**URL:** llms-txt#setup-your-agent-using-a-reasoning-model

agent = Agent(
    model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
    markdown=True,
)

---

## Create Knowledge Instance with ChromaDB

**URL:** llms-txt#create-knowledge-instance-with-chromadb

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with ChromaDB",
    vector_db=ChromaDb(
        collection="vectors", path="tmp/chromadb", persistent_client=True
    ),
)

knowledge.add_content(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"doc_type": "recipe_book"},
    )

---

## Websearch Builtin Tool

**URL:** llms-txt#websearch-builtin-tool

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/websearch-builtin-tool

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run team and return the response as a variable

**URL:** llms-txt#run-team-and-return-the-response-as-a-variable

response = team.run(input="What is the weather in Tokyo?")

---

## Agent responds with current, accurate policy details

**URL:** llms-txt#agent-responds-with-current,-accurate-policy-details

**Contents:**
  - Contextual Understanding

**Examples:**

Example 1 (unknown):
```unknown
### Contextual Understanding

The agent understands the context of questions and searches for the most relevant information, not just keyword matches.
```

---

## Confirmation Required with Multiple Tools

**URL:** llms-txt#confirmation-required-with-multiple-tools

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-multiple-tools

This example demonstrates human-in-the-loop functionality with multiple tools that require confirmation. It shows how to handle user confirmation during tool execution and gracefully cancel operations based on user choice.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Confirmation Required with Toolkit

**URL:** llms-txt#confirmation-required-with-toolkit

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-toolkit

This example demonstrates human-in-the-loop functionality using toolkit-based tools that require confirmation. It shows how to handle user confirmation when working with pre-built tool collections like DuckDuckGoTools.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## JSON for Workflows

**URL:** llms-txt#json-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/json/usage/json-for-workflow

Agno supports using local JSON files as a storage backend for Workflows using the `JsonDb` class.

```python json_for_workflows.py theme={null}
from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Get spans and print tree

**URL:** llms-txt#get-spans-and-print-tree

**Contents:**
- See Also

spans = db.get_spans(trace_id=trace.trace_id)
print_span_tree(spans)
```

* [Trace Reference](/reference/tracing/trace) - Complete execution trace
* [DB Functions](/basics/tracing/db-functions) - Query functions for traces and spans

---

## Collaborative Image Generation Team

**URL:** llms-txt#collaborative-image-generation-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/generate-image-with-team

This example demonstrates how a team can collaborate to generate high-quality images using a prompt engineer to optimize prompts and an image creator to generate images with DALL-E.

```python cookbook/examples/teams/multimodal/generate_image_with_team.py theme={null}
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.dalle import DalleTools
from agno.utils.common import dataclass_to_dict
from rich.pretty import pprint

image_generator = Agent(
    name="Image Creator",
    role="Generate images using DALL-E",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DalleTools()],
    instructions=[
        "Use the DALL-E tool to create high-quality images",
        "Return image URLs in markdown format: `![description](URL)`",
    ],
)

prompt_engineer = Agent(
    name="Prompt Engineer",
    role="Optimize and enhance image generation prompts",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Enhance user prompts for better image generation results",
        "Consider artistic style, composition, and technical details",
    ],
)

---

## Print the response

**URL:** llms-txt#print-the-response

**Contents:**
- Notes

print(response.content)
```

* **Environment Variables**: Ensure your environment variable is correctly set for the AgentOps API key.
* **Initialization**: Call `agentops.init()` to initialize AgentOps.
* **AgentOps Docs**: [AgentOps Docs](https://docs.agentops.ai/v2/integrations/agno)

Following these steps will integrate Agno with AgentOps, providing comprehensive logging and visualization for your AI agentsâ€™ model calls.

---

## Final Synthesizer Agent - Specialized in optimal synthesis

**URL:** llms-txt#final-synthesizer-agent---specialized-in-optimal-synthesis

final_synthesizer = Agent(
    name="Final Synthesizer",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Synthesize reranked results into optimal comprehensive responses",
    instructions=[
        "Synthesize information from all team members into optimal responses.",
        "Leverage the reranked and analyzed results for maximum quality.",
        "Create responses that demonstrate the benefits of advanced reranking.",
        "Ensure optimal information organization and presentation.",
        "Include confidence levels and source quality indicators.",
    ],
    markdown=True,
)

---

## Example prompts to try:

**URL:** llms-txt#example-prompts-to-try:

**Contents:**
- Usage

"""
Explore Agno's capabilities with these queries:
1. "What are the different types of agents in Agno?"
2. "How does Agno handle knowledge base management?"
3. "What embedding models does Agno support?"
4. "How can I implement custom tools in Agno?"
5. "What storage options are available for workflow caching?"
6. "How does Agno handle streaming responses?"
7. "What types of LLM providers does Agno support?"
8. "How can I implement custom knowledge sources?"
"""

bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    pip install -U agno openai lancedb tantivy inquirer
    bash Mac theme={null}
      python cookbook/examples/agents/deep_knowledge.py
      bash Windows theme={null}
      python cookbook/examples/agents/deep_knowledge.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Conversational Workflow with Conditional Step

**URL:** llms-txt#conversational-workflow-with-conditional-step

Source: https://docs.agno.com/basics/workflows/usage/conversational-workflow-with-conditional-step

This example demonstrates a conversational workflow with a conditional step.

This example shows how to use the `WorkflowAgent` to create a conversational workflow with a conditional step.

```python conversational_workflow_with_conditional_step.py theme={null}
import asyncio

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.workflow import WorkflowAgent
from agno.workflow.condition import Condition
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## OpenCL

**URL:** llms-txt#opencl

**Contents:**
  - Model Quantization
- Troubleshooting
  - Server Connection Issues
  - Model Loading Problems
  - Performance Issues

make LLAMA_CLBLAST=1
bash check server theme={null}
curl http://127.0.0.1:8080/v1/models
```

### Model Loading Problems

* Verify the model file exists and is in GGML format
* Check available memory for large models
* Ensure the model is compatible with your LlamaCpp version

### Performance Issues

* Adjust batch sizes (`-b`, `-ub`) based on your hardware
* Use GPU acceleration if available
* Consider using quantized models for faster inference

**Examples:**

Example 1 (unknown):
```unknown
### Model Quantization

Use quantized models for better performance:

* `Q4_K_M`: Balanced size and quality
* `Q8_0`: Higher quality, larger size
* `Q2_K`: Smallest size, lower quality

## Troubleshooting

### Server Connection Issues

Ensure the LlamaCpp server is running and accessible:
```

---

## NVIDIA GPU (CUDA)

**URL:** llms-txt#nvidia-gpu-(cuda)

---

## Query the knowledge base

**URL:** llms-txt#query-the-knowledge-base

**Contents:**
- Usage
- Params

agent.print_response(
    "What are the main topics discussed in the videos?",
    markdown=True
)
bash  theme={null}
    pip install -U youtube-transcript-api pytube sqlalchemy psycopg pgvector agno openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python examples/basics/knowledge/readers/youtube_reader_sync.py
      bash Windows theme={null}
      python examples/basics/knowledge/readers/youtube_reader_sync.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="youtube-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Tool Use Gpt 5

**URL:** llms-txt#tool-use-gpt-5

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/tool-use-gpt-5

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## All messages excluding those marked as from_history

**URL:** llms-txt#all-messages-excluding-those-marked-as-from_history

chat_history = agent.get_chat_history()

---

## AgentOS Connection Issues

**URL:** llms-txt#agentos-connection-issues

**Contents:**
- Browser Compatibility
  - Recommended Browsers
  - Browsers with Known Issues
- Solutions
  - For Brave Users
  - For Other Browsers

Source: https://docs.agno.com/faq/agentos-connection

If you're experiencing connection issues with AgentOS, particularly when trying to connect to **local instances**, this guide will help you resolve them.

## Browser Compatibility

Some browsers have security restrictions that prevent connections to localhost domains due to mixed content security issues. Here's what you need to know about different browsers:

### Recommended Browsers

* **Chrome & Edge**: These browsers work well with local connections by default and are our recommended choices
* **Firefox**: Generally works well with local connections

### Browsers with Known Issues

* **Safari**: May block local connections due to its strict security policies
* **Brave**: Blocks local connections by default due to its shield feature

If you're using Brave browser, you can try these steps:

1. Click on the Brave shield icon in the address bar
2. Turn off the shield for the current site
3. Refresh the endpoint and try connecting again

<video autoPlay muted controls className="w-full aspect-video" src="https://mintcdn.com/agno-v2/Xj0hQoiFt0n7bXOq/videos/agentos-brave-issue.mp4?fit=max&auto=format&n=Xj0hQoiFt0n7bXOq&q=85&s=80ec713d1ca11cc06366c5460388fdd8" data-path="videos/agentos-brave-issue.mp4" />

### For Other Browsers

If you're using Safari or experiencing issues with other browsers, you can use one of these solutions:

#### 1. Use Chrome or Edge

The simplest solution is to use Chrome or Edge browsers which have better support for local connections.

#### 2. Use Tunneling Services

You can use tunneling services to expose your local endpoint to the internet:

1. Install ngrok from [ngrok.com](https://ngrok.com)
2. Run your local server
3. Create a tunnel with ngrok:

4. Use the provided ngrok URL on [AgentOS](https://os.agno.com).

##### Using Cloudflare Tunnel

1. Install Cloudflare Tunnel (cloudflared) from [Cloudflare's website](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation/)
2. Authenticate with Cloudflare
3. Create a tunnel:

4. Use the provided Cloudflare URL on [AgentOS](https://os.agno.com).

**Examples:**

Example 1 (unknown):
```unknown
4. Use the provided ngrok URL on [AgentOS](https://os.agno.com).

##### Using Cloudflare Tunnel

1. Install Cloudflare Tunnel (cloudflared) from [Cloudflare's website](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation/)
2. Authenticate with Cloudflare
3. Create a tunnel:
```

---

## Clickhouse

**URL:** llms-txt#clickhouse

Source: https://docs.agno.com/reference/vector-db/clickhouse

<Snippet file="vector-db-clickhouse-reference.mdx" />

---

## Create the team

**URL:** llms-txt#create-the-team

**Contents:**
- Run your Team

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o"),
    instructions="Coordinate with team members to provide comprehensive information. Delegate tasks based on the user's request."
)

team.print_response("What's the latest news and weather in Tokyo?", stream=True)
python  theme={null}
team.print_response("What's the latest news and weather in Tokyo?")
python  theme={null}
from typing import Iterator
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response

news_agent = Agent(name="News Agent", role="Get the latest news", tools=[DuckDuckGoTools()])
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days", tools=[DuckDuckGoTools()])

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o")
)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  It is recommended to specify the `id`, `name` and the `role` fields of each team member, for better identification by the team leader when delegating requests.
  The `id` is used to identify the team member in the team and in the team leader's context.
</Tip>

<Note>
  Team members inherit the `model` parameter from their parent `Team` if not specified. Members with an explicitly assigned `model` retain their own. In nested team structures, inheritance always happens from the direct parent.
  Teams without a defined model default to OpenAI `gpt-4o`.

  The `reasoning_model`, `parser_model`, and `output_model` must be explicitly defined for each team or team member.

  See the [model inheritance example](/basics/teams/usage/other/model-inheritance).
</Note>

## Run your Team

When running your team, use the `Team.print_response()` method to print the response in the terminal.

You can pass `show_members_responses=True` to also print the responses from the team members.

For example:
```

Example 2 (unknown):
```unknown
This is only for development purposes and not recommended for production use. In production, use the `Team.run()` or `Team.arun()` methods.

For example:
```

---

## Performance with Teams

**URL:** llms-txt#performance-with-teams

Source: https://docs.agno.com/basics/evals/performance/usage/performance-team-instantiation

Example showing how to analyze the runtime and memory usage of an Agno Team.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Pass a dictionary - it will be automatically validated against ResearchProject schema

**URL:** llms-txt#pass-a-dictionary---it-will-be-automatically-validated-against-researchproject-schema

research_team.print_response(
    input={
        "project_name": "AI Framework Comparison 2024",
        "research_topics": ["LangChain", "CrewAI", "AutoGen", "Agno"],
        "target_audience": "AI Engineers and Developers",
        "depth_level": "intermediate",
        "max_sources": 15,
        "include_recent_only": True,
    }
)

print("\n=== Example 2: Pydantic Model Input (direct pass-through) ===")

---

## Run async optimization

**URL:** llms-txt#run-async-optimization

**Contents:**
- Optimization Strategies
  - Summarize Strategy (Default)
  - Custom Strategies

optimized = asyncio.run(optimize_user_memories())

Memory 1: "The user's name is John Doe"
Memory 2: "The user likes to play basketball"
Memory 3: "The user's favorite color is blue"
Memory 4: "The user works as a software engineer"
Memory 5: "The user lives in San Francisco"

Memory 1: "John Doe is a software engineer who lives in San Francisco. 
          He enjoys playing basketball and his favorite color is blue."
python  theme={null}
from agno.memory.strategies import MemoryOptimizationStrategy
from agno.db.schemas import UserMemory
from agno.models.base import Model
from typing import List

class CustomOptimizationStrategy(MemoryOptimizationStrategy):
    def optimize(
        self,
        memories: List[UserMemory],
        model: Model,
    ) -> List[UserMemory]:
        # Your custom optimization logic
        # Return optimized list of UserMemory objects
        pass
    
    async def aoptimize(
        self,
        memories: List[UserMemory],
        model: Model,
    ) -> List[UserMemory]:
        # Async version
        pass

**Examples:**

Example 1 (unknown):
```unknown
## Optimization Strategies

### Summarize Strategy (Default)

The `SUMMARIZE` strategy combines all memories into a single comprehensive summary:

* **Best for:** Most use cases, maximum token reduction
* **Result:** All memories combined into 1 memory
* **Preserves:** All factual information, topics, metadata

**Before Optimization:**
```

Example 2 (unknown):
```unknown
**After Optimization:**
```

Example 3 (unknown):
```unknown
**Token Savings:**

* Before: \~50 tokens Ã— 5 memories = 250 tokens
* After: \~40 tokens Ã— 1 memory = 40 tokens
* **Savings: 84% reduction**

### Custom Strategies

You can create custom optimization strategies by implementing the `MemoryOptimizationStrategy` interface:
```

---

## Discord Bot

**URL:** llms-txt#discord-bot

**Contents:**
- Setup Steps
  - Example Usage
- Core Components
- `DiscordClient` Class
  - Initialization Parameters
- Event Handling
  - Message Events
  - Supported Media Types
- Environment Variables
- Message Processing

Source: https://docs.agno.com/integrations/discord/overview

Host agents as Discord Bots.

The Discord Bot integration allows you to serve Agents or Teams via Discord, using the discord.py library to handle Discord events and send messages.

<Snippet file="setup-discord-app.mdx" />

Create an agent, wrap it with `DiscordClient`, and run it:

* `DiscordClient`: Wraps Agno agents/teams for Discord integration using discord.py.
* `DiscordClient.serve`: Starts the Discord bot client with the provided token.

## `DiscordClient` Class

Main entry point for Agno Discord bot applications.

### Initialization Parameters

| Parameter | Type              | Default | Description            |
| --------- | ----------------- | ------- | ---------------------- |
| `agent`   | `Optional[Agent]` | `None`  | Agno `Agent` instance. |
| `team`    | `Optional[Team]`  | `None`  | Agno `Team` instance.  |

*Provide `agent` or `team`, not both.*

The Discord bot automatically handles various Discord events:

* **Description**: Processes all incoming messages from users
* **Media Support**: Handles images, videos, audio files, and documents
* **Threading**: Automatically creates threads for conversations
* **Features**:
  * Automatic thread creation for each conversation
  * Media processing and forwarding to agents
  * Message splitting for responses longer than 1500 characters
  * Support for reasoning content display
  * Context enrichment with username and message URL

### Supported Media Types

* **Images**: Direct URL processing for image analysis
* **Videos**: Downloads and processes video content
* **Audio**: URL-based audio processing
* **Files**: Downloads and processes document attachments

## Environment Variables

Ensure the following environment variable is set:

## Message Processing

The bot processes messages with the following workflow:

1. **Message Reception**: Receives messages from Discord channels
2. **Media Processing**: Downloads and processes any attached media
3. **Thread Management**: Creates or uses existing threads for conversations
4. **Agent/Team Execution**: Forwards the message and media to the configured agent or team
5. **Response Handling**: Sends the response back to Discord, splitting long messages if necessary
6. **Reasoning Display**: Shows reasoning content in italics if available

### Automatic Thread Creation

* Creates a new thread for each user's first message
* Maintains conversation context within threads
* Uses the format: `{username}'s thread`

* **Images**: Passed as `Image` objects with URLs
* **Videos**: Downloaded and passed as `Video` objects with content
* **Audio**: Passed as `Audio` objects with URLs
* **Files**: Downloaded and passed as `File` objects with content

### Message Formatting

* Long messages (>1500 characters) are automatically split
* Reasoning content is displayed in italics
* Batch numbering for split messages: `[1/3] message content`

## Testing the Integration

1. Set up your Discord bot token: `export DISCORD_BOT_TOKEN="your-token"`
2. Run your application: `python your_discord_bot.py`
3. Invite the bot to your Discord server
4. Send a message in any channel where the bot has access
5. The bot will automatically create a thread and respond

**Examples:**

Example 1 (unknown):
```unknown
## Core Components

* `DiscordClient`: Wraps Agno agents/teams for Discord integration using discord.py.
* `DiscordClient.serve`: Starts the Discord bot client with the provided token.

## `DiscordClient` Class

Main entry point for Agno Discord bot applications.

### Initialization Parameters

| Parameter | Type              | Default | Description            |
| --------- | ----------------- | ------- | ---------------------- |
| `agent`   | `Optional[Agent]` | `None`  | Agno `Agent` instance. |
| `team`    | `Optional[Team]`  | `None`  | Agno `Team` instance.  |

*Provide `agent` or `team`, not both.*

## Event Handling

The Discord bot automatically handles various Discord events:

### Message Events

* **Description**: Processes all incoming messages from users
* **Media Support**: Handles images, videos, audio files, and documents
* **Threading**: Automatically creates threads for conversations
* **Features**:
  * Automatic thread creation for each conversation
  * Media processing and forwarding to agents
  * Message splitting for responses longer than 1500 characters
  * Support for reasoning content display
  * Context enrichment with username and message URL

### Supported Media Types

* **Images**: Direct URL processing for image analysis
* **Videos**: Downloads and processes video content
* **Audio**: URL-based audio processing
* **Files**: Downloads and processes document attachments

## Environment Variables

Ensure the following environment variable is set:
```

---

## Create an agent with Searxng

**URL:** llms-txt#create-an-agent-with-searxng

agent = Agent(tools=[searxng])

---

## Cohere

**URL:** llms-txt#cohere

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/cohere

The Cohere model provides access to Cohere's language models.

| Parameter               | Type                       | Default                    | Description                                                      |
| ----------------------- | -------------------------- | -------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`                      | `"command-r-plus-08-2024"` | The id of the Cohere model to use                                |
| `name`                  | `str`                      | `"CohereChat"`             | The name of the model                                            |
| `provider`              | `str`                      | `"Cohere"`                 | The provider of the model                                        |
| `api_key`               | `Optional[str]`            | `None`                     | The API key for Cohere (defaults to COHERE\_API\_KEY env var)    |
| `max_tokens`            | `Optional[int]`            | `None`                     | Maximum number of tokens to generate                             |
| `temperature`           | `Optional[float]`          | `None`                     | Controls randomness in the model's output (0.0 to 1.0)           |
| `p`                     | `Optional[float]`          | `None`                     | Controls diversity via nucleus sampling (0.0 to 1.0)             |
| `k`                     | `Optional[int]`            | `None`                     | Controls diversity via top-k sampling                            |
| `seed`                  | `Optional[int]`            | `None`                     | Random seed for deterministic sampling                           |
| `frequency_penalty`     | `Optional[float]`          | `None`                     | Reduces repetition by penalizing frequent tokens (0.0 to 1.0)    |
| `presence_penalty`      | `Optional[float]`          | `None`                     | Reduces repetition by penalizing present tokens (0.0 to 1.0)     |
| `stop_sequences`        | `Optional[List[str]]`      | `None`                     | List of strings that stop generation                             |
| `response_format`       | `Optional[Dict[str, Any]]` | `None`                     | Specifies the format of the response (e.g., JSON)                |
| `citation_options`      | `Optional[Dict[str, Any]]` | `None`                     | Options for citation generation                                  |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`                     | Additional parameters to include in the request                  |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`                     | Additional parameters for client configuration                   |
| `retries`               | `int`                      | `0`                        | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`                      | `1`                        | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`                     | `False`                    | If True, the delay between retries is doubled each time          |

---

## Now provide the adjusted Memory Manager to your Agent

**URL:** llms-txt#now-provide-the-adjusted-memory-manager-to-your-agent

**Contents:**
- Memories and Context

agent = Agent(
    db=db,
    memory_manager=memory_manager,
    enable_user_memories=True,
)

agent.print_response("My name is John Doe and I like to play basketball on the weekends.")

agent.print_response("What's do I do in weekends?")
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

**Examples:**

Example 1 (unknown):
```unknown
In this example, the memory manager will store memories about hobbies, but won't include the user's actual name. This is useful for healthcare, legal, or other privacy-sensitive applications.

## Memories and Context

When enabled, memories about the current user are automatically added to the agent's context on each request. But in some scenarios, like when you're building analytics on memories or want the agent to explicitly search for memories using tools, you might want to store memories without auto-including them.

Use `add_memories_to_context=False` to collect memories in the background while keeping the agent's context lean:
```

---

## Set the local collector endpoint

**URL:** llms-txt#set-the-local-collector-endpoint

os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"

---

## MongoDB Vector Database

**URL:** llms-txt#mongodb-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/mongodb/overview

Learn how to use MongoDB as a vector database for your Knowledge Base

Follow the instructions in the [MongoDB Setup Guide](https://www.mongodb.com/docs/atlas/getting-started/) to get connection string

Install MongoDB packages

```python agent_with_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.mongodb import MongoVectorDb

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Audio Input Output

**URL:** llms-txt#audio-input-output

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/audio/usage/audio-input-output

```python  theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

---

## Todoist

**URL:** llms-txt#todoist

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/todoist

**TodoistTools** enables an Agent to interact with [Todoist](https://www.todoist.com/).

The following example requires the `todoist-api-python` library. and a Todoist API token which can be obtained from the [Todoist Developer Portal](https://app.todoist.com/app/settings/integrations/developer).

The following agent will create a new task in Todoist.

```python cookbook/tools/todoist.py theme={null}
"""
Example showing how to use the Todoist Tools with Agno

Requirements:
- Sign up/login to Todoist and get a Todoist API Token (get from https://app.todoist.com/app/settings/integrations/developer)
- pip install todoist-api-python

Usage:
- Set the following environment variables:
    export TODOIST_API_TOKEN="your_api_token"

- Or provide them when creating the TodoistTools instance
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.todoist import TodoistTools

todoist_agent = Agent(
    name="Todoist Agent",
    role="Manage your todoist tasks",
    instructions=[
        "When given a task, create a todoist task for it.",
        "When given a list of tasks, create a todoist task for each one.",
        "When given a task to update, update the todoist task.",
        "When given a task to delete, delete the todoist task.",
        "When given a task to get, get the todoist task.",
    ],
    id="todoist-agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[TodoistTools()],
    markdown=True,
    debug_mode=True,
    )

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will create a new task in Todoist.
```

---

## Image to Structured Movie Script Team

**URL:** llms-txt#image-to-structured-movie-script-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/image-to-structured-output

This example demonstrates how a team can collaborate to analyze images and create structured movie scripts using Pydantic models for consistent output format.

```python cookbook/examples/teams/multimodal/image_to_structured_output.py theme={null}
from typing import List

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.team import Team
from pydantic import BaseModel, Field
from rich.pretty import pprint

class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

image_analyst = Agent(
    name="Image Analyst",
    role="Analyze visual content and extract key elements",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Analyze images for visual elements, setting, and characters",
        "Focus on details that can inspire creative content",
    ],
)

script_writer = Agent(
    name="Script Writer",
    role="Create structured movie scripts from visual inspiration",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Transform visual analysis into compelling movie concepts",
        "Follow the structured output format precisely",
    ],
)

---

## Huggingface Embedder

**URL:** llms-txt#huggingface-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/huggingface/usage/huggingface-embedder

```python  theme={null}
from agno.knowledge.embedder.huggingface import HuggingfaceCustomEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = HuggingfaceCustomEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Serialized to JSON

**URL:** llms-txt#serialized-to-json

**Contents:**
- Using Filters Through the API
  - Dictionary Filters (Simple Approach)

{
  "op": "AND",
  "conditions": [
    {"op": "EQ", "key": "status", "value": "published"},
    {"op": "GT", "key": "views", "value": 1000}
  ]
}
python Python Client theme={null}
  import requests
  import json

# Simple dict filter
  filter_dict = {"docs": "agno", "status": "published"}

# Serialize to JSON
  filter_json = json.dumps(filter_dict)

# Send request
  response = requests.post(
      "http://localhost:7777/agents/agno-knowledge-agent/runs",
      data={
          "message": "What are agno's key features?",
          "stream": "false",
          "knowledge_filters": filter_json,
      }
  )

result = response.json()
  bash cURL theme={null}
  curl -X 'POST' \
    'http://localhost:7777/agents/agno-knowledge-agent/runs' \
    -H 'accept: application/json' \
    -H 'Content-Type: multipart/form-data' \
    -F 'message=What are agno'\''s key features?' \
    -F 'stream=false' \
    -F 'session_id=' \
    -F 'user_id=' \
    -F 'knowledge_filters={"docs": "agno"}'
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  The presence of the `"op"` key tells the API to deserialize the filter as a filter expression. Regular dict filters (without `"op"`) continue to work for backward compatibility.
</Note>

## Using Filters Through the API

In all examples below, you pass the serialized JSON string via the `knowledge_filters` field when creating a run.

### Dictionary Filters (Simple Approach)

For basic filtering, send a JSON object with key-value pairs. All conditions are combined with AND logic:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

**More Dict Filter Examples:**
```

---

## Async Tool Confirmation Required

**URL:** llms-txt#async-tool-confirmation-required

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-async

This example demonstrates how to implement human-in-the-loop functionality with async agents, requiring user confirmation before executing tool operations.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 1: Using IN operator with teams

**URL:** llms-txt#example-1:-using-in-operator-with-teams

print("Using IN operator")
team_with_knowledge.print_response(
    "Tell me about the candidate's work and experience",
    knowledge_filters=[
        IN(
            "user_id",
            [
                "jordan_mitchell",
                "taylor_brooks",
                "morgan_lee",
                "casey_jordan",
                "alex_rivera",
            ],
        )
    ],
    markdown=True,
)

---

## Azure OpenAI o3

**URL:** llms-txt#azure-openai-o3

Source: https://docs.agno.com/basics/reasoning/usage/models/azure-openai/o3

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set your Azure OpenAI credentials">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set your Azure OpenAI credentials">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create agent

**URL:** llms-txt#create-agent

research_agent = Agent(
    id="user-agent",
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    tools=[get_user_details],
    instructions="You are a user agent that can get user details if the user asks for them.",
)

agent_os = AgentOS(
    description="JWT Protected AgentOS",
    agents=[research_agent],
)

---

## Tool Use O3

**URL:** llms-txt#tool-use-o3

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/tool-use-o3

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Team with Tool Hooks

**URL:** llms-txt#team-with-tool-hooks

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/usage/team-with-tool-hooks

This example demonstrates how to use tool hooks with teams and agents for intercepting and monitoring tool function calls, providing logging, timing, and other observability features.

```python cookbook/examples/teams/tools/02_team_with_tool_hooks.py theme={null}
"""
This example demonstrates how to use tool hooks with teams and agents.

Tool hooks allow you to intercept and monitor tool function calls, providing
logging, timing, and other observability features.
"""

import time
from typing import Any, Callable, Dict
from uuid import uuid4

from agno.agent.agent import Agent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reddit import RedditTools
from agno.utils.log import logger

def logger_hook(function_name: str, function_call: Callable, arguments: Dict[str, Any]):
    """
    Tool hook that logs function calls and measures execution time.

Args:
        function_name: Name of the function being called
        function_call: The actual function to call
        arguments: Arguments passed to the function

Returns:
        The result of the function call
    """
    if function_name == "delegate_task_to_member":
        member_id = arguments.get("member_id")
        logger.info(f"Delegating task to member {member_id}")

# Start timer
    start_time = time.time()
    result = function_call(**arguments)
    # End timer
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"Function {function_name} took {duration:.2f} seconds to execute")
    return result

---

## Create your AgentOS app

**URL:** llms-txt#create-your-agentos-app

agent_os = AgentOS(agents=[agent])
app = agent_os.get_app()

---

## Gemini

**URL:** llms-txt#gemini

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/gemini

The Gemini model provides access to Google's Gemini models.

| Parameter               | Type                       | Default              | Description                                                      |
| ----------------------- | -------------------------- | -------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`                      | `"gemini-1.5-flash"` | The id of the Gemini model to use                                |
| `name`                  | `str`                      | `"Gemini"`           | The name of the model                                            |
| `provider`              | `str`                      | `"Google"`           | The provider of the model                                        |
| `api_key`               | `Optional[str]`            | `None`               | The API key for Google AI (defaults to GOOGLE\_API\_KEY env var) |
| `generation_config`     | `Optional[Dict[str, Any]]` | `None`               | Generation configuration parameters for the model                |
| `safety_settings`       | `Optional[List[Dict]]`     | `None`               | Safety settings to filter content                                |
| `tools`                 | `Optional[List[Dict]]`     | `None`               | Tools available to the model                                     |
| `tool_config`           | `Optional[Dict[str, Any]]` | `None`               | Configuration for tool use                                       |
| `system_instruction`    | `Optional[str]`            | `None`               | System instruction for the model                                 |
| `cached_content`        | `Optional[str]`            | `None`               | Cached content identifier for context caching                    |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`               | Additional parameters to include in the request                  |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`               | Additional parameters for client configuration                   |
| `thinking_enabled`      | `Optional[bool]`           | `None`               | Whether to enable thinking mode for supported models             |
| `retries`               | `int`                      | `0`                  | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`                      | `1`                  | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`                     | `False`              | If True, the delay between retries is doubled each time          |

---

## Reasoning

**URL:** llms-txt#reasoning

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/reasoning

ReasoningTools provides step-by-step reasoning capabilities for agents to think through complex problems systematically.

The following agent can use structured reasoning to solve complex problems:

| Parameter           | Type            | Default | Description                                 |
| ------------------- | --------------- | ------- | ------------------------------------------- |
| `enable_think`      | `bool`          | `True`  | Enable the think reasoning function.        |
| `enable_analyze`    | `bool`          | `True`  | Enable the analyze reasoning function.      |
| `instructions`      | `Optional[str]` | `None`  | Custom instructions for reasoning behavior. |
| `add_instructions`  | `bool`          | `False` | Whether to add instructions to the agent.   |
| `add_few_shot`      | `bool`          | `False` | Whether to include few-shot examples.       |
| `few_shot_examples` | `Optional[str]` | `None`  | Custom few-shot examples for reasoning.     |

| Function  | Description                                                  |
| --------- | ------------------------------------------------------------ |
| `think`   | Perform step-by-step reasoning about a problem or situation. |
| `analyze` | Conduct detailed analysis with structured reasoning steps.   |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/reasoning.py)
* [Agno Reasoning Framework](https://docs.agno.com/reasoning)

---

## Agents with Knowledge

**URL:** llms-txt#agents-with-knowledge

**Contents:**
- Knowledge for Agents

Source: https://docs.agno.com/basics/knowledge/agents/overview

Understanding knowledge and how to use it with Agno agents

**Knowledge** stores domain-specific content that can be added to the context of the agent to enable better decision making.

<Note>
  Agno has a generic knowledge solution that supports many forms of content.

See more details in the [knowledge](/basics/knowledge/overview) documentation.
</Note>

The Agent can **search** this knowledge at runtime to make better decisions and provide more accurate responses. This **searching on demand** pattern is called Agentic RAG.

<Tip>
  Example: Say we are building a Text2Sql Agent, we'll need to give the table schemas, column names, data types, example queries, etc to the agent to help it generate the best-possible SQL query.

It is not viable to put this all in the system message, instead we store this information as knowledge and let the Agent query it at runtime.

Using this information, the Agent can then generate the best-possible SQL query. This is called **dynamic few-shot learning**.
</Tip>

## Knowledge for Agents

Agno Agents use **Agentic RAG** by default, meaning when we provide `knowledge` to an Agent, it will search this knowledge base, at runtime, for the specific information it needs to achieve its task.

```python  theme={null}
import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

---

## Maxim

**URL:** llms-txt#maxim

**Contents:**
- Integrating Agno with Maxim
- Prerequisites
- Sending Traces to Maxim
  - Example: Basic Maxim Integration

Source: https://docs.agno.com/integrations/observability/maxim

Connect Agno with Maxim to monitor, trace, and evaluate your agent's activity and performance.

## Integrating Agno with Maxim

Maxim AI provides comprehensive agent monitoring, evaluation, and observability for your Agno applications. With Maxim's one-line integration, you can easily trace and analyse agent interactions, performance metrics, and more.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

Or install Maxim separately:

2. **Setup Maxim Account**

* Sign up for an account at [Maxim](https://getmaxim.ai/).
   * Generate your API key from the Maxim dashboard.
   * Create a repository to store your traces.

3. **Set Environment Variables**

Configure your environment with the Maxim API key:

## Sending Traces to Maxim

### Example: Basic Maxim Integration

This example demonstrates how to instrument your Agno agent with Maxim and send traces for monitoring and evaluation.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

try:
    from maxim import Maxim
    from maxim.logger.agno import instrument_agno
except ImportError:
    raise ImportError(
        "`maxim` not installed. Please install using `pip install maxim-py`"
    )

**Examples:**

Example 1 (unknown):
```unknown
Or install Maxim separately:
```

Example 2 (unknown):
```unknown
2. **Setup Maxim Account**

   * Sign up for an account at [Maxim](https://getmaxim.ai/).
   * Generate your API key from the Maxim dashboard.
   * Create a repository to store your traces.

3. **Set Environment Variables**

   Configure your environment with the Maxim API key:
```

Example 3 (unknown):
```unknown
## Sending Traces to Maxim

### Example: Basic Maxim Integration

This example demonstrates how to instrument your Agno agent with Maxim and send traces for monitoring and evaluation.
```

---

## MongoDB Atlas connection string

**URL:** llms-txt#mongodb-atlas-connection-string

**Contents:**
- MongoDB Params
- Developer Resources

"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost/?directConnection=true"
"""
mdb_connection_string = ""

knowledge_base = Knowledge(
    vector_db=MongoVectorDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        wait_until_index_ready_in_seconds=60,
        wait_after_insert_in_seconds=300
    ),
)  # adjust wait_after_insert_in_seconds and wait_until_index_ready_in_seconds to your needs

if __name__ == "__main__":
    knowledge_base.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

agent = Agent(knowledge=knowledge_base)
    agent.print_response("How to make Thai curry?", markdown=True)
python async_mongodb.py theme={null}
    import asyncio

from agno.agent import Agent
    from agno.knowledge.knowledge import Knowledge
    from agno.vectordb.mongodb import MongoVectorDb

# MongoDB Atlas connection string
    """
    Example connection strings:
    "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
    "mongodb://localhost:27017/agno?authSource=admin"
    """
    mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

knowledge_base = Knowledge(
        vector_db=MongoVectorDb(
            collection_name="recipes",
            db_url=mdb_connection_string,
        ),
    )

# Create and use the agent
    agent = Agent(knowledge=knowledge_base)

if __name__ == "__main__":
        # Load knowledge base asynchronously
        asyncio.run(knowledge_base.add_content_async(
                url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
            )
        )

# Create and use the agent asynchronously
        asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))
    ```

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_mongodb_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/mongo_db/mongo_db.py)
* View [Cookbook (Async)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/mongo_db/async_mongo_db.py)

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      MongoDB also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## PII Detection Guardrail

**URL:** llms-txt#pii-detection-guardrail

Source: https://docs.agno.com/basics/guardrails/usage/agent/pii-detection

This example demonstrates how to use Agno's built-in PII detection guardrail to protect sensitive data like SSNs, credit cards, emails, and phone numbers.

This example demonstrates how to use Agno's built-in PII detection guardrail with an Agent.

This example shows how to:

1. Detect and block personally identifiable information (PII) in input
2. Protect sensitive data like SSNs, credit cards, emails, and phone numbers
3. Handle different types of PII violations with appropriate error messages

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Workflow with History Enabled for Steps

**URL:** llms-txt#workflow-with-history-enabled-for-steps

Source: https://docs.agno.com/basics/chat-history/workflow/usage/workflow-with-history-enabled-for-steps

This example demonstrates a workflow with history enabled for specific steps.

This example shows how to use the `add_workflow_history_to_steps` flag to add workflow history to multiple steps in the workflow.
In this case we have a workflow with three steps.

* The first step is a meal suggester that suggests meal categories and cuisines.
* The second step is a preference analysis step that analyzes the conversation history to understand user food preferences.
* The third step is a recipe specialist that provides recipe recommendations based on the user's preferences.

```python 02_workflow_with_history_enabled_for_steps.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## OpenAI with Reasoning Tools

**URL:** llms-txt#openai-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/openai-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MySQL Workflow Storage

**URL:** llms-txt#mysql-workflow-storage

**Contents:**
- Usage
  - Run MySQL

Source: https://docs.agno.com/integrations/database/mysql/usage/mysql-for-workflow

Agno supports using MySQL as a storage backend for Workflows using the `MysqlDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MySQL** on port **3306** using:

```python mysql_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.mysql import MySQLDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"

db = MySQLDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Image to Image Agent

**URL:** llms-txt#image-to-image-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/images/usage/image-to-image

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## With Podman

**URL:** llms-txt#with-podman

---

## Knowledge Searcher Agent - Specialized in finding relevant information

**URL:** llms-txt#knowledge-searcher-agent---specialized-in-finding-relevant-information

knowledge_searcher = Agent(
    name="Knowledge Searcher",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Search and retrieve relevant information from the knowledge base",
    knowledge=knowledge,
    search_knowledge=True,
    instructions=[
        "You are responsible for searching the knowledge base thoroughly.",
        "Find all relevant information for the user's query.",
        "Provide detailed search results with context and sources.",
        "Focus on comprehensive information retrieval.",
    ],
    markdown=True,
)

---

## Will load the session state from the session with the id "user_2_session_1"

**URL:** llms-txt#will-load-the-session-state-from-the-session-with-the-id-"user_2_session_1"

**Contents:**
- Overwriting the state in the db

team.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")
python overwriting_session_state_in_db.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
## Overwriting the state in the db

By default, if you pass `session_state` to the run methods, this new state will be merged with the `session_state` in the db.

You can change that behavior if you want to overwrite the `session_state` in the db:
```

---

## Pubmed

**URL:** llms-txt#pubmed

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/pubmed

**PubmedTools** enable an Agent to search for Pubmed for articles.

The following agent will search Pubmed for articles related to "ulcerative colitis".

| Parameter              | Type   | Default                    | Description                                                            |
| ---------------------- | ------ | -------------------------- | ---------------------------------------------------------------------- |
| `email`                | `str`  | `"your_email@example.com"` | Specifies the email address to use.                                    |
| `max_results`          | `int`  | `None`                     | Optional parameter to specify the maximum number of results to return. |
| `enable_search_pubmed` | `bool` | `True`                     | Enable the search\_pubmed functionality.                               |
| `all`                  | `bool` | `False`                    | Enable all functionality.                                              |

| Function        | Description                                                                                                                                                                                                                                                                                 |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search_pubmed` | Searches PubMed for articles based on a specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results to return (default is 10). Returns a JSON string containing the search results, including publication date, title, and summary. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/pubmed.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/pubmed_tools.py)

---

## Define your workflow

**URL:** llms-txt#define-your-workflow

**Contents:**
  - Event Types
  - Storing Events

async def main():
    try:
        response: AsyncIterator[WorkflowRunOutputEvent] = basic_workflow.arun(
            message="Recent breakthroughs in quantum computing",
            stream=True,
            stream_events=True,
        )
        async for event in response:
            if event.event == WorkflowRunEvent.condition_execution_started.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.condition_execution_completed.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.workflow_started.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.step_started.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.step_completed.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.workflow_completed.value:
                print(event)
                print()
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from agno.run.workflow import WorkflowRunEvent

**Examples:**

Example 1 (unknown):
```unknown
See the [Async Streaming](/basics/workflows/usage/async-events-streaming) example for more details.

### Event Types

The following events are yielded by the `Workflow.run()` and `Workflow.arun()` functions depending on the workflow's configuration:

#### Core Events

| Event Type          | Description                                         |
| ------------------- | --------------------------------------------------- |
| `WorkflowStarted`   | Indicates the start of a workflow run               |
| `WorkflowCompleted` | Signals successful completion of the workflow run   |
| `WorkflowError`     | Indicates an error occurred during the workflow run |

#### Step Events

| Event Type      | Description                               |
| --------------- | ----------------------------------------- |
| `StepStarted`   | Indicates the start of a step             |
| `StepCompleted` | Signals successful completion of a step   |
| `StepError`     | Indicates an error occurred during a step |

#### Step Output Events (For custom functions)

| Event Type   | Description                    |
| ------------ | ------------------------------ |
| `StepOutput` | Indicates the output of a step |

#### Parallel Execution Events

| Event Type                   | Description                                      |
| ---------------------------- | ------------------------------------------------ |
| `ParallelExecutionStarted`   | Indicates the start of a parallel step           |
| `ParallelExecutionCompleted` | Signals successful completion of a parallel step |

#### Condition Execution Events

| Event Type                    | Description                                  |
| ----------------------------- | -------------------------------------------- |
| `ConditionExecutionStarted`   | Indicates the start of a condition           |
| `ConditionExecutionCompleted` | Signals successful completion of a condition |

#### Loop Execution Events

| Event Type                    | Description                                       |
| ----------------------------- | ------------------------------------------------- |
| `LoopExecutionStarted`        | Indicates the start of a loop                     |
| `LoopIterationStartedEvent`   | Indicates the start of a loop iteration           |
| `LoopIterationCompletedEvent` | Signals successful completion of a loop iteration |
| `LoopExecutionCompleted`      | Signals successful completion of a loop           |

#### Router Execution Events

| Event Type                 | Description                               |
| -------------------------- | ----------------------------------------- |
| `RouterExecutionStarted`   | Indicates the start of a router           |
| `RouterExecutionCompleted` | Signals successful completion of a router |

#### Steps Execution Events

| Event Type                | Description                                        |
| ------------------------- | -------------------------------------------------- |
| `StepsExecutionStarted`   | Indicates the start of `Steps` being executed      |
| `StepsExecutionCompleted` | Signals successful completion of `Steps` execution |

See detailed documentation in the [WorkflowRunOutputEvent](/reference/workflows/run-output) documentation.

### Storing Events

Workflows can automatically store all execution events for analysis, debugging, and audit purposes. Filter specific event types to reduce noise and storage overhead while maintaining essential execution records.

Access stored events via `workflow.run_response.events` and in the `runs` column of your workflow's session database (SQLite, PostgreSQL, etc.).

* `store_events=True`: Automatically stores all workflow events in the database
* `events_to_skip=[]`: Filter out specific event types to reduce storage and noise

Access all stored events via `workflow.run_response.events`

**Available Events to Skip:**
```

---

## SessionSummaryManager

**URL:** llms-txt#sessionsummarymanager

**Contents:**
- SessionSummaryManager Attributes
- SessionSummaryManager Methods
  - `create_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`
  - `acreate_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`
- SessionSummary Object

Source: https://docs.agno.com/reference/session/summary_manager

The `SessionSummaryManager` is responsible for generating and managing session summaries. It uses a model to analyze conversations and create concise summaries with optional topic extraction.

## SessionSummaryManager Attributes

| Parameter                 | Type              | Default                                      | Description                                                                        |
| ------------------------- | ----------------- | -------------------------------------------- | ---------------------------------------------------------------------------------- |
| `model`                   | `Optional[Model]` | `None`                                       | Model used for session summary generation                                          |
| `session_summary_prompt`  | `Optional[str]`   | `None`                                       | Custom prompt for session summary generation. If not provided, uses default prompt |
| `summary_request_message` | `str`             | `"Provide the summary of the conversation."` | User message prompt for requesting the summary                                     |
| `summaries_updated`       | `bool`            | `False`                                      | Whether session summaries were created in the last run                             |

## SessionSummaryManager Methods

### `create_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`

Creates a summary of the session synchronously.

* `session`: The agent or team session to summarize

* `Optional[SessionSummary]`: A SessionSummary object containing the summary text, topics, and timestamp, or None if generation fails

### `acreate_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`

Creates a summary of the session asynchronously.

* `session`: The agent or team session to summarize

* `Optional[SessionSummary]`: A SessionSummary object containing the summary text, topics, and timestamp, or None if generation fails

## SessionSummary Object

The `SessionSummary` object returned by the summary manager contains:

| Attribute    | Type                  | Description                                                      |
| ------------ | --------------------- | ---------------------------------------------------------------- |
| `summary`    | `str`                 | Concise summary of the session focusing on important information |
| `topics`     | `Optional[List[str]]` | List of topics discussed in the session                          |
| `updated_at` | `Optional[datetime]`  | Timestamp when the summary was created                           |

---

## Now returns Step(s) to execute

**URL:** llms-txt#now-returns-step(s)-to-execute

def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

# Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

if any(keyword in topic for keyword in tech_keywords):
        print(f"ðŸ” Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f"ðŸŒ General topic detected: Using web research for '{topic}'")
        return [research_web]

workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning"
    )
```

This was a synchronous non-streaming example of this pattern. To checkout async and streaming versions, see the cookbooks-

* [Router Steps Workflow (sync streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/sync/router_steps_workflow_stream.py)
* [Router Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/async/router_steps_workflow.py)
* [Router Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/async/router_steps_workflow_stream.py)

---

## Run a keyword-based query

**URL:** llms-txt#run-a-keyword-based-query

**Contents:**
- Usage

results = keyword_db.search("chicken coconut soup", limit=5)
print("Keyword Search Results:", results)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/search_type/keyword_search.py
      bash Windows theme={null}
      python cookbook/knowledge/search_type/keyword_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Firestore for Agent

**URL:** llms-txt#firestore-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/firestore/usage/firestore-for-agent

Agno supports using Firestore as a storage backend for Agents using the `FirestoreDb` class.

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.tools.duckduckgo import DuckDuckGoTools

PROJECT_ID = "agno-os-test"  # Use your project ID here

---

## Calculate reduction

**URL:** llms-txt#calculate-reduction

**Contents:**
- Usage

if tokens_before > 0:
    reduction_pct = ((tokens_before - tokens_after) / tokens_before) * 100
    tokens_saved = tokens_before - tokens_after
    print(f"  Reduction: {reduction_pct:.1f}% ({tokens_saved} tokens saved)")

if memories_after:
    print("\nSummarized memory:")
    print(f"  {memories_after[0].memory}")
else:
    print("\n No memories found after optimization")
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python memory-optimization.py
      bash Windows theme={null}
      python memory-optimization.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Output Validation Post-Hook

**URL:** llms-txt#output-validation-post-hook

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/hooks/usage/team/output-validation-post-hook

This example demonstrates how to use a post-hook to validate the output of an Team, before it is returned to the user.

This example shows how to:

1. Validate team responses for quality and safety
2. Ensure outputs meet minimum standards before being returned
3. Raise OutputCheckError when validation fails

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Generate Images with Intermediate Steps

**URL:** llms-txt#generate-images-with-intermediate-steps

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/images/usage/generate-image

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add from local file to the knowledge base

**URL:** llms-txt#add-from-local-file-to-the-knowledge-base

asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        skip_if_exists=True,  # True by default
    )
)

---

## Add JWT middleware to the app

**URL:** llms-txt#add-jwt-middleware-to-the-app

---

## Example 2: Using NOT operator

**URL:** llms-txt#example-2:-using-not-operator

print("Using NOT operator")
sales_agent.print_response(
    "Describe revenue performance for the region",
    knowledge_filters=[NOT(IN("region", ["north_america"]))],
    markdown=True,
)

---

## Setup your Agent with Agentic Memory

**URL:** llms-txt#setup-your-agent-with-agentic-memory

**Contents:**
- Storage: Where Memories Live
  - Custom Table Names

agent = Agent(
    db=db,
    enable_agentic_memory=True, # This enables Agentic Memory for the Agent
)
python  theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

**Examples:**

Example 1 (unknown):
```unknown
With agentic memory, the agent is equipped with tools to manage memories when it deems relevant. This gives more flexibility but requires the agent to make intelligent decisions about what to remember.

**Best for:** Complex workflows, multi-turn interactions where the agent needs to decide what's worth remembering based on context.

<Note>
  **Important:** Don't enable both `enable_user_memories` and `enable_agentic_memory` at the same time, as they're mutually exclusive. While nothing will break if you set both, `enable_agentic_memory` will always take precedence and `enable_user_memories` will be ignored.
</Note>

## Storage: Where Memories Live

Memories are stored in the database you connect to your agent. Agno supports all major database systems: Postgres, SQLite, MongoDB, and more. Check the [Storage documentation](/basics/database/overview) for the full list of supported databases and setup instructions.

By default, memories are stored in the `agno_memories` table (or collection, for document databases). If this table doesn't exist when your agent first tries to store a memory, Agno creates it automatically with no manual schema setup required.

### Custom Table Names

You can specify a custom table name for storing memories:
```

---

## Example usage with detailed research request

**URL:** llms-txt#example-usage-with-detailed-research-request

if __name__ == "__main__":
    research_agent.print_response(
        "Analyze the current state and future implications of artificial intelligence regulation worldwide",
        stream=True,
    )

---

## Pydantic Models as Team Output

**URL:** llms-txt#pydantic-models-as-team-output

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/pydantic-model-output

This example demonstrates how to use Pydantic models as output from teams, showing how structured data can be returned as responses for more precise and validated output handling.

```python cookbook/examples/teams/structured_input_output/00_pydantic_model_output.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response
from pydantic import BaseModel

class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str

class CompanyAnalysis(BaseModel):
    company_name: str
    analysis: str

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-5-mini"),
    output_schema=StockAnalysis,
    role="Searches for information on stocks and provides price analysis.",
    tools=[DuckDuckGoTools()],
)

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches for information about companies and recent news.",
    output_schema=CompanyAnalysis,
    tools=[DuckDuckGoTools()],
)

class StockReport(BaseModel):
    symbol: str
    company_name: str
    analysis: str

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
)

---

## Test knowledge filtering

**URL:** llms-txt#test-knowledge-filtering

**Contents:**
- Usage

team_with_knowledge.print_response(
    "Tell me about Jordan Mitchell's work and experience"
)
bash  theme={null}
    pip install agno openai lancedb
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/knowledge/02_team_with_knowledge_filters.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Get trace for a run

**URL:** llms-txt#get-trace-for-a-run

**Contents:**
- See Also

trace = db.get_trace(run_id=response.run_id)

if trace:
    print(f"Trace: {trace.name} ({trace.duration_ms}ms)")
    
    # Get all spans
    spans = db.get_spans(trace_id=trace.trace_id)
    
    # Print execution tree
    for span in sorted(spans, key=lambda s: s.start_time):
        indent = "  " if span.parent_span_id else ""
        print(f"{indent}- {span.name} ({span.duration_ms}ms)")
```

* [Trace Reference](/reference/tracing/trace) - Full `Trace` object definition
* [Span Reference](/reference/tracing/span) - Full `Span` object definition
* [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/integrations/observability/trace_to_database.py) - Full example code

---

## - "What are the top 3 HN stories right now?"

**URL:** llms-txt#--"what-are-the-top-3-hn-stories-right-now?"

---

## Session Tracking

**URL:** llms-txt#session-tracking

**Contents:**
- Overview
- Accessing Sessions
- Troubleshooting
- Useful Links

Source: https://docs.agno.com/agent-os/features/session-tracking

Monitor, analyze, and manage agent sessions through the AgentOS interface

Sessions are durable conversation timelines that bind inputs, model outputs, tools, files, metrics, and summaries under a single `session_id`. AgentOS persists sessions for Agents, Teams, and Workflows so you can resume work, audit behavior, and analyze quality over time.

* A session collects ordered runs (each run contains messages, tool calls, and metrics).
* Summaries and metadata help you search, group, and reason about long histories.
* Token usage can be monitored per session via the metrics tab.
* Inspect details about the agent and models tied to each session.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/xm93WWN8gg4nzCGE/videos/agentos-session-management.mp4?fit=max&auto=format&n=xm93WWN8gg4nzCGE&q=85&s=70dda8ee349f38e48272eff8cdd4719a" type="video/mp4" data-path="videos/agentos-session-management.mp4" />
  </video>
</Frame>

## Accessing Sessions

* Open the `Sessions` section in the sidebar.
* If multiple session databases are configured, pick one from the database selector in the header.
* Switch between `Agents` and `Teams` using the header tabs.
* Click `Reload page` (Refresh) to sync the list and statuses.

* Sessions not loading: Ensure your OS is connected and active, select a session database, then click `Reload page`.
* No sessions yet: Start a conversation to generate sessions.
* Wrong list: Check the `Agents` vs `Teams` tab and sorting.
* Configuration errors: If you see endpoint or database errors, verify your OS endpoint and database settings.

<CardGroup cols={2}>
  <Card title="Sessions" icon="user" href="/basics/sessions/overview">
    Learn about sessions and multi-turn conversations
  </Card>

<Card title="Chat History" icon="users" href="/basics/chat-history/overview">
    Understand chat history and multi-turn conversations
  </Card>
</CardGroup>

---

## Upstash

**URL:** llms-txt#upstash

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/upstash/usage/upstash-db

```python cookbook/knowledge/vector_db/upstash_db/upstash_db.py theme={null}
import os

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.upstashdb import UpstashVectorDb

---

## Create MCP-enabled agent

**URL:** llms-txt#create-mcp-enabled-agent

agent = Agent(
    id="agno-agent",
    name="Agno Agent",
    tools=[mcp_tools],
)

---

## Spotify

**URL:** llms-txt#spotify

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/spotify

**SpotifyTools** enable an Agent to access Spotify API to search for songs.

The following example requires the `spotify` library and an API key from [Spotify](https://developer.spotify.com/).

The only external library needed for spotify is the `httpx`. Which is already a dependency of Agno.

The following agent will search for songs using Spotify API.

| Parameter        | Type  | Default | Description                               |
| ---------------- | ----- | ------- | ----------------------------------------- |
| `access_token`   | `str` | -       | Access token for authentication purposes. |
| `default_market` | `str` | `"US"`  | Default market for the search.            |
| `timeout`        | `int` | `30`    | Timeout for the search.                   |

| Function                      | Description                                         |
| ----------------------------- | --------------------------------------------------- |
| `search_tracks`               | Search for tracks on Spotify                        |
| `search_playlists`            | Search for playlists on Spotify                     |
| `search_artists`              | Search for artists on Spotify                       |
| `search_albums`               | Search for albums on Spotify                        |
| `get_user_playlists`          | Get playlists for a specific user                   |
| `get_track_recommendations`   | Get track recommendations based on seed data        |
| `get_artist_top_tracks`       | Get top tracks for a specific artist                |
| `get_album_tracks`            | Get tracks from a specific album                    |
| `get_my_top_tracks`           | Get current user's top tracks                       |
| `get_my_top_artists`          | Get current user's top artists                      |
| `create_playlist`             | Create a new playlist for the user                  |
| `add_tracks_to_playlist`      | Add tracks to an existing playlist                  |
| `get_playlist`                | Get details of a specific playlist                  |
| `update_playlist_details`     | Update playlist name, description, or other details |
| `remove_tracks_from_playlist` | Remove tracks from an existing playlist             |
| `get_current_user`            | Get current user's profile information              |
| `play_track`                  | Play a specific track (requires Spotify Premium)    |
| `get_currently_playing`       | Get information about currently playing track       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/spotify.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/spotify_tools.py)

---

## Agent with Input Schema

**URL:** llms-txt#agent-with-input-schema

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/input-schema-on-agent

This example demonstrates how to define an input schema for an agent using Pydantic models, ensuring structured input validation.

```python input_schema_on_agent.py theme={null}
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field

class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)

---

## Router Steps

**URL:** llms-txt#router-steps

Source: https://docs.agno.com/reference/workflows/router-steps

| Parameter     | Type                                                                                                                                                   | Default  | Description                                                                   |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | -------- | ----------------------------------------------------------------------------- |
| `selector`    | `Union[Callable[[StepInput], Union[WorkflowSteps, List[WorkflowSteps]]], Callable[[StepInput], Awaitable[Union[WorkflowSteps, List[WorkflowSteps]]]]]` | Required | Function to select steps dynamically (supports both sync and async functions) |
| `choices`     | `WorkflowSteps`                                                                                                                                        | Required | Available steps for selection                                                 |
| `name`        | `Optional[str]`                                                                                                                                        | `None`   | Name of the router step                                                       |
| `description` | `Optional[str]`                                                                                                                                        | `None`   | Description of the router step                                                |

---

## Video to Shorts Generation

**URL:** llms-txt#video-to-shorts-generation

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/agent/usage/video-to-shorts

This example demonstrates how to analyze a video and automatically generate short-form content segments optimized for platforms like YouTube Shorts and Instagram Reels.

```python video_to_shorts.py theme={null}
"""
1. Install dependencies using: `pip install opencv-python google-geneai sqlalchemy`
2. Install ffmpeg `brew install ffmpeg`
2. Run the script using: `python cookbook/agent_basics/video_to_shorts.py`
"""

import subprocess
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger

video_path = Path(__file__).parent.joinpath("sample_video.mp4")
output_dir = Path("tmp/shorts")

agent = Agent(
    name="Video2Shorts",
    description="Process videos and generate engaging shorts.",
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    instructions=[
        "Analyze the provided video directlyâ€”do NOT reference or analyze any external sources or YouTube videos.",
        "Identify engaging moments that meet the specified criteria for short-form content.",
        """Provide your analysis in a **table format** with these columns:
   - Start Time | End Time | Description | Importance Score""",
        "Ensure all timestamps use MM:SS format and importance scores range from 1-10. ",
        "Focus only on segments between 15 and 60 seconds long.",
        "Base your analysis solely on the provided video content.",
        "Deliver actionable insights to improve the identified segments for short-form optimization.",
    ],
)

---

## Set Client

**URL:** llms-txt#set-client

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/local/ollama/usage/set-client

```python cookbook/models/ollama/set_client.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama
from ollama import Client as OllamaClient

agent = Agent(
    model=Ollama(id="llama3.1:8b", client=OllamaClient()),
    markdown=True,
)

---

## Create Knowledge Instance with Weaviate

**URL:** llms-txt#create-knowledge-instance-with-weaviate

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with Weaviate",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
    skip_if_exists=True,
)

---

## Load an example large system message from S3. A large prompt like this would benefit from caching.

**URL:** llms-txt#load-an-example-large-system-message-from-s3.-a-large-prompt-like-this-would-benefit-from-caching.

txt_path = Path(__file__).parent.joinpath("system_prompt.txt")
download_file(
    "https://agno-public.s3.amazonaws.com/prompts/system_promt.txt",
    str(txt_path),
)
system_message = txt_path.read_text()

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        cache_system_prompt=True,  # Activate prompt caching for Anthropic to cache the system prompt
    ),
    system_message=system_message,
    markdown=True,
)

---

## Interfaces

**URL:** llms-txt#interfaces

**Contents:**
- Available Interfaces
- How Interfaces Work
- Using Interfaces

Source: https://docs.agno.com/agent-os/interfaces/overview

Expose Agno agents through various communication protocols and platforms

Interfaces enable exposing Agno agents, teams, and workflows through various communication protocols and platforms. Each interface provides a standardized way to connect Agno agents to external systems, messaging platforms, and frontend applications.

## Available Interfaces

<CardGroup cols={2}>
  <Card title="AG-UI" icon="desktop" href="/agent-os/interfaces/ag-ui/introduction">
    Connect agents to frontend applications using the Agent-User Interaction Protocol
  </Card>

<Card title="Slack" icon="slack" href="/agent-os/interfaces/slack/introduction">
    Deploy agents as Slack applications for team collaboration
  </Card>

<Card title="WhatsApp" icon="whatsapp" href="/agent-os/interfaces/whatsapp/introduction">
    Serve agents via WhatsApp for direct messaging interactions
  </Card>

<Card title="A2A" icon="code" href="/agent-os/interfaces/a2a/introduction">
    Expose agents via the Agent-to-Agent Protocol for inter-agent communication
  </Card>
</CardGroup>

## How Interfaces Work

Interfaces are FastAPI routers that mount protocol-specific endpoints on an AgentOS instance. Each interface:

* Wraps Agno agents, teams, or workflows into protocol-compatible endpoints
* Handles authentication and request validation for the target platform
* Manages session tracking and context preservation
* Streams responses back to clients in the appropriate format

Interfaces are added to an AgentOS instance through the `interfaces` parameter:

Multiple interfaces can be added to a single AgentOS instance, allowing the same agents to be exposed through different protocols simultaneously.

---

## Run Agent Infra AWS on AWS

**URL:** llms-txt#run-agent-infra-aws-on-aws

**Contents:**
- Next

Source: https://docs.agno.com/templates/agent-infra-aws/run-aws

<Snippet file="run-agent-infra-aws-prd.mdx" />

For managing the AWS infrastructure, make sure to look at the infra management documentation [here](/templates/infra-management).

Congratulations on running your Agent Infra AWS. Next Steps:

* Read how to [update infra settings](/templates/infra-management/infra-settings)
* Read how to [create a git repository for your workspace](/templates/infra-management/git-repo)
* Read how to [manage the development application](/templates/infra-management/development-app)
* Read how to [format and validate your code](/templates/infra-management/format-and-validate)
* Read how to [add python libraries](/templates/infra-management/install)
* Chat with us on [discord](https://agno.link/discord)

---

## Transcription Agent

**URL:** llms-txt#transcription-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/groq/usage/transcription-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Synchronous reading

**URL:** llms-txt#synchronous-reading

documents = reader.read("file.pdf")

---

## Beta Features

**URL:** llms-txt#beta-features

**Contents:**
- Example

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/betas

Learn how to use Anthropic's beta features with Agno.

Anthropic's `betas` are experimental features extending the capabilities of Claude models.

They are supported via the `betas` parameter when initializing the `Claude` model:

```python  theme={null}
import anthropic
from agno.agent import Agent
from agno.models.anthropic import Claude

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Workflow

**URL:** llms-txt#workflow

**Contents:**
- Parameters
- Functions
  - `run`
  - `arun`
  - `print_response`
  - `aprint_response`
  - `cli_app`
  - `acli_app`
  - `cancel_run`
  - `get_run`

Source: https://docs.agno.com/reference/workflows/workflow

| Parameter                       | Type                                                              | Default | Description                                                                                              |
| ------------------------------- | ----------------------------------------------------------------- | ------- | -------------------------------------------------------------------------------------------------------- |
| `name`                          | `Optional[str]`                                                   | `None`  | Workflow name                                                                                            |
| `id`                            | `Optional[str]`                                                   | `None`  | Workflow ID (autogenerated if not set)                                                                   |
| `description`                   | `Optional[str]`                                                   | `None`  | Workflow description                                                                                     |
| `steps`                         | `Optional[WorkflowSteps]`                                         | `None`  | Workflow steps - can be a callable function, Steps object, or list of steps                              |
| `db`                            | `Optional[BaseDb]`                                                | `None`  | Database to use for this workflow                                                                        |
| `session_id`                    | `Optional[str]`                                                   | `None`  | Default session\_id to use for this workflow (autogenerated if not set)                                  |
| `user_id`                       | `Optional[str]`                                                   | `None`  | Default user\_id to use for this workflow                                                                |
| `session_state`                 | `Optional[Dict[str, Any]]`                                        | `None`  | Default session state (stored in the database to persist across runs)                                    |
| `debug_mode`                    | `Optional[bool]`                                                  | `False` | If True, the workflow runs in debug mode                                                                 |
| `stream`                        | `Optional[bool]`                                                  | `None`  | Stream the response from the Workflow                                                                    |
| `stream_events`                 | `bool`                                                            | `False` | Stream the intermediate steps from the Workflow                                                          |
| `stream_executor_events`        | `bool`                                                            | `True`  | Stream the events emitted by the Step executor (the agent/team events) together with the Workflow events |
| `store_events`                  | `bool`                                                            | `False` | Persist the events on the run response                                                                   |
| `events_to_skip`                | `Optional[List[Union[WorkflowRunEvent, RunEvent, TeamRunEvent]]]` | `None`  | Events to skip when persisting the events on the run response                                            |
| `store_executor_outputs`        | `bool`                                                            | `True`  | Control whether to store executor responses (agent/team responses) in flattened runs                     |
| `websocket_handler`             | `Optional[WebSocketHandler]`                                      | `None`  | WebSocket handler for real-time communication                                                            |
| `input_schema`                  | `Optional[Type[BaseModel]]`                                       | `None`  | Input schema to validate the input to the workflow                                                       |
| `metadata`                      | `Optional[Dict[str, Any]]`                                        | `None`  | Metadata stored with this workflow                                                                       |
| `add_workflow_history_to_steps` | `bool`                                                            | `False` | If True, add the workflow history to the steps                                                           |
| `num_history_runs`              | `int`                                                             | `None`  | Number of runs to include in the workflow history, if not provided, all history runs are included        |
| `cache_session`                 | `bool`                                                            | `False` | If True, cache the current workflow session in memory for faster access                                  |
| `telemetry`                     | `bool`                                                            | `True`  | Log minimal telemetry for analytics                                                                      |

Execute the workflow synchronously with optional streaming.

* `input` (Optional\[Union\[str, Dict\[str, Any], List\[Any], BaseModel]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (bool): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps

* `Union[WorkflowRunOutput, Iterator[WorkflowRunOutputEvent]]`: Either a WorkflowRunOutput or an iterator of WorkflowRunOutputEvents, depending on the `stream` parameter

Execute the workflow asynchronously with optional streaming.

* `input` (Optional\[Union\[str, Dict\[str, Any], List\[Any], BaseModel, List\[Message]]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (bool): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `background` (Optional\[bool]): Whether to run in background
* `websocket` (Optional\[WebSocket]): WebSocket for real-time communication

* `Union[WorkflowRunOutput, AsyncIterator[WorkflowRunOutputEvent]]`: Either a WorkflowRunOutput or an iterator of WorkflowRunOutputEvents, depending on the `stream` parameter

Print workflow execution with rich formatting and optional streaming.

* `input` (Union\[str, Dict\[str, Any], List\[Any], BaseModel, List\[Message]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (Optional\[bool]): Whether to stream the response content
* `markdown` (bool): Whether to render content as markdown
* `show_time` (bool): Whether to show execution time
* `show_step_details` (bool): Whether to show individual step outputs
* `console` (Optional\[Any]): Rich console instance (optional)

### `aprint_response`

Print workflow execution with rich formatting and optional streaming asynchronously.

* `input` (Union\[str, Dict\[str, Any], List\[Any], BaseModel, List\[Message]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (Optional\[bool]): Whether to stream the response content
* `markdown` (bool): Whether to render content as markdown
* `show_time` (bool): Whether to show execution time
* `show_step_details` (bool): Whether to show individual step outputs
* `console` (Optional\[Any]): Rich console instance (optional)

Run an interactive command-line interface to interact with the workflow.

* `input` (Optional\[str]): The input to send to the workflow
* `session_id` (Optional\[str]): Session ID to use
* `user_id` (Optional\[str]): User ID to use
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":technologist:")
* `stream` (Optional\[bool]): Whether to stream the response content
* `markdown` (bool): Whether to render content as markdown (default: True)
* `show_time` (bool): Whether to show execution time (default: True)
* `show_step_details` (bool): Whether to show individual step outputs (default: True)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

Run an interactive command-line interface to interact with the workflow asynchronously.

* `input` (Optional\[str]): The input to send to the workflow
* `session_id` (Optional\[str]): Session ID to use
* `user_id` (Optional\[str]): User ID to use
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":technologist:")
* `stream` (Optional\[bool]): Whether to stream the response content
* `markdown` (bool): Whether to render content as markdown (default: True)
* `show_time` (bool): Whether to show execution time (default: True)
* `show_step_details` (bool): Whether to show individual step outputs (default: True)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

Cancel a running workflow execution.

* `run_id` (str): The run\_id to cancel

* `bool`: True if the run was found and marked for cancellation, False otherwise

Get the status and details of a background workflow run.

* `run_id` (str): The run ID to get

* `Optional[WorkflowRunOutput]`: The workflow run output if found

Get a WorkflowRunOutput from the database.

* `run_id` (str): The run ID
* `session_id` (Optional\[str]): Session ID to use

* `Optional[WorkflowRunOutput]`: The run output

### `get_last_run_output`

Get the last run response from the database for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Optional[WorkflowRunOutput]`: The last run output

### `get_chat_history`

Return a list of dictionaries containing the input and output for each run in the session.

* `session_id` (Optional\[str]): The session ID to get the chat history for. If not provided, the current cached session ID is used
* `last_n_runs` (Optional\[int]): Number of recent runs to include. If None, all runs will be considered

* `List[WorkflowChatInteraction]`: A list of dictionaries containing the input and output for each run

Get the session for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Optional[WorkflowSession]`: The workflow session

### `get_session_state`

Get the session state for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Dict[str, Any]`: The session state

### `get_session_name`

Get the session name for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `str`: The session name

### `set_session_name`

Set the session name and save to storage.

* `session_id` (Optional\[str]): Session ID to use
* `autogenerate` (bool): Whether to autogenerate the name
* `session_name` (Optional\[str]): The name to set

* `WorkflowSession`: The updated session

### `get_session_metrics`

Get the session metrics for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Optional[Metrics]`: The session metrics

* `session_id` (str): Session ID to delete

Save the WorkflowSession to storage.

* `session` (WorkflowSession): The session to save

Convert workflow to dictionary representation.

* `Dict[str, Any]`: Dictionary representation of the workflow

---

## Define agents with specific roles

**URL:** llms-txt#define-agents-with-specific-roles

researcher = Agent(
    name="Researcher",
    instructions="Find relevant information about the topic",
    tools=[DuckDuckGoTools()]
)

writer = Agent(
    name="Writer",
    instructions="Write a clear, engaging article based on the research"
)

---

## 2. Continue with approval (use the run_id and tool_call_id from response)

**URL:** llms-txt#2.-continue-with-approval-(use-the-run_id-and-tool_call_id-from-response)

**Contents:**
- Usage
- Learn More

curl -X POST http://localhost:7777/agents/data_manager/runs/{run_id}/continue \
  -F "tools=[{\"tool_call_id\": \"{tool_call_id}\", \"confirmed\": true}]" \
  -F "session_id=test_session" \
  -F "stream=false"
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno fastapi uvicorn sqlalchemy pgvector psycopg openai
    bash  theme={null}
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash  theme={null}
    python hitl_confirmation.py
    ```
  </Step>
</Steps>

<CardGroup cols={3}>
  <Card title="HITL Getting Started" icon="hand" href="/examples/getting-started/12-human-in-the-loop">
    Learn HITL basics with a simple example
  </Card>

<Card title="HITL Concepts" icon="book" href="/basics/hitl/overview">
    Understand HITL patterns and best practices
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example">
```

---

## Configure Exa for broader web intelligence

**URL:** llms-txt#configure-exa-for-broader-web-intelligence

**Contents:**
- Step 3: Define Intelligence Strategy
  - 3a. Define the Data Collection Strategy

exa_tools = ExaTools(
    num_results=10,               # Comprehensive but not overwhelming
    include_domains=["reddit.com", "news.ycombinator.com", "medium.com"],
)

print("ExaTools configured for web search")
python  theme={null}
from textwrap import dedent

**Examples:**

Example 1 (unknown):
```unknown
**Why these specific domains?**

* **Reddit**: Early discussion indicators, community sentiment
* **HackerNews**: Tech industry insights, developer opinions
* **Medium**: Thought leadership, analysis articles

## Step 3: Define Intelligence Strategy

**Why do we need instructions?** We need to describe the strategy that the agent should take to collect and analyze content. Without clear instructions, the agent won't know how to use the tools effectively or what kind of analysis to provide.

### 3a. Define the Data Collection Strategy
```

---

## Create team with tool hooks

**URL:** llms-txt#create-team-with-tool-hooks

**Contents:**
- Usage

company_info_team = Team(
    name="Company Info Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        reddit_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
    tool_hooks=[logger_hook],
)

if __name__ == "__main__":
    company_info_team.print_response(
        "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        stream=True,
    )
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    export ANTHROPIC_API_KEY=****
    export REDDIT_CLIENT_ID=****
    export REDDIT_CLIENT_SECRET=****
    export REDDIT_USER_AGENT=****
    bash  theme={null}
    python cookbook/examples/teams/tools/02_team_with_tool_hooks.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Define agents with response models

**URL:** llms-txt#define-agents-with-response-models

research_agent = Agent(
    name="AI Research Specialist",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    role="Research AI trends and extract structured insights",
    output_schema=ResearchFindings,
    instructions=[
        "Research the given topic thoroughly using available tools",
        "Provide structured findings with confidence scores",
        "Focus on recent developments and market trends",
        "Make sure to structure your response according to the ResearchFindings model",
    ],
)

strategy_agent = Agent(
    name="Content Strategy Expert",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Create content strategies based on research findings",
    output_schema=ContentStrategy,
    instructions=[
        "Analyze the research findings provided from the previous step",
        "Create a comprehensive content strategy based on the structured research data",
        "Focus on audience engagement and brand building",
        "Structure your response according to the ContentStrategy model",
    ],
)

planning_agent = Agent(
    name="Content Planning Specialist",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Create detailed content plans and calendars",
    output_schema=FinalContentPlan,
    instructions=[
        "Use the content strategy from the previous step to create a detailed implementation plan",
        "Include specific timelines and success metrics",
        "Consider budget and resource constraints",
        "Structure your response according to the FinalContentPlan model",
    ],
)

---

## Use an embedder in a knowledge base

**URL:** llms-txt#use-an-embedder-in-a-knowledge-base

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="gemini_embeddings",
        embedder=GeminiEmbedder(),
    ),
    max_results=2,
)
```

For more details on configuring different model providers, check our [Embeddings documentation](/basics/knowledge/embedder/)

---

## Named steps for better tracking

**URL:** llms-txt#named-steps-for-better-tracking

**Contents:**
- Developer Resources
- Reference

workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Step(name="Research Phase", team=researcher),
        Step(name="Analysis Phase", executor=custom_function),
        Step(name="Writing Phase", agent=writer),
    ]
)

workflow.print_response(
    "AI trends in 2024",
    markdown=True,
)
```

## Developer Resources

* [Sequence of Steps](/basics/workflows/usage/sequence-of-steps)
* [Step with a Custom Function](/basics/workflows/usage/step-with-function)

For complete API documentation, see [Step Reference](/reference/workflows/step).

---

## Check current memories

**URL:** llms-txt#check-current-memories

print("\nBefore optimization:")
memories_before = agent.get_user_memories(user_id=user_id)
print(f"  Memory count: {len(memories_before)}")

---

## All logs will be written to tmp/log.txt

**URL:** llms-txt#all-logs-will-be-written-to-tmp/log.txt

**Contents:**
- Multiple Loggers

log_info("This is using our file logger!")

agent = Agent()
agent.print_response("Tell me a fun fact")
python  theme={null}
import logging

from agno.agent import Agent
from agno.team import Team
from agno.workflow import Workflow
from agno.workflow.step import Step
from agno.utils.log import configure_agno_logging, log_info

**Examples:**

Example 1 (unknown):
```unknown
## Multiple Loggers

You can configure different loggers for your Agents, Teams and Workflows:
```

---

## WhatsApp Agent with User Memory

**URL:** llms-txt#whatsapp-agent-with-user-memory

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/whatsapp/agent-with-user-memory

Personalized WhatsApp agent that remembers user information and preferences

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Memory Management**: Remembers user names, hobbies, preferences, and activities
* **DuckDuckGo Search**: Access to current information during conversations
* **Personalized Responses**: Uses stored memories for contextualized replies
* **Friendly AI**: Acts as personal AI friend with engaging conversation
* **Gemini Powered**: Fast, intelligent responses with multimodal capabilities

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Executor functions

**URL:** llms-txt#executor-functions

---

## run_response: Iterator[RunOutputEvent] = agent.run("Share a 2 sentence horror story", stream=True)

**URL:** llms-txt#run_response:-iterator[runoutputevent]-=-agent.run("share-a-2-sentence-horror-story",-stream=true)

---

## Basic WhatsApp Agent

**URL:** llms-txt#basic-whatsapp-agent

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/whatsapp/basic

Create a basic AI agent that integrates with WhatsApp Business API

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **WhatsApp Integration**: Responds to messages automatically
* **Conversation History**: Maintains context with last 3 interactions
* **Persistent Memory**: SQLite database for session storage
* **DateTime Context**: Time-aware responses
* **Markdown Support**: Rich text formatting in messages

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Toolkit Index

**URL:** llms-txt#toolkit-index

**Contents:**
- Search
- Social
- Web Scraping
- Data
- Local
- Native Model Toolkit
- Additional Toolkits

Source: https://docs.agno.com/integrations/toolkits/overview

Index of all toolkits supported by Agno.

A **Toolkit** is a collection of functions that can be added to an Agent. The functions in a Toolkit are designed to work together, share internal state and provide a better development experience.

The following **Toolkits** are available to use

<CardGroup cols={3}>
  <Card title="Arxiv" icon="book" iconType="duotone" href="/integrations/toolkits/search/arxiv">
    Tools to read arXiv papers.
  </Card>

<Card title="BaiduSearch" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/baidusearch">
    Tools to search the web using Baidu.
  </Card>

<Card title="DuckDuckGo" icon="duck" iconType="duotone" href="/integrations/toolkits/search/duckduckgo">
    Tools to search the web using DuckDuckGo.
  </Card>

<Card title="Exa" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/exa">
    Tools to search the web using Exa.
  </Card>

<Card title="HackerNews" icon="newspaper" iconType="duotone" href="/integrations/toolkits/search/hackernews">
    Tools to read Hacker News articles.
  </Card>

<Card title="Pubmed" icon="file-medical" iconType="duotone" href="/integrations/toolkits/search/pubmed">
    Tools to search Pubmed.
  </Card>

<Card title="SearxNG" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/searxng">
    Tools to search the web using SearxNG.
  </Card>

<Card title="SerperApi" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/serpapi">
    Tools to search Google using SerperApi.
  </Card>

<Card title="Serpapi" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/serper">
    Tools to search Google, YouTube, and more using Serpapi.
  </Card>

<Card title="Tavily" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/tavily">
    Tools to search the web using Tavily.
  </Card>

<Card title="Linkup" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/linkup">
    Tools to search the web using Linkup.
  </Card>

<Card title="Valyu" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/search/valyu">
    Tools to search academic papers and web content using Valyu.
  </Card>

<Card title="Wikipedia" icon="book" iconType="duotone" href="/integrations/toolkits/search/wikipedia">
    Tools to search Wikipedia.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Discord" icon="comment" iconType="duotone" href="/integrations/toolkits/social/discord">
    Tools to interact with Discord.
  </Card>

<Card title="Email" icon="envelope" iconType="duotone" href="/integrations/toolkits/social/email">
    Tools to send emails.
  </Card>

<Card title="Gmail" icon="envelope" iconType="duotone" href="/integrations/toolkits/social/gmail">
    Tools to interact with Gmail.
  </Card>

<Card title="Slack" icon="slack" iconType="duotone" href="/integrations/toolkits/social/slack">
    Tools to interact with Slack.
  </Card>

<Card title="Telegram" icon="telegram" iconType="brands" href="/integrations/toolkits/social/telegram">
    Tools to interact with Telegram.
  </Card>

<Card title="Twilio" icon="mobile-screen-button" iconType="duotone" href="/integrations/toolkits/social/twilio">
    Tools to interact with Twilio services.
  </Card>

<Card title="WhatsApp" icon="whatsapp" iconType="brands" href="/integrations/toolkits/social/whatsapp">
    Tools to interact with WhatsApp.
  </Card>

<Card title="Webex" icon="message" iconType="duotone" href="/integrations/toolkits/social/webex">
    Tools to interact with Cisco Webex.
  </Card>

<Card title="X (Twitter)" icon="x-twitter" iconType="brands" href="/integrations/toolkits/social/x">
    Tools to interact with X.
  </Card>

<Card title="Reddit" icon="reddit" iconType="brands" href="/integrations/toolkits/social/reddit">
    Tools to interact with Reddit.
  </Card>

<Card title="Zoom" icon="video" iconType="duotone" href="/integrations/toolkits/social/zoom">
    Tools to interact with Zoom.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="AgentQL" icon="magnifying-glass" iconType="duotone" href="/integrations/toolkits/web-scrape/agentql">
    Browse and scrape websites using AgentQL.
  </Card>

<Card title="BrowserBase" icon="browser" iconType="duotone" href="/integrations/toolkits/web-scrape/browserbase">
    Tools to interact with BrowserBase.
  </Card>

<Card title="Crawl4AI" icon="spider" iconType="duotone" href="/integrations/toolkits/web-scrape/crawl4ai">
    Tools to crawl web data.
  </Card>

<Card title="Jina Reader" icon="robot" iconType="duotone" href="/integrations/toolkits/web-scrape/jina-reader">
    Tools for neural search and AI services using Jina.
  </Card>

<Card title="Newspaper" icon="newspaper" iconType="duotone" href="/integrations/toolkits/web-scrape/newspaper">
    Tools to read news articles.
  </Card>

<Card title="Newspaper4k" icon="newspaper" iconType="duotone" href="/integrations/toolkits/web-scrape/newspaper4k">
    Tools to read articles using Newspaper4k.
  </Card>

<Card title="Website" icon="globe" iconType="duotone" href="/integrations/toolkits/web-scrape/website">
    Tools to scrape websites.
  </Card>

<Card title="Firecrawl" icon="fire" iconType="duotone" href="/integrations/toolkits/web-scrape/firecrawl">
    Tools to crawl the web using Firecrawl.
  </Card>

<Card title="Spider" icon="spider" iconType="duotone" href="/integrations/toolkits/web-scrape/spider">
    Tools to crawl websites.
  </Card>

<Card title="Trafilatura" icon="text" iconType="duotone" href="/integrations/toolkits/web-scrape/trafilatura">
    Tools to extract text content from web pages.
  </Card>

<Card title="BrightData" icon="screen-users" iconType="duotone" href="/integrations/toolkits/web-scrape/brightdata">
    Tools to scrape websites using BrightData.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="CSV" icon="file-csv" iconType="duotone" href="/integrations/toolkits/database/csv">
    Tools to work with CSV files.
  </Card>

<Card title="DuckDb" icon="server" iconType="duotone" href="/integrations/toolkits/database/duckdb">
    Tools to run SQL using DuckDb.
  </Card>

<Card title="Pandas" icon="table" iconType="duotone" href="/integrations/toolkits/database/pandas">
    Tools to manipulate data using Pandas.
  </Card>

<Card title="Postgres" icon="database" iconType="duotone" href="/integrations/toolkits/database/postgres">
    Tools to interact with PostgreSQL databases.
  </Card>

<Card title="Redshift" icon="database" iconType="duotone" href="/concepts/tools/toolkits/database/redshift">
    Tools to interact with Amazon Redshift data warehouses.
  </Card>

<Card title="SQL" icon="database" iconType="duotone" href="/integrations/toolkits/database/sql">
    Tools to run SQL queries.
  </Card>

<Card title="Google BigQuery" icon="database" iconType="duotone" href="/integrations/toolkits/database/google-bigquery">
    Tools to query large datasets using Google BigQuery.
  </Card>

<Card title="Neo4j" icon="project-diagram" iconType="duotone" href="/integrations/toolkits/database/neo4j">
    Tools to interact with Neo4j graph databases.
  </Card>

<Card title="Zep" icon="memory" iconType="duotone" href="/integrations/toolkits/database/zep">
    Tools to interact with Zep.
  </Card>

<Card title="MCP Toolbox" icon="database" iconType="duotone" href="/basics/tools/mcp/mcp-toolbox">
    Tools to connect to MCP Toolbox for Databases with filtering capabilities.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Calculator" icon="calculator" iconType="duotone" href="/integrations/toolkits/local/calculator">
    Tools to perform calculations.
  </Card>

<Card title="Docker" icon="docker" iconType="duotone" href="/integrations/toolkits/local/docker">
    Tools to interact with Docker.
  </Card>

<Card title="File" icon="file" iconType="duotone" href="/integrations/toolkits/local/file">
    Tools to read and write files.
  </Card>

<Card title="Python" icon="code" iconType="duotone" href="/integrations/toolkits/local/python">
    Tools to write and run Python code.
  </Card>

<Card title="Shell" icon="terminal" iconType="duotone" href="/integrations/toolkits/local/shell">
    Tools to run shell commands.
  </Card>

<Card title="Local File System" icon="file" iconType="duotone" href="/integrations/toolkits/local/local-file-system">
    Tools to write files to the local file system.
  </Card>

<Card title="Sleep" icon="bed" iconType="duotone" href="/integrations/toolkits/local/sleep">
    Tools to pause execution for a given number of seconds.
  </Card>
</CardGroup>

## Native Model Toolkit

<CardGroup cols={3}>
  <Card title="Azure OpenAI" icon="microsoft" iconType="brands" href="/integrations/toolkits/models/azure-openai">
    Tools to generate images using Azure OpenAI DALL-E.
  </Card>

<Card title="Groq" icon="groq" iconType="brands" href="/integrations/toolkits/models/groq">
    Tools to interact with Groq.
  </Card>

<Card title="Morph" icon="code" iconType="duotone" href="/integrations/toolkits/models/morph">
    Tools to modify code using Morph AI.
  </Card>

<Card title="Nebius" icon="image" iconType="duotone" href="/integrations/toolkits/models/nebius">
    Tools to generate images using Nebius Token Factory.
  </Card>
</CardGroup>

## Additional Toolkits

<CardGroup cols={3}>
  <Card title="Airflow" icon="wind" iconType="duotone" href="/integrations/toolkits/others/airflow">
    Tools to manage Airflow DAGs.
  </Card>

<Card title="Apify" icon="gear" iconType="duotone" href="/integrations/toolkits/others/apify">
    Tools to use Apify Actors.
  </Card>

<Card title="AWS Lambda" icon="server" iconType="duotone" href="/integrations/toolkits/others/aws-lambda">
    Tools to run serverless functions using AWS Lambda.
  </Card>

<Card title="AWS SES" icon="envelope" iconType="duotone" href="/integrations/toolkits/others/aws-ses">
    Tools to send emails using AWS SES
  </Card>

<Card title="CalCom" icon="calendar" iconType="duotone" href="/integrations/toolkits/others/calcom">
    Tools to interact with the Cal.com API.
  </Card>

<Card title="Cartesia" icon="waveform" iconType="duotone" href="/integrations/toolkits/others/cartesia">
    Tools for integrating voice AI.
  </Card>

<Card title="Composio" icon="code-branch" iconType="duotone" href="/integrations/toolkits/others/composio">
    Tools to compose complex workflows.
  </Card>

<Card title="Confluence" icon="file" iconType="duotone" href="/integrations/toolkits/others/confluence">
    Tools to manage Confluence pages.
  </Card>

<Card title="Custom API" icon="puzzle-piece" iconType="duotone" href="/integrations/toolkits/others/custom-api">
    Tools to call any custom HTTP API.
  </Card>

<Card title="Dalle" icon="eye" iconType="duotone" href="/integrations/toolkits/others/dalle">
    Tools to interact with Dalle.
  </Card>

<Card title="Eleven Labs" icon="headphones" iconType="duotone" href="/integrations/toolkits/others/eleven-labs">
    Tools to generate audio using Eleven Labs.
  </Card>

<Card title="E2B" icon="server" iconType="duotone" href="/integrations/toolkits/others/e2b">
    Tools to interact with E2B.
  </Card>

<Card title="Fal" icon="video" iconType="duotone" href="/integrations/toolkits/others/fal">
    Tools to generate media using Fal.
  </Card>

<Card title="Financial Datasets" icon="dollar-sign" iconType="duotone" href="/integrations/toolkits/others/financial-datasets">
    Tools to access and analyze financial data.
  </Card>

<Card title="Giphy" icon="image" iconType="duotone" href="/integrations/toolkits/others/giphy">
    Tools to search for GIFs on Giphy.
  </Card>

<Card title="GitHub" icon="github" iconType="brands" href="/integrations/toolkits/others/github">
    Tools to interact with GitHub.
  </Card>

<Card title="Google Maps" icon="map" iconType="duotone" href="/integrations/toolkits/others/google-maps">
    Tools to search for places on Google Maps.
  </Card>

<Card title="Google Calendar" icon="calendar" iconType="duotone" href="/integrations/toolkits/others/googlecalendar">
    Tools to manage Google Calendar events.
  </Card>

<Card title="Google Sheets" icon="google" iconType="duotone" href="/integrations/toolkits/others/google-sheets">
    Tools to work with Google Sheets.
  </Card>

<Card title="Jira" icon="jira" iconType="brands" href="/integrations/toolkits/others/jira">
    Tools to interact with Jira.
  </Card>

<Card title="Linear" icon="list" iconType="duotone" href="/integrations/toolkits/others/linear">
    Tools to interact with Linear.
  </Card>

<Card title="Lumalabs" icon="lightbulb" iconType="duotone" href="/integrations/toolkits/others/lumalabs">
    Tools to interact with Lumalabs.
  </Card>

<Card title="MLX Transcribe" icon="headphones" iconType="duotone" href="/integrations/toolkits/others/mlx-transcribe">
    Tools to transcribe audio using MLX.
  </Card>

<Card title="ModelsLabs" icon="video" iconType="duotone" href="/integrations/toolkits/others/models-labs">
    Tools to generate videos using ModelsLabs.
  </Card>

<Card title="Notion" icon="database" iconType="duotone" href="/integrations/toolkits/others/notion">
    Tools to interact with Notion database.
  </Card>

<Card title="Nano Banana" icon="image" iconType="duotone" href="/integrations/toolkits/others/nano-banana">
    Tools to generate images using Google Gemini.
  </Card>

<Card title="OpenBB" icon="chart-bar" iconType="duotone" href="/integrations/toolkits/others/openbb">
    Tools to search for stock data using OpenBB.
  </Card>

<Card title="Openweather" icon="cloud-sun" iconType="duotone" href="/integrations/toolkits/others/openweather">
    Tools to search for weather data using Openweather.
  </Card>

<Card title="Replicate" icon="robot" iconType="duotone" href="/integrations/toolkits/others/replicate">
    Tools to interact with Replicate.
  </Card>

<Card title="Resend" icon="paper-plane" iconType="duotone" href="/integrations/toolkits/others/resend">
    Tools to send emails using Resend.
  </Card>

<Card title="Todoist" icon="list" iconType="duotone" href="/integrations/toolkits/others/todoist">
    Tools to interact with Todoist.
  </Card>

<Card title="YFinance" icon="dollar-sign" iconType="duotone" href="/integrations/toolkits/others/yfinance">
    Tools to search Yahoo Finance.
  </Card>

<Card title="YouTube" icon="youtube" iconType="brands" href="/integrations/toolkits/others/youtube">
    Tools to search YouTube.
  </Card>

<Card title="Bitbucket" icon="bitbucket" iconType="brands" href="/integrations/toolkits/others/bitbucket">
    Tools to interact with Bitbucket repositories.
  </Card>

<Card title="Brandfetch" icon="trademark" iconType="duotone" href="/integrations/toolkits/others/brandfetch">
    Tools to retrieve brand information and logos.
  </Card>

<Card title="ClickUp" icon="tasks" iconType="duotone" href="/integrations/toolkits/others/clickup">
    Tools to manage ClickUp tasks and projects.
  </Card>

<Card title="Desi Vocal" icon="microphone" iconType="duotone" href="/integrations/toolkits/others/desi-vocal">
    Tools for Indian text-to-speech synthesis.
  </Card>

<Card title="EVM" icon="coins" iconType="duotone" href="/integrations/toolkits/others/evm">
    Tools to interact with Ethereum blockchain.
  </Card>

<Card title="Knowledge" icon="brain" iconType="duotone" href="/integrations/toolkits/others/knowledge">
    Tools to search and analyze knowledge bases.
  </Card>

<Card title="Mem0" icon="memory" iconType="duotone" href="/integrations/toolkits/others/mem0">
    Tools for advanced memory management.
  </Card>

<Card title="Memori" icon="brain" iconType="duotone" href="/integrations/memory/memori">
    Tools for persistent conversation memory.
  </Card>

<Card title="OpenCV" icon="camera" iconType="duotone" href="/integrations/toolkits/others/opencv">
    Tools for computer vision and camera operations.
  </Card>

<Card title="Reasoning" icon="brain" iconType="duotone" href="/integrations/toolkits/others/reasoning">
    Tools for structured logical analysis.
  </Card>

<Card title="Spotify" icon="spotify" iconType="brands" href="/integrations/toolkits/others/spotify">
    Tools to search for songs using Spotify.
  </Card>

<Card title="User Control Flow" icon="user-check" iconType="duotone" href="/integrations/toolkits/others/user-control-flow">
    Tools for interactive user input collection.
  </Card>

<Card title="Visualization" icon="chart-bar" iconType="duotone" href="/integrations/toolkits/others/visualization">
    Tools for data visualization and charting.
  </Card>

<Card title="WebTools" icon="globe" iconType="duotone" href="/integrations/toolkits/others/webtools">
    Tools for URL expansion and web utilities.
  </Card>

<Card title="Zendesk" icon="headphones" iconType="duotone" href="/integrations/toolkits/others/zendesk">
    Tools to search Zendesk.
  </Card>
</CardGroup>

---

## Define the vector search index

**URL:** llms-txt#define-the-vector-search-index

search_index = SearchIndex(
    name="vector_search",
    source_type="gocbcore",
    idx_type="fulltext-index",
    source_name="recipe_bucket",
    plan_params={"index_partitions": 1, "num_replicas": 0},
    params={
        "doc_config": {
            "docid_prefix_delim": "",
            "docid_regexp": "",
            "mode": "scope.collection.type_field",
            "type_field": "type",
        },
        "mapping": {
            "default_analyzer": "standard",
            "default_datetime_parser": "dateTimeOptional",
            "index_dynamic": True,
            "store_dynamic": True,
            "default_mapping": {"dynamic": True, "enabled": False},
            "types": {
                "recipe_scope.recipes": {
                    "dynamic": False,
                    "enabled": True,
                    "properties": {
                        "content": {
                            "enabled": True,
                            "fields": [
                                {
                                    "docvalues": True,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "content",
                                    "store": True,
                                    "type": "text",
                                }
                            ],
                        },
                        "embedding": {
                            "enabled": True,
                            "dynamic": False,
                            "fields": [
                                {
                                    "vector_index_optimized_for": "recall",
                                    "docvalues": True,
                                    "dims": 1536,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "embedding",
                                    "similarity": "dot_product",
                                    "store": True,
                                    "type": "vector",
                                }
                            ],
                        },
                        "meta": {
                            "dynamic": True,
                            "enabled": True,
                            "properties": {
                                "name": {
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "docvalues": True,
                                            "include_in_all": False,
                                            "include_term_vectors": False,
                                            "index": True,
                                            "name": "name",
                                            "store": True,
                                            "analyzer": "keyword",
                                            "type": "text",
                                        }
                                    ],
                                }
                            },
                        },
                    },
                }
            },
        },
    },
)
vector_db = CouchbaseSearch(
    bucket_name="recipe_bucket",
    scope_name="recipe_scope",
    collection_name="recipes",
    couchbase_connection_string=connection_string,
    cluster_options=cluster_options,
    search_index=search_index,
    embedder=OpenAIEmbedder(
        dimensions=1536,
    ),
    wait_until_index_ready=60,
    overwrite=True,
)

knowledge = Knowledge(
    name="Couchbase Knowledge Base",
    description="This is a knowledge base that uses a Couchbase DB",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=True,
)

agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

vector_db.delete_by_name("Recipes")

---

## === Request/Response Logging Middleware ===

**URL:** llms-txt#===-request/response-logging-middleware-===

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """
    Request/response logging middleware with timing and basic info.
    """

def __init__(self, app, log_body: bool = False, log_headers: bool = False):
        super().__init__(app)
        self.log_body = log_body
        self.log_headers = log_headers
        self.request_count = 0

async def dispatch(self, request: Request, call_next) -> Response:
        self.request_count += 1
        start_time = time.time()

# Basic request info
        client_ip = request.client.host if request.client else "unknown"
        print(
            f"ðŸ” Request #{self.request_count}: {request.method} {request.url.path} from {client_ip}"
        )

# Optional: Log headers
        if self.log_headers:
            print(f"ðŸ“‹ Headers: {dict(request.headers)}")

# Optional: Log request body
        if self.log_body and request.method in ["POST", "PUT", "PATCH"]:
            body = await request.body()
            if body:
                print(f"ðŸ“ Body: {body.decode()}")

# Process request
        response = await call_next(request)

# Log response info
        duration = time.time() - start_time
        status_emoji = "âœ…" if response.status_code < 400 else "âŒ"
        print(
            f"{status_emoji} Response: {response.status_code} in {duration * 1000:.1f}ms"
        )

# Add request count to response header
        response.headers["X-Request-Count"] = str(self.request_count)

---

## Asynchronous reading - better for I/O intensive operations

**URL:** llms-txt#asynchronous-reading---better-for-i/o-intensive-operations

documents = await reader.async_read("file.pdf")

---

## Context Editing

**URL:** llms-txt#context-editing

**Contents:**
- Working example

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/context-management

Learn how to use Anthropic's context editing capabilities with Agno.

With Anthropic's [context editing capabilities](https://docs.anthropic.com/en/docs/build-with-claude/context-editing), you can automatically manage your context size.

When your context grows larger, previous tool results and thinking blocks will be removed.

This is useful to reduce costs, improve performance, and reduce the chances of hitting context limits.

```python anthropic_context_management.py theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-5",
        # Activate and configure the context management feature
        betas=["context-management-2025-06-27"],
        context_management={
            "edits": [
                {
                    "type": "clear_tool_uses_20250919",
                    "trigger": {"type": "tool_uses", "value": 2},
                    "keep": {"type": "tool_uses", "value": 1},
                }
            ]
        },
    ),
    instructions="You are a helpful assistant with access to the web.",
    tools=[DuckDuckGoTools()],
    session_id="context-editing",
    add_history_to_context=True,
    markdown=True,
)

agent.print_response(
    "Search for AI regulation in US. Make multiple searches to find the latest information."
)

---

## Sequential Workflows

**URL:** llms-txt#sequential-workflows

Source: https://docs.agno.com/basics/workflows/workflow-patterns/sequential

Linear, deterministic processes where each step depends on the output of the previous step.

Sequential workflows ensure predictable execution order and clear data flow between steps.

**Example Flow**: Research â†’ Data Processing â†’ Content Creation â†’ Final Review

Sequential workflows ensure predictable execution order and clear data flow between steps.

<Note>
  For more information on how to use custom functions, refer to the
  [Workflow with custom function step](/basics/workflows/workflow-patterns/custom-function-step-workflow) page.
</Note>

* [Sequence of Functions and Agents](/basics/workflows/usage/function-instead-of-steps) - Complete workflow with functions and agents

<Note>
  `StepInput` and `StepOutput` provides standardized interfaces for data flow between steps:
  So if you make a custom function as an executor for a step, make sure that the input and output types are compatible with the `StepInput` and `StepOutput` interfaces.
  This will ensure that your custom function can seamlessly integrate into the workflow system.

Take a look at the schemas for [`StepInput`](/reference/workflows/step_input) and [`StepOutput`](/reference/workflows/step_output).
</Note>

---

## Anthropic Claude

**URL:** llms-txt#anthropic-claude

**Contents:**
- Authentication
- Example
- Beta Features
- Prompt caching
- Structured Outputs
- Params

Source: https://docs.agno.com/integrations/models/native/anthropic/overview

Learn how to use Anthropic Claude models in Agno.

Claude is a family of foundational AI models by Anthropic that can be used in a variety of applications.
See their model comparisons [here](https://docs.anthropic.com/en/docs/about-claude/models#model-comparison-table).

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `claude-sonnet-4-20250514` model is good for most use-cases and supports image input.
* `claude-opus-4-1-20250805` model is their best model.
* `claude-3-5-haiku-20241022` model is their fastest model.

Anthropic has rate limits on their APIs. See the [docs](https://docs.anthropic.com/en/api/rate-limits#response-headers) for more information.

<Note>
  Claude API expects a `max_tokens` param to be sent with each request. Unless
  set as a param, Agno will default to 8192. See the
  [docs](https://docs.claude.com/en/api/messages) for more information.
</Note>

Set your `ANTHROPIC_API_KEY` environment. You can get one [from Anthropic here](https://console.anthropic.com/settings/keys).

Use `Claude` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

You can use Anthropic's beta features with Agno by setting the `betas` parameter:

Read more about beta features with Agno `Claude` model [here](/integrations/models/native/anthropic/usage/betas).

You can enable system prompt caching by setting `cache_system_prompt` to `True`:

Read more about prompt caching with Agno's `Claude` model [here](/integrations/models/native/anthropic/usage/prompt-caching).

## Structured Outputs

Structured outputs are used to ensure that the model's response matches a defined schema.

This is useful to eliminate issues like missing fields or invalid values. Use it for production systems that need reliable, consistent responses in a specific format.

Agno uses Claude's native support for structured outputs. This feature is available for `claude-sonnet-4-5-20250929` and all newer models. See Anthropic's [structured outputs documentation](https://docs.anthropic.com/en/build-with-claude/structured-outputs) for more details.

Read more about structured outputs with Agno's `Claude` model:

* [Basic structured outputs](/integrations/models/native/anthropic/usage/structured-output)
* [Streaming structured outputs](/integrations/models/native/anthropic/usage/structured-output-stream)
* [Structured outputs with strict tools](/integrations/models/native/anthropic/usage/structured-output-strict-tools)

| Parameter             | Type                                     | Default                        | Description                                                                                                                      |
| --------------------- | ---------------------------------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------- |
| `id`                  | `str`                                    | `"claude-3-5-sonnet-20241022"` | The id of the Anthropic Claude model to use                                                                                      |
| `name`                | `str`                                    | `"Claude"`                     | The name of the model                                                                                                            |
| `provider`            | `str`                                    | `"Anthropic"`                  | The provider of the model                                                                                                        |
| `max_tokens`          | `Optional[int]`                          | `4096`                         | Maximum number of tokens to generate in the chat completion                                                                      |
| `thinking`            | `Optional[Dict[str, Any]]`               | `None`                         | Configuration for the thinking (reasoning) process (See [their docs](https://www.anthropic.com/news/visible-extended-thinking))) |
| `temperature`         | `Optional[float]`                        | `None`                         | Controls randomness in the model's output                                                                                        |
| `stop_sequences`      | `Optional[List[str]]`                    | `None`                         | A list of strings that the model should stop generating text at                                                                  |
| `top_p`               | `Optional[float]`                        | `None`                         | Controls diversity via nucleus sampling                                                                                          |
| `top_k`               | `Optional[int]`                          | `None`                         | Controls diversity via top-k sampling                                                                                            |
| `cache_system_prompt` | `Optional[bool]`                         | `False`                        | Whether to cache the system prompt for improved performance                                                                      |
| `extended_cache_time` | `Optional[bool]`                         | `False`                        | Whether to use extended cache time (1 hour instead of default)                                                                   |
| `request_params`      | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters to include in the request                                                                                  |
| `mcp_servers`         | `Optional[List[MCPServerConfiguration]]` | `None`                         | List of MCP (Model Context Protocol) server configurations                                                                       |
| `api_key`             | `Optional[str]`                          | `None`                         | The API key for authenticating with Anthropic                                                                                    |
| `default_headers`     | `Optional[Dict[str, Any]]`               | `None`                         | Default headers to include in all requests                                                                                       |
| `client_params`       | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters for client configuration                                                                                   |
| `client`              | `Optional[AnthropicClient]`              | `None`                         | A pre-configured instance of the Anthropic client                                                                                |
| `async_client`        | `Optional[AsyncAnthropicClient]`         | `None`                         | A pre-configured instance of the async Anthropic client                                                                          |

`Claude` is a subclass of the [Model](/reference/models/model) class and has access to the same params.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Claude` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

## Beta Features

You can use Anthropic's beta features with Agno by setting the `betas` parameter:
```

Example 4 (unknown):
```unknown
Read more about beta features with Agno `Claude` model [here](/integrations/models/native/anthropic/usage/betas).

## Prompt caching

You can enable system prompt caching by setting `cache_system_prompt` to `True`:
```

---

## What is Memory?

**URL:** llms-txt#what-is-memory?

**Contents:**
- How Memory Works
- Getting Started with Memory

Source: https://docs.agno.com/basics/memory/overview

Give your agents the ability to remember user preferences, context, and past interactions for truly personalized experiences.

Imagine a customer support agent that remembers your product preferences from last week, or a personal assistant that knows you prefer morning meetings, but only after you've had coffee. This is the power of Memory in Agno.

When relevant information appears in a conversation, like a user's name, preferences, or habits, an Agent with Memory automatically stores it in your database. Later, when that information becomes relevant again, the agent retrieves and uses it naturally in the conversation. The agent is effectively **learning about each user** across interactions.

<Tip>
  **Memory â‰  Session History:** Memory stores learned user facts ("Sarah prefers email"), [session history](/basics/chat-history/overview) stores conversation messages for continuity ("what did we just discuss?").
</Tip>

## Getting Started with Memory

Setting up memory is straightforward: just connect a database and enable the memory feature. Here's a basic setup:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## Stagehand MCP agent

**URL:** llms-txt#stagehand-mcp-agent

**Contents:**
- Key Features
- Prerequisites
- Setup Instructions
  - 1. Clone and Build Stagehand MCP Server

Source: https://docs.agno.com/basics/tools/mcp/usage/stagehand

A web scraping agent that uses the Stagehand MCP server to automate browser interactions and create a structured content digest from Hacker News.

* **Safe Navigation**: Proper initialization sequence prevents common browser automation errors
* **Structured Data Extraction**: Methodical approach to extracting and organizing web content
* **Flexible Output**: Creates well-structured digests with headlines, summaries, and insights

Before running this example, you'll need:

* **Browserbase Account**: Get API credentials from [Browserbase](https://browserbase.com)
* **OpenAI API Key**: Get an API Key from [OpenAI](https://platform.openai.com/settings/organization/api-keys)

## Setup Instructions

### 1. Clone and Build Stagehand MCP Server

```bash  theme={null}
git clone https://github.com/browserbase/mcp-server-browserbase

---

## "Get the user profile and tell me about their experience level.",

**URL:** llms-txt#"get-the-user-profile-and-tell-me-about-their-experience-level.",

---

## Add in the query and the agent redirects it to the appropriate agent

**URL:** llms-txt#add-in-the-query-and-the-agent-redirects-it-to-the-appropriate-agent

customer_support_team.print_response(
    "Hi Team, I want to build an educational platform where the models are have access to tons of study materials, How can Agno platform help me build this?",
    stream=True,
)

---

## Create and run agents - they're automatically traced!

**URL:** llms-txt#create-and-run-agents---they're-automatically-traced!

**Contents:**
  - Option 2: Using AgentOS
- Dedicated Traces Database
- Processing Modes
  - Batch Processing
  - Simple Processing (Default)
- Next Steps

agent = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a research assistant",
)

response = agent.run("What is quantum computing?")
python  theme={null}
from agno.os import AgentOS

agent_os = AgentOS(
    agents=[my_agent],
    tracing=True,  # Enable tracing
    db=tracing_db,
)
python  theme={null}
  from agno.agent import Agent
  from agno.db.sqlite import SqliteDb
  from agno.models.openai import OpenAIChat
  from agno.tracing import setup_tracing
  from agno.tools.duckduckgo import DuckDuckGoTools
  from agno.tools.hackernews import HackerNewsTools

# Each agent has its own database for sessions/memory
  agent1_db = SqliteDb(db_file="tmp/agent1.db", id="agent1_db")
  agent2_db = SqliteDb(db_file="tmp/agent2.db", id="agent2_db")

# Dedicated database for ALL traces (separate from agent databases)
  tracing_db = SqliteDb(db_file="tmp/traces.db", id="traces_db")

# Enable tracing to the dedicated database
  setup_tracing(
      db=tracing_db,
      batch_processing=True,
      max_queue_size=1024,
      max_export_batch_size=256,
  )

# Agent 1: HackerNews specialist with its own database
  hackernews_agent = Agent(
      name="HackerNews Agent",
      model=OpenAIChat(id="gpt-4o-mini"),
      tools=[HackerNewsTools()],
      instructions="You are a hacker news agent. Answer questions concisely.",
      markdown=True,
      db=agent1_db,  # Agent's own database
  )

# Agent 2: Web search specialist with its own database
  search_agent = Agent(
      name="Web Search Agent",
      model=OpenAIChat(id="gpt-4o-mini"),
      tools=[DuckDuckGoTools()],
      instructions="You are a web search agent. Answer questions concisely.",
      markdown=True,
      db=agent2_db,  # Agent's own database
  )

# Both agents are traced to the same tracing_db
  hackernews_agent.run("What's trending on HackerNews?")
  search_agent.run("Latest AI news")

# Query traces for both agents from one place
  traces, count = tracing_db.get_traces(limit=20)
  print(f"Found {count} traces across all agents")
  python  theme={null}
setup_tracing(
    db=tracing_db,
    batch_processing=True,
    max_queue_size=2048,           # Max traces in memory
    max_export_batch_size=512,     # Traces per batch write
    schedule_delay_millis=5000,    # Export every 5 seconds
)
python  theme={null}
setup_tracing(
    db=tracing_db,
    batch_processing=False
)
```

* Traces appear immediately
* No memory buffering

* More database writes
* Slight performance overhead

<Tip>
  Use **batch processing** in production and **simple processing** for development/debugging when you need immediate trace visibility.
</Tip>

<CardGroup cols={2}>
  <Card title="DB Functions" icon="database" href="/basics/tracing/db-functions">
    Query traces and spans from your database
  </Card>

<Card title="Cookbook" icon="book" href="https://github.com/agno-agi/agno/tree/main/cookbook/integrations/observability/trace_to_database.py">
    Full example code
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  Call `setup_tracing()` **before** creating your agents. This ensures the instrumentation is active when agents are initialized.
</Warning>

### Option 2: Using AgentOS

When deploying agents with AgentOS, you can enable tracing with a simple parameter:
```

Example 2 (unknown):
```unknown
<Info>
  For detailed AgentOS tracing configuration including multi-database setups, see [Tracing in AgentOS](/agent-os/tracing/overview).
</Info>

## Dedicated Traces Database

<Warning>
  **Recommended**: Use a separate database for storing traces, especially when you have multiple agents or teams with their own databases.
</Warning>

When agents and teams each have their own databases for sessions and memory, traces should go to a **dedicated central database**. This ensures:

* **Unified observability**: All traces in one place for cross-agent analysis
* **Simpler querying**: No need to search multiple databases
* **Independent scaling**: Traces can grow independently from agent data
* **Cleaner separation**: Agent data and observability data don't mix

<Accordion title="Example: Multiple Agents with Shared Tracing">
```

Example 3 (unknown):
```unknown
</Accordion>

Once configured, traces and spans are automatically stored in your database. The tracing system creates two tables: `agno_traces` for high-level trace information and `agno_spans` for individual span details.

<Frame caption="Traces stored in SQLite database viewed with TablePlus">
  <img src="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=00d7b09c58d61392fc2bf09b217fb08c" alt="Database view showing agno_spans table with trace data including span_id, trace_id, parent_span_id, and operation names" data-og-width="2038" width="2038" data-og-height="480" height="480" data-path="images/traces-in-db.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?w=280&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=0c62525a7983eab01053b339864e0ded 280w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?w=560&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=c38d5d4e0d13ef88ddee29c0dba9f9d7 560w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?w=840&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=5190bf50a2f5fdaa924ac651c0865faf 840w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?w=1100&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=bc687fbcdbfabb6cf559f91675ae756c 1100w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?w=1650&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=fd048ddb6ee2a5c5931946cac4f5099a 1650w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-db.png?w=2500&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=f14f9ad2dd68b64c7b8440365ed28337 2500w" />
</Frame>

Each span record includes the `trace_id` to group related operations, `parent_span_id` for hierarchy, and the operation `name` (e.g., `Stock_Price_Agent.run`, `OpenAIChat.invoke`, `get_current_stock_price`).

## Processing Modes

Agno supports two trace processing modes:

### Batch Processing

Batch processing collects traces in memory and writes them in batches. This is more efficient and recommended for production:
```

Example 4 (unknown):
```unknown
**Pros:**

* Lower database load
* Better performance
* Minimal impact on agent execution

**Cons:**

* Slight delay before traces appear (default 5 seconds)
* Traces in memory if application crashes before export

### Simple Processing (Default)

Simple processing writes each trace immediately:
```

---

## Display context management metrics

**URL:** llms-txt#display-context-management-metrics

print("\n" + "=" * 60)
print("CONTEXT MANAGEMENT SUMMARY")
print("=" * 60)
response = agent.get_last_run_output()
if response and response.metrics:
    print(f"\nInput tokens: {response.metrics.input_tokens:,}")

---

## session_id=session_id,

**URL:** llms-txt#session_id=session_id,

---

## Example: Ask the agent to search using Searxng

**URL:** llms-txt#example:-ask-the-agent-to-search-using-searxng

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("""
Please search for information about artificial intelligence
and summarize the key points from the top results
""")
```

| Parameter           | Type        | Default | Description                                                        |
| ------------------- | ----------- | ------- | ------------------------------------------------------------------ |
| `host`              | `str`       | -       | The host for the connection.                                       |
| `engines`           | `List[str]` | `[]`    | A list of search engines to use.                                   |
| `fixed_max_results` | `int`       | `None`  | Optional parameter to specify the fixed maximum number of results. |

| Function         | Description                                                                                                                                                                                                                         |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search`         | Performs a general web search using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the search results.                             |
| `image_search`   | Performs an image search using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the image search results.                            |
| `it_search`      | Performs a search for IT-related information using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the IT-related search results.   |
| `map_search`     | Performs a search for maps using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the map search results.                            |
| `music_search`   | Performs a search for music-related information using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the music search results.     |
| `news_search`    | Performs a search for news using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the news search results.                           |
| `science_search` | Performs a search for science-related information using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the science search results. |
| `video_search`   | Performs a search for videos using the specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results (default is 5). Returns the video search results.                        |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/searxng.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/searxng_tools.py)

---

## Readme Generator

**URL:** llms-txt#readme-generator

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/readme_generator

The README Generator Agent is an intelligent automation tool that creates comprehensive, professional README files for open source projects. This agent leverages the power of AI to analyze GitHub repositories and generate well-structured documentation automatically.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create team with agentic knowledge filters enabled

**URL:** llms-txt#create-team-with-agentic-knowledge-filters-enabled

team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[
        web_agent
    ],  # If you omit the member, the leader will search the knowledge base itself.
    model=OpenAIChat(id="gpt-5-mini"),
    knowledge=knowledge,
    show_members_responses=True,
    markdown=True,
    enable_agentic_knowledge_filters=True,  # Allow AI to determine filters
)

---

## Create a knowledge base with Postgres and PgVector

**URL:** llms-txt#create-a-knowledge-base-with-postgres-and-pgvector

knowledge = Knowledge(
    contents_db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5432/db",
        knowledge_table="knowledge_contents",
    ),
    vector_db=PgVector(
        table_name="knowledge_documents",
        db_url="postgresql+psycopg://ai:ai@localhost:5432/db"
    ),
)

---

## End condition function

**URL:** llms-txt#end-condition-function

def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

# Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

# Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f"âœ… Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

print(
        f"âŒ Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False

---

## Create our News Reporter with a fun personality

**URL:** llms-txt#create-our-news-reporter-with-a-fun-personality

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! ðŸ—½
        Think of yourself as a mix between a witty comedian and a sharp journalist.

Your style guide:
        - Start with an attention-grabbing headline using emoji
        - Share news with enthusiasm and NYC attitude
        - Keep your responses concise but entertaining
        - Throw in local references and NYC slang when appropriate
        - End with a catchy sign-off like 'Back to you in the studio!' or 'Reporting live from the Big Apple!'

Remember to verify all facts while keeping that NYC energy high!\
    """),
    markdown=True,
)

---

## wget https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg

**URL:** llms-txt#wget-https://upload.wikimedia.org/wikipedia/commons/b/bf/krakow_-_kosciol_mariacki.jpg

**Contents:**
- Usage

image_path = Path(__file__).parent.joinpath("Krakow_-_Kosciol_Mariacki.jpg")
image_file: file_types.File = upload_file(image_path)
print(f"Uploaded image: {image_file}")

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[Image(content=image_file)],
    stream=True,
)
bash  theme={null}
    wget https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg
    bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai ddgs agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/image_input_file_upload.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/image_input_file_upload.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Download the image">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your API key">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## ClickHouse Async

**URL:** llms-txt#clickhouse-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/clickhouse/usage/async-clickhouse-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run ClickHouse">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run ClickHouse">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Embed each chunk and load it into the vector store

**URL:** llms-txt#embed-each-chunk-and-load-it-into-the-vector-store

Chroma.from_documents(
    documents, OpenAIEmbeddings(), persist_directory=str(chroma_db_dir)
)

---

## Weaviate Async

**URL:** llms-txt#weaviate-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/weaviate/usage/async-weaviate-db

```python cookbook/knowledge/vector_db/weaviate_db/async_weaviate_db.py theme={null}

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

vector_db = Weaviate(
    collection="recipes_async",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)

---

## Deep Knowledge

**URL:** llms-txt#deep-knowledge

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/deep_knowledge

This agent performs iterative searches through its knowledge base, breaking down complex
queries into sub-questions, and synthesizing comprehensive answers. It's designed to explore
topics deeply and thoroughly by following chains of reasoning.

In this example, the agent uses the Agno documentation as a knowledge base

* Iteratively searches a knowledge base
* Source attribution and citations

```python cookbook/examples/agents/deep_knowledge.py theme={null}
from textwrap import dedent
from typing import List, Optional

import inquirer
import typer
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType
from rich import print

def initialize_knowledge_base():
    """Initialize the knowledge base with your preferred documentation or knowledge source
    Here we use Agno docs as an example, but you can replace with any relevant URLs
    """
    agent_knowledge = Knowledge(
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="deep_knowledge_knowledge",
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        ),
    )
    agent_knowledge.add_content(
        url="https://docs.agno.com/llms-full.txt",
    )
    return agent_knowledge

def get_agent_db():
    """Return agent storage"""
    return SqliteDb(session_table="deep_knowledge_sessions", db_file="tmp/agents.db")

def create_agent(session_id: Optional[str] = None) -> Agent:
    """Create and return a configured DeepKnowledge agent."""
    agent_knowledge = initialize_knowledge_base()
    agent_db = get_agent_db()
    return Agent(
        name="DeepKnowledge",
        session_id=session_id,
        model=OpenAIChat(id="gpt-5-mini"),
        description=dedent("""\
        You are DeepKnowledge, an advanced reasoning agent designed to provide thorough,
        well-researched answers to any query by searching your knowledge base.

Your strengths include:
        - Breaking down complex topics into manageable components
        - Connecting information across multiple domains
        - Providing nuanced, well-researched answers
        - Maintaining intellectual honesty and citing sources
        - Explaining complex concepts in clear, accessible terms"""),
        instructions=dedent("""\
        Your mission is to leave no stone unturned in your pursuit of the correct answer.

To achieve this, follow these steps:
        1. **Analyze the input and break it down into key components**.
        2. **Search terms**: You must identify at least 3-5 key search terms to search for.
        3. **Initial Search:** Searching your knowledge base for relevant information. You must make atleast 3 searches to get all relevant information.
        4. **Evaluation:** If the answer from the knowledge base is incomplete, ambiguous, or insufficient - Ask the user for clarification. Do not make informed guesses.
        5. **Iterative Process:**
            - Continue searching your knowledge base till you have a comprehensive answer.
            - Reevaluate the completeness of your answer after each search iteration.
            - Repeat the search process until you are confident that every aspect of the question is addressed.
        4. **Reasoning Documentation:** Clearly document your reasoning process:
            - Note when additional searches were triggered.
            - Indicate which pieces of information came from the knowledge base and where it was sourced from.
            - Explain how you reconciled any conflicting or ambiguous information.
        5. **Final Synthesis:** Only finalize and present your answer once you have verified it through multiple search passes.
            Include all pertinent details and provide proper references.
        6. **Continuous Improvement:** If new, relevant information emerges even after presenting your answer,
            be prepared to update or expand upon your response.

**Communication Style:**
        - Use clear and concise language.
        - Organize your response with numbered steps, bullet points, or short paragraphs as needed.
        - Be transparent about your search process and cite your sources.
        - Ensure that your final answer is comprehensive and leaves no part of the query unaddressed.

Remember: **Do not finalize your answer until every angle of the question has been explored.**"""),
        additional_context=dedent("""\
        You should only respond with the final answer and the reasoning process.
        No need to include irrelevant information.

- User ID: {user_id}
        - Memory: You have access to your previous search results and reasoning process.
        """),
        knowledge=agent_knowledge,
        db=agent_db,
        add_history_to_context=True,
        num_history_runs=3,
        read_chat_history=True,
        markdown=True,
    )

def get_example_topics() -> List[str]:
    """Return a list of example topics for the agent."""
    return [
        "What are AI agents and how do they work in Agno?",
        "What chunking strategies does Agno support for text processing?",
        "How can I implement custom tools in Agno?",
        "How does knowledge retrieval work in Agno?",
        "What types of embeddings does Agno support?",
    ]

def handle_session_selection() -> Optional[str]:
    """Handle session selection and return the selected session ID."""
    agent_db = get_agent_db()

new = typer.confirm("Do you want to start a new session?", default=True)
    if new:
        return None

existing_sessions: List[str] = agent_db.get_sessions(session_type=SessionType.AGENT)
    if not existing_sessions:
        print("No existing sessions found. Starting a new session.")
        return None

print("\nExisting sessions:")
    for i, session in enumerate(existing_sessions, 1):
        print(f"{i}. {session}")

session_idx = typer.prompt(
        "Choose a session number to continue (or press Enter for most recent)",
        default=1,
    )

try:
        return existing_sessions[int(session_idx) - 1]
    except (ValueError, IndexError):
        return existing_sessions[0]

def run_interactive_loop(agent: Agent):
    """Run the interactive question-answering loop."""
    example_topics = get_example_topics()

while True:
        choices = [f"{i + 1}. {topic}" for i, topic in enumerate(example_topics)]
        choices.extend(["Enter custom question...", "Exit"])

questions = [
            inquirer.List(
                "topic",
                message="Select a topic or ask a different question:",
                choices=choices,
            )
        ]
        answer = inquirer.prompt(questions)

if answer["topic"] == "Exit":
            break

if answer["topic"] == "Enter custom question...":
            questions = [inquirer.Text("custom", message="Enter your question:")]
            custom_answer = inquirer.prompt(questions)
            topic = custom_answer["custom"]
        else:
            topic = example_topics[int(answer["topic"].split(".")[0]) - 1]

agent.print_response(topic, stream=True)

def deep_knowledge_agent():
    """Main function to run the DeepKnowledge agent."""

session_id = handle_session_selection()
    agent = create_agent(session_id)

print("\nðŸ¤” Welcome to DeepKnowledge - Your Advanced Research Assistant! ðŸ“š")
    if session_id is None:
        session_id = agent.session_id
        if session_id is not None:
            print(f"[bold green]Started New Session: {session_id}[/bold green]\n")
        else:
            print("[bold green]Started New Session[/bold green]\n")
    else:
        print(f"[bold blue]Continuing Previous Session: {session_id}[/bold blue]\n")

run_interactive_loop(agent)

if __name__ == "__main__":
    typer.run(deep_knowledge_agent)

---

## Setup team members

**URL:** llms-txt#setup-team-members

research_agent = Agent(
    name="Research Agent",
    role="Analyze candidate information",
    knowledge=knowledge_base
)

---

## Note: you can see all beta features available in your Anthropic version like this:

**URL:** llms-txt#note:-you-can-see-all-beta-features-available-in-your-anthropic-version-like-this:

all_betas = anthropic.types.AnthropicBetaParam
print("\n=== All available Anthropic beta features ===")
print(f"- {'\n- '.join(all_betas.__args__[1].__args__)}")
print("=============================================\n")

agent = Agent(model=model, debug_mode=True)

---

## Input Transformation Pre-Hook

**URL:** llms-txt#input-transformation-pre-hook

**Contents:**
- Code

Source: https://docs.agno.com/basics/hooks/usage/team/input-transformation-pre-hook

This example demonstrates how to use a pre-hook to transform the input of an Team, before it is presented to the LLM.

```python  theme={null}
from typing import Optional

from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.run.team import RunInput
from agno.session.team import TeamSession
from agno.utils.log import log_debug

def transform_input(
    run_input: RunInput,
    session: TeamSession,
    user_id: Optional[str] = None,
    debug_mode: Optional[bool] = None,
) -> None:
    """
    Pre-hook: Rewrite the input to be more relevant to the team's purpose.

This hook rewrites the input to be more relevant to the team's purpose.
    """
    log_debug(
        f"Transforming input: {run_input.input_content} for user {user_id} and session {session.session_id}"
    )

# Input transformation team
    transformer_team = Team(
        name="Input Transformer",
        model=OpenAIChat(id="gpt-5-mini"),
        instructions=[
            "You are an input transformation specialist.",
            "Rewrite the user request to be more relevant to the team's purpose.",
            "Use known context engineering standards to rewrite the input.",
            "Keep the input as concise as possible.",
            "The team's purpose is to provide investment guidance and financial planning advice.",
        ],
        debug_mode=debug_mode,
    )

transformation_result = transformer_team.run(
        input=f"Transform this user request: '{run_input.input_content}'"
    )

# Overwrite the input with the transformed input
    run_input.input_content = transformation_result.content
    log_debug(f"Transformed input: {run_input.input_content}")

print("ðŸš€ Input Transformation Pre-Hook Example")
print("=" * 60)

---

## Load documentation asynchronously

**URL:** llms-txt#load-documentation-asynchronously

asyncio.run(
    knowledge.add_content_async(
        name="Agno Docs", 
        url="https://docs.agno.com/llms-full.txt"
    )
)

---

## Async Sqlite for Agent

**URL:** llms-txt#async-sqlite-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/async-sqlite/usage/async-sqlite-for-agent

Agno supports using Sqlite asynchronously as a storage backend for Agents, with the `AsyncSqliteDb` class.

```python async_sqlite_for_agent.py theme={null}
"""
Run `pip install openai ddgs sqlalchemy aiosqlite` to install dependencies.
"""
import asyncio

from agno.agent import Agent
from agno.db.sqlite import AsyncSqliteDb
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Start instrumenting agno

**URL:** llms-txt#start-instrumenting-agno

AgnoInstrumentor().instrument()

---

## OpenWeather

**URL:** llms-txt#openweather

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/openweather

**OpenWeatherTools** enable an Agent to access weather data from the OpenWeatherMap API.

The following example requires the `requests` library and an API key which can be obtained from [OpenWeatherMap](https://openweathermap.org/api). Once you sign up the mentioned api key will be activated in a few hours so please be patient.

The following agent will use OpenWeatherMap to get current weather information for Tokyo.

```python cookbook/tools/openweather_tools.py theme={null}
from agno.agent import Agent
from agno.tools.openweather import OpenWeatherTools

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use OpenWeatherMap to get current weather information for Tokyo.
```

---

## Team execution creates parent span with child spans for each agent

**URL:** llms-txt#team-execution-creates-parent-span-with-child-spans-for-each-agent

result = team.run("Write a brief overview of OpenTelemetry observability")
print(result.content)
python  theme={null}
from traceloop.sdk import Traceloop
from traceloop.sdk.decorators import workflow
from agno.agent import Agent
from agno.models.openai import OpenAIChat

Traceloop.init(app_name="agno_workflows")

agent = Agent(
    name="AnalysisAgent",
    model=OpenAIChat(id="gpt-4o-mini"),
    debug_mode=True,
)

@workflow(name="data_analysis_pipeline")
def analyze_data(query: str) -> str:
    """Custom workflow that wraps agent execution."""
    response = agent.run(query)
    return response.content

**Examples:**

Example 1 (unknown):
```unknown
* ### Example: Using Workflow Decorators

Use the `@workflow` decorator to create custom spans for organizing your traces:
```

---

## 2. MEDICAL CONSULTATION WORKFLOW

**URL:** llms-txt#2.-medical-consultation-workflow

---

## LanceDB Hybrid Search

**URL:** llms-txt#lancedb-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/lancedb/usage/lance-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom landing page (conflicts with AgentOS home route)

**URL:** llms-txt#custom-landing-page-(conflicts-with-agentos-home-route)

@app.get("/")
async def get_custom_home():
    return {
        "message": "Custom FastAPI App",
        "note": "Using on_route_conflict=\"preserve_base_app\" to preserve custom routes",
    }

---

## Create workflow

**URL:** llms-txt#create-workflow

**Contents:**
- Developer Resources

workflow = Workflow(
    name="Content Workflow",
    db=db,
    steps=[research_step, content_step],
)

workflow.print_response("AI trends in 2024")
```

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/in_memory/in_memory_storage_for_workflow.py)

---

## Agent Input as Messages List

**URL:** llms-txt#agent-input-as-messages-list

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/input-output/agent/usage/input-as-messages-list

This example demonstrates how to pass input to an agent as a list of Message objects, allowing for multi-turn conversations and context setup.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Create the Agent

**URL:** llms-txt#create-the-agent

agno_agent = Agent(
    name="Agno Agent",
    model=Claude(id="claude-sonnet-4-5"),
    # Add a database to the Agent
    db=SqliteDb(db_file="agno.db"),
    # Add the Agno MCP server to the Agent
    tools=[MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")],
    # Add the previous session history to the context
    add_history_to_context=True,
    markdown=True,
)

---

## More example prompts to explore:

**URL:** llms-txt#more-example-prompts-to-explore:

**Contents:**
- What to Expect
- Usage
- Next Steps

"""
Tutorial Analysis:
1. "Break down this Python tutorial with focus on code examples"
2. "Create a learning path from this web development course"
3. "Extract all practical exercises from this programming guide"
4. "Identify key concepts and implementation examples"

Educational Content:
1. "Create a study guide with timestamps for this math lecture"
2. "Extract main theories and examples from this science video"
3. "Break down this historical documentary into key events"
4. "Summarize the main arguments in this academic presentation"

Tech Reviews:
1. "List all product features mentioned with timestamps"
2. "Compare pros and cons discussed in this review"
3. "Extract technical specifications and benchmarks"
4. "Identify key comparison points and conclusions"

Creative Content:
1. "Break down the techniques shown in this art tutorial"
2. "Create a timeline of project steps in this DIY video"
3. "List all tools and materials mentioned with timestamps"
4. "Extract tips and tricks with their demonstrations"
"""
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai youtube_transcript_api
    bash Mac theme={null}
      python youtube_agent.py
      bash Windows theme={null}
      python youtube_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Try analyzing different video types (tutorials, lectures, reviews)
* Modify `instructions` to focus on specific content types
* Combine with other tools for enhanced analysis
* Explore [Tools](/basics/tools/overview) for additional capabilities

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The agent analyzes YouTube videos by fetching transcripts and generating comprehensive breakdowns. For a typical video, you'll receive:

* Video metadata (title, duration, type, and audience)
* High-level structure overview
* Timestamped breakdown of major topics with key examples
* Content organization showing recurring themes
* Practical highlights and actionable takeaways

Analysis typically takes 30-60 seconds depending on video length and complexity.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ag infra create

**URL:** llms-txt#ag-infra-create

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/create

Create a new infra in the current directory.

<ResponseField name="name" type="str">
  Name of the new infra. `--name` `-n`
</ResponseField>

<ResponseField name="template" type="str">
  Starter template for the infra. `--template` `-t`
</ResponseField>

<ResponseField name="url" type="str">
  URL of the starter template. `--url` `-u`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

---

## Books Recommender

**URL:** llms-txt#books-recommender

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/books-recommender

This example shows how to create an intelligent book recommendation system that provides
comprehensive literary suggestions based on your preferences. The agent combines book databases,
ratings, reviews, and upcoming releases to deliver personalized reading recommendations.

Example prompts to try:

* "I loved 'The Seven Husbands of Evelyn Hugo' and 'Daisy Jones & The Six', what should I read next?"
* "Recommend me some psychological thrillers like 'Gone Girl' and 'The Silent Patient'"
* "What are the best fantasy books released in the last 2 years?"
* "I enjoy historical fiction with strong female leads, any suggestions?"
* "Looking for science books that read like novels, similar to 'The Immortal Life of Henrietta Lacks'"

```python books_recommender.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

book_recommendation_agent = Agent(
    name="Shelfie",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
        You are Shelfie, a passionate and knowledgeable literary curator with expertise in books worldwide! ðŸ“š

Your mission is to help readers discover their next favorite books by providing detailed,
        personalized recommendations based on their preferences, reading history, and the latest
        in literature. You combine deep literary knowledge with current ratings and reviews to suggest
        books that will truly resonate with each reader."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:

1. Analysis Phase ðŸ“–
           - Understand reader preferences from their input
           - Consider mentioned favorite books' themes and styles
           - Factor in any specific requirements (genre, length, content warnings)

2. Search & Curate ðŸ”
           - Use Exa to search for relevant books
           - Ensure diversity in recommendations
           - Verify all book data is current and accurate

3. Detailed Information ðŸ“
           - Book title and author
           - Publication year
           - Genre and subgenres
           - Goodreads/StoryGraph rating
           - Page count
           - Brief, engaging plot summary
           - Content advisories
           - Awards and recognition

4. Extra Features âœ¨
           - Include series information if applicable
           - Suggest similar authors
           - Mention audiobook availability
           - Note any upcoming adaptations

Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar books together
        - Add emoji indicators for genres (ðŸ“š ðŸ”® ðŸ’• ðŸ”ª)
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
        - Highlight diversity in authors and perspectives
        - Note trigger warnings when relevant"""),
    markdown=True,
    add_datetime_to_context=True,
    )

---

## Unknown operator

**URL:** llms-txt#unknown-operator

curl ... -F 'knowledge_filters={"op": "UNKNOWN", "key": "status", "value": "x"}'

---

## Configure the tracer provider

**URL:** llms-txt#configure-the-tracer-provider

tracer_provider = TracerProvider()
tracer_provider.add_span_processor(
    SimpleSpanProcessor(OTLPSpanExporter(endpoint=endpoint, headers=headers))
)
trace_api.set_tracer_provider(tracer_provider=tracer_provider)

---

## Chain them together in a workflow

**URL:** llms-txt#chain-them-together-in-a-workflow

content_workflow = Workflow(
    name="Content Creation",
    steps=[researcher, writer]
)

---

## YouTube Agent

**URL:** llms-txt#youtube-agent

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/agents/youtube-agent

Build an AI agent that analyzes YouTube videos and creates structured summaries with accurate timestamps. This agent extracts key insights from video content, making it easy to navigate educational videos, tutorials, and presentations without watching them in full.

By building this agent, you'll understand:

* How to integrate YouTube transcript extraction into agents
* How to structure prompts for consistent timestamp generation
* How to organize video content into logical sections
* How to create agents that transform unstructured media into searchable content

Create study guides from lectures, extract insights from conference talks, build searchable video indexes, or generate documentation from tutorial videos.

The agent uses YouTubeTools to fetch video transcripts and metadata, then analyzes the content to:

1. **Extract**: Gets video metadata (title, duration) and full transcript
2. **Analyze**: Identifies video type and content structure
3. **Organize**: Creates timestamps for major topic transitions
4. **Summarize**: Generates section-based summaries with key points

The structured output makes long-form video content quickly scannable and searchable.

```python youtube_agent.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.youtube import YouTubeTools

youtube_agent = Agent(
    name="YouTube Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YouTubeTools()],
    instructions=dedent("""\
        You are an expert YouTube content analyst with a keen eye for detail! ðŸŽ“
        Follow these steps for comprehensive video analysis:
        1. Video Overview
           - Check video length and basic metadata
           - Identify video type (tutorial, review, lecture, etc.)
           - Note the content structure
        2. Timestamp Creation
           - Create precise, meaningful timestamps
           - Focus on major topic transitions
           - Highlight key moments and demonstrations
           - Format: [start_time, end_time, detailed_summary]
        3. Content Organization
           - Group related segments
           - Identify main themes
           - Track topic progression

Your analysis style:
        - Begin with a video overview
        - Use clear, descriptive segment titles
        - Include relevant emojis for content types:
          ðŸ“š Educational
          ðŸ’» Technical
          ðŸŽ® Gaming
          ðŸ“± Tech Review
          ðŸŽ¨ Creative
        - Highlight key learning points
        - Note practical demonstrations
        - Mark important references

Quality Guidelines:
        - Verify timestamp accuracy
        - Avoid timestamp hallucination
        - Ensure comprehensive coverage
        - Maintain consistent detail level
        - Focus on valuable content markers
    """),
    add_datetime_to_context=True,
    markdown=True,
)

---

## Create agents and team

**URL:** llms-txt#create-agents-and-team

research_agent = Agent(
    name="Research Agent",
    model=OpenAIChat("gpt-5-mini"),
    tools=[HackerNewsTools(), DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Agent",
    model=OpenAIChat("gpt-5-mini"),
)

---

## Print metrics per message

**URL:** llms-txt#print-metrics-per-message

if run_output.messages:
    for message in run_output.messages:  # type: ignore
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

---

## What is Chunking?

**URL:** llms-txt#what-is-chunking?

**Contents:**
- Available Chunking Strategies
- Using Chunking Strategies

Source: https://docs.agno.com/basics/knowledge/chunking/overview

Chunking is the process of breaking down large documents into smaller pieces for effective vector search and retrieval.

Chunking is the process of dividing content into manageable pieces before converting them into embeddings and storing them in vector databases. The chunking strategy you choose directly impacts search quality and retrieval accuracy.

Different chunking strategies serve different purposes. For example, when processing a recipe book, different strategies produce different results:

* **Fixed Size**: Splits text every 500 characters (which may break recipes mid-instruction)
* **Semantic**: Keeps complete recipes together based on meaning
* **Document**: Each page becomes a chunk

The strategy affects whether you get complete, relevant results or fragmented pieces.

## Available Chunking Strategies

<CardGroup cols={3}>
  <Card title="Fixed Size Chunking" icon="ruler" href="/basics/knowledge/chunking/fixed-size-chunking">
    Split content into uniform chunks with specified size and overlap.
  </Card>

<Card title="Semantic Chunking" icon="brain" href="/basics/knowledge/chunking/semantic-chunking">
    Use semantic similarity to identify natural breakpoints in content.
  </Card>

<Card title="Recursive Chunking" icon="sitemap" href="/basics/knowledge/chunking/recursive-chunking">
    Recursively split content using multiple separators for hierarchical processing.
  </Card>

<Card title="Document Chunking" icon="file-lines" href="/basics/knowledge/chunking/document-chunking">
    Preserve document structure by treating sections as individual chunks.
  </Card>

<Card title="CSV Row Chunking" icon="table" href="/basics/knowledge/chunking/csv-row-chunking">
    Splits CSV files by treating each row as an individual chunk. Only compatible with CSVs.
  </Card>

<Card title="Markdown Chunking" icon="markdown" href="/basics/knowledge/chunking/markdown-chunking">
    Split markdown content while preserving heading structure and hierarchy. Only compatible with Markdown files.
  </Card>

<Card title="Agentic Chunking" icon="robot" href="/basics/knowledge/chunking/agentic-chunking">
    Use AI to intelligently determine optimal chunk boundaries.
  </Card>

<Card title="Custom Chunking" icon="code" href="/basics/knowledge/chunking/custom-chunking">
    Build your own chunking strategy for specialized use cases.
  </Card>
</CardGroup>

## Using Chunking Strategies

Chunking strategies are configured when setting up readers for your knowledge base:

```python  theme={null}
from agno.knowledge.chunking.semantic import SemanticChunking
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.db.postgres import PostgresDb

---

## SurreabDB

**URL:** llms-txt#surreabdb

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/surrealdb/overview

Learn to use SurreabDB as a database for your Agents

Agno supports using [SurreabDB](https://surrealdb.com/) as a database with the `SurrealDb` class.

You can get started with SurreabDB following their [documentation](https://surrealdb.com/docs).

Run SurreabDB locally with the following command:

```python surrealdb_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.surrealdb import SurrealDb

**Examples:**

Example 1 (unknown):
```unknown
## Usage
```

---

## Crawl4AI

**URL:** llms-txt#crawl4ai

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/crawl4ai

**Crawl4aiTools** enable an Agent to perform web crawling and scraping tasks using the Crawl4ai library.

The following example requires the `crawl4ai` library.

The following agent will scrape the content from the [https://github.com/agno-agi/agno](https://github.com/agno-agi/agno) webpage:

| Parameter           | Type    | Default              | Description                                                                               |
| ------------------- | ------- | -------------------- | ----------------------------------------------------------------------------------------- |
| `max_length`        | `int`   | `1000`               | Specifies the maximum length of the text from the webpage to be returned.                 |
| `timeout`           | `int`   | `60`                 | Timeout in seconds for web crawling operations.                                           |
| `use_pruning`       | `bool`  | `False`              | Enable content pruning to remove less relevant content.                                   |
| `pruning_threshold` | `float` | `0.48`               | Threshold for content pruning relevance scoring.                                          |
| `bm25_threshold`    | `float` | `1.0`                | BM25 scoring threshold for content relevance.                                             |
| `headless`          | `bool`  | `True`               | Run browser in headless mode.                                                             |
| `wait_until`        | `str`   | `"domcontentloaded"` | Browser wait condition before crawling (e.g., "domcontentloaded", "load", "networkidle"). |
| `enable_crawl`      | `bool`  | `True`               | Enable the web crawling functionality.                                                    |
| `all`               | `bool`  | `False`              | Enable all available functions. When True, all enable flags are ignored.                  |

| Function      | Description                                                                                                                                                                                                      |
| ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `web_crawler` | Crawls a website using crawl4ai's WebCrawler. Parameters include 'url' for the URL to crawl and an optional 'max\_length' to limit the length of extracted content. The default value for 'max\_length' is 1000. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/crawl4ai.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/crawl4ai_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will scrape the content from the [https://github.com/agno-agi/agno](https://github.com/agno-agi/agno) webpage:
```

---

## For batch loading

**URL:** llms-txt#for-batch-loading

**Contents:**
  - 3. Use Metadata Filters

knowledge.add_contents(
    paths=["docs/", "policies/"],
    skip_if_exists=True,
    include=["*.pdf", "*.md"],
    exclude=["*temp*", "*draft*"]
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3. Use Metadata Filters

Narrow searches before vector comparison for faster, more accurate results:
```

---

## Run the Agent. This will store a session in our "my_memory_table"

**URL:** llms-txt#run-the-agent.-this-will-store-a-session-in-our-"my_memory_table"

**Contents:**
  - Manual Memory Retrieval

agent.print_response("Hi! My name is John Doe and I like to play basketball on the weekends.")

agent.print_response("What are my hobbies?")
python  theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

**Examples:**

Example 1 (unknown):
```unknown
### Manual Memory Retrieval

While memories are automatically recalled during conversations, you can also manually retrieve them using the `get_user_memories` method. This is useful for debugging, displaying user profiles, or building custom memory interfaces:
```

---

## Memories

**URL:** llms-txt#memories

**Contents:**
- Overview
- Accessing Memories
- Memory Management
- Privacy and Control
- Useful Links

Source: https://docs.agno.com/agent-os/features/memories

View and manage persistent memory storage for your agents in AgentOS

The Memories feature in AgentOS provides a centralized view of information that agents have learned and stored about you as a user. Memory gives agents the ability to recall information about you across conversations, enabling personalized and contextual interactions.

* Memories are created and updated during an agent run
* Each memory is tied to a specific user ID and contains learned information
* Memories include content, topics, timestamps, and the input that generated them
* Agents with memory enabled can learn about you and provide more relevant responses over time

<Note>
  <strong>Prerequisites</strong>: Your AgentOS must be connected and active. If
  you see "Disconnected" or "Inactive," review your{" "}
  <a href="/agent-os/connecting-your-os">connection settings</a>.
</Note>

## Accessing Memories

* Open the `Memory` section in the sidebar.
* View all stored memories in a chronological table format.
* Click the `Refresh` button to sync the latest memory updates.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/04J6ekYOTyb3RbcL/videos/memory-table-usage.mp4?fit=max&auto=format&n=04J6ekYOTyb3RbcL&q=85&s=4afbe866cc00ed3799f270e642fa842f" type="video/mp4" data-path="videos/memory-table-usage.mp4" />
  </video>
</Frame>

The memory interface allows you to:

1. **Create memories** - Create memories your agents can reference during chat sessions
2. **View by topics** - See memories organized by thematic categories
3. **Edit memories** - Update or correct stored information as needed
4. **Delete memories** - Remove outdated or incorrect information
5. **Monitor memory creation** - See when and from what inputs memories were generated

<Tip>
  Memories are automatically generated from your conversations, but you can also
  manually create, edit, or remove them.
</Tip>

## Privacy and Control

* All memories are tied to a specific user ID and stored in your AgentOS database
* Memories are only accessible to agents within your connected OS instance
* Memory data remains within your deployment and is never shared externally

<CardGroup cols={3}>
  <Card title="Memory Concepts" icon="brain" href="/basics/memory/overview">
    Learn how memory works and how agents learn about users
  </Card>

<Card title="Privacy & Security" icon="shield-check" href="/agent-os/security">
    Understand data protection and privacy features
  </Card>
</CardGroup>

---

## Use the agent - traces will be printed to console

**URL:** llms-txt#use-the-agent---traces-will-be-printed-to-console

**Contents:**
  - Example: Multi-Agent Team Tracing

agent.print_response("What are the latest developments in AI agents?")
python  theme={null}
import openlit
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

**Examples:**

Example 1 (unknown):
```unknown
### Example: Multi-Agent Team Tracing

OpenLIT automatically traces complex multi-agent workflows:
```

---

## Step 1: Custom classifier function to assign tags

**URL:** llms-txt#step-1:-custom-classifier-function-to-assign-tags

def classify_query(step_input: StepInput) -> StepOutput:
    """
    Classify the user query into one of the predefined tags.

Available tags: travel, tech, general-blogs, fashion, documents
    """
    # Get the user query from step_input
    query = step_input.input

# Create an agent to classify the query
    classifier_agent = Agent(
        model=OpenAIChat(id="gpt-4o-mini"),
        instructions=[
            "You are a query classifier.",
            "Classify the given query into ONE of these tags: travel, tech, general-blogs, fashion, documents",
            "Only respond with the tag name, nothing else.",
            "Classification rules:",
            "- travel: Anything related to destinations, tours, trips, locations, hotels, travel guides, places to visit",
            "- tech: Programming, software, AI, machine learning, coding, development, technology topics",
            "- fashion: Clothing, style, trends, outfits, fashion industry",
            "- documents: Resumes, CVs, reports, official documents, contracts",
            "- general-blogs: Personal thoughts, opinions, life advice, miscellaneous content",
            "",
            "Examples:",
            "- 'Best places to visit in Italy' -> travel",
            "- 'Ha Giang loop tour Vietnam guide' -> travel",
            "- 'Add travel guide website link' -> travel",
            "- 'How to build a React app' -> tech",
            "- 'The rise of AI and machine learning' -> tech",
            "- 'Fashion trends 2025' -> fashion",
            "- 'My resume and CV' -> documents",
            "- 'Random thoughts about life' -> general-blogs",
        ],
    )

# Get classification
    response = classifier_agent.run(query)
    tag = response.content.strip().lower()

# Validate the tag
    valid_tags = ["travel", "tech", "general-blogs", "fashion", "documents"]
    if tag not in valid_tags:
        tag = "general-blogs"  # Default fallback

# Return structured data using Pydantic model
    result = ClassificationResult(
        query=str(query), tag=tag, message=f"Query classified as: {tag}"
    )

return StepOutput(content=result)

---

## Get the combined app with both AgentOS and your routes

**URL:** llms-txt#get-the-combined-app-with-both-agentos-and-your-routes

**Contents:**
  - Adding Middleware
  - Running with FastAPI CLI
  - Running in Production
- Adding Custom Routers

app = agent_os.get_app()
bash Install FastAPI CLI theme={null}
pip install "fastapi[standard]"
bash Run with FastAPI CLI theme={null}
  fastapi run your_app.py
  bash Run with auto-reload theme={null}
  fastapi run your_app.py --reload
  bash Custom host and port theme={null}
  fastapi run your_app.py --host 0.0.0.0 --port 8000
  bash Uvicorn theme={null}
  uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
  bash Gunicorn with Uvicorn workers theme={null}
  gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
  bash FastAPI CLI (Production) theme={null}
  fastapi run main.py --host 0.0.0.0 --port 8000
  python custom_fastapi_app.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import FastAPI

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Your custom FastAPI app can have its own middleware and dependencies.

  If you have your own CORS middleware, it will be updated to include the AgentOS allowed origins, to make the AgentOS instance compatible with the Control Plane.
  Otherwise, the appropriate CORS middleware will be added to the app.
</Tip>

### Adding Middleware

You can add any FastAPI middleware to your custom FastAPI app and it will be respected by AgentOS.

Agno also provides some built-in middleware for common use cases, including authentication.

See the [Middleware](/agent-os/middleware/overview) page for more details.

### Running with FastAPI CLI

AgentOS applications are compatible with the [FastAPI CLI](https://fastapi.tiangolo.com/deployment/manually/) for development.

First, install the FastAPI CLI:
```

Example 2 (unknown):
```unknown
Then run the app:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## PDF Input Bytes Agent

**URL:** llms-txt#pdf-input-bytes-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/pdf-input-bytes

```python cookbook/models/anthropic/pdf_input_bytes.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

---

## Create a team for collaborative video caption generation

**URL:** llms-txt#create-a-team-for-collaborative-video-caption-generation

**Contents:**
- Usage

caption_team = Team(
    name="Video Caption Team",
    members=[video_processor, caption_generator],
    model=OpenAIChat(id="gpt-5-mini"),
    description="Team that generates and embeds captions for videos",
    instructions=[
        "Process videos to generate captions in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)

caption_team.print_response(
    "Generate captions for {video with location} and embed them in the video"
)
bash  theme={null}
    pip install agno moviepy ffmpeg-python
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/video_caption_generation.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Step 1: Initialize knowledge with documents and metadata

**URL:** llms-txt#step-1:-initialize-knowledge-with-documents-and-metadata

---

## What is Culture?

**URL:** llms-txt#what-is-culture?

**Contents:**
- How Culture Works
- Why Use Culture?
- Getting Started with Culture

Source: https://docs.agno.com/basics/culture/overview

Enable your agents to share universal knowledge, principles, and best practices that compound across all interactions.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.10" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.10">v2.1.10</Tooltip>
</Badge>

Imagine agents that learn from every interaction and share those learnings with each other.
A support agent discovers that customers prefer step-by-step solutions with code examples.
A technical writer agent learns that "Operational Thinking" produces better documentation.
These insights shouldn't be lostâ€”they should become part of a shared culture that benefits all agents, forever. Culture turns these patterns into reusable rules your agents can follow from day one.

Culture provides a shared knowledge layer where agents store universal principles, best practices, and reusable insights that apply across all interactions. Unlike Memory, which stores user-specific facts ("Sarah prefers email"), Culture stores universal knowledge that benefits everyone ("Always provide actionable solutions with clear next steps").

When an agent completes a task, it can reflect on what worked well and distill that into cultural knowledge. Later, when any agent faces a similar situation, it automatically accesses this shared culture and applies those learnings.

<Tip>
  **Culture â‰  Memory:** Culture stores universal principles and best practices that apply to all interactions. [Memory](/basics/memory/overview) stores user-specific facts and preferences. Think of Culture as "how we do things here" and Memory as "what I know about you."
</Tip>

<Note>
  **Notice:** Culture is an experimental feature and is subject to change. The current goal is helping agents stay consistent in tone, reasoning, and behavior. The eventual goal is to transform isolated agents into a living, evolving system of collective intelligence.
</Note>

Culture enables intelligence to compound. Instead of each agent starting from scratch, they build on collective experience:

* **Consistency:** All agents follow the same communication standards, formatting rules, and best practices
* **Evolution:** Your agent system improves over time as agents learn what works
* **Efficiency:** Agents don't re-learn the same lessons repeatedly

**Example Use Cases:**

* Technical documentation agents that maintain consistent style and structure
* Customer support teams that apply proven problem-solving patterns
* Development assistants that follow your organization's coding standards
* Content generation that adheres to brand voice and formatting guidelines

## Getting Started with Culture

Setting up culture is straightforward: connect a database and enable the culture feature. Here's a basic setup:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## Context Management with DateTime Instructions

**URL:** llms-txt#context-management-with-datetime-instructions

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/context/agent/usage/datetime-instructions

This example demonstrates how to add current date and time context to agent instructions, enabling the agent to provide time-aware responses.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## -*- Print memories and summary

**URL:** llms-txt#-*--print-memories-and-summary

if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

---

## Reliability with Teams

**URL:** llms-txt#reliability-with-teams

Source: https://docs.agno.com/basics/evals/reliability/usage/reliability-with-teams

Example showing how to assert an Agno Team is making the expected tool calls.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print the response in markdown format

**URL:** llms-txt#print-the-response-in-markdown-format

**Contents:**
- Run Output
- Streaming

pprint_run_response(response, markdown=True)
python  theme={null}
from typing import Iterator
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat

news_agent = Agent(name="News Agent", role="Get the latest news")
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o")
)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  You can also run the team asynchronously using `Team.arun()`. This means members will run concurrently if the team leader delegates to multiple members in one request.
</Tip>

<Tip>
  See the [Input & Output](/basics/input-output/overview) docs for more information, and to see how to use structured input and output with teams.
</Tip>

## Run Output

The `Team.run()` function returns a `TeamRunOutput` object when not streaming. This object contains the output content, the list of messages sent to the model, the metrics of the run, the model used for the run, and an optional list of member responses.

See the detailed schema in the [TeamRunOutput](/reference/teams/team-response) documentation.

## Streaming

To enable streaming, set `stream=True` when calling `run()`. This will return an iterator of `TeamRunOutputEvent` objects instead of a single `TeamRunOutput` object.
```

---

## Create stock research agent

**URL:** llms-txt#create-stock-research-agent

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a stock.",
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com", "wsj.com"],
            text=False,
            show_results=True,
            highlights=False,
        )
    ],
)

---

## Demo Deepseek R1

**URL:** llms-txt#demo-deepseek-r1

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/local/ollama/usage/demo-deepseek-r1

```python cookbook/models/ollama/demo_deepseek_r1.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="deepseek-r1:14b"), markdown=True)

---

## Exa

**URL:** llms-txt#exa

**Contents:**
- Prerequisites
- Example
- Toolkit Functions
- Toolkit Params
  - Categories
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/exa

**ExaTools** enable an Agent to search the web using Exa, retrieve content from URLs, find similar content, and get AI-powered answers.

The following examples require the `exa-py` library and an API key which can be obtained from [Exa](https://exa.ai).

The following agent will search Exa for AAPL news and print the response.

| Function       | Description                                                      |
| -------------- | ---------------------------------------------------------------- |
| `search_exa`   | Searches Exa for a query with optional category filtering        |
| `get_contents` | Retrieves detailed content from specific URLs                    |
| `find_similar` | Finds similar content to a given URL                             |
| `exa_answer`   | Gets an AI-powered answer to a question using Exa search results |

| Parameter              | Type                  | Default    | Description                                        |
| ---------------------- | --------------------- | ---------- | -------------------------------------------------- |
| `enable_search`        | `bool`                | `True`     | Enable search functionality                        |
| `enable_get_contents`  | `bool`                | `True`     | Enable content retrieval                           |
| `enable_find_similar`  | `bool`                | `True`     | Enable finding similar content                     |
| `enable_answer`        | `bool`                | `True`     | Enable AI-powered answers                          |
| `enable_research`      | `bool`                | `True`     | Enable research functionality                      |
| `all`                  | `bool`                | `False`    | Enable all functionality                           |
| `text`                 | `bool`                | `True`     | Include text content in results                    |
| `text_length_limit`    | `int`                 | `1000`     | Maximum length of text content per result          |
| `highlights`           | `bool`                | `True`     | Include highlighted snippets                       |
| `summary`              | `bool`                | `False`    | Include result summaries                           |
| `num_results`          | `Optional[int]`       | `None`     | Default number of results                          |
| `livecrawl`            | `str`                 | `"always"` | Livecrawl behavior                                 |
| `start_crawl_date`     | `Optional[str]`       | `None`     | Include results crawled after date (YYYY-MM-DD)    |
| `end_crawl_date`       | `Optional[str]`       | `None`     | Include results crawled before date (YYYY-MM-DD)   |
| `start_published_date` | `Optional[str]`       | `None`     | Include results published after date (YYYY-MM-DD)  |
| `end_published_date`   | `Optional[str]`       | `None`     | Include results published before date (YYYY-MM-DD) |
| `use_autoprompt`       | `Optional[bool]`      | `None`     | Enable autoprompt features                         |
| `type`                 | `Optional[str]`       | `None`     | Content type filter (e.g., article, blog, video)   |
| `category`             | `Optional[str]`       | `None`     | Category filter (e.g., news, research paper)       |
| `include_domains`      | `Optional[List[str]]` | `None`     | Restrict results to these domains                  |
| `exclude_domains`      | `Optional[List[str]]` | `None`     | Exclude results from these domains                 |
| `show_results`         | `bool`                | `False`    | Log search results for debugging                   |
| `model`                | `Optional[str]`       | `None`     | Search model to use ('exa' or 'exa-pro')           |

Available categories for filtering:

* company
* research paper
* news
* pdf
* github
* tweet
* personal site
* linkedin profile
* financial report

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/exa.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/exa_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search Exa for AAPL news and print the response.
```

---

## Option A: Use CultureManager with a model to process principles

**URL:** llms-txt#option-a:-use-culturemanager-with-a-model-to-process-principles

culture_manager = CultureManager(
    db=db,
    model=Claude(id="claude-sonnet-4-5"),
)

message = """
All technical guidance should follow 'Operational Thinking':
1. State the Objective â€” What outcome and why
2. Show the Procedure â€” Clear, reproducible steps
3. Surface Pitfalls â€” What usually fails
4. Define Validation â€” How to confirm it works
5. Close the Loop â€” Suggest next iterations
"""

culture_manager.create_cultural_knowledge(message=message)

---

## Claude with Reasoning Tools

**URL:** llms-txt#claude-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/claude-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Anthropic API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Anthropic API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Preview optimization without saving

**URL:** llms-txt#preview-optimization-without-saving

preview = memory_manager.optimize_memories(
    user_id="user_123",
    strategy=MemoryOptimizationStrategyType.SUMMARIZE,
    apply=False,  # Don't save changes
)

---

## Build a Social Media Intelligence Agent with Agno, X Tools, and Exa

**URL:** llms-txt#build-a-social-media-intelligence-agent-with-agno,-x-tools,-and-exa

**Contents:**
- What You'll Build
- Prerequisites and Setup

Source: https://docs.agno.com/tutorials/social-media-agent

Create a professional-grade social media intelligence system using Agno.

In this tutorial, we will build a multi-agent intelligence system. It will monitor X (Twitter), perform sentiment analysis, and generate reports using Agno framework.

We will be using the following components:

* **Agno** - The fastest framework for building agents.
* **X Tools** - Provides real-time, structured data directly from Twitter/X API with engagement metrics
* **Exa Tools** - Deliver semantic web search for broader context discovery across blogs, forums, and news
* **GPT-5 Mini** - OpenAI's new model. Well suited for contextually-aware sentiment analysis and strategic pattern detection

This system will combine direct social media data with broader web intelligence, to provide comprehensive brand monitoring that captures both immediate social sentiment and emerging discussions before they reach mainstream attention.

Your social media intelligence system will:

* Track brand and competitor mentions across X and the broader web
* Perform weighted sentiment analysis that accounts for influence and engagement
* Detect viral content, controversy signals, and high-influence discussions
* Generate executive-ready reports with strategic recommendations
* Serve insights via [AgentOS](/agent-os/introduction) API for integration with your applications

## Prerequisites and Setup

Before we get started, we need to setup our environment:

1. Install Python, Git and get your API keys:

* Install **Python >= 3.9** and **Git**
* Get API keys for:
  * **X (Twitter) Developer Account** ([Apply here](https://developer.twitter.com/en/apply-for-access))
  * **OpenAI API** ([Get key](https://platform.openai.com/api-keys))
  * **Exa API** ([Sign up](https://exa.ai))

2. Setup your Python environment:

3. Install our Python dependencies:

````bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
3. Install our Python dependencies:
```

---

## 7. Print results

**URL:** llms-txt#7.-print-results

**Contents:**
- Usage

print("\n--- Generated Shorts ---")
for short in shorts:
    print(f"Short at {short['path']}")
    print(f"Description: {short['description']}")
    print(f"Engagement Score: {short['score']}/10\n")
bash  theme={null}
    pip install -U agno opencv-python sqlalchemy
    bash Mac theme={null}
      brew install ffmpeg
      bash Windows   theme={null}
      # Install ffmpeg from https://ffmpeg.org/download.html
      bash Mac/Linux theme={null}
       export GOOGLE_API_KEY="your_google_api_key_here"
      bash Windows theme={null}
        $Env:GOOGLE_API_KEY="your_google_api_key_here"
      bash  theme={null}
    touch video_to_shorts.py
    bash Mac theme={null}
      python video_to_shorts.py
      bash Windows   theme={null}
      python video_to_shorts.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install ffmpeg">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Export your GOOGLE API key">
    <CodeGroup>
```

---

## Run the same query twice to demonstrate caching

**URL:** llms-txt#run-the-same-query-twice-to-demonstrate-caching

**Contents:**
- Usage

for i in range(1, 3):
    print(f"\n{'=' * 60}")
    print(
        f"Run {i}: {'Cache Miss (First Request)' if i == 1 else 'Cache Hit (Cached Response)'}"
    )
    print(f"{'=' * 60}\n")

response = agent.run(
        "Write me a short story about a cat that can talk and solve problems."
    )
    print(response.content)
    print(f"\n Elapsed time: {response.metrics.duration:.3f}s")

# Small delay between iterations for clarity
    if i == 1:
        time.sleep(0.5)
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
        python cache_model_response.py
      bash Windows theme={null}
        python cache_model_response.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## AgentOS Authentication

**URL:** llms-txt#agentos-authentication

**Contents:**
- Authentication Methods
  - Using Bearer Token Authentication
  - Configuration
- Developer Resources

Source: https://docs.agno.com/agent-os/api/authentication

Learn how to authenticate with AgentOS

AgentOS supports bearer-token authentication to secure your API endpoints and protect your agentic systems from unauthorized access.

## Authentication Methods

When a Security Key is configured, all API routes require an `Authorization: Bearer <token>` header for access. Without a key configured, authentication is disabled and all endpoints are publicly accessible.

### Using Bearer Token Authentication

Include the authorization header in your API requests:

Set your security key when initializing AgentOS or through the configuration file. See the [AgentOS Security](/agent-os/security) guide for detailed setup instructions.

## Developer Resources

* See [AgentOS Security](/agent-os/security) for comprehensive security configuration

---

## PromptInjectionGuardrail

**URL:** llms-txt#promptinjectionguardrail

**Contents:**
- Parameters
- Injection patterns

Source: https://docs.agno.com/reference/hooks/prompt-injection-guardrail

| Parameter            | Type                  | Default | Description                                                                              |
| -------------------- | --------------------- | ------- | ---------------------------------------------------------------------------------------- |
| `injection_patterns` | `Optional[List[str]]` | `None`  | A list of patterns to check for. Defaults to a list of common prompt injection patterns. |

## Injection patterns

The default list of injection patterns handled by the guardrail are:

* "ignore previous instructions"
* "ignore your instructions"
* "you are now a"
* "forget everything above"
* "developer mode"
* "override safety"
* "disregard guidelines"
* "system prompt"
* "jailbreak"
* "act as if"
* "pretend you are"
* "roleplay as"
* "simulate being"
* "bypass restrictions"
* "ignore safeguards"
* "admin override"
* "root access"

---

## Create an agent with knowledge

**URL:** llms-txt#create-an-agent-with-knowledge

**Contents:**
- Step 2: Add Your Content
- Step 3: Chat with Your Agent

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    knowledge=knowledge,
    # Enable automatic knowledge search
    search_knowledge=True,
    instructions=[
        "Always search your knowledge base before answering questions",
        "Include source references in your responses when possible"
    ]
)
python  theme={null}
  from agno.vectordb.pgvector import PgVector

# Local vector storage - no database setup required
  knowledge = Knowledge(
      vector_db=PgVector(db_url="postgresql+psycopg://ai:ai@localhost:5432/ai"),
  )
  python  theme={null}
    # Add a specific file
    knowledge.add_content(
        path="path/to/your/document.pdf"
    )

# Add an entire directory
    knowledge.add_content(
        path="path/to/documents/"
    )
    python  theme={null}
    # Add content from a website
    knowledge.add_content(
        url="https://docs.agno.com/introduction"
    )

# Add a PDF from the web
    knowledge.add_content(
        url="https://example.com/document.pdf"
    )
    python  theme={null}
    # Add text content directly
    knowledge.add_content(
        text_content="""
        Company Policy: Remote Work Guidelines

1. Remote work is available to all full-time employees
        2. Employees must maintain regular communication with their team
        3. Home office equipment is provided up to $1000 annually
        """
    )
    python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Don't have PostgreSQL? Use LanceDB instead">
  For a quick start without setting up PostgreSQL, use LanceDB which stores data locally:
```

Example 2 (unknown):
```unknown
</Accordion>

## Step 2: Add Your Content

Now let's add some knowledge to your agent. You can add content from various sources:

<Tabs>
  <Tab title="From Local Files">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="From URLs">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="From Text">
```

---

## Wait for the vector index to sync with KV

**URL:** llms-txt#wait-for-the-vector-index-to-sync-with-kv

---

## Get Memory by ID

**URL:** llms-txt#get-memory-by-id

Source: https://docs.agno.com/reference-api/schema/memory/get-memory-by-id

get /memories/{memory_id}
Retrieve detailed information about a specific user memory by its ID.

---

## -*- Share personal information

**URL:** llms-txt#-*--share-personal-information

agent.print_response("I'm going to a concert tomorrow?", stream=True)

---

## Create output directory

**URL:** llms-txt#create-output-directory

output_dir = Path(output_dir)
output_dir.mkdir(parents=True, exist_ok=True)

---

## Capture Reasoning Content with Knowledge Tools

**URL:** llms-txt#capture-reasoning-content-with-knowledge-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/capture-reasoning-content-knowledge-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The agent will use strict tool validation and return structured output

**URL:** llms-txt#the-agent-will-use-strict-tool-validation-and-return-structured-output

**Contents:**
- Usage

agent.print_response("What's the weather like in San Francisco?")
bash  theme={null}
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U anthropic agno
    bash  theme={null}
    python structured_output_strict_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Secondary knowledge base for targeted search

**URL:** llms-txt#secondary-knowledge-base-for-targeted-search

knowledge_secondary = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_secondary",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=InfinityReranker(
            base_url="http://localhost:7997/rerank", model="BAAI/bge-reranker-base"
        ),
    ),
)

---

## Create workflow with conditional early termination

**URL:** llms-txt#create-workflow-with-conditional-early-termination

workflow = Workflow(
    name="Data Processing with Early Exit",
    description="Process data but stop early if validation fails",
    steps=[
        data_validator,  # Step 1: Validate data
        early_exit_validator,  # Step 2: Check validation and possibly stop early
        data_processor,  # Step 3: Process data (only if validation passed)
        report_generator,  # Step 4: Generate report (only if processing completed)
    ],
)

if __name__ == "__main__":
    print("\n=== Testing with INVALID data ===")
    workflow.print_response(
        input="Process this data: {'user_count': -50, 'revenue': 'invalid_amount', 'date': 'bad_date'}"
    )

print("=== Testing with VALID data ===")
    workflow.print_response(
        input="Process this data: {'user_count': 1000, 'revenue': 50000, 'date': '2024-01-15'}"
    )
```

To checkout async version, see the cookbook-

* [Early Stop Workflow with Loop](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_basics/_02_early_stopping/early_stop_workflow_with_loop.py)
* [Early Stop Workflow with Parallel](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_basics/_02_early_stopping/early_stop_workflow_with_parallel.py)
* [Early Stop Workflow with Router](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_basics/_02_early_stopping/early_stop_workflow_with_router.py)
* [Early Stop Workflow with Step](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_basics/_02_early_stopping/early_stop_workflow_with_step.py)
* [Early Stop Workflow with Steps](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_basics/_02_early_stopping/early_stop_workflow_with_steps.py)

---

## Team with Chain of Thought

**URL:** llms-txt#team-with-chain-of-thought

Source: https://docs.agno.com/basics/reasoning/usage/agents/team-cot

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## response = agent.run(

**URL:** llms-txt#response-=-agent.run(

---

## Delete Multiple Sessions

**URL:** llms-txt#delete-multiple-sessions

Source: https://docs.agno.com/reference-api/schema/sessions/delete-multiple-sessions

delete /sessions
Delete multiple sessions by their IDs in a single operation. This action cannot be undone and will permanently remove all specified sessions and their runs.

---

## Upload and process video

**URL:** llms-txt#upload-and-process-video

video_file = upload_file(video_path)
while video_file.state.name == "PROCESSING":
    time.sleep(2)
    video_file = get_file(video_file.name)

---

## Async Basic Streaming Agent

**URL:** llms-txt#async-basic-streaming-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/perplexity/usage/async-basic-stream

```python cookbook/models/perplexity/async_basic_stream.py theme={null}
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

---

## Session State In Context

**URL:** llms-txt#session-state-in-context

Source: https://docs.agno.com/basics/state/agent/usage/session-state-in-context

This example demonstrates how to use session state with PostgreSQL database and manage user context across different sessions. It shows how session state persists and can be retrieved for different users and sessions.

<Steps>
  <Step title="Create a Python file">
    Create a file called `session_state_in_context.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Dynamic Instructions Based on Session State

**URL:** llms-txt#dynamic-instructions-based-on-session-state

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/context/agent/usage/dynamic-instructions

This example demonstrates how to create dynamic instructions that change based on session state, allowing personalized agent behavior for different users.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Configuration for the Memory page

**URL:** llms-txt#configuration-for-the-memory-page

memory:
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        display_name: <DISPLAY_NAME>
    ...

---

## Example: Ask the agent to list tables

**URL:** llms-txt#example:-ask-the-agent-to-list-tables

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "List the tables in the database and describe the sales table"
)
```

| Parameter            | Type            | Default  | Description                                                          |
| -------------------- | --------------- | -------- | -------------------------------------------------------------------- |
| `host`               | `Optional[str]` | `None`   | Redshift cluster endpoint. Uses `REDSHIFT_HOST`.                     |
| `port`               | `int`           | `5439`   | Port for the database connection.                                    |
| `database`           | `Optional[str]` | `None`   | Database name. Uses `REDSHIFT_DATABASE`.                             |
| `user`               | `Optional[str]` | `None`   | Username for standard authentication.                                |
| `password`           | `Optional[str]` | `None`   | Password for standard authentication.                                |
| `iam`                | `bool`          | `False`  | Enable IAM authentication.                                           |
| `cluster_identifier` | `Optional[str]` | `None`   | Cluster identifier for IAM auth. Uses `REDSHIFT_CLUSTER_IDENTIFIER`. |
| `region`             | `Optional[str]` | `None`   | AWS region for IAM auth. Uses `AWS_REGION`.                          |
| `db_user`            | `Optional[str]` | `None`   | Database user for IAM auth. Uses `REDSHIFT_DB_USER`.                 |
| `access_key_id`      | `Optional[str]` | `None`   | AWS access key for IAM auth. Uses `AWS_ACCESS_KEY_ID`.               |
| `secret_access_key`  | `Optional[str]` | `None`   | AWS secret key for IAM auth. Uses `AWS_SECRET_ACCESS_KEY`.           |
| `session_token`      | `Optional[str]` | `None`   | AWS session token for IAM auth. Uses `AWS_SESSION_TOKEN`.            |
| `profile`            | `Optional[str]` | `None`   | AWS profile for IAM auth. Uses `AWS_PROFILE`.                        |
| `ssl`                | `bool`          | `True`   | Enable SSL for the connection.                                       |
| `table_schema`       | `str`           | `public` | Schema name to search for tables.                                    |

| Function               | Description                                                                                                                                                                                                                                                                            |
| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `show_tables`          | Retrieves and displays a list of tables in the configured schema. Returns the list of tables.                                                                                                                                                                                          |
| `describe_table`       | Describes the structure of a specified table by returning its columns, data types, and nullability. Parameters include `table` (str) to specify the table name. Returns the table description.                                                                                         |
| `summarize_table`      | Summarizes a table by computing aggregates such as min, max, average, standard deviation, and non-null counts for numeric columns, or unique values and average length for text columns. Parameters include `table` (str) to specify the table name. Returns the summary of the table. |
| `inspect_query`        | Inspects an SQL query by returning the query plan using EXPLAIN. Parameters include `query` (str) to specify the SQL query. Returns the query plan.                                                                                                                                    |
| `run_query`            | Executes a read-only SQL query and returns the result. Parameters include `query` (str) to specify the SQL query. Returns the result of the query execution.                                                                                                                           |
| `export_table_to_path` | Exports a specified table in CSV format to a given path. Parameters include `table` (str) to specify the table name and `path` (str) to specify where to save the file. Returns the result of the export operation.                                                                    |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Example](/examples/concepts/tools/database/redshift)

---

## - Real-time memory updates during conversation

**URL:** llms-txt#--real-time-memory-updates-during-conversation

---

## Hybrid Search- Combining Keyword and Vector Search

**URL:** llms-txt#hybrid-search--combining-keyword-and-vector-search

**Contents:**
- What exactly is Hybrid Search?
- Keyword Search vs Vector Search vs Hybrid Search
- Vector DBs in Agno that Support Hybrid Search
- Example: Hybrid Search using `pgvector`

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/hybrid-search

Understanding Hybrid Search and its benefits in combining keyword and vector search for better results.

With Hybrid search, get the precision of exact matching with the intelligence of semantic understanding. Combining both approaches will deliver more comprehensive and relevant results in many cases.

## What exactly is Hybrid Search?

**Hybrid search** is a retrieval technique that combines the strengths of both **vector search** (semantic search) and **keyword search** (lexical search) to find the most relevant results for a query.

* Vector search uses embeddings (dense vectors) to capture the semantic meaning of text, enabling the system to find results that are similar in meaning, even if the exact words don't match.
* Keyword search (BM25, TF-IDF, etc.) matches documents based on the presence and frequency of exact words or phrases in the query.

Hybrid search blends these approaches, typically by scoring and/or ranking results from both methods, to maximize both precision and recall.

## Keyword Search vs Vector Search vs Hybrid Search

| Feature       | Keyword Search                  | Vector Search                             | Hybrid Search                             |
| ------------- | ------------------------------- | ----------------------------------------- | ----------------------------------------- |
| Based On      | Lexical matching (BM25, TF-IDF) | Embedding similarity (cosine, dot)        | Both                                      |
| Strength      | Exact matches, relevance        | Contextual meaning                        | Balanced relevance + meaning              |
| Weakness      | No semantic understanding       | Misses exact keywords                     | Slightly heavier in compute               |
| Example Match | "chicken soup" = *chicken soup* | "chicken soup" = *hot broth with chicken* | Both literal and related concepts         |
| Best Use Case | Legal docs, structured data     | Chatbots, Q\&A, semantic search           | Multimodal, real-world messy user queries |

<Note>
  Why Hybrid Search might be better for your application-

* **Improved Recall**: Captures more relevant results missed by pure keyword or vector search.
  * **Balanced Precision**: Exact matches get priority while also including semantically relevant results.
  * **Robust to Ambiguity**: Handles spelling variations, synonyms, and fuzzy user intent.
  * **Best of Both Worlds**: Keywords matter when they should, and meaning matters when needed.

**Perfect for **real-world apps** like recipe search, customer support, legal discovery, etc.**
</Note>

## Vector DBs in Agno that Support Hybrid Search

The following vector databases support hybrid search natively or via configurations:

| Database   | Hybrid Search Support       |
| ---------- | --------------------------- |
| `pgvector` | âœ… Yes                       |
| `milvus`   | âœ… Yes                       |
| `lancedb`  | âœ… Yes                       |
| `qdrantdb` | âœ… Yes                       |
| `weaviate` | âœ… Yes                       |
| `mongodb`  | âœ… Yes (Atlas Vector Search) |

## Example: Hybrid Search using `pgvector`

```python  theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

---

## Notion Knowledge Manager

**URL:** llms-txt#notion-knowledge-manager

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/workflows/notion-knowledge-manager

Build a knowledge management workflow that automatically creates, updates, and searches Notion pages based on content classification. This workflow uses AI to categorize information and organize it in your Notion database.

By building this workflow, you'll understand:

* How to integrate Notion API for automated knowledge management
* How to use agents for content classification and tagging
* How to implement conditional logic based on classification results
* How to coordinate multiple Notion operations (create, update, search)

Build knowledge base managers, automated documentation systems, content organization tools, or research note-taking applications.

The workflow manages Notion pages through a multi-step classification and organization process:

1. **Classify**: Classification agent analyzes content and determines tags/categories
2. **Route**: Based on classification, decides whether to create, update, or search
3. **Execute**: Notion agent performs the appropriate operation (create page, update tags, or search)
4. **Organize**: Content is automatically organized in your Notion database with proper tags

The workflow uses NotionTools to interact with your Notion database programmatically.

```python notion_knowledge_manager.py theme={null}
import os

from agno.agent.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.os import AgentOS
from agno.tools.notion import NotionTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel

---

## Web Search Agent

**URL:** llms-txt#web-search-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/langdb/usage/tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize and connect to the Streamable HTTP MCP server

**URL:** llms-txt#initialize-and-connect-to-the-streamable-http-mcp-server

mcp_tools = MCPTools(url="https://docs.agno.com/mcp", transport="streamable-http")
await mcp_tools.connect()

try:
    agent = Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[mcp_tools])
    await agent.aprint_response("What can you tell me about MCP support in Agno?", stream=True)
finally:
    # Always close the connection when done
    await mcp_tools.close()
python  theme={null}
from agno.tools.mcp import MCPTools, StreamableHTTPClientParams

server_params = StreamableHTTPClientParams(
    url=...,
    headers=...,
    timeout=...,
    sse_read_timeout=...,
    terminate_on_close=...,
)

**Examples:**

Example 1 (unknown):
```unknown
You can also use the `server_params` argument to define the MCP connection. This way you can specify the headers to send to the MCP server with every request, and the timeout values:
```

---

## Use the sequence in a workflow

**URL:** llms-txt#use-the-sequence-in-a-workflow

**Contents:**
- Steps with Router

workflow = Workflow(
    name="Article Creation Workflow",
    steps=[article_creation_sequence]  # Single sequence
)

workflow.print_response("Write an article about renewable energy", markdown=True)
python  theme={null}
from agno.workflow import Steps, Router, Step, Workflow

**Examples:**

Example 1 (unknown):
```unknown
## Steps with Router

This is where `Steps` really shines - creating distinct sequences for different content types or workflows:
```

---

## Now if we pass a new session_state, it will overwrite the stored session_state.

**URL:** llms-txt#now-if-we-pass-a-new-session_state,-it-will-overwrite-the-stored-session_state.

**Contents:**
- Team Member Interactions
- Developer Resources

agent.print_response(
    "Can you tell me what is in your session_state?",
    session_state={"secret_number": 43},
    stream=True,
)
print(f"Stored session state: {agent.get_session_state()}")
python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team

from agno.db.sqlite import SqliteDb
from agno.tools.duckduckgo import DuckDuckGoTools

db = SqliteDb(db_file="tmp/agents.db")

web_research_agent = Agent(
    name="Web Research Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions="You are a web research agent that can answer questions from the web.",
)

report_agent = Agent(
    name="Report Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="You are a report agent that can write a report from the web research.",
)

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    members=[web_research_agent, report_agent],
    share_member_interactions=True,
    instructions=[
        "You are a team of agents that can research the web and write a report.",
        "First, research the web for information about the topic.",
        "Then, use your report agent to write a report from the web research.",
    ],
    show_members_responses=True,
    debug_mode=True,
)

team.print_response("How are LEDs made?")
```

## Developer Resources

* View the [Team schema](/reference/teams/team)
* View the [RunContext schema](/reference/run/run-context)

**Examples:**

Example 1 (unknown):
```unknown
## Team Member Interactions

Agent Teams can share interactions between members, allowing agents to learn from each other's outputs:
```

---

## Model Index

**URL:** llms-txt#model-index

**Contents:**
  - Native Model Providers
  - Local Model Providers
  - Cloud Model Providers
  - Model Gateways & Aggregators

Source: https://docs.agno.com/integrations/models/model-index

Index of all models supported by Agno.

Agno supports the following model providers organized by category:

### Native Model Providers

<CardGroup cols={3}>
  <Card title="Anthropic" icon="brain" iconType="duotone" href="/integrations/models/native/anthropic/overview">
    Anthropic Claude models integration.
  </Card>

<Card title="Cohere" icon="robot" iconType="duotone" href="/integrations/models/native/cohere/overview">
    Cohere language models integration.
  </Card>

<Card title="DashScope" icon="robot" iconType="duotone" href="/integrations/models/native/dashscope/overview">
    Alibaba Cloud DashScope models.
  </Card>

<Card title="DeepSeek" icon="robot" iconType="duotone" href="/integrations/models/native/deepseek/overview">
    DeepSeek AI models integration.
  </Card>

<Card title="Google Gemini" icon="google" iconType="duotone" href="/integrations/models/native/google/overview">
    Google Gemini models integration.
  </Card>

<Card title="Meta" icon="meta" iconType="duotone" href="/integrations/models/native/meta/overview">
    Meta AI models integration.
  </Card>

<Card title="Mistral" icon="wind" iconType="duotone" href="/integrations/models/native/mistral/overview">
    Mistral AI models integration.
  </Card>

<Card title="OpenAI" icon="robot" iconType="duotone" href="/integrations/models/native/openai/completion/overview">
    OpenAI models integration.
  </Card>

<Card title="OpenAI Responses" icon="robot" iconType="duotone" href="/integrations/models/native/openai/responses/overview">
    OpenAI response format handling.
  </Card>

<Card title="Perplexity" icon="question" iconType="duotone" href="/integrations/models/native/perplexity/overview">
    Perplexity AI models integration.
  </Card>

<Card title="Vercel" icon="robot" iconType="duotone" href="/integrations/models/native/vercel/overview">
    Vercel AI models integration.
  </Card>

<Card title="xAI" icon="x" iconType="duotone" href="/integrations/models/native/xai/overview">
    xAI models integration.
  </Card>
</CardGroup>

### Local Model Providers

<CardGroup cols={3}>
  <Card title="LlamaCpp" icon="robot" iconType="duotone" href="/integrations/models/local/llama-cpp/overview">
    LlamaCpp local model inference.
  </Card>

<Card title="LM Studio" icon="laptop" iconType="duotone" href="/integrations/models/local/lmstudio/overview">
    LM Studio local model integration.
  </Card>

<Card title="Ollama" icon="robot" iconType="duotone" href="/integrations/models/local/ollama/overview">
    Ollama local model integration.
  </Card>

<Card title="VLLM" icon="server" iconType="duotone" href="/integrations/models/local/vllm/overview">
    VLLM high-throughput inference.
  </Card>
</CardGroup>

### Cloud Model Providers

<CardGroup cols={3}>
  <Card title="AWS Bedrock" icon="cloud" iconType="duotone" href="/integrations/models/cloud/aws-bedrock/overview">
    Amazon Web Services Bedrock models.
  </Card>

<Card title="Claude via AWS Bedrock" icon="brain" iconType="duotone" href="/integrations/models/cloud/aws-claude/overview">
    Anthropic Claude models via AWS Bedrock.
  </Card>

<Card title="Azure AI Foundry" icon="microsoft" iconType="duotone" href="/integrations/models/cloud/azure-ai-foundry/overview">
    Microsoft Azure AI Foundry models.
  </Card>

<Card title="Azure OpenAI" icon="microsoft" iconType="duotone" href="/integrations/models/cloud/azure-openai/overview">
    Microsoft Azure OpenAI models.
  </Card>

<Card title="Vertex AI Claude" icon="brain" iconType="duotone" href="/integrations/models/cloud/vertexai-claude/overview">
    Anthropic Claude models via Google Vertex AI.
  </Card>

<Card title="IBM WatsonX" icon="robot" iconType="duotone" href="/integrations/models/cloud/ibm-watsonx/overview">
    IBM WatsonX models integration.
  </Card>
</CardGroup>

### Model Gateways & Aggregators

<CardGroup cols={3}>
  <Card title="AI/ML API" icon="robot" iconType="duotone" href="/integrations/models/gateways/aimlapi/overview">
    AI/ML API model provider integration.
  </Card>

<Card title="Cerebras" icon="robot" iconType="duotone" href="/integrations/models/gateways/cerebras/overview">
    Cerebras AI models integration.
  </Card>

<Card title="Cerebras OpenAI" icon="robot" iconType="duotone" href="/integrations/models/gateways/cerebras-openai/overview">
    Cerebras OpenAI-compatible models.
  </Card>

<Card title="CometAPI" icon="comet" iconType="duotone" href="/integrations/models/gateways/cometapi/overview">
    CometAPI model provider integration.
  </Card>

<Card title="DeepInfra" icon="infinity" iconType="duotone" href="/integrations/models/gateways/deepinfra/overview">
    DeepInfra model provider integration.
  </Card>

<Card title="Fireworks" icon="fire" iconType="duotone" href="/integrations/models/gateways/fireworks/overview">
    Fireworks AI models integration.
  </Card>

<Card title="Groq" icon="bolt" iconType="duotone" href="/integrations/models/gateways/groq/overview">
    Groq fast inference models.
  </Card>

<Card title="Hugging Face" icon="robot" iconType="duotone" href="/integrations/models/gateways/huggingface/overview">
    Hugging Face models integration.
  </Card>

<Card title="LangDB" icon="database" iconType="duotone" href="/integrations/models/gateways/langdb/overview">
    LangDB model provider integration.
  </Card>

<Card title="LiteLLM" icon="lightbulb" iconType="duotone" href="/integrations/models/gateways/litellm/overview">
    LiteLLM unified model interface.
  </Card>

<Card title="LiteLLM OpenAI" icon="lightbulb" iconType="duotone" href="/integrations/models/gateways/litellm-openai/overview">
    LiteLLM OpenAI-compatible models.
  </Card>

<Card title="Nebius Token Factory" icon="star" iconType="duotone" href="/integrations/models/gateways/nebius/overview">
    Nebius Token Factory models.
  </Card>

<Card title="Nexus" icon="link" iconType="duotone" href="/integrations/models/gateways/nexus/overview">
    Nexus model provider integration.
  </Card>

<Card title="NVIDIA" icon="robot" iconType="duotone" href="/integrations/models/gateways/nvidia/overview">
    NVIDIA AI models integration.
  </Card>

<Card title="OpenRouter" icon="route" iconType="duotone" href="/integrations/models/gateways/openrouter/overview">
    OpenRouter model aggregation.
  </Card>

<Card title="Portkey" icon="key" iconType="duotone" href="/integrations/models/gateways/portkey/overview">
    Portkey model gateway integration.
  </Card>

<Card title="Requesty" icon="robot" iconType="duotone" href="/integrations/models/gateways/requesty/overview">
    Requesty model provider integration.
  </Card>

<Card title="Sambanova" icon="server" iconType="duotone" href="/integrations/models/gateways/sambanova/overview">
    SambaNova AI models integration.
  </Card>

<Card title="SiliconFlow" icon="robot" iconType="duotone" href="/integrations/models/gateways/siliconflow/overview">
    SiliconFlow model provider.
  </Card>

<Card title="Together" icon="users" iconType="duotone" href="/integrations/models/gateways/together/overview">
    Together AI models integration.
  </Card>
</CardGroup>

Each provider offers a different set of models, with different capabilities and features. By default, Agno supports all models provided by the mentioned providers.

---

## Response Caching

**URL:** llms-txt#response-caching

**Contents:**
- Basic Usage

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/cache-response

Learn how to cache model responses to avoid redundant API calls and reduce costs.

<Note>
  For a conceptual overview of response caching, see [Response Caching](/basics/models/cache-response).
</Note>

Response caching allows you to cache model responses, which can significantly improve response times and reduce API costs during development and testing.

Enable caching by setting `cache_response=True` when initializing the model. The first call will hit the API and cache the response, while subsequent identical calls will return the cached result.

```python cache_model_response.py theme={null}
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o", cache_response=True))

---

## Validate filters to catch typos

**URL:** llms-txt#validate-filters-to-catch-typos

**Contents:**
  - 4. Match Chunking Strategy to Your Content

valid_filters, invalid_keys = knowledge.validate_filters({
    "department": "engineering",
    "invalid_key": "value"  # This gets flagged
})
python  theme={null}
from agno.knowledge.chunking.fixed import FixedSizeChunking
from agno.knowledge.chunking.semantic import SemanticChunking

**Examples:**

Example 1 (unknown):
```unknown
### 4. Match Chunking Strategy to Your Content

Different strategies have different performance characteristics:

| Strategy       | Speed  | Quality | Best For                            |
| -------------- | ------ | ------- | ----------------------------------- |
| **Fixed Size** | Fast   | Good    | Uniform content, when speed matters |
| **Semantic**   | Slower | Best    | Complex docs, when quality matters  |
| **Recursive**  | Fast   | Good    | Structured docs, good balance       |
```

---

## run_response_strem: Iterator[RunOutputEvent] = agent.run("What is the stock price of NVDA", stream=True)

**URL:** llms-txt#run_response_strem:-iterator[runoutputevent]-=-agent.run("what-is-the-stock-price-of-nvda",-stream=true)

---

## Setup your evaluator Agent

**URL:** llms-txt#setup-your-evaluator-agent

**Contents:**
- Accuracy with Tools
- Accuracy with given output
- Accuracy with asynchronous functions

evaluator_agent = Agent(
    model=OpenAIChat(id="gpt-5"),
    output_schema=AccuracyAgentResponse,  # We want the evaluator agent to return an AccuracyAgentResponse
    # You can provide any additional evaluator instructions here:
    # instructions="",
)

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[CalculatorTools()]),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    # Use your evaluator Agent
    evaluator_agent=evaluator_agent,
    # Further adjusting the guidelines
    additional_guidelines="Agent output should include the steps and the final answer.",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
python accuracy_with_tools.py theme={null}
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Tools Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[CalculatorTools()],
    ),
    input="What is 10!?",
    expected_output="3628800",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
python accuracy_with_given_answer.py theme={null}
from typing import Optional

from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat

evaluation = AccuracyEval(
    name="Given Answer Evaluation",
    model=OpenAIChat(id="o4-mini"),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
)
result_with_given_answer: Optional[AccuracyResult] = evaluation.run_with_output(
    output="2500", print_results=True
)
assert result_with_given_answer is not None and result_with_given_answer.avg_score >= 8
python async_accuracy.py theme={null}
"""This example shows how to run an Accuracy evaluation asynchronously."""

import asyncio
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[CalculatorTools()],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=3,
)

**Examples:**

Example 1 (unknown):
```unknown
<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=60f989f94bfe8b9147e0fe439e1d27d2" style={{ borderRadius: '8px' }} data-og-width="2046" data-og-height="1354" data-path="images/evals/accuracy_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=a37037fd28a47087de5fedc599c4346b 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=b73acf915f02a759ae8c2353fdf6ec77 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=dc6425d6eb0243364b9fafd500495a15 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=33690c95f75c292fb1347da676cfaa53 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=28e03442ad38b0576c1607a6abcd1593 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/accuracy_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=60af642df61c82e5f7eaa7833d9df73f 2500w" />
</Frame>

## Accuracy with Tools

You can also run the `AccuracyEval` with tools.
```

Example 2 (unknown):
```unknown
## Accuracy with given output

For comprehensive evaluation, run with a given output:
```

Example 3 (unknown):
```unknown
## Accuracy with asynchronous functions

Evaluate accuracy with asynchronous functions:
```

---

## PDF Input URL Agent

**URL:** llms-txt#pdf-input-url-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/pdf-input-url

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define specialized customer service agents

**URL:** llms-txt#define-specialized-customer-service-agents

tech_support_agent = Agent(
    name="Technical Support Specialist",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a technical support specialist with deep product knowledge.",
        "You have access to the full conversation history with this customer.",
        "Reference previous interactions to provide better help.",
        "Build on any troubleshooting steps already attempted.",
        "Be patient and provide step-by-step technical guidance.",
    ],
)

billing_agent = Agent(
    name="Billing & Account Specialist",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a billing and account specialist.",
        "You have access to the full conversation history with this customer.",
        "Reference any account details or billing issues mentioned previously.",
        "Build on any payment or account information already discussed.",
        "Be helpful with billing questions, refunds, and account changes.",
    ],
)

general_support_agent = Agent(
    name="General Customer Support",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a general customer support representative.",
        "You have access to the full conversation history with this customer.",
        "Handle general inquiries, product information, and basic support.",
        "Reference the conversation context - build on what was discussed.",
        "Be friendly and acknowledge their previous interactions.",
    ],
)

---

## Agent that uses structured outputs with strict_output=True (default)

**URL:** llms-txt#agent-that-uses-structured-outputs-with-strict_output=true-(default)

structured_output_agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

---

## Create an agent with the RedshiftTools

**URL:** llms-txt#create-an-agent-with-the-redshifttools

agent = Agent(tools=[redshift_tools])

---

## Get a trace

**URL:** llms-txt#get-a-trace

**Contents:**
- See Also

trace = db.get_trace(run_id=response.run_id)

if trace:
    print(f"Trace ID: {trace.trace_id}")
    print(f"Name: {trace.name}")
    print(f"Duration: {trace.duration_ms}ms")
    print(f"Status: {trace.status}")
    print(f"Total Spans: {trace.total_spans}")
    print(f"Errors: {trace.error_count}")
```

* [Span Reference](/reference/tracing/span) - Individual operations within a trace
* [DB Functions](/basics/tracing/db-functions) - Query functions for traces and spans

---

## List Content

**URL:** llms-txt#list-content

Source: https://docs.agno.com/reference-api/schema/knowledge/list-content

get /knowledge/content
Retrieve paginated list of all content in the knowledge base with filtering and sorting options. Filter by status, content type, or metadata properties.

---

## What are Knowledge Filters?

**URL:** llms-txt#what-are-knowledge-filters?

**Contents:**
- Why Use Knowledge Filters?
- How Do Knowledge Filters Work?
- Ways to Apply Filters
- Filters in Traditional RAG vs. Agentic RAG
- Best Practices
- Manual vs. Agentic Filtering
- Developer Resources

Source: https://docs.agno.com/basics/knowledge/filters/overview

Use knowledge filters to restrict and refine searches

Knowledge filters allow you to restrict and refine searches within your knowledge base using metadata such as user IDs, document types, years, and more. This feature is especially useful when you have a large collection of documents and want to retrieve information relevant to specific users or contexts.

## Why Use Knowledge Filters?

* **Personalization:** Retrieve information for a specific user or group.
* **Security:** Restrict access to sensitive documents.
* **Efficiency:** Reduce noise by narrowing down search results.

## How Do Knowledge Filters Work?

When you load documents into your knowledge base, you can attach metadata (like user ID, document type, year, etc.). Later, when querying, you can specify filters to only search documents matching certain criteria.

**Example Metadata:**

## Ways to Apply Filters

You can apply knowledge filters in two main ways:

1. **Manual Filters:** Explicitly pass filters when querying.
2. **Agentic Filters:** Let the Agent automatically extract filters from your query.

> **Tip:** You can combine multiple filters for more precise results!

## Filters in Traditional RAG vs. Agentic RAG

When configuring your Agent it is important to choose the right approach for your use case. There are two broad approaches to RAG with Agno agents: traditional RAG and agentic RAG. With a traditional RAG approach you set `add_knowledge_to_context=True` to ensure that references are included in the system message sent to the LLM. For Agentic RAG, you set `search_knowledge=True` to leverage the agent's ability search the knowledge base directly.

<Check>
  Remember to use only one of these configurations at a time, setting the other to false. By default, `search_knowledge=True` is preferred as it offers a more dynamic and interactive experience.
  Checkout an example [here](/basics/knowledge/filters/usage/filtering) of how to set up knowledge filters in a Traditional RAG system
</Check>

* Make your prompts descriptive (e.g., include user names, document types, years).
* Use agentic filtering for interactive applications or chatbots.

## Manual vs. Agentic Filtering

| Manual Filtering         | Agentic Filtering                |
| ------------------------ | -------------------------------- |
| Explicit filters in code | Filters inferred from query text |
| Full control             | More natural, less code          |
| Good for automation      | Good for user-facing apps        |

<Note>
  ðŸš¦ **Currently, knowledge filtering is supported on the following vector databases:**

* **Qdrant**
  * **LanceDB**
  * **PgVector**
  * **MongoDB**
  * **Pinecone**
  * **Weaviate**
  * **ChromaDB**
  * **Milvus**
</Note>

## Developer Resources

See the detailed cookbooks [here](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/filters/README.md)

**Examples:**

Example 1 (unknown):
```unknown
## Ways to Apply Filters

You can apply knowledge filters in two main ways:

1. **Manual Filters:** Explicitly pass filters when querying.
2. **Agentic Filters:** Let the Agent automatically extract filters from your query.

> **Tip:** You can combine multiple filters for more precise results!

## Filters in Traditional RAG vs. Agentic RAG

When configuring your Agent it is important to choose the right approach for your use case. There are two broad approaches to RAG with Agno agents: traditional RAG and agentic RAG. With a traditional RAG approach you set `add_knowledge_to_context=True` to ensure that references are included in the system message sent to the LLM. For Agentic RAG, you set `search_knowledge=True` to leverage the agent's ability search the knowledge base directly.

Example:
```

---

## Create a Context-Aware Agent that can access real-time HackerNews data

**URL:** llms-txt#create-a-context-aware-agent-that-can-access-real-time-hackernews-data

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    # Each function in the context is evaluated when the agent is run,
    # think of it as dependency injection for Agents
    dependencies={"top_hackernews_stories": get_top_hackernews_stories},
    # Alternatively, you can manually add the context to the instructions. This gets resolved automatically
    instructions=dedent("""\
        You are an insightful tech trend observer! ðŸ“°

Here are the top stories on HackerNews:
        {top_hackernews_stories}\
    """),
    markdown=True,
)

---

## Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.

**URL:** llms-txt#give-a-sentiment-analysis-of-this-audio-conversation.-use-speaker-a,-speaker-b-to-identify-speakers.

**Contents:**
- Key Features
- Use Cases

agent.print_response(
    "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)

agent.print_response(
    "What else can you tell me about this audio conversation?",
    stream=True,
)
```

* **Audio Processing**: Downloads and processes audio files from remote URLs
* **Sentiment Analysis**: Analyzes emotional tone and sentiment in conversations
* **Speaker Identification**: Distinguishes between different speakers in the conversation
* **Persistent Sessions**: Maintains conversation history using SQLite database
* **Streaming Response**: Real-time response generation for better user experience

* Customer service call analysis
* Meeting sentiment tracking
* Interview evaluation
* Call center quality monitoring

---

## --- Agents ---

**URL:** llms-txt#----agents----

research_agent = Agent(
    name="Blog Research Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    description=dedent("""\
    You are BlogResearch-X, an elite research assistant specializing in discovering
    high-quality sources for compelling blog content. Your expertise includes:

- Finding authoritative and trending sources
    - Evaluating content credibility and relevance
    - Identifying diverse perspectives and expert opinions
    - Discovering unique angles and insights
    - Ensuring comprehensive topic coverage
    """),
    instructions=dedent("""\
    1. Search Strategy ðŸ”
       - Find 10-15 relevant sources and select the 5-7 best ones
       - Prioritize recent, authoritative content
       - Look for unique angles and expert insights
    2. Source Evaluation ðŸ“Š
       - Verify source credibility and expertise
       - Check publication dates for timeliness
       - Assess content depth and uniqueness
    3. Diversity of Perspectives ðŸŒ
       - Include different viewpoints
       - Gather both mainstream and expert opinions
       - Find supporting data and statistics
    """),
    output_schema=SearchResults,
)

content_scraper_agent = Agent(
    name="Content Scraper Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[Newspaper4kTools()],
    description=dedent("""\
    You are ContentBot-X, a specialist in extracting and processing digital content
    for blog creation. Your expertise includes:

- Efficient content extraction
    - Smart formatting and structuring
    - Key information identification
    - Quote and statistic preservation
    - Maintaining source attribution
    """),
    instructions=dedent("""\
    1. Content Extraction ðŸ“‘
       - Extract content from the article
       - Preserve important quotes and statistics
       - Maintain proper attribution
       - Handle paywalls gracefully
    2. Content Processing ðŸ”„
       - Format text in clean markdown
       - Preserve key information
       - Structure content logically
    3. Quality Control âœ…
       - Verify content relevance
       - Ensure accurate extraction
       - Maintain readability
    """),
    output_schema=ScrapedArticle,
)

blog_writer_agent = Agent(
    name="Blog Writer Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
    You are BlogMaster-X, an elite content creator combining journalistic excellence
    with digital marketing expertise. Your strengths include:

- Crafting viral-worthy headlines
    - Writing engaging introductions
    - Structuring content for digital consumption
    - Incorporating research seamlessly
    - Optimizing for SEO while maintaining quality
    - Creating shareable conclusions
    """),
    instructions=dedent("""\
    1. Content Strategy ðŸ“
       - Craft attention-grabbing headlines
       - Write compelling introductions
       - Structure content for engagement
       - Include relevant subheadings
    2. Writing Excellence âœï¸
       - Balance expertise with accessibility
       - Use clear, engaging language
       - Include relevant examples
       - Incorporate statistics naturally
    3. Source Integration ðŸ”
       - Cite sources properly
       - Include expert quotes
       - Maintain factual accuracy
    4. Digital Optimization ðŸ’»
       - Structure for scanability
       - Include shareable takeaways
       - Optimize for SEO
       - Add engaging subheadings

Format your blog post with this structure:
    # {Viral-Worthy Headline}

## Introduction
    {Engaging hook and context}

## {Compelling Section 1}
    {Key insights and analysis}
    {Expert quotes and statistics}

## {Engaging Section 2}
    {Deeper exploration}
    {Real-world examples}

## {Practical Section 3}
    {Actionable insights}
    {Expert recommendations}

## Key Takeaways
    - {Shareable insight 1}
    - {Practical takeaway 2}
    - {Notable finding 3}

## Sources
    {Properly attributed sources with links}
    """),
    markdown=True,
)

---

## Recommended: Single memory processing after conversation

**URL:** llms-txt#recommended:-single-memory-processing-after-conversation

agent = Agent(
    db=db,
    enable_user_memories=True  # Processes memories once at end
)

---

## Initialize GCSJsonDb with explicit credentials, unique bucket name, and project.

**URL:** llms-txt#initialize-gcsjsondb-with-explicit-credentials,-unique-bucket-name,-and-project.

db = GcsJsonDb(
    bucket_name=unique_bucket_name,
    prefix="agent/",
    project=project_id,
    credentials=credentials,
)

---

## Router Team with Direct Response

**URL:** llms-txt#router-team-with-direct-response

Source: https://docs.agno.com/basics/teams/usage/basic-flows/respond-directly

This example demonstrates a team of AI agents working together to answer questions in different languages.

The team consists of six specialized agents:

1. **English Agent** - Can only answer in English
2. **Japanese Agent** - Can only answer in Japanese
3. **Chinese Agent** - Can only answer in Chinese
4. **Spanish Agent** - Can only answer in Spanish
5. **French Agent** - Can only answer in French
6. **German Agent** - Can only answer in German

The team leader routes the user's question to the appropriate language agent. With `respond_directly=True`, the selected agent responds directly without the team leader processing the response.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Fetch the audio file and convert it to a base64 encoded string

**URL:** llms-txt#fetch-the-audio-file-and-convert-it-to-a-base64-encoded-string

url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

---

## Background Hooks

**URL:** llms-txt#background-hooks

**Contents:**
- Why Use Background Hooks?
- Enabling Background Tasks
  - Option 1: Global Setting via AgentOS
  - Option 2: Per-Hook Setting via Decorator
- How It Works
  - Data Isolation
  - Error Handling
- Developer Resources

Source: https://docs.agno.com/agent-os/background-tasks/overview

Run agent hooks as non-blocking background tasks in AgentOS

When serving agents or teams through AgentOS, you can configure pre-hooks and post-hooks to run as background tasks. This means the API response is returned immediately to the user while the hooks continue executing in the background.

## Why Use Background Hooks?

By default, hooks used by agents and teams in your AgentOS are in the execution path and block the response:

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=615b75c6fd7b2bc946dd12cb853f41b6" alt="Background tasks not enabled" data-og-width="2895" width="2895" data-og-height="906" height="906" data-path="images/background-tasks-notenabled-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?w=280&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=c3dcec05473d3750f9c6f9a8a2766239 280w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?w=560&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=942c15c932e0d71652f874088a2fcbea 560w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?w=840&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=84d728448bc3646060cb3af299d6117d 840w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?w=1100&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=e9816cdb46d2394bd0e453840a7c61be 1100w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?w=1650&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=d6ff0b683d01424eca2cc5112985f458 1650w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-light.png?w=2500&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=76d678d154f114a7cf27a93a093220f9 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=510be4326e46562ee9d0e527a76feb33" alt="Background tasks not enabled" data-og-width="2895" width="2895" data-og-height="906" height="906" data-path="images/background-tasks-notenabled-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?w=280&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=49cf4e5f2c43a56342ef13e3781317b5 280w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?w=560&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=1b511f27bef977b7caebef20f7df5eff 560w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?w=840&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=1cc96f5f92036c66a7fb3dadf51f3af9 840w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?w=1100&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=94e25e9536ca3685ea2b4a1dec0f9660 1100w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?w=1650&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=de03d910764ae08d2102b26ad6bfcbb0 1650w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-notenabled-dark.png?w=2500&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=8fddb0de142ac887091432456fb46e55 2500w" />

With background hooks enabled, your hooks won't block the response, increasing response speed:

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=b274e516d035f89d422e312945a29aaa" alt="Background tasks enabled" data-og-width="2271" width="2271" data-og-height="906" height="906" data-path="images/background-tasks-enabled-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?w=280&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=0f9a9b3b3aeb4e353b7f3bf992e89da7 280w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?w=560&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=38c9e23810aaf50738deae73d851c4fb 560w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?w=840&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=cf7a55219acaedd0560d2db624f5e81f 840w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?w=1100&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=7cd4428ab9c6ed95bee09fc7cd132219 1100w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?w=1650&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=297ff38f65490b1ab71844f7019b9efd 1650w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-light.png?w=2500&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=62a4622b6eaa8134427799ce9af58c4d 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=dddbd20dd6bccab04949979587e67042" alt="Background tasks enabled" data-og-width="2271" width="2271" data-og-height="906" height="906" data-path="images/background-tasks-enabled-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?w=280&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=dcfb71d67b3b3dabc6d94d9952b56b95 280w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?w=560&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=98cb92e42b038280f3207a9517c9f2b5 560w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?w=840&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=fa3436b0d198f8c36c055ab86320a127 840w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?w=1100&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=9da5f237c3adcd2ddfc2df78d628f112 1100w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?w=1650&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=5bf6395194962d35f0137167e7c3a5bc 1650w, https://mintcdn.com/agno-v2/I_OqdwByK40CFkbk/images/background-tasks-enabled-dark.png?w=2500&fit=max&auto=format&n=I_OqdwByK40CFkbk&q=85&s=b2b2258684ff3ac029fd00825294bacf 2500w" />

* **Agent Evaluation**: Evaluate the agent's responses without affecting the responses themselves
* **Analytics and logging**: Track usage patterns without affecting response time
* **Notifications**: Send emails, Slack messages, or webhook calls
* **External API calls**: Sync data with third-party services
* **Non-critical data processing**: Tasks that don't affect the response

## Enabling Background Tasks

There are two ways to enable background execution for hooks:

### Option 1: Global Setting via AgentOS

Enable background execution for **all** hooks across all agents and teams (even as part of a workflow):

When enabled, this setting automatically propagates to:

* All agents registered with AgentOS
* All teams and their member agents (including nested teams)
* All workflows and the agents/teams within their steps

See [Global Background Hooks Example](/agent-os/usage/background-hooks-global) for an example.

<Note>
  Note that pre-hooks are typically used for validation or modification of the input of a run. If you use them as background tasks, they will execute after the run has already been initiated.

If you have hooks that should not run as background tasks, you should use the second option and mark only the specific hooks to run in background.
</Note>

### Option 2: Per-Hook Setting via Decorator

Mark specific hooks to run in background using the `@hook` decorator:

This approach gives you fine-grained control: critical hooks are executed during the run while non-critical hooks run in the background.

See [Per-Hook Background Example](/agent-os/usage/background-hooks-decorator) for an example.

<Check>
  **Background tasks require AgentOS.** When running agents directly (not through AgentOS), the `@hook(run_in_background=True)` decorator has no effect - hooks will run synchronously.
</Check>

AgentOS uses FastAPI's [BackgroundTasks](https://fastapi.tiangolo.com/tutorial/background-tasks/) to schedule hooks for execution after the response is sent.

Background tasks execute **sequentially** after the response is sent. If you have multiple background hooks, they run one after another.

<Warning>
  **Pre- and post-hooks in background mode cannot modify the request or response.** Any modifications to `run_input` or `run_output` won't affect the agent's processing. Only use background mode for pre- and post-hooks that perform logging or monitoring. This means background mode is not suitable for Guardrails.
</Warning>

When hooks run in the background, AgentOS automatically creates deep copies of:

* `run_input` - The input to the agent run
* `run_context` - The current run context
* `run_output` - The output from the agent

This prevents race conditions where background hooks might accidentally modify data that's being used elsewhere.

Errors in background tasks don't affect the API response (since it's already been sent). Make sure to implement proper error handling and logging in your background hooks:

## Developer Resources

* View [Global Background Hooks Example](/agent-os/usage/background-hooks-global)
* View [Per-Hook Background Example](/agent-os/usage/background-hooks-decorator)
* View [@hook Decorator Reference](/reference/hooks/hook-decorator)
* View [Hooks Overview](/basics/hooks/overview)
* View [Cookbook Examples](https://github.com/agno-agi/agno/tree/main/cookbook/agent_os/background_tasks)

**Examples:**

Example 1 (unknown):
```unknown
When enabled, this setting automatically propagates to:

* All agents registered with AgentOS
* All teams and their member agents (including nested teams)
* All workflows and the agents/teams within their steps

See [Global Background Hooks Example](/agent-os/usage/background-hooks-global) for an example.

<Note>
  Note that pre-hooks are typically used for validation or modification of the input of a run. If you use them as background tasks, they will execute after the run has already been initiated.

  If you have hooks that should not run as background tasks, you should use the second option and mark only the specific hooks to run in background.
</Note>

### Option 2: Per-Hook Setting via Decorator

Mark specific hooks to run in background using the `@hook` decorator:
```

Example 2 (unknown):
```unknown
This approach gives you fine-grained control: critical hooks are executed during the run while non-critical hooks run in the background.

See [Per-Hook Background Example](/agent-os/usage/background-hooks-decorator) for an example.

<Check>
  **Background tasks require AgentOS.** When running agents directly (not through AgentOS), the `@hook(run_in_background=True)` decorator has no effect - hooks will run synchronously.
</Check>

## How It Works

AgentOS uses FastAPI's [BackgroundTasks](https://fastapi.tiangolo.com/tutorial/background-tasks/) to schedule hooks for execution after the response is sent.

Background tasks execute **sequentially** after the response is sent. If you have multiple background hooks, they run one after another.

<Warning>
  **Pre- and post-hooks in background mode cannot modify the request or response.** Any modifications to `run_input` or `run_output` won't affect the agent's processing. Only use background mode for pre- and post-hooks that perform logging or monitoring. This means background mode is not suitable for Guardrails.
</Warning>

### Data Isolation

When hooks run in the background, AgentOS automatically creates deep copies of:

* `run_input` - The input to the agent run
* `run_context` - The current run context
* `run_output` - The output from the agent

This prevents race conditions where background hooks might accidentally modify data that's being used elsewhere.

### Error Handling

Errors in background tasks don't affect the API response (since it's already been sent). Make sure to implement proper error handling and logging in your background hooks:
```

---

## Configure SingleStore DB connection

**URL:** llms-txt#configure-singlestore-db-connection

USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)

db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
db = SingleStoreDb(db_url=db_url)

---

## Giphy

**URL:** llms-txt#giphy

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/giphy

**GiphyTools** enables an Agent to search for GIFs on GIPHY.

The following agent will search GIPHY for a GIF appropriate for a birthday message.

| Parameter            | Type   | Default | Description                                       |
| -------------------- | ------ | ------- | ------------------------------------------------- |
| `api_key`            | `str`  | `None`  | If you want to manually supply the GIPHY API key. |
| `limit`              | `int`  | `1`     | The number of GIFs to return in a search.         |
| `enable_search_gifs` | `bool` | `True`  | Enable the search\_gifs functionality.            |
| `all`                | `bool` | `False` | Enable all functionality.                         |

| Function      | Description                                         |
| ------------- | --------------------------------------------------- |
| `search_gifs` | Searches GIPHY for a GIF based on the query string. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/giphy.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/giphy_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will search GIPHY for a GIF appropriate for a birthday message.
```

---

## Define team-level tools

**URL:** llms-txt#define-team-level-tools

def list_items(run_context: RunContext) -> str:
    """List all items in the shopping list."""
    if not run_context.session_state:
        run_context.session_state = {}

# Access shared state (not private state)
    shopping_list = run_context.session_state["shopping_list"]

if not shopping_list:
        return "The shopping list is empty."

items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"

def add_chore(run_context: RunContext, chore: str) -> str:
    """Add a completed chore to the team's private log."""
    if not run_context.session_state:
        run_context.session_state = {}

# Access team's private state
    if "chores" not in run_context.session_state:
        run_context.session_state["chores"] = []

run_context.session_state["chores"].append(chore)
    return f"Logged chore: {chore}"

---

## Azure OpenAI Embedder

**URL:** llms-txt#azure-openai-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/azure-openai/usage/azure-embedder

```python  theme={null}
from agno.knowledge.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = AzureOpenAIEmbedder(id="text-embedding-3-small").get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Access metrics from the response

**URL:** llms-txt#access-metrics-from-the-response

---

## Zep

**URL:** llms-txt#zep

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/database/zep

The ZepTools toolkit enables an Agent to interact with a Zep memory system, providing capabilities to store, retrieve, and search memory data associated with user sessions.

**ZepTools** enable an Agent to interact with a Zep memory system, providing capabilities to store, retrieve, and search memory data associated with user sessions.

The ZepTools require the `zep-cloud` Python package and a Zep API key.

The following example demonstrates how to create an agent with access to Zep memory:

```python cookbook/tools/zep_tools.py theme={null}
import time

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.zep import ZepTools

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following example demonstrates how to create an agent with access to Zep memory:
```

---

## Initialize knowledge base with vector database

**URL:** llms-txt#initialize-knowledge-base-with-vector-database

agno_docs_knowledge = Knowledge(
    vector_db=LanceDb(
        uri=str(tmp_dir.joinpath("lancedb")),
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## Airbnb Mcp

**URL:** llms-txt#airbnb-mcp

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/airbnb_mcp

ðŸ  MCP Airbnb Agent - Search for Airbnb listings!

This example shows how to create an agent that uses MCP and Llama 4 to search for Airbnb listings.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ---------------------------------------------------------

**URL:** llms-txt#---------------------------------------------------------

---

## A2A

**URL:** llms-txt#a2a

**Contents:**
- Setup

Source: https://docs.agno.com/agent-os/interfaces/a2a/introduction

Expose Agno agents via the A2A protocol

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.2" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.2">v2.1.2</Tooltip>
</Badge>

Google's [Agent-to-Agent Protocol (A2A)](https://a2a-protocol.org/latest/topics/what-is-a2a/) provides a standard way for agents to communicate with each other.

Agno integrates with A2A, enabling Agno agents and teams to be exposed in an A2A-compatible format.

The `A2A` interface works with the [AgentOS](/agent-os/introduction) runtime to provide this functionality.

Set `a2a_interface=True` when creating an `AgentOS` instance:

By default, all agents, teams, and workflows in the AgentOS are exposed via A2A.

Specific agents, teams, and workflows can be exposed by initializing the interface explicitly:

```python a2a-interface-initialization.py theme={null}
from agno.agent import Agent
from agno.os import AgentOS
from agno.os.interfaces.a2a import A2A

agent = Agent(name="My Agno Agent")

**Examples:**

Example 1 (unknown):
```unknown
By default, all agents, teams, and workflows in the AgentOS are exposed via A2A.

Specific agents, teams, and workflows can be exposed by initializing the interface explicitly:
```

---

## Yfinance

**URL:** llms-txt#yfinance

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/yfinance

**YFinanceTools** enable an Agent to access stock data, financial information and more from Yahoo Finance.

The following example requires the `yfinance` library.

The following agent will provide information about the stock price and analyst recommendations for NVDA (Nvidia Corporation).

The YFinanceTools toolkit does not require any configuration parameters. All functions are enabled by default and do not have individual enable/disable flags. Simply instantiate the toolkit without any parameters.

| Function                      | Description                                                      |
| ----------------------------- | ---------------------------------------------------------------- |
| `get_current_stock_price`     | This function retrieves the current stock price of a company.    |
| `get_company_info`            | This function retrieves detailed information about a company.    |
| `get_historical_stock_prices` | This function retrieves historical stock prices for a company.   |
| `get_stock_fundamentals`      | This function retrieves fundamental data about a stock.          |
| `get_income_statements`       | This function retrieves income statements of a company.          |
| `get_key_financial_ratios`    | This function retrieves key financial ratios for a company.      |
| `get_analyst_recommendations` | This function retrieves analyst recommendations for a stock.     |
| `get_company_news`            | This function retrieves the latest news related to a company.    |
| `get_technical_indicators`    | This function retrieves technical indicators for stock analysis. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/yfinance.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/yfinance_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will provide information about the stock price and analyst recommendations for NVDA (Nvidia Corporation).
```

---

## Streaming Basic Agent

**URL:** llms-txt#streaming-basic-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/cloud/ibm-watsonx/usage/basic-stream

```python cookbook/models/ibm/watsonx/basic_stream.py theme={null}
from typing import Iterator
from agno.agent import Agent, RunOutput
from agno.models.ibm import WatsonX

agent = Agent(model=WatsonX(id="ibm/granite-20b-code-instruct"), markdown=True)

---

## OpenCV

**URL:** llms-txt#opencv

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/opencv

OpenCVTools enables agents to capture images and videos from webcam using OpenCV computer vision library.

The following agent can capture images and videos from your webcam:

| Parameter              | Type   | Default | Description                                           |
| ---------------------- | ------ | ------- | ----------------------------------------------------- |
| `show_preview`         | `bool` | `False` | Whether to show camera preview window during capture. |
| `enable_capture_image` | `bool` | `True`  | Enable image capture functionality.                   |
| `enable_capture_video` | `bool` | `True`  | Enable video capture functionality.                   |

| Function        | Description                                              |
| --------------- | -------------------------------------------------------- |
| `capture_image` | Capture a single image from the webcam.                  |
| `capture_video` | Record a video from the webcam for a specified duration. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/opencv.py)
* [OpenCV Documentation](https://docs.opencv.org/)

---

## Ollama Cloud

**URL:** llms-txt#ollama-cloud

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/integrations/models/local/ollama/usage/cloud

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set up Ollama Cloud API Key">
    Sign up at [ollama.com](https://ollama.com) and get your API key, then export it:

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **No local setup required**: Access powerful models instantly without downloading or managing local installations
* **Production-ready**: Enterprise-grade infrastructure with reliable uptime and performance
* **Wide model selection**: Access to powerful models including GPT-OSS and other optimized cloud models
* **Automatic configuration**: When `api_key` is provided, the host automatically defaults to `https://ollama.com`

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up Ollama Cloud API Key">
    Sign up at [ollama.com](https://ollama.com) and get your API key, then export it:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## PDF Reader Async

**URL:** llms-txt#pdf-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/pdf-reader-async

The **PDF Reader** with asynchronous processing allows you to handle PDF files efficiently and integrate them with knowledge bases.

```python examples/basics/knowledge/readers/pdf_reader_async.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## JSON Reader Async

**URL:** llms-txt#json-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/json-reader-async

The **JSON Reader** with asynchronous processing allows you to handle JSON files efficiently and integrate them with knowledge bases.

```python examples/basics/knowledge/readers/json_reader_async.py theme={null}
import json
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.json_reader import JSONReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Agent with Async Tool Usage

**URL:** llms-txt#agent-with-async-tool-usage

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/meta/usage/async-tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your LLAMA API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your LLAMA API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Slack Events

**URL:** llms-txt#slack-events

Source: https://docs.agno.com/reference-api/schema/slack/slack-events

post /slack/events
Process incoming Slack events

---

## Filtering on Pinecone

**URL:** llms-txt#filtering-on-pinecone

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-pinecone

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in Pinecone.

```python  theme={null}
from os import getenv

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pineconedb import PineconeDb

---

## Workflow-level: limit history for all steps

**URL:** llms-txt#workflow-level:-limit-history-for-all-steps

workflow = Workflow(
    add_workflow_history_to_steps=True,
    num_history_runs=5  # Only last 5 runs
)

---

## StepInput

**URL:** llms-txt#stepinput

**Contents:**
- Helper Functions

Source: https://docs.agno.com/reference/workflows/step_input

| Parameter               | Type                                                         | Description                                                                |
| ----------------------- | ------------------------------------------------------------ | -------------------------------------------------------------------------- |
| `input`                 | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel]]` | Primary input message (can be any format)                                  |
| `previous_step_content` | `Optional[Any]`                                              | Content from the last step                                                 |
| `previous_step_outputs` | `Optional[Dict[str, StepOutput]]`                            | All previous step outputs by name                                          |
| `additional_data`       | `Optional[Dict[str, Any]]`                                   | Additional context data                                                    |
| `images`                | `Optional[List[Image]]`                                      | Media inputs - images (accumulated from workflow input and previous steps) |
| `videos`                | `Optional[List[Video]]`                                      | Media inputs - videos (accumulated from workflow input and previous steps) |
| `audio`                 | `Optional[List[Audio]]`                                      | Media inputs - audio (accumulated from workflow input and previous steps)  |
| `files`                 | `Optional[List[File]]`                                       | File inputs (accumulated from workflow input and previous steps)           |

| Method                                        | Return Type                            | Description                                                     |
| --------------------------------------------- | -------------------------------------- | --------------------------------------------------------------- |
| `get_step_output(step_name: str)`             | `Optional[StepOutput]`                 | Get the complete StepOutput object from a specific step by name |
| `get_step_content(step_name: str)`            | `Optional[Union[str, Dict[str, str]]]` | Get content from a specific step by name                        |
| `get_all_previous_content()`                  | `str`                                  | Get all previous step content combined                          |
| `get_last_step_content()`                     | `Optional[str]`                        | Get content from the immediate previous step                    |
| `get_workflow_history(num_runs: int)`         | `List[Tuple[str, str]]`                | Get the workflow history as a list of tuples                    |
| `get_workflow_history_context(num_runs: int)` | `str`                                  | Get the workflow history as a formatted context string          |

---

## Set environment variables for Langfuse

**URL:** llms-txt#set-environment-variables-for-langfuse

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://us.cloud.langfuse.com/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

---

## Filtering on PgVector

**URL:** llms-txt#filtering-on-pgvector

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-pgvector

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in PgVector.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pgvector import PgVector

---

## Code Execution Tool

**URL:** llms-txt#code-execution-tool

**Contents:**
- Working example

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/code-execution

Learn how to use Anthropic's code execution tool with Agno.

With Anthropic's [code execution tool](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool), your model can execute Python code in a secure, sandboxed environment.
This is useful for your model to perform tasks as analyzing data, creating visualizations, or performing complex calculations.

---

## Setup your Memory Manager, to adjust how memories are created

**URL:** llms-txt#setup-your-memory-manager,-to-adjust-how-memories-are-created

memory_manager = MemoryManager(
    db=db,
    # Select the model used for memory creation and updates. If unset, the default model of the Agent is used.
    model=OpenAIChat(id="gpt-5-mini"),
    # You can also provide additional instructions
    additional_instructions="Don't store the user's real name",
)

---

## Define agents for use in custom functions

**URL:** llms-txt#define-agents-for-use-in-custom-functions

hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[HackerNewsTools()],
    instructions="Extract key insights and content from Hackernews posts",
    db=InMemoryDb(),
)

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Search the web for the latest news and trends",
    db=InMemoryDb(),
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
    db=InMemoryDb(),
)

async def hackernews_research_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function for HackerNews research with enhanced processing and streaming
    """
    message = step_input.input

research_prompt = f"""
        HACKERNEWS RESEARCH REQUEST:

Research Tasks:
        1. Search for relevant HackerNews posts and discussions
        2. Extract key insights and trends
        3. Identify popular opinions and debates
        4. Summarize technical developments
        5. Note community sentiment and engagement levels

Please provide comprehensive HackerNews research results.
    """

try:
        # Stream the agent response
        response_iterator = hackernews_agent.arun(
            research_prompt, stream=True, stream_events=True
        )
        async for event in response_iterator:
            yield event

# Get the final response
        response = hackernews_agent.get_last_run_output()

# Check if response and content exist
        response_content = ""
        if response and hasattr(response, "content") and response.content:
            response_content = response.content
        else:
            response_content = "No content available from HackerNews research"

enhanced_content = f"""
            ## HackerNews Research Results

**Research Topic:** {message}
            **Source:** HackerNews Community Analysis
            **Processing:** Enhanced with custom streaming function

**Findings:**
            {response_content}

**Custom Function Enhancements:**
            - Community Focus: HackerNews developer perspectives
            - Technical Depth: High-level technical discussions
            - Trend Analysis: Developer sentiment and adoption patterns
            - Streaming: Real-time research progress updates
        """.strip()

yield StepOutput(content=enhanced_content)

except Exception as e:
        yield StepOutput(
            content=f"HackerNews research failed: {str(e)}",
            success=False,
        )

async def web_search_research_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function for web search research with enhanced processing and streaming
    """
    message = step_input.input

research_prompt = f"""
        WEB SEARCH RESEARCH REQUEST:

Research Tasks:
        1. Search for the latest news and articles
        2. Identify market trends and business implications
        3. Find expert opinions and analysis
        4. Gather statistical data and reports
        5. Note mainstream media coverage and public sentiment

Please provide comprehensive web research results.
    """

try:
        # Stream the agent response
        response_iterator = web_agent.arun(
            research_prompt, stream=True, stream_events=True
        )
        async for event in response_iterator:
            yield event

# Get the final response
        response = web_agent.get_last_run_output()

# Check if response and content exist
        response_content = ""
        if response and hasattr(response, "content") and response.content:
            response_content = response.content
        else:
            response_content = "No content available from web search research"

enhanced_content = f"""
            ## Web Search Research Results

**Research Topic:** {message}
            **Source:** General Web Search Analysis
            **Processing:** Enhanced with custom streaming function

**Findings:**
            {response_content}

**Custom Function Enhancements:**
            - Market Focus: Business and mainstream perspectives
            - Trend Analysis: Public adoption and market signals
            - Data Integration: Statistical and analytical insights
            - Streaming: Real-time research progress updates
        """.strip()

yield StepOutput(content=enhanced_content)

except Exception as e:
        yield StepOutput(
            content=f"Web search research failed: {str(e)}",
            success=False,
        )

async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness and streaming
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

# Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

Core Topic: {message}

Research Results: {previous_step_content[:1000] if previous_step_content else "No research results"}

Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

Please create a detailed, actionable content plan.
    """

try:
        # Stream the agent response
        response_iterator = content_planner.arun(
            planning_prompt, stream=True, stream_events=True
        )
        async for event in response_iterator:
            yield event

# Get the final response
        response = content_planner.get_last_run_output()

# Check if response and content exist
        response_content = ""
        if response and hasattr(response, "content") and response.content:
            response_content = response.content
        else:
            response_content = "No content available from content planning"

enhanced_content = f"""
            ## Strategic Content Plan

**Planning Topic:** {message}

**Research Integration:** {"âœ“ Multi-source research" if previous_step_content else "âœ— No research foundation"}

**Content Strategy:**
            {response_content}

**Custom Planning Enhancements:**
            - Research Integration: {"High (Parallel sources)" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
            - Source Diversity: HackerNews + Web + Social insights
            - Streaming: Real-time planning progress updates
        """.strip()

yield StepOutput(content=enhanced_content)

except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )

---

## Fast: Filter first, then search

**URL:** llms-txt#fast:-filter-first,-then-search

results = knowledge.search(
    query="deployment process",
    max_results=10,
    filters={"department": "engineering", "type": "procedure"}
)

---

## agent.print_response("Create a simple web app that displays a random number between 1 and 100.")

**URL:** llms-txt#agent.print_response("create-a-simple-web-app-that-displays-a-random-number-between-1-and-100.")

**Contents:**
- Usage

bash  theme={null}
    export V0_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/vercel/basic.py
      bash Windows theme={null}
      python cookbook/models/vercel/basic.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add a directory of PDFs using the PDF reader

**URL:** llms-txt#add-a-directory-of-pdfs-using-the-pdf-reader

**Contents:**
- Best Practices for Search & Retrieval
  - Content Strategy
  - Technical Optimization
  - Monitoring and Improvement
- Next Steps

knowledge.add_content(path="presentations/", reader=PdfReader())
```

## Best Practices for Search & Retrieval

* Organize logically; group related content
* Use consistent terminology
* Include context and cross-references
* Keep content current; retire outdated docs

### Technical Optimization

* Choose appropriate chunk sizes and strategies
* Select a quality embedder for your domain
* Configure VectorDB search type (vector/keyword/hybrid)
* Add a reranker for better ordering

### Monitoring and Improvement

* Test with real user queries regularly
* Observe what agents retrieve
* Gather feedback when answers lack context
* Iterate on chunking, metadata, and search configuration

<CardGroup cols={3}>
  <Card title="Advanced Filtering" icon="filter" href="/basics/knowledge/filters/advanced-filtering">
    Use filter expressions for complex logical conditions
  </Card>

<Card title="Content Database" icon="database" href="/basics/knowledge/content-db">
    Learn how content metadata is tracked and managed
  </Card>

<Card title="Vector Databases" icon="vector-square" href="/basics/vectordb/overview">
    Explore storage options for your knowledge base
  </Card>

<Card title="Hybrid Search" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/hybrid-search">
    Deep dive into advanced search strategies
  </Card>

<Card title="Performance Tips" icon="gauge" href="/basics/knowledge/performance-tips">
    Optimize your knowledge base for speed and accuracy
  </Card>
</CardGroup>

---

## Setup team with in-memory database

**URL:** llms-txt#setup-team-with-in-memory-database

**Contents:**
- Developer Resources

db = InMemoryDb()
team = Team(
    name="Research Team",
    members=[hn_researcher, web_searcher],
    db=db,
)

team.print_response("Find top AI news")
```

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/in_memory/in_memory_storage_for_team.py)

---

## Custom Middleware

**URL:** llms-txt#custom-middleware

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/middleware/custom-middleware

AgentOS with custom middleware for rate limiting, logging, and monitoring

This example demonstrates how to create and add custom middleware to your AgentOS application. We implement two common middleware types: rate limiting and request/response logging.

```python custom_middleware.py theme={null}
import time
from collections import defaultdict, deque
from typing import Dict

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import Request, Response
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware

---

## Generate Video using Models Lab

**URL:** llms-txt#generate-video-using-models-lab

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/video/usage/generate-video-models-lab

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Vertex AI Claude

**URL:** llms-txt#vertex-ai-claude

**Contents:**
- Authentication
- Example
- Parameters

Source: https://docs.agno.com/integrations/models/cloud/vertexai-claude/overview

Learn how to use Vertex AI Claude models with Agno.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.3" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.3">v2.1.3</Tooltip>
</Badge>

Use Claude models through Vertex AI. This provides a native Claude integration optimized for Vertex AI infrastructure.

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `claude-sonnet-4@20250514` model is good for most use-cases.
* `claude-opus-4@20250805` model is their best model.

Set your `GOOGLE_CLOUD_PROJECT` and `CLOUD_ML_REGION` environment variables.

And then authenticate your CLI session:

Use `Claude` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note>View more examples [here](/integrations/models/cloud/vertexai-claude/usage/basic-stream). </Note>

| Parameter    | Type  | Default                      | Description                                        |
| ------------ | ----- | ---------------------------- | -------------------------------------------------- |
| `id`         | `str` | `"claude-sonnet-4@20250514"` | The specific Vertex AI Claude model ID to use      |
| `name`       | `str` | `"Claude"`                   | The name identifier for the Vertex AI Claude model |
| `provider`   | `str` | `"VertexAI"`                 | The provider of the model                          |
| `region`     | `str` | `"None`                      | The region to use for the model                    |
| `project_id` | `str` | `None`                       | The project ID to use for the model                |
| `base_url`   | `str` | `None`                       | The base URL to use for the model                  |

`Claude` (Vertex AI) extends the [Anthropic Claude](/integrations/models/native/anthropic/overview) model with Vertex AI integration and has access to most of the same parameters.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And then authenticate your CLI session:
```

Example 3 (unknown):
```unknown
## Example

Use `Claude` with your `Agent`:

<CodeGroup>
```

---

## Print workflow duration

**URL:** llms-txt#print-workflow-duration

if response.metrics and response.metrics.duration:
    print(f"\nTotal execution time: {response.metrics.duration:.2f} seconds")

---

## Security dependency

**URL:** llms-txt#security-dependency

security = HTTPBearer()

async def verify_token(token: str = Depends(security)):
    if token.credentials != "your-secret-token":
        raise HTTPException(status_code=401, detail="Invalid token")
    return token

---

## Welcome to Agno

**URL:** llms-txt#welcome-to-agno

Source: https://docs.agno.com/index

Build, run, and manage multi-agent systems with Agno. The leading framework for creating AI agents, teams, and workflows with powerful tools and deployment options.

export const CodeTabs = ({ tabs, defaultTab }) => {
  const [activeTab, setActiveTab] = React.useState(defaultTab || tabs[0]?.id);
  const currentTab = tabs.find((tab) => tab.id === activeTab) || tabs[0];

return (
    <div className="flex flex-col gap-6 w-full">
      <style dangerouslySetInnerHTML={{__html: `
        .code-tab-btn {
          padding: 0;
          border: none;
          background: transparent;
          cursor: pointer;
          font-family: inherit;
          font-size: 14px;
          transition: color 0.2s ease;
          color: #71717a;
          font-weight: 400;
        }
        .code-tab-btn.active {
          color: #18181b;
          font-weight: 600;
        }
        .dark .code-tab-btn.active {
          color: #f4f4f5;
        }
        .code-tab-bracket {
          margin-right: 4px;
          visibility: hidden;
        }
        .code-tab-bracket-right {
          margin-left: 4px;
          visibility: hidden;
        }
        .code-tab-btn.active .code-tab-bracket,
        .code-tab-btn.active .code-tab-bracket-right {
          visibility: visible;
        }
        .kw { color: #ff4017; }
        .str { color: #c3e88d; }
        .num { color: #c792ea; }
        .cmt { color: #71717a; }
      `}} />
      <div className="grid grid-cols-2 gap-4 md:flex md:gap-8">
        {tabs.map((tab) => {
          const isActive = activeTab === tab.id;
          return (
            <button
              key={tab.id}
              onClick={() => setActiveTab(tab.id)}
              className={isActive ? 'code-tab-btn active' : 'code-tab-btn'}
            >
              <span className="code-tab-bracket">[</span>
              {tab.label}
              <span className="code-tab-bracket-right">]</span>
            </button>
          );
        })}
      </div>

<style dangerouslySetInnerHTML={{__html: `
        .code-box {
          position: relative;
          border-radius: 16px;
          border: 1px solid #27272a;
          background: #09090b;
          padding: 20px;
          overflow: auto;
        }
        .code-box pre {
          margin: 0;
          padding: 0;
          overflow: auto;
          font-size: 13px;
          line-height: 1.8;
          font-family: "JetBrains Mono", "Fira Code", Monaco, Consolas, monospace;
        }
        .code-box code {
          color: #e5e7eb;
          white-space: pre;
        }
        .code-lang {
          display: block;
          font-size: 10px;
          text-transform: uppercase;
          letter-spacing: 0.15em;
          color: #ff4017;
          margin-bottom: 16px;
          font-weight: 600;
        }
      `}} />
      <div className="code-box">
        <pre>
          <code>
            {currentTab?.language && (
              <span className="code-lang">{currentTab.language}</span>
            )}
            {currentTab?.code}
          </code>
        </pre>
      </div>
    </div>
  );
};

export const codeTabs = [
    {
        id: 'agents',
        label: 'Agents',
        language: 'python',
        code: (
          <>
            <span style={{color:'#ff4017'}}>from</span> agno.agent <span style={{color:'#ff4017'}}>import</span> Agent{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.models.anthropic <span style={{color:'#ff4017'}}>import</span> Claude{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.tools.hackernews <span style={{color:'#ff4017'}}>import</span> HackerNewsTools{'\n'}
            {'\n'}
            agent = Agent({'\n'}
            {'    '}model=Claude(id=<span style={{color:'#c3e88d'}}>"claude-sonnet-4-5"</span>),{'\n'}
            {'    '}tools=[HackerNewsTools()],{'\n'}
            {'    '}markdown=<span style={{color:'#ff4017'}}>True</span>,{'\n'}
            ){'\n'}
            agent.print_response(<span style={{color:'#c3e88d'}}>"Write a report on trending startups."</span>, stream=<span style={{color:'#ff4017'}}>True</span>)
          </>
        ),
    },
    {
        id: 'teams',
        label: 'Teams',
        language: 'python',
        code: (
          <>
            <span style={{color:'#ff4017'}}>from</span> agno.agent <span style={{color:'#ff4017'}}>import</span> Agent{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.team <span style={{color:'#ff4017'}}>import</span> Team{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.tools.hackernews <span style={{color:'#ff4017'}}>import</span> HackerNewsTools{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.tools.newspaper4k <span style={{color:'#ff4017'}}>import</span> Newspaper4kTools{'\n'}
            {'\n'}
            hn_researcher = Agent({'\n'}
            {'    '}name=<span style={{color:'#c3e88d'}}>"HackerNews Researcher"</span>,{'\n'}
            {'    '}tools=[HackerNewsTools()],{'\n'}
            ){'\n'}
            article_reader = Agent({'\n'}
            {'    '}name=<span style={{color:'#c3e88d'}}>"Article Reader"</span>,{'\n'}
            {'    '}tools=[Newspaper4kTools()],{'\n'}
            ){'\n'}
            {'\n'}
            team = Team(members=[hn_researcher, article_reader]){'\n'}
            team.print_response(<span style={{color:'#c3e88d'}}>"Research AI trends and summarize the top articles"</span>)
          </>
        ),
    },
    {
        id: 'workflows',
        label: 'Workflows',
        language: 'python',
        code: (
          <>
            <span style={{color:'#ff4017'}}>from</span> agno.agent <span style={{color:'#ff4017'}}>import</span> Agent{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.workflow <span style={{color:'#ff4017'}}>import</span> Workflow{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.tools.duckduckgo <span style={{color:'#ff4017'}}>import</span> DuckDuckGoTools{'\n'}
            {'\n'}
            researcher = Agent(name=<span style={{color:'#c3e88d'}}>"Researcher"</span>, tools=[DuckDuckGoTools()]){'\n'}
            writer = Agent(name=<span style={{color:'#c3e88d'}}>"Writer"</span>, instructions=<span style={{color:'#c3e88d'}}>"Write engaging content"</span>){'\n'}
            {'\n'}
            workflow = Workflow({'\n'}
            {'    '}name=<span style={{color:'#c3e88d'}}>"Content Workflow"</span>,{'\n'}
            {'    '}description=<span style={{color:'#c3e88d'}}>"A workflow for creating content"</span>,{'\n'}
            {'    '}steps=[researcher, writer],{'\n'}
            ){'\n'}
            {'\n'}
            workflow.print_response(<span style={{color:'#c3e88d'}}>"Create a blog post about AI agents"</span>, stream=<span style={{color:'#ff4017'}}>True</span>)
          </>
        ),
    },
    {
        id: 'agentos',
        label: 'AgentOS',
        language: 'python',
        code: (
          <>
            <span style={{color:'#ff4017'}}>from</span> agno.agent <span style={{color:'#ff4017'}}>import</span> Agent{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.db.sqlite <span style={{color:'#ff4017'}}>import</span> SqliteDb{'\n'}
            <span style={{color:'#ff4017'}}>from</span> agno.os <span style={{color:'#ff4017'}}>import</span> AgentOS{'\n'}
            {'\n'}
            agent = Agent({'\n'}
            {'    '}name=<span style={{color:'#c3e88d'}}>"Support Agent"</span>,{'\n'}
            {'    '}db=SqliteDb(db_file=<span style={{color:'#c3e88d'}}>"agno.db"</span>),{'\n'}
            {'    '}add_history_to_context=<span style={{color:'#ff4017'}}>True</span>,{'\n'}
            ){'\n'}
            {'\n'}
            agent_os = AgentOS(agents=[agent]){'\n'}
            app = agent_os.get_app()  <span style={{color:'#71717a'}}># FastAPI app ready to deploy</span>
          </>
        ),
    },
];

<div className="relative mx-[calc(50%-50vw)] bg-[#f6f7f9] dark:bg-[#111113] py-16 md:py-16 border-y border-[#e5e7eb] dark:border-zinc-800 overflow-hidden">
  <div
    className="absolute inset-0 pointer-events-none opacity-30 dark:opacity-0"
    style={{
    backgroundImage:
      'linear-gradient(#d1d5db 1px, transparent 1px), linear-gradient(90deg, #d1d5db 1px, transparent 1px)',
    backgroundSize: '290px 290px',
    backgroundPosition: 'center',
  }}
  />

<div className="relative mx-auto flex max-w-6xl flex-col px-5 sm:px-8 lg:px-10">
    <header className="flex flex-col gap-3 rounded-full border border-[#e5e7eb] dark:border-zinc-700 bg-white dark:bg-zinc-900 px-5 py-3 shadow-sm sm:flex-row sm:items-center sm:justify-between">
      <div className="flex items-center gap-3">
        <span className="text-lg font-semibold tracking-tight text-zinc-900 dark:text-zinc-100">Agno</span>

<span className="rounded-full border border-[#e5e7eb] dark:border-zinc-700 px-3 py-1 text-xs font-semibold uppercase tracking-[0.2em] text-zinc-500 dark:text-zinc-400">
          Docs
        </span>
      </div>

<div className="flex items-center gap-3">
        <a href="/introduction" className="rounded-full border border-[#d1d5db] dark:border-zinc-600 bg-white dark:bg-zinc-800 px-4 py-2 text-sm font-semibold text-zinc-800 dark:text-zinc-100 transition hover:bg-zinc-50 dark:hover:bg-zinc-700">
          Read the docs
        </a>

<a href="https://os.agno.com" className="rounded-full border border-[#ff4017] bg-[#ff4017] px-4 py-2 text-sm font-semibold text-white shadow-[0_10px_20px_rgba(255,64,23,0.25)] transition hover:bg-[#ff2e00]">
          Try AgentOS
        </a>
      </div>
    </header>

<section className="mt-20 flex flex-col items-center gap-6 text-center">
      <h1 className="max-w-4xl text-4xl font-semibold leading-[1.05] text-zinc-900 dark:text-zinc-100 sm:text-5xl md:text-[64px]">
        Build. Run. Manage.<br />Multi-Agent Systems

{/* Blinking Cursor Animation */}

{(() => {
                  const [visible, setVisible] = React.useState(true);
                  React.useEffect(() => {
                    const interval = setInterval(() => setVisible(v => !v), 600);
                    return () => clearInterval(interval);
                  }, []);
                  return (
                    <span style={{
                      display: 'inline-block',
                      width: '0.5em',
                      height: '0.1em',
                      backgroundColor: '#ff4017',
                      marginLeft: '0.1em',
                      verticalAlign: 'baseline',
                      position: 'relative',
                      top: '0.10em',
                      opacity: visible ? 1 : 0,
                      transition: 'opacity 0.5s ease-in-out',
                    }} />
                  );
                })()}
      </h1>

{/* ============================================
                QUICK START CARDS
                ============================================ */}

<div className="mt-12 grid w-full max-w-5xl grid-cols-1 gap-4 md:grid-cols-3">
        {/* Getting Started Card */}

<a href="/introduction" className="group relative flex flex-col gap-2 rounded-xl bg-[#f6f7f9] dark:bg-zinc-900 border border-[#e5e7eb] dark:border-zinc-700 p-5 transition hover:border-[#e5e7eb] dark:hover:border-zinc-600">
          <span className="pointer-events-none absolute -left-2 -top-2 h-4 w-4 border-l-2 border-t-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -right-2 -top-2 h-4 w-4 border-r-2 border-t-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -bottom-2 -left-2 h-4 w-4 border-b-2 border-l-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -bottom-2 -right-2 h-4 w-4 border-b-2 border-r-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<h3 className="text-base font-semibold text-start text-zinc-900 dark:text-zinc-100">Get Started</h3>

<p className="text-xs leading-relaxed text-start text-zinc-500 dark:text-zinc-400">
            Learn the basics and build your first Agent
          </p>
        </a>

<a href="/agent-os/introduction" className="group relative flex flex-col gap-2 rounded-xl border border-[#e5e7eb] dark:border-zinc-700 bg-[#f6f7f9] dark:bg-zinc-900 p-5 transition hover:border-[#e5e7eb] dark:hover:border-zinc-600">
          <span className="pointer-events-none absolute -left-2 -top-2 h-4 w-4 border-l-2 border-t-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -right-2 -top-2 h-4 w-4 border-r-2 border-t-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -bottom-2 -left-2 h-4 w-4 border-b-2 border-l-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -bottom-2 -right-2 h-4 w-4 border-b-2 border-r-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<h3 className="text-base font-semibold text-start text-zinc-900 dark:text-zinc-100">AgentOS</h3>

<p className="text-xs leading-relaxed text-start text-zinc-500 dark:text-zinc-400">
            Deploy and manage Agents at scale in your cloud
          </p>
        </a>

{/* Reference Card */}

<a href="/reference/agents/agent" className="group relative flex flex-col gap-2 rounded-xl border border-[#e5e7eb] dark:border-zinc-700 bg-[#f6f7f9] dark:bg-zinc-900 p-5 transition hover:border-[#e5e7eb] dark:hover:border-zinc-600">
          <span className="pointer-events-none absolute -left-2 -top-2 h-4 w-4 border-l-2 border-t-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -right-2 -top-2 h-4 w-4 border-r-2 border-t-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -bottom-2 -left-2 h-4 w-4 border-b-2 border-l-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<span className="pointer-events-none absolute -bottom-2 -right-2 h-4 w-4 border-b-2 border-r-2 border-[#ff4017] opacity-0 transition-opacity group-hover:opacity-100" />

<h3 className="text-base font-semibold text-start text-zinc-900 dark:text-zinc-100">Reference</h3>

<p className="text-xs leading-relaxed text-start text-zinc-500 dark:text-zinc-400">
            Complete SDK and API reference for developers
          </p>
        </a>
      </div>
    </section>

<section className="mt-20 flex flex-col gap-6">
      <div className="flex flex-col gap-2 text-center">
        <p className="text-xs font-semibold uppercase tracking-[0.35em] text-zinc-500 dark:text-zinc-400">Build with agno</p>
      </div>

<div className=" lg:p-10">
        <CodeTabs tabs={codeTabs} defaultTab="agents" />
      </div>
    </section>

<section className="mt-20 pb-12">
      {/* Footer Links Grid */}

<div className="mx-auto grid max-w-6xl grid-cols-2 gap-8 px-5 sm:px-8 lg:grid-cols-5 lg:px-10">
        <div>
          <h3 className="mb-4 text-xs font-semibold uppercase tracking-wider text-zinc-900 dark:text-zinc-100">Get Started</h3>

<ul className="space-y-3">
            <li><a href="/get-started/quickstart" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Quickstart</a></li>
            <li><a href="/get-started/getting-help" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Getting Help</a></li>
          </ul>
        </div>

<div>
          <h3 className="mb-4 text-xs font-semibold uppercase tracking-wider text-zinc-900 dark:text-zinc-100">Concepts</h3>

<ul className="space-y-3">
            <li><a href="/basics/agents/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Agents</a></li>
            <li><a href="/basics/teams/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Teams</a></li>
            <li><a href="/basics/workflows/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Workflows</a></li>
            <li><a href="/basics/tools/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Tools</a></li>
          </ul>
        </div>

<div>
          <h3 className="mb-4 text-xs font-semibold uppercase tracking-wider text-zinc-900 dark:text-zinc-100">AgentOS</h3>

<ul className="space-y-3">
            <li><a href="/agent-os/introduction" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Overview</a></li>
            <li><a href="/agent-os/creating-your-first-os" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Getting Started</a></li>
            <li><a href="/agent-os/config" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Configuration</a></li>
            <li><a href="/deploy/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Deploy</a></li>
          </ul>
        </div>

<div>
          <h3 className="mb-4 text-xs font-semibold uppercase tracking-wider text-zinc-900 dark:text-zinc-100">Reference</h3>

<ul className="space-y-3">
            <li><a href="/reference/agents/agent" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">SDK Reference</a></li>
            <li><a href="/reference-api/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">API Reference</a></li>
            <li><a href="/examples/use-cases/agents/overview" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Examples</a></li>

{/* <li><a href="/templates/overview" className="text-sm text-zinc-500 transition hover:text-[#ff4017]">Templates</a></li> */}
          </ul>
        </div>

<div>
          <h3 className="mb-4 text-xs font-semibold uppercase tracking-wider text-zinc-900 dark:text-zinc-100">Community</h3>

<ul className="space-y-3">
            <li><a href="https://github.com/agno-agi/agno" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">GitHub</a></li>
            <li><a href="https://agno.link/discord" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">Discord</a></li>
            <li><a href="https://x.com/AgnoAgi" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">X (Twitter)</a></li>
            <li><a href="https://agno.link/youtube" className="text-sm text-zinc-500 dark:text-zinc-400 transition hover:text-[#ff4017]">YouTube</a></li>
          </ul>
        </div>
      </div>

{/* Footer Bottom - Copyright & Social Links */}

<div className="mx-auto mt-12 flex max-w-6xl items-center justify-between px-5 pt-8 sm:px-8 lg:px-10">
        <p className="text-xs text-zinc-400 dark:text-zinc-500">Â© 2025 Agno. All rights reserved.</p>

<div className="flex items-center gap-4">
          <a href="https://x.com/AgnoAgi" className="text-zinc-400 dark:text-zinc-500 transition hover:text-[#ff4017]">
            <svg className="h-5 w-5" fill="currentColor" viewBox="0 0 24 24">
              <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" />
            </svg>
          </a>

<a href="https://github.com/agno-agi/agno" className="text-zinc-400 dark:text-zinc-500 transition hover:text-[#ff4017]">
            <svg className="h-5 w-5" fill="currentColor" viewBox="0 0 24 24">
              <path d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" />
            </svg>
          </a>

<a href="https://agno.link/discord" className="text-zinc-400 dark:text-zinc-500 transition hover:text-[#ff4017]">
            <svg className="h-5 w-5" fill="currentColor" viewBox="0 0 24 24">
              <path d="M20.317 4.37a19.791 19.791 0 00-4.885-1.515.074.074 0 00-.079.037c-.21.375-.444.864-.608 1.25a18.27 18.27 0 00-5.487 0 12.64 12.64 0 00-.617-1.25.077.077 0 00-.079-.037A19.736 19.736 0 003.677 4.37a.07.07 0 00-.032.027C.533 9.046-.32 13.58.099 18.057a.082.082 0 00.031.057 19.9 19.9 0 005.993 3.03.078.078 0 00.084-.028 14.09 14.09 0 001.226-1.994.076.076 0 00-.041-.106 13.107 13.107 0 01-1.872-.892.077.077 0 01-.008-.128 10.2 10.2 0 00.372-.292.074.074 0 01.077-.01c3.928 1.793 8.18 1.793 12.062 0a.074.074 0 01.078.01c.12.098.246.198.373.292a.077.077 0 01-.006.127 12.299 12.299 0 01-1.873.892.077.077 0 00-.041.107c.36.698.772 1.362 1.225 1.993a.076.076 0 00.084.028 19.839 19.839 0 006.002-3.03.077.077 0 00.032-.054c.5-5.177-.838-9.674-3.549-13.66a.061.061 0 00-.031-.03zM8.02 15.33c-1.183 0-2.157-1.085-2.157-2.419 0-1.333.956-2.419 2.157-2.419 1.21 0 2.176 1.096 2.157 2.42 0 1.333-.956 2.418-2.157 2.418zm7.975 0c-1.183 0-2.157-1.085-2.157-2.419 0-1.333.955-2.419 2.157-2.419 1.21 0 2.176 1.096 2.157 2.42 0 1.333-.946 2.418-2.157 2.418z" />
            </svg>
          </a>

<a href="https://agno.link/youtube" className="text-zinc-400 dark:text-zinc-500 transition hover:text-[#ff4017]">
            <svg className="h-5 w-5" fill="currentColor" viewBox="0 0 24 24">
              <path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z" />
            </svg>
          </a>
        </div>
      </div>
    </section>
  </div>
</div>

---

## Background Workflow Execution

**URL:** llms-txt#background-workflow-execution

**Contents:**
- Example

Source: https://docs.agno.com/basics/workflows/background-execution

How to execute workflows as non-blocking background tasks

Execute workflows as non-blocking background tasks by passing `background=True` to `Workflow.arun()`. This returns a `WorkflowRunOutput` object with a `run_id` for polling the workflow status until completion.

<Note>
  Background execution requires async workflows using `.arun()`. Poll for results using `workflow.get_run(run_id)` and check completion status with `.has_completed()`.

Ideal for long-running operations like large-scale data processing, multi-step research, or batch operations that shouldn't block your main application thread.
</Note>

```python  theme={null}
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Image Model Output

**URL:** llms-txt#image-model-output

**Contents:**
- Image Output Modality

Source: https://docs.agno.com/basics/multimodal/images/image-output

Learn how to use images from models as output with Agno agents.

Similar to providing image inputs, you can also get image outputs from an agent. Take a look at the [compatibility matrix](/basics/models/compatibility#multimodal-support) to see which models support image as output.

## Image Output Modality

The following example demonstrates how some models can directly generate images as part of their response.

```python image_agent.py theme={null}
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini
from PIL import Image

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"], # Generate both images and text in the response
    )
)

---

## Audio Sentiment Analysis

**URL:** llms-txt#audio-sentiment-analysis

Source: https://docs.agno.com/basics/multimodal/agent/usage/audio-sentiment-analysis

This example demonstrates how to perform sentiment analysis on audio conversations using Agno agents with multimodal capabilities.

```python audio_sentiment_analysis.py theme={null}
import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import Audio
from agno.models.google import Gemini

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    add_history_to_context=True,
    markdown=True,
    db=SqliteDb(
        session_table="audio_sentiment_analysis_sessions",
        db_file="tmp/audio_sentiment_analysis.db",
    ),
)

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

response = requests.get(url)
audio_content = response.content

---

## Primary knowledge base for main retrieval

**URL:** llms-txt#primary-knowledge-base-for-main-retrieval

primary_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_primary",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## Shell

**URL:** llms-txt#shell

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/local/shell

**ShellTools** enable an Agent to interact with the shell to run commands.

The following agent will run a shell command and show contents of the current directory.

<Note>
  Mention your OS to the agent to make sure it runs the correct command.
</Note>

| Parameter                  | Type   | Default | Description                                 |                                            |
| -------------------------- | ------ | ------- | ------------------------------------------- | ------------------------------------------ |
| `base_dir`                 | \`Path | str\`   | `None`                                      | Base directory for shell command execution |
| `enable_run_shell_command` | `bool` | `True`  | Enables functionality to run shell commands |                                            |
| `all`                      | `bool` | `False` | Enables all functionality when set to True  |                                            |

| Function            | Description                                           |
| ------------------- | ----------------------------------------------------- |
| `run_shell_command` | Runs a shell command and returns the output or error. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/shell.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/shell_tools.py)

---

## Access Multiple Previous Steps Output

**URL:** llms-txt#access-multiple-previous-steps-output

**Contents:**
- Key Features:

Source: https://docs.agno.com/basics/workflows/usage/access-multiple-previous-steps-output

This example demonstrates **Workflows 2.0** advanced data flow capabilities

This example demonstrates **Workflows 2.0** shows how to:

1. Access outputs from **specific named steps** (`get_step_content()`)
2. Aggregate **all previous outputs** (`get_all_previous_content()`)
3. Create comprehensive reports by combining multiple research sources

* **Step Output Access**: Retrieve data from any previous step by name or collectively.
* **Custom Reporting**: Combine and analyze outputs from parallel or sequential steps.
* **Streaming Support**: Real-time updates during execution.

```python access_multiple_previous_steps_output.py theme={null}
from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## BaseGuardrail

**URL:** llms-txt#baseguardrail

**Contents:**
- Methods
  - `check`
  - `async_check`

Source: https://docs.agno.com/reference/hooks/base-guardrail

Perform the guardrail checks synchronously.

* `run_input` (RunInput | TeamRunInput): The input provided to the Agent or Team when invoking the run.

Perform the guardrail checks asynchronously.

* `run_input` (RunInput | TeamRunInput): The input provided to the Agent or Team when invoking the run.

---

## Delete Content by ID

**URL:** llms-txt#delete-content-by-id

Source: https://docs.agno.com/reference-api/schema/knowledge/delete-content-by-id

delete /knowledge/content/{content_id}
Permanently remove a specific content item from the knowledge base. This action cannot be undone.

---

## Analyze aggregated team metrics

**URL:** llms-txt#analyze-aggregated-team-metrics

print("=" * 50)
print("AGGREGATED TEAM METRICS")
print("=" * 50)
pprint(run_output.metrics)

---

## Run the Agent. This will store a memory in our "my_memory_table"

**URL:** llms-txt#run-the-agent.-this-will-store-a-memory-in-our-"my_memory_table"

agent.print_response("I love sushi!", user_id="123")

---

## Agent with Image Input

**URL:** llms-txt#agent-with-image-input

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/cloud/aws-bedrock/usage/image-agent

AWS Bedrock supports image input with models like `amazon.nova-pro-v1:0`. You can use this to analyze images and get information about them.

```python image_agent.py theme={null}
from pathlib import Path
from agno.agent import Agent
from agno.media import Image
from agno.models.aws import AwsBedrock
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=AwsBedrock(id="amazon.nova-pro-v1:0"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

---

## Create test JSON data

**URL:** llms-txt#create-test-json-data

**Contents:**
- Usage
- Params

json_path = Path("tmp/test.json")
json_path.parent.mkdir(exist_ok=True)
test_data = {
    "users": [
        {"id": 1, "name": "John Doe", "role": "Developer"},
        {"id": 2, "name": "Jane Smith", "role": "Designer"}
    ],
    "project": "Knowledge Base System"
}
json_path.write_text(json.dumps(test_data, indent=2))

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="json_documents",
        db_url=db_url,
    ),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

async def main():
    # Add JSON content to knowledge base
    await knowledge.add_content_async(
        path=json_path,
        reader=JSONReader(),
    )

# Query the knowledge base
    await agent.aprint_response(
        "What information is available about the users?",
        markdown=True
    )

if __name__ == "__main__":
    asyncio.run(main())
bash  theme={null}
    pip install -U sqlalchemy psycopg pgvector agno openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python examples/basics/knowledge/readers/json_reader_async.py
      bash Windows theme={null}
      python examples/basics/knowledge/readers/json_reader_async.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="json-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## When you add content

**URL:** llms-txt#when-you-add-content

knowledge.add_content(
    name="Product Manual",
    path="docs/manual.pdf",
    metadata={"department": "engineering", "version": "2.1"}
)

---

## Set up your database

**URL:** llms-txt#set-up-your-database

db = SQLiteDb(db_file="agno.db")

---

## CI/CD

**URL:** llms-txt#ci/cd

**Contents:**
- Test and Validate on every PR
- Build Docker Images with Github Releases
- Build ECR Images with Github Releases

Source: https://docs.agno.com/templates/infra-management/ci-cd

Agno templates come pre-configured with [Github Actions](https://docs.github.com/en/actions) for CI/CD. We can

1. [Test and Validate on every PR](#test-and-validate-on-every-pr)
2. [Build Docker Images with Github Releases](#build-docker-images-with-github-releases)
3. [Build ECR Images with Github Releases](#build-ecr-images-with-github-releases)

## Test and Validate on every PR

Whenever a PR is opened against the `main` branch, a validate script runs that ensures

1. The changes are formatted using ruff
2. All unit-tests pass
3. The changes don't have any typing or linting errors.

Checkout the `.github/workflows/validate.yml` file for more information.

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=01d9c697e3f87a8248fa8daa6fac3922" alt="validate-cicd" data-og-width="940" width="940" data-og-height="353" height="353" data-path="images/validate-cicd.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=e8bf732e3895a4377b65766816624fc5 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=5dd89160c72ebf2f088d75bd8dfe52dc 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=8a27817ed129343cad278184145eb5ee 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=eb51747558d308ee09444057ba4f7f85 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=57ed9ba5331f3170a62cdfe304d9c05d 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=6b995305ffe38e39b14bbe7ef18ce4ba 2500w" />

## Build Docker Images with Github Releases

If you're using [Dockerhub](https://hub.docker.com/) for images, you can buld and push the images throug a Github Release. This action is defined in the `.github/workflows/docker-images.yml` file.

1. Create a [Docker Access Token](https://hub.docker.com/settings/security) for Github Actions

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=870118e1906c093108643eceaaf577e6" alt="docker-access-token" data-og-width="742" width="742" data-og-height="568" height="568" data-path="images/docker-access-token.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=44ecd6f45c24f63b65c47d63d5dda04e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7010dc82d6907e7a0e4c9b727a5b1f14 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f35b57f4d86867cd143d00861e9a188d 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ae91c75b8692c79f3f67b7c949a87305 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ce6d88dc5073bcbb6ec943c2ff9f1750 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c730143dfa3232d2daffbe8f04b77eb1 2500w" />

2. Create secret variables `DOCKERHUB_REPO`, `DOCKERHUB_TOKEN` and `DOCKERHUB_USERNAME` in your github repo. These variables are used by the action in `.github/workflows/docker-images.yml`

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=70015ffb61a3816c45806f85c8c44877" alt="github-actions-docker-secrets" data-og-width="1143" width="1143" data-og-height="822" height="822" data-path="images/github-actions-docker-secrets.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=98a34ba0df0ee5fa4ad3ed51852978d1 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=025950971a2df62fd409c74a6bf265df 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=fdb266b2ef9b01200bbf0704a607af87 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3a78c11b5d5fe0e1294bdea54fd03004 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e37288626fd82e5fa083c0bc48cf00c9 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f854830e5040e368d1154389162e173e 2500w" />

3. Run workflow using a Github Release

This workflow is configured to run when a release is created. Create a new release using:

<Note>
  Confirm the image name in the `.github/workflows/docker-images.yml` file before running
</Note>

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2f96812f7b12f8e1f9d831152556c5d7" alt="github-actions-build-docker" data-og-width="1042" width="1042" data-og-height="732" height="732" data-path="images/github-actions-build-docker.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ec6400a97ff55a3a44da52d9e53473ca 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=6568fab440d3f4794aecce651f2a3a0e 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c88fbcfbeb5e647b45e56d064c8066b6 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c7d8bc7f1d25a8167dcbe7920bfca3e6 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ab0a319474f8e5380c2dc9740715b916 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0a29ea2ec1f152a8142a2988768316da 2500w" />

<Note>
  You can also run the workflow using `gh workflow run`
</Note>

## Build ECR Images with Github Releases

If you're using ECR for images, you can buld and push the images through a Github Release. This action is defined in the `.github/workflows/ecr-images.yml` file and uses the new OpenID Connect (OIDC) approach to request the access token, without using IAM access keys.

We will follow this [guide](https://aws.amazon.com/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/) to create an IAM role which will be used by the github action.

1. Open the IAM console.
2. In the left navigation menu, choose Identity providers.
3. In the Identity providers pane, choose Add provider.
4. For Provider type, choose OpenID Connect.
5. For Provider URL, enter the URL of the GitHub OIDC IdP: [https://token.actions.githubusercontent.com](https://token.actions.githubusercontent.com)
6. Get thumbprint to verify the server certificate
7. For Audience, enter sts.amazonaws.com.

Verify the information matches the screenshot below and Add provider

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3eda54501351859a9afc8a041dc82139" alt="github-oidc-provider" data-og-width="1125" width="1125" data-og-height="799" height="799" data-path="images/github-oidc-provider.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=4c8f4ac0f7afae5f6c02d105fb827306 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2e714e3b3ef8993d0d0db50b9975eb12 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=96bbb52cb5fd2bd262fcb1d2eb2caf66 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=69a06894592bebeefa2170cdf776f424 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7cf6268657b4073faa5671f6710dcbff 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff74cc7bd14c9412ff7f9ef72d069e15 2500w" />

8. Assign a Role to the provider.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=dbd84c74dc15c2dd74311e69afd6a6cd" alt="github-oidc-provider-assign-role" data-og-width="1347" width="1347" data-og-height="587" height="587" data-path="images/github-oidc-provider-assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=fc0720d26b0176b03192881e0d00d4c7 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=305275efad18dd80e182f7442bfcb292 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0dc5facedf7d84a8e35ad5faa29409e7 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7723cea354fe2bc26fed0bccdf406853 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9038f7189b3752d863fdb6772d34ceae 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e0e20507f59fa7b57970bdd1b187072e 2500w" />

9. Create a new role.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e7b3d4a069f97ba3dbfe8bc08e8a534f" alt="github-oidc-provider-create-new-role" data-og-width="604" width="604" data-og-height="278" height="278" data-path="images/github-oidc-provider-create-new-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0b0bbe7da72790aeaa23eca25e846e12 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7536bf15de67ff8826aaaaa336d3b2ff 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=65907ad9152fa24dcd1fe791d6a1980d 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7396e3e8f50462a000d5cd3131cf7e94 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd64cc43cbf45bf06a66f40db3976251 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c8037ba2ea7bfc3e8f03dcb19ed66c9f 2500w" />

10. Confirm that Web identity is already selected as the trusted entity and the Identity provider field is populated with the IdP. In the Audience list, select sts.amazonaws.com, and then select Next.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3fe69db526ec7276382189d8d063561f" alt="github-oidc-provider-trusted-entity" data-og-width="1300" width="1300" data-og-height="934" height="934" data-path="images/github-oidc-provider-trusted-entity.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1eb0d8ae46efdbb4f0ce072de01a4287 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a54123b0b191d9587345115f28a5c2e2 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=74e9c6b7764f1ee331fc692808e898d0 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a8d61a562ca252b89940f25c67e94c4c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c2904811b03b257358ba3578dc0a4c8e 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd5780393d9adde78a009499ef3ba6bf 2500w" />

11. Add the `AmazonEC2ContainerRegistryPowerUser` permission to this role.

12. Create the role with the name `GithubActionsRole`.

13. Find the role `GithubActionsRole` and copy the ARN.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff1efeba61931aa435c13062d91a8f0b" alt="github-oidc-role" data-og-width="1389" width="1389" data-og-height="710" height="710" data-path="images/github-oidc-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1c1afadf6e661558e3cc861e2353a38d 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2231af7fff49341e8393eb7b49b610b1 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=43d85fdcd8f72dbe0ed7948a95793d38 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f05dc967abe03969118e755451c43a4a 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b5e1b433d11d461a5fc24c8b14e6bc91 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=8b7072639f56d42f00d00b8b735cb375 2500w" />

14. Create the ECR Repositories: `llm` and `jupyter-llm` which are built by the workflow.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c68ceb3a9b6784fd519cc04b0e38caf1" alt="create-ecr-image" data-og-width="1389" width="1389" data-og-height="408" height="408" data-path="images/create-ecr-image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ce02d48da7e53a6c335736a17ebec6e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f0b4d1687849a637c0a595c4a8d0690a 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=26b1f13eb8b6f9b09a06b9e6bb1eeb27 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e53f084201341a7c92738fa62efdb64c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c9e2477e1befaf12f81d4d345dac5a26 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a22af7830053ba9139cfb6d0d4017d4a 2500w" />

15. Update the workflow with the `GithubActionsRole` ARN and ECR Repository.

16. Update the `docker-images` workflow to **NOT** run on a release

17. Run workflow using a Github Release

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=aff1bdde8baea8591770b0f6b5ac036b" alt="github-actions-build-ecr" data-og-width="1389" width="1389" data-og-height="710" height="710" data-path="images/github-actions-build-ecr.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7440b9e93662a242501bdf7eb37f0620 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a62d3057b07926a447999d1b75a092dd 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=494393ce11efce791ab0d62f19b1b1fb 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2a85f548629bf8dc605f9964f99329fc 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=27878c2ccf71bb63426dc120b9233cd3 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9116b3c2eb2bf07b4e8f4c5070e3c1fa 2500w" />

<Note>
  You can also run the workflow using `gh workflow run`
</Note>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2f96812f7b12f8e1f9d831152556c5d7" alt="github-actions-build-docker" data-og-width="1042" width="1042" data-og-height="732" height="732" data-path="images/github-actions-build-docker.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ec6400a97ff55a3a44da52d9e53473ca 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=6568fab440d3f4794aecce651f2a3a0e 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c88fbcfbeb5e647b45e56d064c8066b6 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c7d8bc7f1d25a8167dcbe7920bfca3e6 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ab0a319474f8e5380c2dc9740715b916 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0a29ea2ec1f152a8142a2988768316da 2500w" />

<Note>
  You can also run the workflow using `gh workflow run`
</Note>

## Build ECR Images with Github Releases

If you're using ECR for images, you can buld and push the images through a Github Release. This action is defined in the `.github/workflows/ecr-images.yml` file and uses the new OpenID Connect (OIDC) approach to request the access token, without using IAM access keys.

We will follow this [guide](https://aws.amazon.com/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/) to create an IAM role which will be used by the github action.

1. Open the IAM console.
2. In the left navigation menu, choose Identity providers.
3. In the Identity providers pane, choose Add provider.
4. For Provider type, choose OpenID Connect.
5. For Provider URL, enter the URL of the GitHub OIDC IdP: [https://token.actions.githubusercontent.com](https://token.actions.githubusercontent.com)
6. Get thumbprint to verify the server certificate
7. For Audience, enter sts.amazonaws.com.

Verify the information matches the screenshot below and Add provider

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3eda54501351859a9afc8a041dc82139" alt="github-oidc-provider" data-og-width="1125" width="1125" data-og-height="799" height="799" data-path="images/github-oidc-provider.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=4c8f4ac0f7afae5f6c02d105fb827306 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2e714e3b3ef8993d0d0db50b9975eb12 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=96bbb52cb5fd2bd262fcb1d2eb2caf66 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=69a06894592bebeefa2170cdf776f424 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7cf6268657b4073faa5671f6710dcbff 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff74cc7bd14c9412ff7f9ef72d069e15 2500w" />

8. Assign a Role to the provider.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=dbd84c74dc15c2dd74311e69afd6a6cd" alt="github-oidc-provider-assign-role" data-og-width="1347" width="1347" data-og-height="587" height="587" data-path="images/github-oidc-provider-assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=fc0720d26b0176b03192881e0d00d4c7 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=305275efad18dd80e182f7442bfcb292 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0dc5facedf7d84a8e35ad5faa29409e7 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7723cea354fe2bc26fed0bccdf406853 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9038f7189b3752d863fdb6772d34ceae 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e0e20507f59fa7b57970bdd1b187072e 2500w" />

9. Create a new role.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e7b3d4a069f97ba3dbfe8bc08e8a534f" alt="github-oidc-provider-create-new-role" data-og-width="604" width="604" data-og-height="278" height="278" data-path="images/github-oidc-provider-create-new-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0b0bbe7da72790aeaa23eca25e846e12 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7536bf15de67ff8826aaaaa336d3b2ff 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=65907ad9152fa24dcd1fe791d6a1980d 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7396e3e8f50462a000d5cd3131cf7e94 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd64cc43cbf45bf06a66f40db3976251 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c8037ba2ea7bfc3e8f03dcb19ed66c9f 2500w" />

10. Confirm that Web identity is already selected as the trusted entity and the Identity provider field is populated with the IdP. In the Audience list, select sts.amazonaws.com, and then select Next.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3fe69db526ec7276382189d8d063561f" alt="github-oidc-provider-trusted-entity" data-og-width="1300" width="1300" data-og-height="934" height="934" data-path="images/github-oidc-provider-trusted-entity.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1eb0d8ae46efdbb4f0ce072de01a4287 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a54123b0b191d9587345115f28a5c2e2 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=74e9c6b7764f1ee331fc692808e898d0 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a8d61a562ca252b89940f25c67e94c4c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c2904811b03b257358ba3578dc0a4c8e 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd5780393d9adde78a009499ef3ba6bf 2500w" />

11. Add the `AmazonEC2ContainerRegistryPowerUser` permission to this role.

12. Create the role with the name `GithubActionsRole`.

13. Find the role `GithubActionsRole` and copy the ARN.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff1efeba61931aa435c13062d91a8f0b" alt="github-oidc-role" data-og-width="1389" width="1389" data-og-height="710" height="710" data-path="images/github-oidc-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1c1afadf6e661558e3cc861e2353a38d 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2231af7fff49341e8393eb7b49b610b1 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=43d85fdcd8f72dbe0ed7948a95793d38 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f05dc967abe03969118e755451c43a4a 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b5e1b433d11d461a5fc24c8b14e6bc91 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=8b7072639f56d42f00d00b8b735cb375 2500w" />

14. Create the ECR Repositories: `llm` and `jupyter-llm` which are built by the workflow.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c68ceb3a9b6784fd519cc04b0e38caf1" alt="create-ecr-image" data-og-width="1389" width="1389" data-og-height="408" height="408" data-path="images/create-ecr-image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ce02d48da7e53a6c335736a17ebec6e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f0b4d1687849a637c0a595c4a8d0690a 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=26b1f13eb8b6f9b09a06b9e6bb1eeb27 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e53f084201341a7c92738fa62efdb64c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c9e2477e1befaf12f81d4d345dac5a26 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a22af7830053ba9139cfb6d0d4017d4a 2500w" />

15. Update the workflow with the `GithubActionsRole` ARN and ECR Repository.
```

Example 3 (unknown):
```unknown
16. Update the `docker-images` workflow to **NOT** run on a release
```

Example 4 (unknown):
```unknown
17. Run workflow using a Github Release

<CodeGroup>
```

---

## Run Response Events

**URL:** llms-txt#run-response-events

Source: https://docs.agno.com/basics/agents/usage/run-response-events

This example demonstrates how to handle different types of events during agent run streaming. It shows how to capture and process content events, tool call started events, and tool call completed events.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Anthropic API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Anthropic API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize the research agent with advanced journalistic capabilities

**URL:** llms-txt#initialize-the-research-agent-with-advanced-journalistic-capabilities

research_agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools(), Newspaper4kTools()],
    description=dedent("""\
        You are an elite investigative journalist with decades of experience at the New York Times.
        Your expertise encompasses: ðŸ“°

- Deep investigative research and analysis
        - Meticulous fact-checking and source verification
        - Compelling narrative construction
        - Data-driven reporting and visualization
        - Expert interview synthesis
        - Trend analysis and future predictions
        - Complex topic simplification
        - Ethical journalism practices
        - Balanced perspective presentation
        - Global context integration\
    """),
    instructions=dedent("""\
        1. Research Phase ðŸ”
           - Search for 10+ authoritative sources on the topic
           - Prioritize recent publications and expert opinions
           - Identify key stakeholders and perspectives

2. Analysis Phase ðŸ“Š
           - Extract and verify critical information
           - Cross-reference facts across multiple sources
           - Identify emerging patterns and trends
           - Evaluate conflicting viewpoints

3. Writing Phase âœï¸
           - Craft an attention-grabbing headline
           - Structure content in NYT style
           - Include relevant quotes and statistics
           - Maintain objectivity and balance
           - Explain complex concepts clearly

4. Quality Control âœ“
           - Verify all facts and attributions
           - Ensure narrative flow and readability
           - Add context where necessary
           - Include future implications
    """),
    expected_output=dedent("""\
        # {Compelling Headline} ðŸ“°

## Executive Summary
        {Concise overview of key findings and significance}

## Background & Context
        {Historical context and importance}
        {Current landscape overview}

## Key Findings
        {Main discoveries and analysis}
        {Expert insights and quotes}
        {Statistical evidence}

## Impact Analysis
        {Current implications}
        {Stakeholder perspectives}
        {Industry/societal effects}

## Future Outlook
        {Emerging trends}
        {Expert predictions}
        {Potential challenges and opportunities}

## Expert Insights
        {Notable quotes and analysis from industry leaders}
        {Contrasting viewpoints}

## Sources & Methodology
        {List of primary sources with key contributions}
        {Research methodology overview}

---
        Research conducted by AI Investigative Journalist
        New York Times Style Report
        Published: {current_date}
        Last Updated: {current_time}\
    """),
    markdown=True,
    add_datetime_to_context=True,
)

---

## Prompt Caching

**URL:** llms-txt#prompt-caching

**Contents:**
- Usage
- Extended cache
- Working example

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/prompt-caching

Learn how to use prompt caching with Anthropic models and Agno.

Prompt caching can help reducing processing time and costs. Consider it if you are using the same prompt multiple times in any flow.

You can read more about prompt caching with Anthropic models [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching).

To use prompt caching in your Agno setup, pass the `cache_system_prompt` argument when initializing the `Claude` model:

Notice that for prompt caching to work, the prompt needs to be of a certain length. You can read more about this on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).

You can also use Anthropic's extended cache beta feature. This updates the cache duration from 5 minutes to 1 hour. To activate it, pass the `extended_cache_time` argument and the following beta header:

```python cookbook/models/anthropic/prompt_caching_extended.py theme={null}
from pathlib import Path
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.media import download_file

**Examples:**

Example 1 (unknown):
```unknown
Notice that for prompt caching to work, the prompt needs to be of a certain length. You can read more about this on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).

## Extended cache

You can also use Anthropic's extended cache beta feature. This updates the cache duration from 5 minutes to 1 hour. To activate it, pass the `extended_cache_time` argument and the following beta header:
```

Example 2 (unknown):
```unknown
## Working example
```

---

## Define a tool that uses dependencies claims

**URL:** llms-txt#define-a-tool-that-uses-dependencies-claims

def get_user_details(dependencies: dict):
    """
    Get the current user's details.
    """
    return {
        "name": dependencies.get("name"),
        "email": dependencies.get("email"),
        "roles": dependencies.get("roles"),
    }

---

## Complex workflow combining multiple patterns

**URL:** llms-txt#complex-workflow-combining-multiple-patterns

workflow = Workflow(
    name="Advanced Multi-Pattern Workflow",
    steps=[
        Parallel(
            Condition(
                name="Tech Check",
                evaluator=is_tech_topic,
                steps=[Step(name="Tech Research", agent=tech_researcher)]
            ),
            Condition(
                name="Business Check",
                evaluator=is_business_topic,
                steps=[
                    Loop(
                        name="Deep Business Research",
                        steps=[Step(name="Market Research", agent=market_researcher)],
                        end_condition=research_quality_check,
                        max_iterations=3
                    )
                ]
            ),
            name="Conditional Research Phase"
        ),
        Step(
            name="Research Post-Processing",
            executor=research_post_processor,
            description="Consolidate and analyze research findings with quality metrics"
        ),
        Router(
            name="Content Type Router",
            selector=content_type_selector,
            choices=[blog_post_step, social_media_step, report_step]
        ),
        Step(name="Final Review", agent=reviewer),
    ]
)

workflow.print_response("Create a comprehensive analysis of sustainable technology trends and their business impact for 2024", markdown=True)
```

* [Condition and Parallel Steps (Streaming Example)](/basics/workflows/usage/condition-and-parallel-steps-stream)
* [Loop with Parallel Steps (Streaming Example)](/basics/workflows/usage/loop-with-parallel-steps-stream)
* [Router with Loop Steps](/basics/workflows/usage/router-steps-workflow)

---

## Pydantic Models as Team Input

**URL:** llms-txt#pydantic-models-as-team-input

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/pydantic-model-as-input

This example demonstrates how to use Pydantic models as input to teams, showing how structured data can be passed as messages for more precise and validated input handling.

```python cookbook/examples/teams/structured_input_output/01_pydantic_model_as_input.py theme={null}
"""
This example demonstrates how to use Pydantic models as input to teams.

Shows how structured data can be passed as messages to teams for more
precise and validated input handling.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field

class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements."""

topic: str = Field(description="The main research topic")
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)

---

## Db

**URL:** llms-txt#db

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/db

```python cookbook/models/openai/responses/db.py theme={null}
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Example showing market analysis

**URL:** llms-txt#example-showing-market-analysis

agent.print_response(
    "What are the top gainers in the market today?"
)

---

## Configure Memory page with custom display names

**URL:** llms-txt#configure-memory-page-with-custom-display-names

memory:
  display_name: "User Memory Store"
  dbs:
    - db_id: db-0001
      tables: ["custom_memory_table"]  # Optional: specify custom table names
      domain_config:
        display_name: Main app user memories
    - db_id: db-0002
      domain_config:
        display_name: Support flow user memories

---

## You can also get the user memories from the agent

**URL:** llms-txt#you-can-also-get-the-user-memories-from-the-agent

---

## Print the metrics

**URL:** llms-txt#print-the-metrics

**Contents:**
- Usage

print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_output.metrics)  # type: ignore
bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U groq yfinance agno
    bash Mac theme={null}
      python cookbook/models/groq/metrics.py
      bash Windows theme={null}
      python cookbook/models/groq/metrics.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create a WorkflowAgent that will decide when to run the workflow

**URL:** llms-txt#create-a-workflowagent-that-will-decide-when-to-run-the-workflow

workflow_agent = WorkflowAgent(model=OpenAIChat(id="gpt-4o-mini"), num_history_runs=4)

---

## Image Generation Agent

**URL:** llms-txt#image-generation-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/image-generation-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print step-level metrics

**URL:** llms-txt#print-step-level-metrics

print("Step Metrics")
if response.metrics:
    for step_name, step_metrics in response.metrics.steps.items():
        print(f"\nStep: {step_name}")
        print(f"Executor: {step_metrics.executor_name} ({step_metrics.executor_type})")
        if step_metrics.metrics:
            print(f"Duration: {step_metrics.metrics.duration:.2f}s")
            print(f"Tokens: {step_metrics.metrics.total_tokens}")

---

## Generate and save an image

**URL:** llms-txt#generate-and-save-an-image

response = widescreen_agent.run(
    "Create a panoramic nature scene with mountains and a lake at sunset",
    markdown=True,
)

---

## Setup your Agents with the same database and Memory enabled

**URL:** llms-txt#setup-your-agents-with-the-same-database-and-memory-enabled

agent_1 = Agent(db=db, enable_user_memories=True)
agent_2 = Agent(db=db, enable_user_memories=True)

---

## Context Compression

**URL:** llms-txt#context-compression

**Contents:**
- The Problem: Verbose Tool Results
- The Solution: Automatic Compression
- How It Works
- Enable Compression
- Custom Compression
- When to Use Context Compression
- Developer Resources

Source: https://docs.agno.com/basics/context-compression/overview

Learn how to compress tool call results to save context space while preserving critical information.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.3" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.3">v2.2.3</Tooltip>
</Badge>

Context Compression allows you to manage your agent context while it is running, helping the agent stay within its context window and avoid rate limits or decreases in response quality.

Think of it like a research assistant who reads lengthy reports and gives you the key bullet points instead of the full documents.

## The Problem: Verbose Tool Results

If you are using tools with large response sizes, without compression, tool results quickly consume your context window:

| Component     | Cumulative Token Count | Notes             |
| ------------- | ---------------------- | ----------------- |
| System Prompt | 1,200 tokens           |                   |
| User Message  | 1,300 tokens           |                   |
| LLM Response  | 1,500 tokens           |                   |
| Tool Call 1   | 2,500 tokens           |                   |
| Tool Call 2   | 5,700 tokens           | 2,500 + 3,200 new |
| Tool Call 3   | 8,500 tokens           | 5,700 + 2,800 new |
| Tool Call 4   | 12,000 tokens          | 8,500 + 3,500 new |

This quickly becomes expensive and hits context limits during complex workflows.

## The Solution: Automatic Compression

Context compression summarizes tool results after a threshold:

* Dramatically reduced token costs
* Stay within context window limits
* Preserve critical facts and data
* Automatic compression

Context compression follows a simple pattern:

<Steps>
  <Step title="Enable Compression">
    Set `compress_tool_results=True` on your agent or team. This comes with a default threshold of 3 tool calls. The system monitors tool call results as they come in.
  </Step>

<Step title="Threshold Reached">
    After the threshold is reached, compression is triggered. Each uncompressed tool call result is individually summarized.
  </Step>

<Step title="Intelligent Summarization">
    The compression model preserves key facts (numbers, dates, entities, URLs) while removing boilerplate, redundancy, and filler text.
  </Step>

<Step title="The LLM loop continues">
    The compressed tool results are used in the next LLM executions, reducing token usage and extending the life of your context window.
  </Step>
</Steps>

<Note>
  When using `arun` on `Agent` or `Team`, compression is handled asynchronously and the uncompressed tool call results are summarised concurrently.
</Note>

## Enable Compression

Turn on `compress_tool_results=True` to automatically compress tool results. This comes with a default threshold of 3 tool calls.

<Info>
  You can also enable `compress_tool_results=True` on individual team members to compress their tool results independently.
</Info>

## Custom Compression

Provide a [`CompressionManager`](/reference/compression/compression-manager) to customize the compression behavior:

<Tip>
  Use a faster, cheaper model like `gpt-4o-mini` for compression to reduce latency and cost while using a more capable model as your Agent's main model.
</Tip>

## When to Use Context Compression

* Agents with tools that return verbose results (web search, APIs)
* Multi-step workflows with many tool calls
* Long-running sessions where context accumulates
* Production systems where cost matters

## Developer Resources

* [CompressionManager Reference](/reference/compression/compression-manager) - Full CompressionManager documentation
* [Agent Reference](/reference/agents/agent) - Agent parameter documentation
* [Team Reference](/reference/teams/team) - Team parameter documentation
* [Cookbook Examples](https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_compression)

**Examples:**

Example 1 (unknown):
```unknown
Tool Call 1: 2,500 tokens
Tool Call 2: 5,700 tokens
Tool Call 3: 8,500 tokens
[Compression triggered]
Tool Call 4: 1,300 tokens (800 compressed + 500 new)
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<Info>
  You can also enable `compress_tool_results=True` on individual team members to compress their tool results independently.
</Info>

## Custom Compression

Provide a [`CompressionManager`](/reference/compression/compression-manager) to customize the compression behavior:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## evaluation.run(print_results=True)

**URL:** llms-txt#evaluation.run(print_results=true)

---

## PgVector

**URL:** llms-txt#pgvector

Source: https://docs.agno.com/reference/vector-db/pgvector

<Snippet file="vector-db-pgvector-reference.mdx" />

---

## Redis for Workflows

**URL:** llms-txt#redis-for-workflows

**Contents:**
- Usage
  - Run Redis

Source: https://docs.agno.com/integrations/database/redis/usage/redis-for-workflow

Agno supports using Redis as a storage backend for Workflows using the `RedisDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Redis** on port **6379** using:

```python redis_for_workflow.py theme={null}
"""
Run: `pip install openai httpx newspaper4k redis agno` to install the dependencies
"""
from agno.agent import Agent
from agno.db.redis import RedisDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Database Migrations

**URL:** llms-txt#database-migrations

Source: https://docs.agno.com/basics/database/migrations

Learn how to migrate your Agno database tables.

You can expect the schemas in your Agno database tables to be stable across versions.

However, in future versions, we may occasionally update or add new columns or tables.

If you need to apply a migration, you can use the `MigrationManager` class:

```python  theme={null}
import asyncio

from agno.db.migrations import MigrationManager
from agno.db.postgres import AsyncPostgresDb

---

## Background Hooks (Per-Hook)

**URL:** llms-txt#background-hooks-(per-hook)

**Contents:**
- What Happens
- Comparison: Global vs Per-Hook

Source: https://docs.agno.com/agent-os/usage/background-hooks-decorator

Run specific hooks as background tasks using the @hook decorator

This example demonstrates how to run **specific** hooks as background tasks using the `@hook` decorator, while other hooks run synchronously.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run the server">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Test the endpoint">

The response will be returned after `log_analytics` completes. Check the server logs to see `log_request` and `send_notification` executing in the background.
  </Step>
</Steps>

1. The agent processes the request
2. `log_analytics` runs synchronously (blocks the response)
3. The response is sent to the user
4. `log_request` and `send_notification` run in the background
5. The user only waits for `log_analytics` to complete

## Comparison: Global vs Per-Hook

| Approach                                | Use Case                                                   |
| --------------------------------------- | ---------------------------------------------------------- |
| `AgentOS(run_hooks_in_background=True)` | All hooks are non-critical, maximize response speed        |
| `@hook(run_in_background=True)`         | Mix of critical (sync) and non-critical (background) hooks |

<Tip>
  Use the `@hook` decorator when you have hooks that must complete before the response (e.g., output validation) alongside hooks that can run later (e.g., notifications).
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Confirmation Required with Mixed Tools

**URL:** llms-txt#confirmation-required-with-mixed-tools

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-mixed-tools

This example demonstrates human-in-the-loop functionality where only some tools require user confirmation. The agent executes tools that don't require confirmation automatically and pauses only for tools that need approval.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cerebras Llama with Reasoning Tools

**URL:** llms-txt#cerebras-llama-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/cerebras-llama-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Cerebras API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Cerebras API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure Scenario defaults (model for user simulator and judge)

**URL:** llms-txt#configure-scenario-defaults-(model-for-user-simulator-and-judge)

**Contents:**
- Usage

scenario.configure(default_model="openai/gpt-4.1-mini")

@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_vegetarian_recipe_agent() -> None:
    # 1. Define an AgentAdapter to wrap your agent
    class VegetarianRecipeAgentAdapter(scenario.AgentAdapter):
        agent: Agent

def __init__(self) -> None:
            self.agent = Agent(
                model=OpenAIChat(id="gpt-4.1-mini"),
                markdown=True,
                debug_mode=True,
                instructions="You are a vegetarian recipe agent.",
            )

async def call(self, input: scenario.AgentInput) -> scenario.AgentReturnTypes:
            response = self.agent.run(
                input=input.last_new_user_message_str(), # Pass only the last user message
                session_id=input.thread_id, # Pass the thread id, this allows the agent to track history
            )
            return response.content

# 2. Run the scenario simulation
    result = await scenario.run(
        name="dinner recipe request",
        description="User is looking for a vegetarian dinner idea.",
        agents=[
            VegetarianRecipeAgentAdapter(),
            scenario.UserSimulatorAgent(),
            scenario.JudgeAgent(
                criteria=[
                    "Agent should not ask more than two follow-up questions",
                    "Agent should generate a recipe",
                    "Recipe should include a list of ingredients",
                    "Recipe should include step-by-step cooking instructions",
                    "Recipe should be vegetarian and not include any sort of meat",
                ]
            ),
        ],
    )

# 3. Assert and inspect the result
    assert result.success
bash  theme={null}
    export OPENAI_API_KEY=xxx
    export LANGWATCH_API_KEY=xxx # Optional, required for Simulation monitoring
    bash  theme={null}
    pip install -U openai agno langwatch-scenario pytest pytest-asyncio
    # or
    uv add agno langwatch-scenario openai pytest
    bash  theme={null}
    pytest cookbook/agent_basics/other/scenario_testing.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Basic Streaming Agent

**URL:** llms-txt#basic-streaming-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/perplexity/usage/basic-stream

```python cookbook/models/perplexity/basic_stream.py theme={null}
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity

agent = Agent(model=Perplexity(id="sonar"), markdown=True)

---

## The second Agent will be able to retrieve the Memory about the user name here:

**URL:** llms-txt#the-second-agent-will-be-able-to-retrieve-the-memory-about-the-user-name-here:

**Contents:**
- Developer Resources

agent_2.print_response("What is my name?")
```

All agents connected to the same database automatically share memories for each user. This works across agent types, teams, and workflows, as long as they use the same `user_id`.

## Developer Resources

* View [Examples](/basics/memory/working-with-memories/usage/memory-creation)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/memory/)

---

## Generate a unique bucket name using a base name and a UUID4 suffix.

**URL:** llms-txt#generate-a-unique-bucket-name-using-a-base-name-and-a-uuid4-suffix.

base_bucket_name = "example-gcs-bucket"
unique_bucket_name = f"{base_bucket_name}-{uuid.uuid4().hex[:12]}"
print(f"Using bucket: {unique_bucket_name}")

---

## Create a research team

**URL:** llms-txt#create-a-research-team

**Contents:**
- Usage

research_team = Team(
    name="Research Team",
    members=[
        Agent(
            name="Sarah",
            role="Data Researcher",
            instructions="Focus on gathering and analyzing data",
        ),
        Agent(
            name="Mike",
            role="Technical Writer",
            instructions="Create clear, concise summaries",
        ),
    ],
    stream=True,
    markdown=True,
)

research_team.print_response(
    [
        Message(
            role="user",
            content="I'm preparing a presentation for my company about renewable energy adoption.",
        ),
        Message(
            role="assistant",
            content="I'd be happy to help with your renewable energy presentation. What specific aspects would you like me to focus on?",
        ),
        Message(
            role="user",
            content="Could you research the latest solar panel efficiency improvements in 2024?",
        ),
        Message(
            role="user",
            content="Also, please summarize the key findings in bullet points for my slides.",
        ),
    ],
    markdown=True,
)
bash  theme={null}
    pip install agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/basic/input_as_messages_list.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create your agent

**URL:** llms-txt#create-your-agent

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    knowledge=knowledge,
    search_knowledge=True,
    instructions=["Always search knowledge before answering"]
)

---

## Audio As Input

**URL:** llms-txt#audio-as-input

Source: https://docs.agno.com/basics/multimodal/audio/audio_input

Learn how to use audio as input with Agno agents.

Agno supports audio as input to agents and teams.  Take a look at the [compatibility matrix](/basics/models/compatibility#multimodal-support) to see which models support audio as input.

Let's create an agent that can understand audio input.

```python audio_agent.py theme={null}
import base64

import requests
from agno.agent import Agent, RunOutput  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat

---

## Configure the language model

**URL:** llms-txt#configure-the-language-model

model = Ollama(id="llama3.1:8b")

---

## Memory with Redis

**URL:** llms-txt#memory-with-redis

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/redis-memory

```python mem-redis-memory.py theme={null}
from agno.agent import Agent
from agno.db.redis import RedisDb

---

## Use when adding content

**URL:** llms-txt#use-when-adding-content

**Contents:**
  - Working with Different Content Types

for file_path in document_files:
    knowledge.add_content(
        path=file_path,
        metadata=assign_metadata(file_path)
    )
python  theme={null}
from agno.knowledge.reader.pdf_reader import PdfReader

**Examples:**

Example 1 (unknown):
```unknown
For advanced filtering with complex logical conditions (OR, NOT, comparisons), see [Advanced Filtering](/basics/knowledge/filters/advanced-filtering).

### Working with Different Content Types

Use appropriate readers; they handle parsing their formats.
```

---

## === WORKFLOW WITH CONDITION ===

**URL:** llms-txt#===-workflow-with-condition-===

workflow = Workflow(
    name="Story Generation with Conditional Editing",
    description="A workflow that generates stories, conditionally edits them, formats them, and adds references",
    agent=workflow_agent,
    steps=[
        write_step,
        Condition(
            name="editing_condition",
            description="Check if story needs editing",
            evaluator=needs_editing,
            steps=[edit_step],
        ),
        format_step,
        add_references,
    ],
    db=PostgresDb(db_url),
)

async def main():
    """Async main function"""
    print("\n" + "=" * 80)
    print("WORKFLOW WITH CONDITION - ASYNC STREAMING")
    print("=" * 80)

# First call - will run the workflow with condition
    print("\n" + "=" * 80)
    print("FIRST CALL: Tell me a story about a brave knight")
    print("=" * 80)
    await workflow.aprint_response(
        "Tell me a story about a brave knight",
        stream=True,
    )

# Second call - should answer from history without re-running workflow
    print("\n" + "=" * 80)
    print("SECOND CALL: What was the knight's name?")
    print("=" * 80)
    await workflow.aprint_response(
        "What was the knight's name?",
        stream=True,
    )

# Third call - new topic, should run workflow again
    print("\n" + "=" * 80)
    print("THIRD CALL: Now tell me about a cat")
    print("=" * 80)
    await workflow.aprint_response(
        "Now tell me about a cat",
        stream=True,
    )

if __name__ == "__main__":
    asyncio.run(main())
```

---

## Create Team Run

**URL:** llms-txt#create-team-run

Source: https://docs.agno.com/reference-api/schema/teams/create-team-run

post /teams/{team_id}/runs
Execute a team collaboration with multiple agents working together on a task.

**Features:**
- Text message input with optional session management
- Multi-media support: images (PNG, JPEG, WebP), audio (WAV, MP3), video (MP4, WebM, etc.)
- Document processing: PDF, CSV, DOCX, TXT, JSON
- Real-time streaming responses with Server-Sent Events (SSE)
- User and session context preservation

**Streaming Response:**
When `stream=true`, returns SSE events with `event` and `data` fields.

---

## Add documents with metadata for agentic filtering

**URL:** llms-txt#add-documents-with-metadata-for-agentic-filtering

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

---

## Sambanova

**URL:** llms-txt#sambanova

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/sambanova

The Sambanova model provides access to Sambanova's language models.

| Parameter               | Type            | Default                         | Description                                                         |
| ----------------------- | --------------- | ------------------------------- | ------------------------------------------------------------------- |
| `id`                    | `str`           | `"Meta-Llama-3.1-8B-Instruct"`  | The id of the SambaNova model to use                                |
| `name`                  | `str`           | `"SambaNova"`                   | The name of the model                                               |
| `provider`              | `str`           | `"SambaNova"`                   | The provider of the model                                           |
| `api_key`               | `Optional[str]` | `None`                          | The API key for SambaNova (defaults to SAMBANOVA\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.sambanova.ai/v1"` | The base URL for the SambaNova API                                  |
| `retries`               | `int`           | `0`                             | Number of retries to attempt before raising a ModelProviderError    |
| `delay_between_retries` | `int`           | `1`                             | Delay between retries, in seconds                                   |
| `exponential_backoff`   | `bool`          | `False`                         | If True, the delay between retries is doubled each time             |

SambaNova extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Install dependencies

**URL:** llms-txt#install-dependencies

pip install "agno[infra]" openai exa_py python-dotenv

4. Create a new project with [AgentOS](/agent-os/introduction):

```bash
ag infra create              # Choose: [1] agent-infra-docker (default)

---

## Configuration for the Session page

**URL:** llms-txt#configuration-for-the-session-page

session:
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        display_name: <DISPLAY_NAME>
    ...

---

## Groq with Reasoning Tools

**URL:** llms-txt#groq-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/groq-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Groq API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Groq API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## List All Workflows

**URL:** llms-txt#list-all-workflows

Source: https://docs.agno.com/reference-api/schema/workflows/list-all-workflows

get /workflows
Retrieve a comprehensive list of all workflows configured in this OS instance.

**Return Information:**
- Workflow metadata (ID, name, description)
- Input schema requirements
- Step sequence and execution flow
- Associated agents and teams

---

## Refresh the context

**URL:** llms-txt#refresh-the-context

agent.context["memory"] = zep_tools.get_zep_memory(memory_type="context")

---

## This may work but with unpredictable results due to message format differences

**URL:** llms-txt#this-may-work-but-with-unpredictable-results-due-to-message-format-differences

gemini_agent.print_response(
    "Continue our discussion about machine learning", session_id=session_id, user_id=user_id
)

---

## === RESEARCH STEPS ===

**URL:** llms-txt#===-research-steps-===

research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

---

## Agno v2.0 Changelog

**URL:** llms-txt#agno-v2.0-changelog

**Contents:**
- General Changes
- Session & Run State
- Storage
- Memory
- Knowledge
- Tools updates
- Media
- Logging
- Agent updates
- Team updates

Source: https://docs.agno.com/how-to/v2-changelog

<img height="200" src="https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=a2223b0ee329d32688b43f6ebe1b7174" data-og-width="323" data-og-height="320" data-path="images/changelogs/agent_os_stack.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=280&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=ebcc3630f1f4c8482631fb06a7d29c91 280w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=560&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=ee6bca0ef5c4acf434bd96854d698456 560w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=840&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=b12920d3ed0141b7eae5364aeefb795f 840w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=1100&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=7d8e536bad2a2e737c907296c9b550c3 1100w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=1650&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=141013bcc891f080fe2e63188ce33e70 1650w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=2500&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=5424dcf79c186f18ec276e514bd0e932 2500w" />

This is a major release that introduces a completely new approach to building multi-agent systems. It also introduces the AgentOS, a runtime for agents.

This is a major rewrite of the Agno library and introduces various new concepts and updates to the existing ones.

Some of the major changes are:

* Agents, Teams and Workflows are now fully stateless.
* Knowledge is now a single solution that supports many forms of content.
* Storage of sessions, memories, evals, etc. has been simplified

<Accordion title="Repo Updates">
  * `/libs/agno` has been restructured to fit the new concepts in Agno and for better organization.
  * All code related to managing workspaces and agent deployment in Agno has been moved to a new package called `agno-infra`. This is a combination of the previous `agno-aws` and `agno-docker` packages, as well as the CLI and other tools.
  * `agno-aws` and `agno-docker` packages have been deprecated and will no-longer be maintained.
  * All code related to the Agno CLI (`ag`) has been moved to this new `agno-infra` package.
  * Added `AgentOS` to `agno` as a comprehensive API solution for building multi-agent systems. This also replaces `Playground` and other Apps. See details below.
  * Cookbook has been completely restructured, with new and more valuable READMEs, better coverage of concepts, and more examples.
</Accordion>

<Accordion title="AgentOS">
  * Introducing `AgentOS`, a system for hosting agents, teams and workflows as a production-ready API. See full details in the [AgentOS](/agent-os/introduction) section.
  * This adds routes for session management, memory management, knowledge management, evals management, and metrics.
  * This enables you to host agents, teams and workflows, and use the [Agent OS UI](https://os.agno.com) to manage them.
</Accordion>

<Accordion title="Apps Deprecations">
  * Removed `Playground`. Its functionality has been replaced by `AgentOS`.
  * Removed `AGUIApp` and replace with `AGUI` interface on `AgentOS`.
  * Removed `SlackApi` and replace with `Slack` interface on `AgentOS`.
  * Removed `WhatsappApi` and replace with `Whatsapp` interface on `AgentOS`.
  * Removed `FastAPIApp`. Its functionality has been replaced by `AgentOS`.
  * `DiscordClient` has been moved to `/integrations/discord`.
</Accordion>

## Session & Run State

* We have made significant changes to the innerworkings of `Agent`, `Team` and `Workflow` to make them completely stateless.
* This means that `agent_session`, `session_metrics`, `session_state`, etc. should not be seen as stateful variables that would be updated during the course of a run, but rather as "defaults" for the agent if they can be set on initialisation.
* `CustomEvent` is now supported and you can inherit from it to create your own custom events that can be yielded from your own tools. See the [documentation](/basics/agents/running-agents#custom-events) for more details.

<Accordion title="Updates to Run Objects">
  For agents:

* `RunResponse` -> `RunOutput`
  * `RunResponseStartedEvent` -> `RunStartedEvent`
  * `RunResponseContentEvent` -> `RunContentEvent`
  * `RunResponseCompletedEvent` -> `RunCompletedEvent`
  * `IntermediateRunResponseContentEvent` -> `IntermediateRunContentEvent`
  * `RunResponseErrorEvent` -> `RunErrorEvent`
  * `RunResponseCancelledEvent` -> `RunCancelledEvent`

* `TeamRunResponse` -> `TeamRunOutput`
  * `RunResponseStartedEvent` -> `RunStartedEvent`
  * `RunResponseContentEvent` -> `RunContentEvent`
  * `RunResponseCompletedEvent` -> `RunCompletedEvent`
  * `IntermediateRunResponseContentEvent` -> `IntermediateRunContentEvent`
  * `RunResponseErrorEvent` -> `RunErrorEvent`
  * `RunResponseCancelledEvent` -> `RunCancelledEvent`

* `WorkflowRunResponse` -> `WorkflowRunOutput`

* `WorkflowRunResponseStartedEvent` -> `WorkflowRunStartedEvent`

* `WorkflowRunResponseContentEvent` -> `WorkflowRunContentEvent`

* `WorkflowRunResponseCompletedEvent` -> `WorkflowRunCompletedEvent`

* `WorkflowRunResponseErrorEvent` -> `WorkflowRunErrorEvent`

* `WorkflowRunResponseCancelledEvent` -> `WorkflowRunCancelledEvent`

* The import location for `RunOutput` (and events) has been moved to `agno.run.agent`.

* For `RunOutput`, `TeamRunOutput` and `WorkflowRunOutput` the `extra_data` attribute has been removed and the internal attributes are now top-level. This is `references`, `additional_input`, `reasoning_steps`, and `reasoning_messages`.

* `metadata` added to `RunOutput`, `TeamRunOutput` and `WorkflowRunOutput`. This represents all the set metadata for the run.
</Accordion>

<Accordion title="Updates to Session Objects">
  * Session storage now stores `AgentSession`, `TeamSession` and `WorkflowSession` with new schemas. See full details in the [Session](/basics/sessions/overview) section.
  * Session objects now have `runs` directly on it.
  * Session objects support new convenience methods:
    * `get_run` -> Get a specific run by ID.
    * `get_session_summary` -> Get the session summary.
    * `get_chat_history` -> Get an aggregated view of all messages for all runs in the session.
</Accordion>

<Accordion title="Updates to Metrics">
  * `SessionMetrics` and `MessageMetrics` have been unified as a single `Metrics` class.
  * `audio_tokens` has been renamed to `audio_total_tokens`.
  * `input_audio_tokens` has been renamed to `audio_input_tokens`.
  * `output_audio_tokens` has been renamed to `audio_output_tokens`.
  * `cached_tokens` has been renamed to `cache_read_tokens`.
  * `prompt_tokens` and `completion_tokens` have been removed (only `input_tokens` and `output_tokens` should be used)
  * `prompt_tokens_details` and `completion_tokens_details` have been removed. Instead `provider_metrics` captures any provider-specific metrics.
  * `time` has been renamed to `duration`.
</Accordion>

<Accordion title="Cancelling Runs">
  * You can now cancel a run by calling `cancel_run` on the `Agent`, `Team` or `Workflow`.
  * This will cancel the run and return a `RunCancelledEvent` during streaming, or set the `RunOutput.status` to `"cancelled"`.
</Accordion>

* `Agent`, `Team`, `Workflow` and the various evals now all support a single `db` parameter. This is to enable storage for the instance of that class. This is required for persistence of sessions, memories, metrics, etc.
* `storage` and `memory` have been removed from `Agent`, `Team` and `Workflow`.

<Accordion title="Updates to Storage Classes">
  - This means all previous storage providers have been reworked. Also session storage, memory storage and eval storage are all a single solution now referred to as a "DB".
  - `PostgresStorage` -> `PostgresDb`
  - `SqliteStorage` -> `SqliteDb`
  - `MysqlStorage` -> `MysqlDb`
  - `RedisStorage` -> `RedisDb`
  - `MongoStorage` -> `MongoDb`
  - `DynamoDBStorage` -> `DynamoDb`
  - `SingleStoreStorage` -> `SingleStoreDb`
  - `InMemoryStorage` -> `InMemoryDb`
  - `JsonStorage` -> `JsonDb`
  - `GCSJsonStorage` -> `GCSJsonDb`
</Accordion>

* With the above changes to storage, memory has been simplified.
* `memory` has been removed from `Agent` and `Team`. Instead memory is enabled with `enable_user_memories: bool` (like before) and persisted in the `db` instance.
* Changes to how memories are created can still be done by overriding the `MemoryManager` class on `Agent` or `Team`. E.g. `Agent(memory_manager=MyMemoryManager())`.
* `AgentMemory` and `TeamMemory` have been removed.

* Knowledge has been completely reworked. See full details in the [Knowledge](/basics/knowledge/) section.
* You now define a single `Knowledge` instance for all types of content. Files (PDF, CSV, etc.), URLs, and other.
* The agent can still use your knowledge base to search for information at runtime. All existing RAG implementations are still supported.
* Added **full `async` support** for embedding models and vector DBs. This has a significant impact on performance and is a major speed improvement when adding content to the knowledge base using `knowledge.add_content_async(...)`.
* `AgentKnowledge` and all other knowledge base classes have been removed.
* Import locations for `embedder`, `document`, `chunking`, `reranker` and `reader` have been moved to `agno.knowledge`. See [examples](/basics/knowledge/overview) for more details.

* General:
  * Since Agents and Teams are now stateless, using attributes from the agent/team object inside a function will give you access to the attributes set on initialisation of that agent/team. E.g. `agent.session_state` should not be used, instead `session_state` can now be directly accessed and would have the "current" state of the session.
  * A new flow allows images, audio and video files generated during tool execution to be passed back in a `FunctionExecutionResult` object and this will ensure these artifacts are made available to the model and agent as needed.
* All tools that handle media (e.g. image generation tools) now correctly add this generated media to the `RunOutput`, but also make it available for subsequent model calls.
* The interface of almost all the toolkits have been updated for a more consistent experience around switching specific tools on and off. The list of changes is too long to list here. We suggest you take a look at the toolkits you use specifically and how they have been updated.
* `show_results` is now `True` by default for all tools. If you just set `stop_after_tool_call=True` then `show_results` will be automatically set to `True`.
* `images`, `videos`, `audio` and `files` are now available as parameters to tools. See the [documentation](/basics/tools/overview) for more details.

* **Removed legacy artifact classes**: `ImageArtifact`, `VideoArtifact`, `AudioArtifact`, and `AudioResponse` classes have been completely removed in favor of unified media classes.

#### New Unified Media Architecture

* **Unified `Image` class**: Now serves all use cases (input, output, artifacts) with standardized `content: Optional[bytes]` field for raw image data
* **Unified `Audio` class**: Replaces both `AudioArtifact` and `AudioResponse` with consistent byte-based content storage and additional fields like `transcript`, `expires_at`, `sample_rate`, and `channels`
* **Unified `Video` class**: Updated to handle all video use cases with standardized content handling and metadata fields
* **Enhanced `File` class**: Updated to work seamlessly across agent, team, workflow, and toolkit contexts

#### New Methods and Features

* **`from_base64()` class method**: Added to `Image`, `Audio`, and `Video` classes for creating instances from base64-encoded content (automatically converts to raw bytes)
* **`get_content_bytes()` method**: Retrieves content as raw bytes, handling loading from URLs or file paths
* **`to_base64()` method**: Converts content to base64 string for transmission/storage
* **`to_dict()` method**: Enhanced serialization with optional base64 content inclusion

#### Content Standardization

* **Byte-based storage**: All media content is now stored as raw bytes (`Optional[bytes]`) instead of mixed string/bytes formats
* **Automatic validation**: Model validators ensure exactly one content source (`url`, `filepath`, or `content`) is provided
* **Auto-generated IDs**: Media objects automatically generate UUIDs when not provided

* Added support for custom loggers. See the [documentation](/basics/custom-logging) for more details.

<Accordion title="Updates to Agent Class">
  * `agent_id` -> `id` -> If `id` is not set, it is autogenerated using the `name` of the agent, or a random UUID if the `name` is not set.
  * `search_previous_sessions_history` -> `search_session_history`
  * `context` -> `dependencies`
  * `add_context` -> `add_dependencies_to_context`
  * `add_history_to_messages` -> `add_history_to_context`
  * `add_name_to_instructions` -> `add_name_to_context`
  * `add_datetime_to_instructions` -> `add_datetime_to_context`
  * `add_location_to_instructions` -> `add_location_to_context`
  * `add_messages` -> `additional_input`
  * `extra_data` -> `metadata`
  * `create_default_system_message` -> `build_context`
  * `create_default_user_message` -> `build_user_context`
  * Added `send_media_to_model` -> `True` by default. Set to False if you don't want to send media (image, audio, video, files) to the model.  This is useful if you only want media for tools.
  * Added `store_media` -> `True` by default. Set to False if you don't want to store any media in the `RunOutput` that is persisted with sessions.
  * `num_history_responses` -> `num_history_runs`
  * Removed `success_criteria` and `goal`
  * Removed `team_session_id` and `workflow_session_id`.
  * Removed `introduction`
  * Removed `show_tool_calls` -> This is now just always enabled.
  * Removed `team` and `team_data`
  * Removed `respond_directly`, `add_transfer_instructions`, `team_response_separator` and `team_session_id` (since team has been removed from `Agent`)
  * Removed all `team` functionality from inside `Agent` (i.e. the deprecated teams implementation has been removed)
  * Removed all `monitoring` from `Agent`. With the new AgentOS platform, monitoring is done using your own data. Go to [os.agno.com](https://os.agno.com) to get started.
</Accordion>

<Accordion title="Updates to Input & Output">
  * `response_model` -> `output_schema`
  * Added `input_schema` (a pydantic model) to validate the input to the agent.
  * Changed `message` to `input` (which also replaces `messages`). `input` can be of type `str`, `list`, `dict`, `Message`, `BaseModel`, or `list[Message]`.
  * If a `dict` and `input_schema` is provided, the dict will be validated against the schema.
  * If a `BaseModel` and `input_schema` is provided, the model will be validated against the schema.
  * `arun` and `acontinue_run` with `stream=True` now return an async iterator of `RunOutputEvent` directly and is not a coroutine anymore.
  * `debug_mode: bool` added to `run`, `arun`, `print_response` and `aprint_response` to enable debug mode for a specific run.
  * `add_history_to_context` added to `run`, `arun`, `print_response` and `aprint_response` to add the chat history to the context for the current run.
  * `dependencies` added to `run`, `arun`, `print_response` and `aprint_response` to add dependencies to the context for the current run.
  * `metadata` added to `run`, `arun`, `print_response` and `aprint_response` to set the metadata for the current run. This is merged with the metadata set on the `Team` object.
  * Added `get_run_output` and `get_last_run_output` to `Agent` to retrieve a run output by ID.
</Accordion>

<Accordion title="Updates to Metrics">
  * Metrics have been simplified and cleaned up.
  * There are now 3 levels of metrics:
    * `Message.metrics` -> Metrics for each message (assistant, tool, etc.).
    * `RunOutput.metrics` -> Aggregated metrics for the whole run.
    * `AgentSession.metrics` -> Aggregated metrics for the whole session.
</Accordion>

<Accordion title="Updates to Knowledge">
  * `knowledge` is now an instance of `Knowledge` instead of `AgentKnowledge`.
  * `retriever` -> `knowledge_retriever` -> For a custom retriever.
  * `add_references` -> `add_knowledge_to_context` -> To enable traditional RAG.
</Accordion>

<Accordion title="Updates to Memory">
  * `add_memory_references` -> `add_memories_to_context`
  * You can set a custom `memory_manager` to use when creating memories.
  * Added `get_user_memories` to retrieve the user memories.
</Accordion>

<Accordion title="Updates to Sessions">
  * `add_session_summary_references` -> `add_session_summary_to_context`
  * You can set a custom `session_summary_manager` to use when creating session summaries.
  * Removed `session_name` and replace with functions `get_session_name` and `rename_session`.
  * Added `get_session` to retrieve a session by ID.
  * Added `get_chat_history` to retrieve the chat history from a session.
  * Added `get_session_metrics` to retrieve the metrics for a session.
  * Added `get_session_state` to retrieve the session state from a session.
  * Added `get_session_summary` to retrieve the session summary from a session.
  * Because `Agent` is now stateless, `agent_session`, `session_metrics`, `run_id`, `run_input`, `run_messages` and `run_response` as "sticky" agent attributes have been removed.
  * Because `Agent` is now stateless, `images`, `videos`, `audio` are no longer available as agent attributes. Instead these can be accessed on the `RunOutput` for a particular run.
  * Removed `team_session_state` and `workflow_session_state`. Only `session_state` is used.
  * Added `enable_agentic_state` to `Agent` and `Team` to allow the agent to update the session state with a tool call.
</Accordion>

<Accordion title="Updates to Team Class">
  * Removed `mode` from `Team`. Instead there are attributes that can be used to control the behavior of the team:
    * `respond_directly` -> If True, the team leader won't process responses from the members and instead will return them directly
    * `delegate_to_all_members` -> If True, the team leader will delegate tasks to all members simultaneously, instead of one by one. When running async (using `arun`) members will run concurrently.
    * `determine_input_for_members` -> `True` by default. Set to False if you want to send the run input directly to the member agents without the team leader synthesizing its own input.
  * `team_id` -> `id` -> If `id` is not set, it is autogenerated using the `name` of the team, or a random UUID if the `name` is not set.
  * `search_previous_sessions_history` -> `search_session_history`
  * `context` -> `dependencies`
  * `add_context` -> `add_dependencies_to_context`
  * `add_history_to_messages` -> `add_history_to_context`
  * `add_name_to_instructions` -> `add_name_to_context`
  * `add_datetime_to_instructions` -> `add_datetime_to_context`
  * `add_location_to_instructions` -> `add_location_to_context`
  * `add_member_tools_to_system_message` -> `add_member_tools_to_context`
  * `extra_data` -> `metadata`
  * Added `additional_input` (works the same as for `Agent`)
  * Added `store_member_responses: bool` to optionally store the member responses on the team run output object.
  * Added `acli_app` to `Team` to enable the CLI app for the team in async mode.
  * Added `send_media_to_model` -> `True` by default. Set to False if you don't want to send media (image, audio, video, files) to the model.  This is useful if you only want media for tools.
  * Added `store_media` -> `True` by default. Set to False if you don't want to store any media in the `RunOutput` that is persisted with sessions.
  * `num_history_responses` -> `num_history_runs`
  * Removed `success_criteria`
  * Removed `team_session_id` and `workflow_session_id`.
  * Removed `enable_team_history`
  * Removed `num_of_interactions_from_history`
  * Removed `show_tool_calls` -> This is now just always enabled.
  * Removed `enable_agentic_context`. `session_state` and `enable_agentic_state` should rather be used to manage state shared between the team and the members.
  * Removed all `monitoring` from `Team`. With the new AgentOS platform, monitoring is done using your own data. Go to [os.agno.com](https://os.agno.com) to get started.
</Accordion>

<Accordion title="Updates to Input & Output">
  * `response_model` -> `output_schema`
  * Added `input_schema` (a pydantic model) to validate the input to the agent.
  * Changed `message` to `input` (which also replaces `messages`). `input` can be of type `str`, `list`, `dict`, `Message`, `BaseModel`, or `list[Message]`.
  * If a `dict` and `input_schema` is provided, the dict will be validated against the schema.
  * If a `BaseModel` and `input_schema` is provided, the model will be validated against the schema.
  * `arun` with `stream=True` now return an async iterator of `TeamRunOutputEvent` directly and is not a coroutine anymore.
  * `debug_mode: bool` added to `run`, `arun`, `print_response` and `aprint_response` to enable debug mode for a specific run.
  * `add_history_to_context` added to `run`, `arun`, `print_response` and `aprint_response` to add the chat history to the context for the current run.
  * `dependencies` added to `run`, `arun`, `print_response` and `aprint_response` to add dependencies to the context for the current run.
  * `metadata` added to `run`, `arun`, `print_response` and `aprint_response` to set the metadata for the current run. This is merged with the metadata set on the `Team` object.
  * Added `get_run_output` and `get_last_run_output` to `Team` to retrieve a run output by ID.
</Accordion>

<Accordion title="Updates to Metrics">
  * Metrics have been simplified and cleaned up.
  * There are now 3 levels of metrics:
    * `Message.metrics` -> Metrics for each message (assistant, tool, etc.).
    * `RunOutput.metrics` -> Aggregated metrics for the whole run.
    * `TeamSession.metrics` -> Aggregated metrics for the whole session.
</Accordion>

<Accordion title="Updates to Knowledge">
  * `knowledge` is now an instance of `Knowledge` instead of `AgentKnowledge`.
  * `retriever` -> `knowledge_retriever` -> For a custom retriever.
  * `add_references` -> `add_knowledge_to_context` -> To enable traditional RAG.
  * Added `update_knowledge` tool to update the knowledge base. Works the same as for `Agent`.
</Accordion>

<Accordion title="Updates to Memory">
  * `add_memory_references` -> `add_memories_to_context`
  * You can set a custom `memory_manager` to use when creating memories.
  * Added `get_user_memories` to retrieve the user memories.
</Accordion>

<Accordion title="Updates to Sessions">
  * `add_session_summary_references` -> `add_session_summary_to_context`
  * You can set a custom `session_summary_manager` to use when creating session summaries.
  * Removed `session_name` and replace with functions `get_session_name` and `rename_session`.
  * Added `get_session` to retrieve a session by ID.
  * Added `get_chat_history` to retrieve the chat history from a session.
  * Added `get_session_metrics` to retrieve the metrics for a session.
  * Added `get_session_state` to retrieve the session state from a session.
  * Added `get_session_summary` to retrieve the session summary from a session.
  * Because `Team` is now stateless, `team_session`, `session_metrics`, `run_id`, `run_input`, `run_messages` and `run_response` as "sticky" team attributes have been removed.
  * Because `Team` is now stateless, `images`, `videos`, `audio` are no longer available as team attributes. Instead these can be accessed on the `TeamRunOutput` for a particular run.
  * Removed `team_session_state` and `workflow_session_state`. Only `session_state` is used.
  * Added `enable_agentic_state` to `Team` to allow the agent to update the session state with a tool call.
</Accordion>

<Accordion title="Updates to Workflow Class">
  * `workflow_id` -> `id` -> If `id` is not set, it is autogenerated using the `name` of the workflow, or a random UUID if the `name` is not set.
  * Workflows "v1" has been completely removed and replaced with `Workflows v2`. See full details in the [Workflows](/basics/workflows) section.
  * This means the import locations for "Workflows v2" is now `agno.workflows`.
  * `extra_data` -> `metadata`
  * Added `store_events` to `Workflow` to optionally store the events on the workflow run output object. Also added `events_to_skip` to skip certain events from being stored. This works the same as for `Agent` and `Team`.
  * Added `store_executor_outputs` to `Workflow` to optionally store the agent/team responses on the workflow run output object.
  * Added `input_schema` to `Workflow` to validate the input to the workflow.
  * Added support for websocket streaming of the workflow. This is appropriate for long-running workflows that need to be streamed to a client. This is only available for `arun`.
  * Removed all `monitoring` from `Workflow`. With the new AgentOS platform, monitoring is done using your own data. Go to [os.agno.com](https://os.agno.com) to get started.
</Accordion>

<Accordion title="Updates to Input & Output">
  * Changed `message` to `input` (which also replaces `messages`). `input` can be of type `str`, `list`, `dict`, or `BaseModel`.
  * If a `dict` and `input_schema` is provided, the dict will be validated against the schema.
  * If a `BaseModel` and `input_schema` is provided, the model will be validated against the schema.
  * `arun` with `stream=True` now return an async iterator of `WorkflowRunOutputEvent` directly and is not a coroutine anymore.
  * `debug_mode: bool` added to `run`, `arun`, `print_response` and `aprint_response` to enable debug mode for a specific run.
  * Added `get_run_output` and `get_last_run_output` to `Workflow` to retrieve a run output by ID.
</Accordion>

<Accordion title="Updates to Sessions">
  * Removed `session_name` and replace with functions `get_session_name` and `rename_session`.
  * Because `Workflow` is now stateless, `workflow_session`, `session_metrics`, `run_id`, `run_input`, `run_messages` and `run_response` as "sticky" workflow attributes have been removed.
  * Because `Workflow` is now stateless, `images`, `videos`, `audio` are no longer available as workflow attributes. Instead these can be accessed on the `WorkflowRunOutput` for a particular run.
  * Added `get_session` to retrieve a session by ID.
  * Added `get_session_metrics` to retrieve the metrics for a session.
  * Added `get_session_state` to retrieve the session state from a session.
  * Added `get_session_summary` to retrieve the session summary from a session.
</Accordion>

---

## Memory with MongoDB

**URL:** llms-txt#memory-with-mongodb

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/mongodb-memory

```python mem-mongodb-memory.py theme={null}
from agno.agent import Agent
from agno.db.mongo import MongoDb

---

## Agent will use custom_agent_logger

**URL:** llms-txt#agent-will-use-custom_agent_logger

agent.print_response("What is 2+2?")

---

## Use the agent to get website content

**URL:** llms-txt#use-the-agent-to-get-website-content

**Contents:**
- Available Apify Tools
  - 1. RAG Web Browser

agent.print_response("What information can you find on https://docs.agno.com/introduction ?", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

agent = Agent(
    tools=[
        ApifyTools(actors=["apify/rag-web-browser"])
    ],
        markdown=True
)

**Examples:**

Example 1 (unknown):
```unknown
## Available Apify Tools

You can easily integrate any Apify Actor as a tool. Here are some examples:

### 1. RAG Web Browser

The [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor is specifically designed for AI and LLM applications. It searches the web for a query or processes a URL, then cleans and formats the content for your agent. This tool is enabled by default.
```

---

## ============================================================================

**URL:** llms-txt#============================================================================

if __name__ == "__main__":
    main()

See more examples in the Agno cookbooks:

* [Basic Conversational Workflow (sync non-streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_basics/_08_workflow_agent/sync/basic_workflow_agent.py)
* [Basic Conversational Workflow (async non-streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_06_advanced_basics/_08_workflow_agent/async/basic_workflow_agent.py)
* [Basic Conversational Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_06_advanced_basics/_08_workflow_agent/async/basic_workflow_agent_stream.py)

---

## Set up your tracing database

**URL:** llms-txt#set-up-your-tracing-database

tracing_db = SqliteDb(db_file="tmp/traces.db")

---

## Usage examples

**URL:** llms-txt#usage-examples

**Contents:**
- Developer Resources
- Reference

media_workflow.print_response("Create an image of a magical forest", markdown=True)
media_workflow.print_response("Create a cinematic video of city timelapse", markdown=True)
```

## Developer Resources

* [`workflow_using_steps.py`](/basics/workflows/usage/workflow-using-steps)
* [`workflow_using_steps_nested.py`](/basics/workflows/usage/workflow-using-steps-nested)
* [`selector_for_image_video_generation_pipelines.py`](/basics/workflows/usage/selector-for-image-video-generation-pipelines)

For complete API documentation, see [Steps Reference](/reference/workflows/steps-step).

---

## Get all spans in a trace

**URL:** llms-txt#get-all-spans-in-a-trace

spans = db.get_spans(trace_id=trace.trace_id)

---

## Print results

**URL:** llms-txt#print-results

**Contents:**
- Usage

print("\n--- Generated Shorts ---")
for short in shorts:
    print(f"Short at {short['path']}")
    print(f"Description: {short['description']}")
    print(f"Engagement Score: {short['score']}/10\n")
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U opencv-python google-generativeai sqlalchemy ffmpeg-python agno
    bash Mac theme={null}
      brew install ffmpeg
      bash Windows theme={null}
      # Install ffmpeg using chocolatey or download from https://ffmpeg.org/download.html
      choco install ffmpeg
      bash Mac theme={null}
      python cookbook/agent_basics/multimodal/video_to_shorts.py
      bash Windows theme={null}
      python cookbook/agent_basics/multimodal/video_to_shorts.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install ffmpeg">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Async Postgres for Agent

**URL:** llms-txt#async-postgres-for-agent

**Contents:**
- Usage
  - Run PgVector
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/async-postgres/usage/async-postgres-for-agent

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

<Snippet file="db-async-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/postgres/async_postgres/async_postgres_for_agent.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Create agents with more specific validation criteria

**URL:** llms-txt#create-agents-with-more-specific-validation-criteria

data_validator = Agent(
    name="Data Validator",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "You are a data validator. Analyze the provided data and determine if it's valid.",
        "For data to be VALID, it must meet these criteria:",
        "- user_count: Must be a positive number (> 0)",
        "- revenue: Must be a positive number (> 0)",
        "- date: Must be in a reasonable date format (YYYY-MM-DD)",
        "",
        "Return exactly 'VALID' if all criteria are met.",
        "Return exactly 'INVALID' if any criteria fail.",
        "Also briefly explain your reasoning.",
    ],
)

data_processor = Agent(
    name="Data Processor",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="Process and transform the validated data.",
)

report_generator = Agent(
    name="Report Generator",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="Generate a final report from processed data.",
)

def early_exit_validator(step_input: StepInput) -> StepOutput:
    """
    Custom function that checks data quality and stops workflow early if invalid
    """
    # Get the validation result from previous step
    validation_result = step_input.previous_step_content or ""

if "INVALID" in validation_result.upper():
        return StepOutput(
            content="âŒ Data validation failed. Workflow stopped early to prevent processing invalid data.",
            stop=True,  # Stop the entire workflow here
        )
    else:
        return StepOutput(
            content="âœ… Data validation passed. Continuing with processing...",
            stop=False,  # Continue normally
        )

---

## Get Content by ID

**URL:** llms-txt#get-content-by-id

Source: https://docs.agno.com/reference-api/schema/knowledge/get-content-by-id

get /knowledge/content/{content_id}
Retrieve detailed information about a specific content item including processing status and metadata.

---

## Reasoning Agent

**URL:** llms-txt#reasoning-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/reasoning-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Web Search Agent: Fetches financial information from the web

**URL:** llms-txt#web-search-agent:-fetches-financial-information-from-the-web

web_search_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    markdown=True,
)

---

## Sparse, hard to filter metadata

**URL:** llms-txt#sparse,-hard-to-filter-metadata

poor_metadata = {
    "type": "doc",
    "id": "12345"
}
python  theme={null}
from datetime import datetime

def assign_metadata(file_path: str) -> dict:
    """Generate metadata based on file characteristics."""
    metadata = {}

# Extract from filename
    if "policy" in file_path.lower():
        metadata["document_type"] = "policy"
    elif "guide" in file_path.lower():
        metadata["document_type"] = "guide"

# Extract department from path
    if "/hr/" in file_path:
        metadata["department"] = "hr"
    elif "/engineering/" in file_path:
        metadata["department"] = "engineering"

# Add timestamp
    metadata["indexed_at"] = datetime.now().isoformat()

**Examples:**

Example 1 (unknown):
```unknown
**Metadata Best Practices:**

* **Be Consistent**: Use standardized values (e.g., always "hr" not sometimes "HR" or "human\_resources")
* **Think Hierarchically**: Use nested categories when appropriate (`department.team`, `location.region`)
* **Include Temporal Data**: Add dates, versions, or other time-based metadata for lifecycle management
* **Add Semantic Tags**: Include searchable tags or keywords that might not appear in the content

**Dynamic Metadata Assignment:**

Add metadata programmatically based on content:
```

---

## Research Workflow

**URL:** llms-txt#research-workflow

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/19-blog-generator-workflow

This advanced example demonstrates how to build a sophisticated blog post generator using
the new workflow v2.0 architecture. The workflow combines web research capabilities with
professional writing expertise using a multi-stage approach:

1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

* Advanced web research and source evaluation
* Content scraping and processing
* Professional writing with SEO optimization
* Automatic content caching for efficiency
* Source attribution and fact verification

```python blog-generator-workflow.py theme={null}
import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

---

## Async Router Team with Direct Response

**URL:** llms-txt#async-router-team-with-direct-response

Source: https://docs.agno.com/basics/teams/usage/async-flows/respond-directly

This example demonstrates an asynchronous route team of AI agents working together to answer questions in different languages. The team consists of six specialized language agents (English, Japanese, Chinese, Spanish, French, and German) with a team leader that routes user questions to the appropriate language agent based on the input language.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/async_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Sqlite for Team

**URL:** llms-txt#sqlite-for-team

**Contents:**
- Usage
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/sqlite/usage/sqlite-for-team

Agno supports using Sqlite as a storage backend for Teams using the `SqliteDb` class.

You need to provide either `db_url`, `db_file` or `db_engine`. The following example uses `db_file`.

<Snippet file="db-sqlite-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/sqlite/sqlite_for_team.py)

---

## Tracing

**URL:** llms-txt#tracing

**Contents:**
- Why is Observability Important?
- Understanding Traces and Spans
  - Trace
  - Span
- What Gets Traced
- Key Features
- Quick Start
- Next Steps

Source: https://docs.agno.com/basics/tracing/overview

Gain deep visibility into your Agno agents with OpenTelemetry-based observability

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.3.5" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.3.5">v2.3.5</Tooltip>
</Badge>

In the systems you build with Agno, agents will make autonomous decisions, interact with external tools, and coordinate with each other in ways that aren't immediately visible from your recorded sessions alone. Observability is key to staying on top of what your agents are doing and how your system is performing.

**Agno Tracing** is the recommended way to introduce observability for your Agno agents and teams.
It automatically captures all relevant execution details, helping you understand what your agents are doing, simplifying debugging and helping you optimize your system.

Tracing leverages [OpenTelemetry](https://opentelemetry.io/) to instrument the Agno agents and teams, capturing traces and spans.

<Frame caption="Traces stored in SQLite database viewed with AgentOS">
  <img src="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=0e007064a21a14cabadeed560a9c207a" alt="Traces viewed in AgentOS" data-og-width="2932" width="2932" data-og-height="1314" height="1314" data-path="images/traces-in-os.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?w=280&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=651a73bbca101d922693ec384ee47083 280w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?w=560&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=b4cb3fd081298932ffc2679447686574 560w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?w=840&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=5bc9389ffb66d6cf7047c41b15a40d87 840w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?w=1100&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=4f06c18c6b7189c26089ca56bbe432af 1100w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?w=1650&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=83cf8de1945335ee4e2ef71a400c29a3 1650w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-in-os.png?w=2500&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=682376786ba9a10592e684390a0f301e 2500w" />
</Frame>

<Note>
  Agno Tracing is currently not supported for Workflows. It is in works and will be added soon.
</Note>

## Why is Observability Important?

As your Agno system become more complex (Agents using multiple tools, making sequential decisions, and coordinating with other agents), understanding its behavior becomes challenging.

**Agno Tracing** solves this:

* **Debugging**: See exactly what went wrong when an agent fails
* **Performance**: Identify bottlenecks in agent execution
* **Cost Tracking**: Monitor token usage and API calls
* **Behavior Analysis**: Understand decision-making patterns
* **Audit Trail**: Track what agents did and why

<Note>
  All Agno Tracing data is stored in your own database. No tracing data will leave your system, or be sent to third parties: you are in complete control.
</Note>

## Understanding Traces and Spans

Think of tracing like a family tree for your agent's execution:

A **trace** represents one complete agent execution from start to finish. Each trace has a unique `trace_id` that groups all related operations together.

A **span** is a single operation within that execution. Spans form a parent-child hierarchy:

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=9154b13c041f6fd5674224fda47065c0" alt="Traces vs spans diagram" data-og-width="1827" width="1827" data-og-height="930" height="930" data-path="images/traces-vs-spans-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?w=280&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=9cfde54b1f868542f84929eaa38d27a9 280w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?w=560&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=7ad28bf2fdaf0313bae5ef3fdfe97ae3 560w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?w=840&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=174cf3379538b44ba11efae87677fc78 840w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?w=1100&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=700830eaa494fa36c945ac0fc07c895b 1100w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?w=1650&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=dc52b7d0d20c4b2c96ec2991c22cee54 1650w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-light.png?w=2500&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=3ad50142e5cf8a6b7370f23ba508842b 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=c513cdba94b11c908653287085397bd0" alt="Traces vs spans diagram" data-og-width="1827" width="1827" data-og-height="930" height="930" data-path="images/traces-vs-spans-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?w=280&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=49fe1ffa407cab97107331cda9101ced 280w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?w=560&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=f8693e2efa8f079a6208d6a2dd947d37 560w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?w=840&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=0e0f880e9a920ad9b11150adcf4c7d22 840w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?w=1100&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=e24b518f77b3bfa4ffcd0ec0291e9078 1100w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?w=1650&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=7b0e4ceed3c629c174f9c4f3ef8f8693 1650w, https://mintcdn.com/agno-v2/gChHNQRMu2Gpq6Xe/images/traces-vs-spans-dark.png?w=2500&fit=max&auto=format&n=gChHNQRMu2Gpq6Xe&q=85&s=08d4acaa62b95a71966216809f102104 2500w" />

* **What happened**: Operation name (e.g., `agent.run`, `model.response`)
* **When**: Start and end timestamps
* **Context**: Input arguments, outputs, token usage
* **Relationships**: Parent span, child spans
* **Metadata**: Agent ID, session ID, run ID

Agno automatically captures:

| Operation               | What Gets Traced                                               |
| ----------------------- | -------------------------------------------------------------- |
| **Agent Runs**          | Every `agent.run()` or `agent.arun()` call with full context   |
| **Model Calls**         | LLM interactions including prompts, responses, and token usage |
| **Tool Executions**     | Tool invocations with arguments and results                    |
| **Team Operations**     | Team coordination and member agent runs                        |
| **Workflow Operations** | Coming soon!                                                   |

* **Zero-Code Instrumentation**: No need to modify your agent code
* **Database Storage**: Traces stored in your Agno database (SQLite, PostgreSQL, etc.)
* **Flexible Querying**: Filter by agent, session, run, or time range
* **OpenTelemetry Standard**: Export to external tools like Arize Phoenix, Langfuse
* **Non-Blocking**: Tracing never slows down your agents
* **Configurable**: Adjust batch sizes and processing for your needs

<Steps>
  <Step title="Install Dependencies">
    
  </Step>

<Step title="Enable Tracing">
    
  </Step>

<Step title="Run Your Agent">
    
  </Step>

<Step title="Query Traces">
    
  </Step>
</Steps>

<CardGroup cols={3}>
  <Card title="Basic Setup" icon="rocket" href="/basics/tracing/basic-setup">
    Configure and enable tracing
  </Card>

<Card title="Tracing in AgentOS" icon="rocket" href="/agent-os/tracing/overview">
    Trace your agents and teams in AgentOS
  </Card>

<Card title="DB Functions" icon="database" href="/basics/tracing/db-functions">
    Query traces and spans from your database
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Enable Tracing">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Your Agent">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Query Traces">
```

---

## Run team as an interactive CLI app

**URL:** llms-txt#run-team-as-an-interactive-cli-app

**Contents:**
- Developer Resources

team.cli_app(stream=True)
```

<Note>
  Use `await team.acli_app()` to run the team asynchronously in an interactive CLI app.
</Note>

## Developer Resources

* View the [Team reference](/reference/teams/team)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/README.md)

---

## Load knowledge base using keyword search

**URL:** llms-txt#load-knowledge-base-using-keyword-search

keyword_db = PgVector(
    table_name="recipes", db_url=db_url, search_type=SearchType.keyword
)
knowledge = Knowledge(
    name="Keyword Search Knowledge Base",
    vector_db=keyword_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

---

## Eleven Labs

**URL:** llms-txt#eleven-labs

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/eleven-labs

**ElevenLabsTools** enable an Agent to perform audio generation tasks using [ElevenLabs](https://elevenlabs.io/docs/product/introduction)

You need to install the `elevenlabs` library and an API key which can be obtained from [Eleven Labs](https://elevenlabs.io/)

Set the `ELEVEN_LABS_API_KEY` environment variable.

The following agent will use Eleven Labs to generate audio based on a user prompt.

```python cookbook/tools/eleven_labs_tools.py theme={null}
from agno.agent import Agent
from agno.tools.eleven_labs import ElevenLabsTools

**Examples:**

Example 1 (unknown):
```unknown
Set the `ELEVEN_LABS_API_KEY` environment variable.
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Eleven Labs to generate audio based on a user prompt.
```

---

## MigrationManager

**URL:** llms-txt#migrationmanager

**Contents:**
- Constructor
  - Parameters
- Properties
- Methods
  - up()
  - down()
- Supported Databases
- Table Types
- See Also

Source: https://docs.agno.com/reference/storage/migrations

API reference for the MigrationManager class used to handle database migrations.

The `MigrationManager` class provides a programmatic way to manage database schema migrations for Agno database tables.

<ResponseField name="db" type="Union[AsyncBaseDb, BaseDb]" required>
  The database instance to run migrations on. Supports both synchronous and asynchronous database classes.
</ResponseField>

<ResponseField name="latest_schema_version" type="Version">
  Returns the latest available schema version from the migration versions list.
</ResponseField>

<ResponseField name="available_versions" type="list[tuple[str, Version]]">
  A list of available migration versions as tuples of (version\_string, parsed\_version).

Currently available versions:

* `v2_0_0` (2.0.0)
  * `v2_3_0` (2.3.0)
</ResponseField>

Executes upgrade migrations to bring database tables to a target schema version.

<ResponseField name="target_version" type="str" optional>
  The version to migrate to (e.g., "2.3.0"). If not provided, migrates to the latest available version.
</ResponseField>

<ResponseField name="table_type" type="str" optional>
  The specific table type to migrate. If not provided, all tables will be migrated.

Valid values: `"memory"`, `"session"`, `"metrics"`, `"eval"`, `"knowledge"`, `"culture"`
</ResponseField>

<ResponseField name="force" type="bool" default="False">
  Force the migration even if the current version is equal to or greater than the target version.
</ResponseField>

Executes downgrade migrations to revert database tables to a target schema version.

<ResponseField name="target_version" type="str" required>
  The version to migrate down to (e.g., "2.0.0"). This parameter is required for down migrations.
</ResponseField>

<ResponseField name="table_type" type="str" optional>
  The specific table type to migrate. If not provided, all tables will be migrated.

Valid values: `"memory"`, `"session"`, `"metrics"`, `"eval"`, `"knowledge"`, `"culture"`
</ResponseField>

<ResponseField name="force" type="bool" default="False">
  Force the migration even if the current version is equal to or less than the target version.
</ResponseField>

## Supported Databases

The `MigrationManager` supports the following database types:

* PostgreSQL (via `PostgresDb` or `AsyncPostgresDb`)
* SQLite (via `SqliteDb` or `AsyncSqliteDb`)
* MySQL (via `MySQLDb`)
* SingleStore (via `SingleStoreDb`)

The following table types can be migrated:

| Table Type  | Description                   |
| ----------- | ----------------------------- |
| `memory`    | Agent memory storage          |
| `session`   | Agent session data            |
| `metrics`   | Performance and usage metrics |
| `eval`      | Evaluation results            |
| `knowledge` | Knowledge base entries        |
| `culture`   | Culture and behavior data     |

* [Database Migrations Guide](/basics/database/migrations)
* [Migration Scripts](https://github.com/agno-agi/agno/tree/main/libs/agno/migrations)

**Examples:**

Example 1 (unknown):
```unknown
### Parameters

<ResponseField name="db" type="Union[AsyncBaseDb, BaseDb]" required>
  The database instance to run migrations on. Supports both synchronous and asynchronous database classes.
</ResponseField>

## Properties

<ResponseField name="latest_schema_version" type="Version">
  Returns the latest available schema version from the migration versions list.
</ResponseField>

<ResponseField name="available_versions" type="list[tuple[str, Version]]">
  A list of available migration versions as tuples of (version\_string, parsed\_version).

  Currently available versions:

  * `v2_0_0` (2.0.0)
  * `v2_3_0` (2.3.0)
</ResponseField>

## Methods

### up()

Executes upgrade migrations to bring database tables to a target schema version.
```

Example 2 (unknown):
```unknown
#### Parameters

<ResponseField name="target_version" type="str" optional>
  The version to migrate to (e.g., "2.3.0"). If not provided, migrates to the latest available version.
</ResponseField>

<ResponseField name="table_type" type="str" optional>
  The specific table type to migrate. If not provided, all tables will be migrated.

  Valid values: `"memory"`, `"session"`, `"metrics"`, `"eval"`, `"knowledge"`, `"culture"`
</ResponseField>

<ResponseField name="force" type="bool" default="False">
  Force the migration even if the current version is equal to or greater than the target version.
</ResponseField>

#### Example
```

Example 3 (unknown):
```unknown
### down()

Executes downgrade migrations to revert database tables to a target schema version.
```

Example 4 (unknown):
```unknown
#### Parameters

<ResponseField name="target_version" type="str" required>
  The version to migrate down to (e.g., "2.0.0"). This parameter is required for down migrations.
</ResponseField>

<ResponseField name="table_type" type="str" optional>
  The specific table type to migrate. If not provided, all tables will be migrated.

  Valid values: `"memory"`, `"session"`, `"metrics"`, `"eval"`, `"knowledge"`, `"culture"`
</ResponseField>

<ResponseField name="force" type="bool" default="False">
  Force the migration even if the current version is equal to or less than the target version.
</ResponseField>

#### Example
```

---

## PIIDetectionGuardrail

**URL:** llms-txt#piidetectionguardrail

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/hooks/pii-guardrail

| Parameter                  | Type   | Default | Description                                                                           |
| -------------------------- | ------ | ------- | ------------------------------------------------------------------------------------- |
| `mask_pii`                 | `bool` | `False` | Whether to mask the PII in the input, rather than raising an error.                   |
| `enable_ssn_check`         | `bool` | `True`  | Whether to check for Social Security Numbers.                                         |
| `enable_credit_card_check` | `bool` | `True`  | Whether to check for credit cards.                                                    |
| `enable_email_check`       | `bool` | `True`  | Whether to check for emails.                                                          |
| `enable_phone_check`       | `bool` | `True`  | Whether to check for phone numbers.                                                   |
| `custom_patterns`          | `dict` | `{}`    | A dictionary of custom PII patterns to detect. This is added to the default patterns. |

---

## Condition with list of steps

**URL:** llms-txt#condition-with-list-of-steps

Source: https://docs.agno.com/basics/workflows/usage/condition-with-list-of-steps

This example demonstrates how to use conditional step to execute multiple steps in parallel.

This example demonstrates **Workflows 2.0** advanced conditional execution where conditions
can trigger multiple steps and run in parallel. Shows how to create sophisticated branching
logic with complex multi-step sequences based on content analysis.

**When to use**: When different topics or content types require completely different
processing pipelines. Ideal for adaptive workflows where the research methodology
should change based on the subject matter or complexity requirements.

```python condition_with_list_of_steps.py theme={null}
from agno.agent.agent import Agent
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

---

## Initial Retriever Agent - Specialized in broad initial retrieval

**URL:** llms-txt#initial-retriever-agent---specialized-in-broad-initial-retrieval

initial_retriever = Agent(
    name="Initial Retriever",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Perform broad initial retrieval to gather candidate information",
    knowledge=reranked_knowledge,
    search_knowledge=True,
    instructions=[
        "Perform comprehensive initial retrieval from the knowledge base.",
        "Cast a wide net to gather all potentially relevant information.",
        "Focus on recall rather than precision in this initial phase.",
        "Retrieve diverse content that might be relevant to the query.",
    ],
    markdown=True,
)

---

## Setup Postgres

**URL:** llms-txt#setup-postgres

**Contents:**
- Usage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    db=db,
    enable_user_memories=True,
)

agent.print_response("My name is John Doe and I like to play basketball on the weekends.")
agent.print_response("What's do I do in weekends?")
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai sqlalchemy 'psycopg[binary]'
    bash Mac/Linux theme={null}
      python mem-postgres-memory.py
      bash Windows theme={null}
      python mem-postgres-memory.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set environment variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Quick start - SQLite

**URL:** llms-txt#quick-start---sqlite

**Contents:**
- Workflow History
  - Enable history for your steps
  - History format
- Session Naming
  - Manual Naming

workflow = Workflow(
    name="Research Pipeline",
    db=SqliteDb(db_file="workflows.db"),
    steps=[...],
)
python  theme={null}
from agno.workflow import Workflow
from agno.db.sqlite import SqliteDb

workflow = Workflow(
    name="Content Pipeline",
    db=SqliteDb(db_file="workflows.db"),
    steps=[...],
    add_workflow_history_to_steps=True,  # Include previous runs
    num_history_runs=5,                  # Limit how many runs to load
)
xml  theme={null}
<workflow_history_context>
[Workflow Run-1]
User input: Create a blog post about AI
Workflow output: [Full output from run]

[Workflow Run-2]
User input: Write about machine learning
Workflow output: [Full output from run]
</workflow_history_context>
python  theme={null}
from agno.workflow import Workflow
from agno.db.sqlite import SqliteDb

workflow = Workflow(
    name="Research Pipeline",
    db=SqliteDb(db_file="workflows.db"),
    steps=[...],
)

workflow.run(input="Analyze AI trends", session_id="session_123")
workflow.set_session_name(session_id="session_123", session_name="AI Trends Analysis Q4 2024")

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Database Configuration Guide" icon="database" href="/basics/database/overview">
  See all supported databases, connection options, and production recommendations
</Card>

## Workflow History

Workflow history lets workflow steps access results from previous runs. When enabled, Agno formats prior run results and prepends them to each step input so future executions can build on that context.

### Enable history for your steps
```

Example 2 (unknown):
```unknown
<Check>
  Why it helps:

  * **Access previous runs** instead of repeating work
  * **Reference past decisions** to keep outputs consistent
  * **Maintain context** across multi-run workflows
  * **Build on prior results** for richer analysis
</Check>

### History format

Agno wraps past runs in a structured XML block before inserting it into each step input:
```

Example 3 (unknown):
```unknown
See the [workflow history implementation guide](/basics/chat-history/workflow/overview) for advanced controls, per-step overrides, and programmatic access patterns.

## Session Naming

Give your workflow sessions meaningful names for easier identification:

### Manual Naming
```

---

## Alert if memory count is unusually high

**URL:** llms-txt#alert-if-memory-count-is-unusually-high

**Contents:**
- Developer Resources

if len(memories) > 500:
    print("âš ï¸ Warning: User has excessive memories. Consider pruning.")
```

## Developer Resources

* View [Examples](/basics/memory/working-with-memories/overview)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/memory/)

---

## Database URL

**URL:** llms-txt#database-url

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Ask the Agent about the user

**URL:** llms-txt#ask-the-agent-about-the-user

**Contents:**
- Toolkit Params
- Toolkit Functions
- Async Toolkit
- Developer Resources

agent.print_response("What do you know about me?")
```

| Parameter                   | Type   | Default | Description                                                 |
| --------------------------- | ------ | ------- | ----------------------------------------------------------- |
| `session_id`                | `str`  | `None`  | Optional session ID. Auto-generated if not provided.        |
| `user_id`                   | `str`  | `None`  | Optional user ID. Auto-generated if not provided.           |
| `api_key`                   | `str`  | `None`  | Zep API key. If not provided, uses ZEP\_API\_KEY env var.   |
| `ignore_assistant_messages` | `bool` | `False` | Whether to ignore assistant messages when adding to memory. |
| `enable_add_zep_message`    | `bool` | `True`  | Add a message to the current Zep session memory.            |
| `enable_get_zep_memory`     | `bool` | `True`  | Retrieve memory for the current Zep session.                |
| `enable_search_zep_memory`  | `bool` | `True`  | Search the Zep memory store for relevant information.       |
| `instructions`              | `str`  | `None`  | Custom instructions for using the Zep tools.                |
| `add_instructions`          | `bool` | `False` | Whether to add default instructions.                        |

| Function            | Description                                                                                                                                                                                                                                |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `add_zep_message`   | Adds a message to the current Zep session memory. Takes `role` (str) for the message sender and `content` (str) for the message text. Returns a confirmation or error message.                                                             |
| `get_zep_memory`    | Retrieves memory for the current Zep session. Takes optional `memory_type` (str) parameter with options "context" (default), "summary", or "messages". Returns the requested memory content or an error.                                   |
| `search_zep_memory` | Searches the Zep memory store for relevant information. Takes `query` (str) to find relevant facts and optional `search_scope` (str) parameter with options "messages" (default) or "summary". Returns search results or an error message. |

The `ZepAsyncTools` class extends the `ZepTools` class and provides asynchronous versions of the toolkit functions.

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/zep.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/zep_tools.py)
* View [Async Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/zep_async_tools.py)

---

## Persisting Sessions

**URL:** llms-txt#persisting-sessions

**Contents:**
- Quick Start
- Supported Databases
- Session IDs

Source: https://docs.agno.com/basics/sessions/persisting-sessions/overview

Store session data in a database for multi-turn conversations

To enable sessions across multiple runs, you need to configure a database. Once configured, Agno automatically stores conversation history, session state, and run metadata.

<Note>
  Database selection, connection strings, credentials management, and operational guidance live in the [Database overview](/basics/database/overview). Reuse that setup hereâ€”this page only adds the session-specific considerations.
</Note>

Once you have a `db` object (Postgres, SQLite, DynamoDB, â€¦) configured per the storage docs, persistence is enabled simply by passing it to your Agent, Team, or Workflow:

## Supported Databases

Nothing new lives hereâ€”simply reuse the database drivers and guidance from `/basics/storage`:

* **PostgreSQL** (recommended) â€“ Production-grade, supports custom `session_table` names for isolating workloads.
* **SQLite** â€“ Great for local development; swap the file path just as you would elsewhere.
* **InMemoryDb** â€“ Useful for tests or demos only. Data disappears when the process exits.

If you need to tune indexing, retention, or connection pools, do that in the shared storage layer so every feature (sessions, memories, knowledge, etc.) benefits from the same configuration.

Sessions are identified by `session_id`. Use the same ID to continue a conversation:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Supported Databases

Nothing new lives hereâ€”simply reuse the database drivers and guidance from `/basics/storage`:

* **PostgreSQL** (recommended) â€“ Production-grade, supports custom `session_table` names for isolating workloads.
* **SQLite** â€“ Great for local development; swap the file path just as you would elsewhere.
* **InMemoryDb** â€“ Useful for tests or demos only. Data disappears when the process exits.

If you need to tune indexing, retention, or connection pools, do that in the shared storage layer so every feature (sessions, memories, knowledge, etc.) benefits from the same configuration.

## Session IDs

Sessions are identified by `session_id`. Use the same ID to continue a conversation:
```

---

## Prompt the agent to solve the problem

**URL:** llms-txt#prompt-the-agent-to-solve-the-problem

**Contents:**
- Usage

reasoning_agent.print_response("Is 9.11 bigger or 9.9?", stream=True)
bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U groq agno
    bash Mac theme={null}
      python cookbook/models/groq/reasoning_agent.py
      bash Windows theme={null}
      python cookbook/models/groq/reasoning_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## print(video_agent.run_response.videos)

**URL:** llms-txt#print(video_agent.run_response.videos)

**Contents:**
- Usage

bash  theme={null}
    pip install -U agno
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch generate_video_using_models_lab.py
    bash Mac theme={null}
      python generate_video_using_models_lab.py
      bash Windows   theme={null}
      python generate_video_using_models_lab.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## PDF Input Local Agent

**URL:** llms-txt#pdf-input-local-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/pdf-input-local

```python cookbook/models/anthropic/pdf_input_local.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

---

## Generate Video Using ModelsLab

**URL:** llms-txt#generate-video-using-modelslab

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/agent/usage/generate-video-using-models-lab

This example demonstrates how to create an AI agent that generates videos using the ModelsLab API.

```python generate_video_using_models_lab.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

video_agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[ModelsLabTools()],
    description="You are an AI agent that can generate videos using the ModelsLabs API.",
    instructions=[
        "When the user asks you to create a video, use the `generate_media` tool to create the video.",
        "The video will be displayed in the UI automatically below your response, so you don't need to show the video URL in your response.",
        "Politely and courteously let the user know that the video has been generated and will be displayed below as soon as its ready.",
    ],
    markdown=True,
)

video_agent.print_response("Generate a video of a cat playing with a ball")

---

## Find recent sales data from specific regions, but exclude drafts

**URL:** llms-txt#find-recent-sales-data-from-specific-regions,-but-exclude-drafts

complex_filter = AND(
    EQ("data_type", "sales"),
    IN("region", ["north_america", "europe"]),
    GT("year", 2022),
    NOT(EQ("status", "draft"))
)

---

## Database connection URL

**URL:** llms-txt#database-connection-url

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## --- Main Execution Function ---

**URL:** llms-txt#----main-execution-function----

async def blog_generation_execution(
    session_state,
    topic: str = None,
    use_search_cache: bool = True,
    use_scrape_cache: bool = True,
    use_blog_cache: bool = True,
) -> str:
    """
    Blog post generation workflow execution function.

Args:
        session_state: The shared session state
        topic: Blog post topic (if not provided, uses execution_input.input)
        use_search_cache: Whether to use cached search results
        use_scrape_cache: Whether to use cached scraped articles
        use_blog_cache: Whether to use cached blog posts
    """

if not blog_topic:
        return "âŒ No blog topic provided. Please specify a topic."

print(f"ðŸŽ¨ Generating blog post about: {blog_topic}")
    print("=" * 60)

# Check for cached blog post first
    if use_blog_cache:
        cached_blog = get_cached_blog_post(session_state, blog_topic)
        if cached_blog:
            print("ðŸ“‹ Found cached blog post!")
            return cached_blog

# Phase 1: Research and gather sources
    print("\nðŸ” PHASE 1: RESEARCH & SOURCE GATHERING")
    print("=" * 50)

search_results = await get_search_results(
        session_state, blog_topic, use_search_cache
    )

if not search_results or len(search_results.articles) == 0:
        return f"âŒ Sorry, could not find any articles on the topic: {blog_topic}"

print(f"ðŸ“Š Found {len(search_results.articles)} relevant sources:")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

# Phase 2: Content extraction
    print("\nðŸ“„ PHASE 2: CONTENT EXTRACTION")
    print("=" * 50)

scraped_articles = await scrape_articles(
        session_state, blog_topic, search_results, use_scrape_cache
    )

if not scraped_articles:
        return f"âŒ Could not extract content from any articles for topic: {blog_topic}"

print(f"ðŸ“– Successfully extracted content from {len(scraped_articles)} articles")

# Phase 3: Blog post writing
    print("\nâœï¸ PHASE 3: BLOG POST CREATION")
    print("=" * 50)

# Prepare input for the writer
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

print("ðŸ¤– AI is crafting your blog post...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

if not writer_response or not writer_response.content:
        return f"âŒ Failed to generate blog post for topic: {blog_topic}"

blog_post = writer_response.content

# Cache the blog post
    cache_blog_post(session_state, blog_topic, blog_post)

print("âœ… Blog post generated successfully!")
    print(f"ðŸ“ Length: {len(blog_post)} characters")
    print(f"ðŸ“š Sources: {len(scraped_articles)} articles")

---

## State Management

**URL:** llms-txt#state-management

**Contents:**
- How State Works
  - Basic Example
- Learn more
- Developer Resources

Source: https://docs.agno.com/basics/state/overview

Persist and share data across agent runs, team coordination, and workflow execution

**State** is data that persists across multiple runs within a session, enabling agents, teams, and workflows to maintain context and remember information.

Common use cases include managing user-specific data like shopping lists, todo lists, preferences, or any information that needs to persist across interactions. State is managed through `session_state`, which can be accessed and updated in tools, then automatically persisted to your database.

State in Agno follows this pattern:

1. **Initialize** - Set default `session_state` when creating agents, teams, or workflows
2. **Access** - Tools access state via `run_context.session_state`
3. **Update** - Modifications are automatically persisted to the database
4. **Load** - Subsequent runs in the same session retrieve the stored state

Here's a simple agent that maintains a shopping list:

<CardGroup cols={3}>
  <Card title="Agent State" icon="robot" iconType="duotone" href="/basics/state/agent">
    Learn about state in agents.
  </Card>

<Card title="Team State" icon="users" iconType="duotone" href="/basics/state/team">
    Learn about state in teams.
  </Card>

<Card title="Workflow State" icon="diagram-project" iconType="duotone" href="/basics/state/workflows/overview">
    Learn about state in workflows.
  </Card>
</CardGroup>

## Developer Resources

* View the [Agent schema](/reference/agents/agent)
* View the [Team schema](/reference/teams/team)
* View the [Workflow schema](/reference/workflows/workflow)
* View the [RunContext schema](/reference/run/run-context)

---

## Image to Audio Agent

**URL:** llms-txt#image-to-audio-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/images/usage/image-to-audio

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cancel Agent Run

**URL:** llms-txt#cancel-agent-run

Source: https://docs.agno.com/reference-api/schema/agents/cancel-agent-run

post /agents/{agent_id}/runs/{run_id}/cancel
Cancel a currently executing agent run. This will attempt to stop the agent's execution gracefully.

**Note:** Cancellation may not be immediate for all operations.

---

## Async Postgres for Team

**URL:** llms-txt#async-postgres-for-team

**Contents:**
- Usage
  - Run PgVector
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/async-postgres/usage/async-postgres-for-team

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

<Snippet file="db-async-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/postgres/async_postgres/async_postgres_for_team.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Firestore for Workflows

**URL:** llms-txt#firestore-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/firestore/usage/firestore-for-workflow

Agno supports using Firestore as a storage backend for Workflows using the `FirestoreDb` class.

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

PROJECT_ID = "agno-os-test"  # Use your project ID here

---

## More example interactions to try:

**URL:** llms-txt#more-example-interactions-to-try:

**Contents:**
- Usage

"""
Try these voice interaction scenarios:
1. "Can you summarize the main points discussed in this recording?"
2. "What emotions or tone do you detect in the speaker's voice?"
3. "Please provide a detailed analysis of the speech patterns and clarity"
4. "Can you identify any background noises or audio quality issues?"
5. "What is the overall context and purpose of this recording?"

Note: You can use your own audio files by converting them to base64 format.
Example for using your own audio file:

with open('your_audio.wav', 'rb') as audio_file:
    audio_data = audio_file.read()
    agent.run("Analyze this audio", audio=[Audio(content=audio_data, format="wav")])
"""
bash  theme={null}
    pip install openai requests agno
    bash  theme={null}
    python audio_input_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Sessions

**URL:** llms-txt#sessions

**Contents:**
- What's a Session?
- Single-Run Example

Source: https://docs.agno.com/basics/sessions/overview

Learn about Agno Sessions and how they work.

When you call `Agent.run()`, it creates a single, stateless interaction. The agent responds to your message and that's it - no memory of what just happened.

But most real applications need **conversations**, not just one-off exchanges. That's where sessions come in.

Think of a session as a conversation thread. It's a collection of back-and-forth interactions (called "runs") between a user and your Agent, Team, or Workflow. Each session gets a unique `session_id` that ties together all the runs, chat history, state, and metrics for that conversation.

Here's the breakdown:

* **Session**: A multi-turn conversation identified by a `session_id`. Contains all the runs, history, state, and metrics for that conversation thread.
* **Run**: A single interaction within a session. Every time you call `Agent.run()`, `Team.run()`, or `Workflow.run()`, a new `run_id` is created. Think of it as one message-and-response pair in the conversation.

<Note>
  Sessions require a database to store history and state. See [Session Storage](/basics/database/overview) for setup details.
</Note>

<Note>
  **Workflow sessions work differently:** Unlike agent and team sessions that store conversation messages, workflow sessions track complete pipeline executions (runs) with inputs and outputs. Because of these unique characteristics, we've created a [dedicated Workflow Sessions section](/basics/sessions/workflow-sessions) that covers workflow-specific features like run-based history, session state, and workflow agents.
</Note>

## Single-Run Example

When you run an agent without specifying a `session_id`, Agno automatically generates both a `run_id` and a `session_id` for you:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

---

## Get the path to our configuration file

**URL:** llms-txt#get-the-path-to-our-configuration-file

cwd = Path(__file__).parent
config_file_path = str(cwd.joinpath("configuration.yaml"))

---

## Create a File Search store and upload documents

**URL:** llms-txt#create-a-file-search-store-and-upload-documents

store = model.create_file_search_store(display_name="My Docs")
operation = model.upload_to_file_search_store(
    file_path=Path("documents/sample.txt"),
    store_name=store.name,
    display_name="Sample Document",
)
model.wait_for_operation(operation)

---

## Example usage with a famous landmark

**URL:** llms-txt#example-usage-with-a-famous-landmark

agent.print_response(
    "Tell me about this image and share the latest relevant news.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

---

## Create a database connection

**URL:** llms-txt#create-a-database-connection

db = SqliteDb(
    db_file="tmp/memory.db"
)

memory_tools = MemoryTools(
    db=db,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[memory_tools],
    markdown=True,
)

agent.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends. "
    "I like to travel to new places and experience different cultures. "
    "I am planning to travel to Africa in December. ",
    user_id="john_doe@example.com",
    stream=True
)

---

## Social Media Agent

**URL:** llms-txt#social-media-agent

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/social_media_agent

Social Media Agent Example with Dummy Dataset

This example demonstrates how to create an agent that:

1. Analyzes a dummy dataset of tweets
2. Leverages LLM capabilities to perform sophisticated sentiment analysis
3. Provides insights about the overall sentiment around a topic

```python cookbook/examples/agents/social_media_agent.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.x import XTools

---

## WhatsApp

**URL:** llms-txt#whatsapp

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/social/whatsapp

**WhatsAppTools** enable an Agent to interact with the WhatsApp Business API, allowing it to send text and template messages.

This cookbook demonstrates how to use WhatsApp integration with Agno. Before running this example,
you'''ll need to complete these setup steps:

1. Create Meta Developer Account
   * Go to [Meta Developer Portal](https://developers.facebook.com/) and create a new account
   * Create a new app at [Meta Apps Dashboard](https://developers.facebook.com/apps/)
   * Enable WhatsApp integration for your app [here](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

2. Set Up WhatsApp Business API
   You can get your WhatsApp Business Account ID from [Business Settings](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

3. Configure Environment
   * Set these environment variables:

* For first-time outreach, you must use pre-approved message templates
  [here](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/send-message-templates)
* Test messages can only be sent to numbers that are registered in your test environment

The example below shows how to send a template message using Agno'''s WhatsApp tools.
For more complex use cases, check out the WhatsApp Cloud API documentation:
[here](https://developers.facebook.com/docs/whatsapp/cloud-api/overview)

The following agent will send a template message using WhatsApp:

```python cookbook/tools/whatsapp_tool.py theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.whatsapp import WhatsAppTools

agent = Agent(
    name="whatsapp",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[WhatsAppTools()]
)

**Examples:**

Example 1 (unknown):
```unknown
Important Notes:

* For first-time outreach, you must use pre-approved message templates
  [here](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/send-message-templates)
* Test messages can only be sent to numbers that are registered in your test environment

The example below shows how to send a template message using Agno'''s WhatsApp tools.
For more complex use cases, check out the WhatsApp Cloud API documentation:
[here](https://developers.facebook.com/docs/whatsapp/cloud-api/overview)

## Example

The following agent will send a template message using WhatsApp:
```

---

## Knowledge

**URL:** llms-txt#knowledge

Source: https://docs.agno.com/reference/knowledge/knowledge

Knowledge is a class that manages knowledge bases for AI agents. It provides comprehensive knowledge management capabilities including adding new content to the knowledge base, searching the knowledge base and deleting content from the knowledge base.

<Snippet file="knowledge-reference.mdx" />

---

## Setup database

**URL:** llms-txt#setup-database

db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

---

## LangChain Async

**URL:** llms-txt#langchain-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/langchain/usage/async-langchain-db

```python cookbook/knowledge/vector_db/langchain/langchain_db.py theme={null}
import asyncio
import pathlib

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.langchaindb import LangChainVectorDb
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings

---

## Memory

**URL:** llms-txt#memory

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/memory

```python cookbook/models/openai/responses/memory.py theme={null}
"""
This recipe shows how to use personalized memories and summaries in an agent.
Steps:
1. Run: `./cookbook/scripts/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector` to install the dependencies
3. Run: `python cookbook/agents/personalized_memories_and_summaries.py` to run the agent
"""

from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIResponses
from rich.pretty import pprint

---

## Working with Memories

**URL:** llms-txt#working-with-memories

**Contents:**
- Customizing the Memory Manager

Source: https://docs.agno.com/basics/memory/working-with-memories/overview

Customize how memories are created, control context inclusion, share memories across agents, and use memory tools for advanced workflows.

The basic memory setup covers most use cases, but sometimes you need more control. This guide covers advanced patterns for customizing memory behavior, controlling what gets stored, and building complex multi-agent systems with shared memory.

## Customizing the Memory Manager

The `MemoryManager` controls which LLM creates and updates memories, plus how those memories are generated. You can customize it to use a specific model, add privacy rules, or change how memories are extracted:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.memory import MemoryManager
from agno.models.openai import OpenAIChat

---

## Filter by metadata

**URL:** llms-txt#filter-by-metadata

**Contents:**
- Next Steps

knowledge.add_content(
    path="docs/",
    filters={"department": "engineering", "clearance": "public"}
)
```

<CardGroup cols={3}>
  <Card title="Content Types" icon="file-lines" href="/basics/knowledge/content-types">
    Learn about different ways to add information to your knowledge base
  </Card>

<Card title="Search & Retrieval" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Understand how agents find and use information
  </Card>

<Card title="Readers" icon="book-open" href="/basics/knowledge/readers">
    Explore content parsing and ingestion options
  </Card>

<Card title="Chunking" icon="scissors" href="/basics/knowledge/chunking/overview">
    Optimize how content is broken down for search
  </Card>
</CardGroup>

---

## Setup paths

**URL:** llms-txt#setup-paths

cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

---

## Video Input (File Upload)

**URL:** llms-txt#video-input-(file-upload)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/video-input-file-upload

```python cookbook/models/google/gemini/video_input_file_upload.py theme={null}
import time
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

---

## Setup a basic agent with the SQLite database

**URL:** llms-txt#setup-a-basic-agent-with-the-sqlite-database

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    add_datetime_to_context=True,
)

---

## Live Search Agent Stream

**URL:** llms-txt#live-search-agent-stream

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/live-search-agent-stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## run: RunOutput = agent.run("New York")

**URL:** llms-txt#run:-runoutput-=-agent.run("new-york")

---

## Create Knowledge with both databases

**URL:** llms-txt#create-knowledge-with-both-databases

**Contents:**
  - Alternative Database Examples

knowledge = Knowledge(
    name="My Knowledge Base",
    vector_db=vector_db,
    contents_db=contents_db  # This enables content tracking!
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Alternative Database Examples
```

---

## "Hi, i want to make a 3 course meal. Can you recommend some recipes. "

**URL:** llms-txt#"hi,-i-want-to-make-a-3-course-meal.-can-you-recommend-some-recipes.-"

---

## Image Generation Tools

**URL:** llms-txt#image-generation-tools

**Contents:**
- Image Generation using a tool
- Developer Resources

Source: https://docs.agno.com/basics/multimodal/images/image-generation

Learn how to use image generation tools with Agno agents.

Similar to providing multimodal inputs, you can also get multimodal outputs from an agent.

## Image Generation using a tool

The following example demonstrates how to generate an image using an OpenAI tool with an agent.

<Check>
  The output of the tool generating a media also goes to the model's input as a
  message so it has access to the media (image, audio, video) and can use it in
  the response. For example, if you say "Generate an image of a dog and tell me
  its color." the model will have access to the image and can use it to describe
  the dog's color in the response in the same run.

That also means you can ask follow-up questions about the image, since it would be available in the history of the agent.
</Check>

## Developer Resources

* View more [Examples](/basics/multimodal/images/usage/generate-image)

---

## Pass a dict that matches the input schema

**URL:** llms-txt#pass-a-dict-that-matches-the-input-schema

hackernews_agent.print_response(
    input={
        "topic": "AI",
        "focus_areas": ["AI", "Machine Learning"],
        "target_audience": "Developers",
        "sources_required": "5",
    }
)

---

## Audio Configuration

**URL:** llms-txt#audio-configuration

SAMPLE_RATE = 24000  # Hz (24kHz)
CHANNELS = 1  # Mono (Change to 2 if Stereo)
SAMPLE_WIDTH = 2  # Bytes (16 bits)

---

## Configuration for the Evals page

**URL:** llms-txt#configuration-for-the-evals-page

evals:
  available_models:
    - <MODEL_STRING>
    ...
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        available_models:
          - <MODEL_STRING>
          ...
        display_name: <DISPLAY_NAME>
    ...

---

## Run the team

**URL:** llms-txt#run-the-team

run_response: TeamRunOutput = team.run(
    "What is going on in the world?"
)
pprint_run_response(run_response, markdown=True)

---

## Create a reasoning agent that uses:

**URL:** llms-txt#create-a-reasoning-agent-that-uses:

---

## Disable telemetry for a specific agent

**URL:** llms-txt#disable-telemetry-for-a-specific-agent

**Contents:**
- Developer Resources

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    telemetry=False
)
```

* **Agents**: `Agent(telemetry=False)`
* **Teams**: `Team(telemetry=False)`
* **Workflows**: `Workflow(telemetry=False)`
* **AgentOS**: `AgentOS(telemetry=False)`

## Developer Resources

* View the [Agent schema](/reference/agents/agent)
* View the [Team schema](/reference/teams/team)
* View the [Workflow schema](/reference/workflows/workflow)

---

## List Traces

**URL:** llms-txt#list-traces

Source: https://docs.agno.com/reference-api/schema/tracing/list-traces

get /traces
Retrieve a paginated list of execution traces with optional filtering.

**Traces provide observability into:**
- Agent execution flows
- Model invocations and token usage
- Tool calls and their results
- Errors and performance bottlenecks

**Filtering Options:**
- By run, session, user, or agent ID
- By status (OK, ERROR)
- By time range

**Pagination:**
- Use `page` (1-indexed) and `limit` parameters
- Response includes pagination metadata (total_pages, total_count, etc.)

**Response Format:**
Returns summary information for each trace. Use GET `/traces/{trace_id}` for detailed hierarchy.

---

## Demo Phi4

**URL:** llms-txt#demo-phi4

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/local/ollama/usage/demo-phi4

```python cookbook/models/ollama/demo_phi4.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="phi4"), markdown=True)

---

## Basic Session State Management

**URL:** llms-txt#basic-session-state-management

Source: https://docs.agno.com/basics/state/agent/usage/session-state-basic

This example demonstrates how to create an agent with basic session state management, maintaining a shopping list across interactions using SQLite storage.

<Steps>
  <Step title="Create a Python file">
    Create a file `session_state_basic.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

  <Step title="Run the agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Hybrid Searcher Agent - Specialized in hybrid search

**URL:** llms-txt#hybrid-searcher-agent---specialized-in-hybrid-search

hybrid_searcher = Agent(
    name="Hybrid Searcher",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Perform hybrid search combining vector and text search",
    knowledge=hybrid_knowledge,
    search_knowledge=True,
    instructions=[
        "Combine vector similarity and text search for comprehensive results.",
        "Find information that matches both semantic and lexical criteria.",
        "Use PostgreSQL's hybrid search capabilities for best coverage.",
        "Ensure retrieval of both conceptually and textually relevant content.",
    ],
    markdown=True,
)

---

## agent.print_response("New York")

**URL:** llms-txt#agent.print_response("new-york")

**Contents:**
- Usage

bash  theme={null}
    export FIREWORKS_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/fireworks/structured_output.py
      bash Windows theme={null}
      python cookbook/models/fireworks/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Include and Exclude Files

**URL:** llms-txt#include-and-exclude-files

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/getting-started/usage/include-exclude-files

```python 08_include_exclude_files.py theme={null}
import asyncio
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## Run the Agent, effectively creating and persisting a session

**URL:** llms-txt#run-the-agent,-effectively-creating-and-persisting-a-session

agent.print_response("What is the capital of France?", session_id="123")

---

## Create document package

**URL:** llms-txt#create-document-package

prompt = (
    "Create a sales report package with 2 documents:\n\n"
    "1. EXCEL SPREADSHEET (sales_report.xlsx):\n"
    "   - Q4 sales data: Oct $450K, Nov $520K, Dec $610K\n"
    "   - Include a total formula\n"
    "   - Add a simple bar chart\n\n"
    "2. WORD DOCUMENT (sales_summary.docx):\n"
    "   - Brief Q4 sales summary\n"
    "   - Total sales: $1.58M\n"
    "   - Growth trend: Strong December performance\n"
)

response = multi_skill_agent.run(prompt)
print(response.content)

---

## Rename Session

**URL:** llms-txt#rename-session

Source: https://docs.agno.com/reference-api/schema/sessions/rename-session

post /sessions/{session_id}/rename
Update the name of an existing session. Useful for organizing and categorizing sessions with meaningful names for better identification and management.

---

## Create an agent with PandasTools

**URL:** llms-txt#create-an-agent-with-pandastools

agent = Agent(tools=[PandasTools()])

---

## Using Podman

**URL:** llms-txt#using-podman

**Contents:**
- Basic Example

podman exec db psql -U toolbox_user -d toolbox_db -c "SELECT COUNT(*) FROM hotels;"
python  theme={null}
import asyncio
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp_toolbox import MCPToolbox

async def main():
    # Connect to the running MCP Toolbox server and filter to hotel tools only
    async with MCPToolbox(
        url="http://127.0.0.1:5001",
        toolsets=["hotel-management"]  # Only load hotel search tools
    ) as toolbox:
        agent = Agent(
            model=OpenAIChat(),
            tools=[toolbox],
            instructions="You help users find hotels. Always mention hotel ID, name, location, and price tier."
        )
        
        # Ask the agent to find hotels
        await agent.aprint_response("Find luxury hotels in Zurich")

**Examples:**

Example 1 (unknown):
```unknown
## Basic Example

Here's the simplest way to use MCPToolbox (after running the Quick Start setup):
```

---

## Configuration for the Knowledge page

**URL:** llms-txt#configuration-for-the-knowledge-page

knowledge:
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        display_name: <DISPLAY_NAME>
    ...

---

## Create distributed PgVector RAG team

**URL:** llms-txt#create-distributed-pgvector-rag-team

**Contents:**
- Usage

distributed_pgvector_team = Team(
    name="Distributed PgVector RAG Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[vector_retriever, hybrid_searcher, data_validator, response_composer],
    instructions=[
        "Work together to provide comprehensive RAG responses using PostgreSQL pgvector.",
        "Vector Retriever: First perform vector similarity search.",
        "Hybrid Searcher: Then perform hybrid search for comprehensive coverage.",
        "Data Validator: Validate and filter the retrieved information quality.",
        "Response Composer: Compose the final response with proper attribution.",
        "Leverage PostgreSQL's scalability and pgvector's performance.",
        "Ensure enterprise-grade reliability and accuracy.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_pgvector_rag_demo():
    """Demonstrate async distributed PgVector RAG processing."""
    print("ðŸ˜ Async Distributed PgVector RAG Demo")
    print("=" * 40)

query = "How do I make chicken and galangal in coconut milk soup? What are the key ingredients and techniques?"

try:
        # Add content to knowledge bases
        await vector_knowledge.add_contents_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        await hybrid_knowledge.add_contents_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        # Run async distributed PgVector RAG
        await distributed_pgvector_team.aprint_response(input=query)
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("ðŸ’¡ Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")

def sync_pgvector_rag_demo():
    """Demonstrate sync distributed PgVector RAG processing."""
    print("ðŸ˜ Distributed PgVector RAG Demo")
    print("=" * 35)

query = "How do I make chicken and galangal in coconut milk soup? What are the key ingredients and techniques?"

try:
        # Add content to knowledge bases
        vector_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        hybrid_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        # Run distributed PgVector RAG
        distributed_pgvector_team.print_response(input=query)
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("ðŸ’¡ Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")

def complex_query_demo():
    """Demonstrate distributed RAG for complex culinary queries."""
    print("ðŸ‘¨â€ðŸ³ Complex Culinary Query with Distributed PgVector RAG")
    print("=" * 60)

query = """I'm planning a Thai dinner party for 8 people. Can you help me plan a complete menu?
    I need appetizers, main courses, and desserts. Please include:
    - Preparation timeline
    - Shopping list
    - Cooking techniques for each dish
    - Any dietary considerations or alternatives"""

try:
        # Add content to knowledge bases
        vector_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        hybrid_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )

distributed_pgvector_team.print_response(input=query)
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("ðŸ’¡ Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")

if __name__ == "__main__":
    # Choose which demo to run

# asyncio.run(async_pgvector_rag_demo())

# complex_query_demo()

sync_pgvector_rag_demo()
bash  theme={null}
    ./cookbook/run_pgvector.sh
    bash  theme={null}
    pip install agno openai sqlalchemy 'psycopg[binary]' pgvector
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/distributed_rag/01_distributed_rag_pgvector.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up PostgreSQL with pgvector">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install required libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create a knowledge base with simplified password handling

**URL:** llms-txt#create-a-knowledge-base-with-simplified-password-handling

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="pdf_documents_password",
        db_url=db_url,
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.us-east-1.amazonaws.com/recipes/ThaiRecipes_protected.pdf",
    auth=ContentAuth(password="ThaiRecipes"),
)

---

## ************* Create AgentOS *************

**URL:** llms-txt#*************-create-agentos-*************

agent_os = AgentOS(agents=[agno_agent])
app = agent_os.get_app()

---

## Audio Input (Bytes Content)

**URL:** llms-txt#audio-input-(bytes-content)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/audio-input-bytes-content

```python cookbook/models/google/gemini/audio_input_bytes_content.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"

---

## Basic

**URL:** llms-txt#basic

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/xai/usage/basic

```python cookbook/models/xai/basic.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

---

## Only use agentic memory when you specifically need:

**URL:** llms-txt#only-use-agentic-memory-when-you-specifically-need:

---

## Have a conversation

**URL:** llms-txt#have-a-conversation

expensive_agent.print_response(
    "Explain quantum computing basics", session_id=session_id, user_id=user_id
)
expensive_agent.print_response(
    "What are the main applications?", session_id=session_id, user_id=user_id
)

---

## Structured outputs

**URL:** llms-txt#structured-outputs

**Contents:**
- Structured Outputs vs. JSON Mode
  - Structured Outputs (Default if supported)
- Example
  - JSON Mode
- Example
  - When to use

Source: https://docs.agno.com/faq/structured-outputs

## Structured Outputs vs. JSON Mode

When working with language models, generating responses that match a specific structure is crucial for building reliable applications. Agno Agents support two methods to achieve this: **Structured Outputs** and **JSON mode**.

### Structured Outputs (Default if supported)

"Structured Outputs" is the **preferred** and most **reliable** way to extract well-formed, schema-compliant responses from a Model. If a model class supports it, Agno Agents use Structured Outputs by default.

With structured outputs, we provide a schema to the model (using Pydantic or JSON Schema), and the modelâ€™s response is guaranteed to **strictly follow** that schema. This eliminates many common issues like missing fields, invalid enum values, or inconsistent formatting. Structured Outputs are ideal when you need high-confidence, well-structured responsesâ€”like entity extraction, content generation for UI rendering, and more.

In this case, the response model is passed as a keyword argument to the model.

In the example above, the model will generate a response that matches the `User` schema using structured outputs via OpenAI's `gpt-5-mini` model. The agent will then return the `User` object as-is.

Some model classes **do not support Structured Outputs**, or you may want to fall back to JSON mode even when the model supports both options. In such cases, you can enable **JSON mode** by setting `use_json_mode=True`.

JSON mode works by injecting a detailed description of the expected JSON structure into the system prompt. The model is then instructed to return a valid JSON object that follows this structure. Unlike Structured Outputs, the response is **not automatically validated** against the schema at the API level.

Use **Structured Outputs** if the model supports it â€” itâ€™s reliable, clean, and validated automatically.

* When the model doesn't support structured outputs. Agno agents do this by default on your behalf.
* When you need broader compatibility, but are okay validating manually.
* When the model does not support tools with structured outputs.

**Examples:**

Example 1 (unknown):
```unknown
In the example above, the model will generate a response that matches the `User` schema using structured outputs via OpenAI's `gpt-5-mini` model. The agent will then return the `User` object as-is.

***

### JSON Mode

Some model classes **do not support Structured Outputs**, or you may want to fall back to JSON mode even when the model supports both options. In such cases, you can enable **JSON mode** by setting `use_json_mode=True`.

JSON mode works by injecting a detailed description of the expected JSON structure into the system prompt. The model is then instructed to return a valid JSON object that follows this structure. Unlike Structured Outputs, the response is **not automatically validated** against the schema at the API level.

## Example
```

---

## LM Studio

**URL:** llms-txt#lm-studio

**Contents:**
- Set up a model
- Example
- Params

Source: https://docs.agno.com/integrations/models/local/lmstudio/overview

Learn how to use LM Studio with Agno.

Run Large Language Models locally with LM Studio

[LM Studio](https://lmstudio.ai) is a fantastic tool for running models locally.

LM Studio supports multiple open-source models. See the library [here](https://lmstudio.ai/models).

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `llama3.3` models are good for most basic use-cases.
* `qwen` models perform specifically well with tool use.
* `deepseek-r1` models have strong reasoning capabilities.
* `phi4` models are powerful, while being really small in size.

Install [LM Studio](https://lmstudio.ai), download the model you want to use, and run it.

After you have the model locally, use the `LM Studio` model class to access it

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/local/lmstudio/usage/basic-stream). </Note>

| Parameter  | Type            | Default                                              | Description                                             |
| ---------- | --------------- | ---------------------------------------------------- | ------------------------------------------------------- |
| `id`       | `str`           | `"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"` | The id of the LMStudio model to use                     |
| `name`     | `str`           | `"LMStudio"`                                         | The name of the model                                   |
| `provider` | `str`           | `"LMStudio"`                                         | The provider of the model                               |
| `api_key`  | `Optional[str]` | `None`                                               | The API key for LMStudio (usually not needed for local) |
| `base_url` | `str`           | `"http://localhost:1234/v1"`                         | The base URL for the local LMStudio server              |

`LM Studio` also supports the params of [OpenAI](/reference/models/openai).

---

## Async Events Streaming

**URL:** llms-txt#async-events-streaming

**Contents:**
- Code

Source: https://docs.agno.com/basics/workflows/usage/async-events-streaming

This example demonstrates how to stream events from a workflow.

```python async_events_streaming.py theme={null}
import asyncio
from textwrap import dedent
from typing import AsyncIterator

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowRunOutputEvent, WorkflowRunEvent
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## Create an AI Voice Interaction Agent

**URL:** llms-txt#create-an-ai-voice-interaction-agent

agent = Agent(
    model=OpenAIChat(
        id="gpt-5-mini-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    description=dedent("""\
        You are an expert in audio processing and voice interaction, capable of understanding
        and analyzing spoken content while providing natural, engaging voice responses.
        You excel at comprehending context, emotion, and nuance in speech.\
    """),
    instructions=dedent("""\
        As a voice interaction specialist, follow these guidelines:
        1. Listen carefully to audio input to understand both content and context
        2. Provide clear, concise responses that address the main points
        3. When generating voice responses, maintain a natural, conversational tone
        4. Consider the speaker's tone and emotion in your analysis
        5. If the audio is unclear, ask for clarification

Focus on creating engaging and helpful voice interactions!\
    """),
)

---

## Alternatively, add all routes from AgentOS app to the current app

**URL:** llms-txt#alternatively,-add-all-routes-from-agentos-app-to-the-current-app

---

## SurrealDB for Agent

**URL:** llms-txt#surrealdb-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/surrealdb/usage/surrealdb-for-agent

Agno supports using SurrealDB as a storage backend for Agents using the `SurrealDb` class.

Run SurreabDB locally with the following command:

```python surrealdb_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.surrealdb import SurrealDb
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Team Metrics

**URL:** llms-txt#team-metrics

**Contents:**
- Example Usage

Source: https://docs.agno.com/basics/sessions/metrics/team

Learn about team run and session metrics.

When you run a team in Agno, the response you get (**TeamRunOutput**) includes detailed metrics about the run. These metrics help you understand resource usage (like **token usage** and **time**), performance, and other aspects of the model and tool calls across both the team leader and team members.

Metrics are available at multiple levels:

* **Per-message**: Each message (assistant, tool, etc.) has its own metrics.
* **Per-member run**: Each team member run has its own metrics. You can make member runs available on the `TeamRunOutput` by setting `store_member_responses=True`,
* **Team-level**: The `TeamRunOutput` aggregates metrics across all team leader and team member messages.
* **Session-level**: Aggregated metrics across all runs in the session, for both the team leader and all team members.

Suppose you have a team that performs some tasks and you want to analyze the metrics after running it. Here's how you can access and print the metrics:

```python  theme={null}
from typing import Iterator

from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

---

## Background Hooks (Global)

**URL:** llms-txt#background-hooks-(global)

**Contents:**
- What Happens

Source: https://docs.agno.com/agent-os/usage/background-hooks-global

Run all agent hooks as background tasks using AgentOS

This example demonstrates how to run **all** hooks as FastAPI background tasks by enabling `run_hooks_in_background` at the AgentOS level.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run the server">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Test the endpoint">

The response will be returned immediately. Check the server logs to see the background hooks executing after the response is sent.
  </Step>
</Steps>

1. The agent processes the request
2. The response is sent immediately to the user
3. All pre-hooks and post-hooks run in the background
4. The user doesn't have to wait for these tasks to complete

<Note>
  With `run_hooks_in_background=True` on AgentOS, **all** hooks for all agents run in the background. Use the `@hook` decorator for more granular control over which hooks run in the background.
</Note>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Team Details

**URL:** llms-txt#get-team-details

Source: https://docs.agno.com/reference-api/schema/teams/get-team-details

get /teams/{team_id}
Retrieve detailed configuration and member information for a specific team.

---

## Delete Evaluation Runs

**URL:** llms-txt#delete-evaluation-runs

Source: https://docs.agno.com/reference-api/schema/evals/delete-evaluation-runs

delete /eval-runs
Delete multiple evaluation runs by their IDs. This action cannot be undone.

---

## Install & Setup

**URL:** llms-txt#install-&-setup

**Contents:**
- Install Agno
- Upgrade Agno

Source: https://docs.agno.com/templates/infra-management/install

* Installing `agno` using `pip` in a python virtual environment.
* Creating an `ai` directory for your ai infra

<Steps>
  <Step title="Create a virtual environment">
    Open the `Terminal` and create an `ai` directory with a python virtual environment.

</CodeGroup>
  </Step>

<Step title="Install Agno">
    Install `agno` using pip

</CodeGroup>
  </Step>

<Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) to run apps locally
  </Step>
</Steps>

<Note>
  If you encounter errors, try updating pip using `python -m pip install --upgrade pip`
</Note>

To upgrade `agno`, run this in your virtual environment

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install Agno">
    Install `agno` using pip

    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) to run apps locally
  </Step>
</Steps>

<br />

<Note>
  If you encounter errors, try updating pip using `python -m pip install --upgrade pip`
</Note>

***

## Upgrade Agno

To upgrade `agno`, run this in your virtual environment
```

---

## Setup basic research agent

**URL:** llms-txt#setup-basic-research-agent

web_research_agent = Agent(
    id="web-research-agent",
    name="Web Research Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    enable_session_summaries=True,
    markdown=True,
)

---

## Download sample documents

**URL:** llms-txt#download-sample-documents

downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

---

## Generate Music using Models Lab

**URL:** llms-txt#generate-music-using-models-lab

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/audio/usage/generate-music-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## - User-directed memory commands ("forget my address")

**URL:** llms-txt#--user-directed-memory-commands-("forget-my-address")

---

## Agent with Vertex AI

**URL:** llms-txt#agent-with-vertex-ai

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/vertexai

```python cookbook/models/google/gemini/vertexai.py theme={null}
"""
To use Vertex AI, with the Gemini Model class, you need to set the following environment variables:

export GOOGLE_GENAI_USE_VERTEXAI="true"
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_CLOUD_LOCATION="your-location"

Or you can set the following parameters in the `Gemini` class:

gemini = Gemini(
    vertexai=True,
    project_id="your-google-cloud-project-id",
    location="your-google-cloud-location",
)
"""

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini

agent = Agent(model=Gemini(id="gemini-2.0-flash-001"), markdown=True)

---

## Seed documentation standards

**URL:** llms-txt#seed-documentation-standards

**Contents:**
  - Customer Communication Tone

doc_standard = CulturalKnowledge(
    name="Documentation Standard",
    summary="All docs follow structure: Example â†’ Explanation â†’ Validation",
    categories=["documentation", "engineering"],
    content=(
        "1. Start with a minimal working example\n"
        "2. Explain key concepts and decisions\n"
        "3. Provide validation steps\n"
        "4. Link to related resources"
    ),
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Customer Communication Tone
```

---

## Configure handlers and formatters for each

**URL:** llms-txt#configure-handlers-and-formatters-for-each

for logger in [custom_agent_logger, custom_team_logger, custom_workflow_logger]:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("[%(name)s] %(levelname)s: %(message)s"))
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    logger.propagate = False

---

## Image to Structured Output

**URL:** llms-txt#image-to-structured-output

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/image-to-structured-output

This example demonstrates how to analyze images and generate structured output using Pydantic models, creating movie scripts based on image content.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Filter to specific candidates

**URL:** llms-txt#filter-to-specific-candidates

**Contents:**
- Advanced Filtering Patterns
  - User-Specific Content

hiring_team.print_response(
    "Compare the experience of our top candidates",
    knowledge_filters=[  # â† List wrapper required
        AND(
            EQ("document_type", "cv"),
            IN("user_id", ["jordan_mitchell", "taylor_brooks"]),
            NOT(EQ("status", "rejected"))
        )
    ]
)
python  theme={null}
from agno.filters import OR, EQ

def get_user_filter(user_id: str, user_department: str):
    """Create filters based on user context."""
    return OR(
        EQ("visibility", "public"),
        EQ("owner", user_id),
        EQ("department", user_department)
    )

**Examples:**

Example 1 (unknown):
```unknown
## Advanced Filtering Patterns

### User-Specific Content

Filter content based on user access or preferences:
```

---

## Returns: [Document(chunk1), Document(chunk2), Document(chunk3), ...]

**URL:** llms-txt#returns:-[document(chunk1),-document(chunk2),-document(chunk3),-...]

**Contents:**
  - Chunking Strategy Support
- Reader Factory and Auto-Selection

python  theme={null}
@classmethod
def get_supported_chunking_strategies(cls) -> List[ChunkingStrategyType]:
    return [
        ChunkingStrategyType.DOCUMENT_CHUNKING,  # Respect document structure
        ChunkingStrategyType.FIXED_SIZE_CHUNKING, # Fixed character/token limits
        ChunkingStrategyType.SEMANTIC_CHUNKING,   # Semantic boundaries
        ChunkingStrategyType.AGENTIC_CHUNKING,    # AI-powered chunking
    ]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Chunking Strategy Support

Different readers support different chunking strategies based on their content type:
```

Example 2 (unknown):
```unknown
## Reader Factory and Auto-Selection

Agno provides intelligent reader selection through the `ReaderFactory`:
```

---

## Human-in-the-Loop Example

**URL:** llms-txt#human-in-the-loop-example

**Contents:**
- Prerequisites
- Code

Source: https://docs.agno.com/agent-os/usage/hitl

AgentOS with tools requiring user confirmation

This example shows how to implement Human-in-the-Loop (HITL) flows in AgentOS. When an agent needs to execute a tool that requires confirmation, the run pauses and waits for user approval before proceeding.

* Python 3.10 or higher
* PostgreSQL with pgvector (setup instructions below)
* OpenAI API key

```python hitl_confirmation.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools import tool

---

## Custom function step that has access to ALL previous step outputs

**URL:** llms-txt#custom-function-step-that-has-access-to-all-previous-step-outputs

def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

# Access original workflow input
    original_topic = step_input.input or ""

# Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

# Or access ALL previous content
    _ = step_input.get_all_previous_content()

# Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

## HackerNews Insights
        {hackernews_data[:500]}...

## Web Research Findings
        {web_data[:500]}...
    """

return StepOutput(
        step_name="comprehensive_report", content=report.strip(), success=True
    )

comprehensive_report_step = Step(
    name="comprehensive_report",
    executor=create_comprehensive_report,
    description="Create comprehensive report from all research sources",
)

---

## X (Twitter)

**URL:** llms-txt#x-(twitter)

**Contents:**
- Prerequisites
- Setup
- Example

Source: https://docs.agno.com/integrations/toolkits/social/x

**XTools** allows an Agent to interact with X, providing functionality for posting, messaging, and searching tweets.

Install the required library:

<Info>Tweepy is a Python library for interacting with the X API.</Info>

1. **Create X Developer Account**

* Visit [developer.x.com](https://developer.x.com) and apply for developer access
   * Create a new project and app in your developer portal

2. **Generate API Credentials**

* Navigate to your app's "Keys and tokens" section
   * Generate and copy these credentials:
     * API Key & Secret
     * Bearer Token
     * Access Token & Secret

3. **Configure Environment**

```python cookbook/tools/x_tools.py theme={null}
from agno.agent import Agent
from agno.tools.x import XTools

**Examples:**

Example 1 (unknown):
```unknown
<Info>Tweepy is a Python library for interacting with the X API.</Info>

## Setup

1. **Create X Developer Account**

   * Visit [developer.x.com](https://developer.x.com) and apply for developer access
   * Create a new project and app in your developer portal

2. **Generate API Credentials**

   * Navigate to your app's "Keys and tokens" section
   * Generate and copy these credentials:
     * API Key & Secret
     * Bearer Token
     * Access Token & Secret

3. **Configure Environment**
```

Example 2 (unknown):
```unknown
## Example
```

---

## Google Sheets

**URL:** llms-txt#google-sheets

**Contents:**
- Prerequisites
- How to Get Credentials
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/google-sheets

**GoogleSheetsTools** enable an Agent to interact with Google Sheets API for reading, creating, updating, and duplicating spreadsheets.

You need to install the required Google API client libraries:

Set up the following environment variables:

## How to Get Credentials

1. Go to Google Cloud Console ([https://console.cloud.google.com](https://console.cloud.google.com))

2. Create a new project or select an existing one

3. Enable the Google Sheets API:
   * Go to "APIs & Services" > "Enable APIs and Services"
   * Search for "Google Sheets API"
   * Click "Enable"

4. Create OAuth 2.0 credentials:
   * Go to "APIs & Services" > "Credentials"
   * Click "Create Credentials" > "OAuth client ID"
   * Go through the OAuth consent screen setup
   * Give it a name and click "Create"
   * You'll receive:
     * Client ID (GOOGLE\_CLIENT\_ID)
     * Client Secret (GOOGLE\_CLIENT\_SECRET)
   * The Project ID (GOOGLE\_PROJECT\_ID) is visible in the project dropdown at the top of the page

The following agent will use Google Sheets to read and update spreadsheet data.

| Parameter                       | Type                    | Default | Description                                                |
| ------------------------------- | ----------------------- | ------- | ---------------------------------------------------------- |
| `scopes`                        | `Optional[List[str]]`   | `None`  | Custom OAuth scopes. If None, uses write scope by default. |
| `spreadsheet_id`                | `Optional[str]`         | `None`  | ID of the target spreadsheet.                              |
| `spreadsheet_range`             | `Optional[str]`         | `None`  | Range within the spreadsheet.                              |
| `creds`                         | `Optional[Credentials]` | `None`  | Pre-existing credentials.                                  |
| `creds_path`                    | `Optional[str]`         | `None`  | Path to credentials file.                                  |
| `token_path`                    | `Optional[str]`         | `None`  | Path to token file.                                        |
| `oauth_port`                    | `int`                   | `0`     | Port to use for OAuth authentication.                      |
| `enable_read_sheet`             | `bool`                  | `True`  | Enable reading from a sheet.                               |
| `enable_create_sheet`           | `bool`                  | `False` | Enable creating a sheet.                                   |
| `enable_update_sheet`           | `bool`                  | `False` | Enable updating a sheet.                                   |
| `enable_create_duplicate_sheet` | `bool`                  | `False` | Enable creating a duplicate sheet.                         |
| `all`                           | `bool`                  | `False` | Enable all tools.                                          |

| Function                 | Description                                                                                                                                                                                                                                                                                 |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `read_sheet`             | Read values from a Google Sheet. Parameters include `spreadsheet_id` (Optional\[str]) for fallback spreadsheet ID and `spreadsheet_range` (Optional\[str]) for fallback range. Returns JSON of list of rows.                                                                                |
| `create_sheet`           | Create a new Google Sheet. Parameters include `title` (str) for the title of the Google Sheet. Returns the ID of the created Google Sheet.                                                                                                                                                  |
| `update_sheet`           | Update data in a Google Sheet. Parameters include `data` (List\[List\[Any]]) for the data to update, `spreadsheet_id` (Optional\[str]) for the ID of the Google Sheet, and `range_name` (Optional\[str]) for the range to update. Returns success or failure message.                       |
| `create_duplicate_sheet` | Create a duplicate of an existing Google Sheet. Parameters include `source_id` (str) for the ID of the source spreadsheet, `new_title` (Optional\[str]) for new title, and `copy_permissions` (bool, default=True) for whether to copy permissions. Returns link to duplicated spreadsheet. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/googlesheets.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/googlesheets_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
Set up the following environment variables:
```

Example 2 (unknown):
```unknown
## How to Get Credentials

1. Go to Google Cloud Console ([https://console.cloud.google.com](https://console.cloud.google.com))

2. Create a new project or select an existing one

3. Enable the Google Sheets API:
   * Go to "APIs & Services" > "Enable APIs and Services"
   * Search for "Google Sheets API"
   * Click "Enable"

4. Create OAuth 2.0 credentials:
   * Go to "APIs & Services" > "Credentials"
   * Click "Create Credentials" > "OAuth client ID"
   * Go through the OAuth consent screen setup
   * Give it a name and click "Create"
   * You'll receive:
     * Client ID (GOOGLE\_CLIENT\_ID)
     * Client Secret (GOOGLE\_CLIENT\_SECRET)
   * The Project ID (GOOGLE\_PROJECT\_ID) is visible in the project dropdown at the top of the page

## Example

The following agent will use Google Sheets to read and update spreadsheet data.
```

---

## Agentic RAG with Reranking

**URL:** llms-txt#agentic-rag-with-reranking

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag-with-reranking

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## InternLM

**URL:** llms-txt#internlm

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/internlm

The InternLM model provides access to the InternLM model.

| Parameter  | Type            | Default                                                | Description                                                       |
| ---------- | --------------- | ------------------------------------------------------ | ----------------------------------------------------------------- |
| `id`       | `str`           | `"internlm/internlm2_5-7b-chat"`                       | The id of the InternLM model to use                               |
| `name`     | `str`           | `"InternLM"`                                           | The name of the model                                             |
| `provider` | `str`           | `"InternLM"`                                           | The provider of the model                                         |
| `api_key`  | `Optional[str]` | `None`                                                 | The API key for InternLM (defaults to INTERNLM\_API\_KEY env var) |
| `base_url` | `str`           | `"https://internlm-chat.intern-ai.org.cn/puyu/api/v1"` | The base URL for the InternLM API                                 |

\| `retries`    | `int`              | `0`                            | Number of retries to attempt before raising a ModelProviderError      |
\| `delay_between_retries` | `int`    | `1`                            | Delay between retries, in seconds                                     |
\| `exponential_backoff` | `bool`     | `False`                        | If True, the delay between retries is doubled each time               |

InternLM extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## More examples:

**URL:** llms-txt#more-examples:

---

## Retrieve and display generated images

**URL:** llms-txt#retrieve-and-display-generated-images

run_response = image_agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput):
    for image_response in run_response.images:
        image_url = image_response.url
        print("image_url: ", image_url)
else:
    print("No images found or images is not a list")

---

## Accuracy Evals

**URL:** llms-txt#accuracy-evals

**Contents:**
- Basic Example
  - Evaluator Agent

Source: https://docs.agno.com/basics/evals/accuracy/overview

Accuracy evals measure how well your Agents and Teams perform against a gold-standard answer using LLM-as-a-judge methodology.

Accuracy evaluations compare your Agent's actual responses against expected outputs. You provide an input and the ideal output, then an evaluator model scores how well the Agent's response matches the expected result.

In this example, the `AccuracyEval` will run the Agent with the input, then use a different model (`o4-mini`) to score the Agent's response according to the guidelines provided.

You can use another agent to evaluate the accuracy of the Agent's response. This strategy is usually referred to as "LLM-as-a-judge".

You can adjust the evaluator Agent to make it fit the criteria you want to evaluate:

```python accuracy_with_evaluator_agent.py theme={null}
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyAgentResponse, AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

**Examples:**

Example 1 (unknown):
```unknown
### Evaluator Agent

You can use another agent to evaluate the accuracy of the Agent's response. This strategy is usually referred to as "LLM-as-a-judge".

You can adjust the evaluator Agent to make it fit the criteria you want to evaluate:
```

---

## Atla

**URL:** llms-txt#atla

**Contents:**
- Prerequisites
- Configuration
- Example

Source: https://docs.agno.com/integrations/observability/atla

Integrate `Atla` with Agno for real-time monitoring, automated evaluation, and performance analytics of your AI agents.

[Atla](https://www.atla-ai.com/) is an advanced observability platform designed specifically for AI agent monitoring and evaluation.
This integration provides comprehensive insights into agent performance, automated quality assessment, and detailed analytics for production AI systems.

* **API Key**: Obtain your API key from the [Atla dashboard](https://app.atla-ai.com)

Install the Atla Insights SDK with Agno support:

Configure your API key as an environment variable:

```python  theme={null}
from os import getenv
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from atla_insights import configure, instrument_agno

**Examples:**

Example 1 (unknown):
```unknown
## Configuration

Configure your API key as an environment variable:
```

Example 2 (unknown):
```unknown
## Example
```

---

## SqliteDb

**URL:** llms-txt#sqlitedb

Source: https://docs.agno.com/reference/storage/sqlite

`SqliteDb` is a class that implements the Db interface using SQLite as the backend storage system. It provides lightweight, file-based storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-sqlite-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## What is AgentOS?

**URL:** llms-txt#what-is-agentos?

**Contents:**
- Getting Started
- Security & Privacy First
  - Complete Data Ownership
- Learn more

Source: https://docs.agno.com/agent-os/introduction

The production runtime and control plane for your agentic systems

AgentOS is Agno's production-ready runtime that runs entirely within your own infrastructure, ensuring complete data privacy and control of your agentic system.
Agno also provides a beautiful web interface for managing, monitoring, and interacting with your AgentOS, with no data ever being persisted outside of your environment.

<Check>
  Behind the scenes, AgentOS is a FastAPI app that you can run locally or in your cloud. It is designed to be easy to deploy and scale.
</Check>

Ready to get started with AgentOS? Here's what you need to do:

<CardGroup cols={2}>
  <Card title="Create Your First OS" icon="plus" href="/agent-os/creating-your-first-os">
    Set up a new AgentOS instance from scratch using our templates
  </Card>

<Card title="Connect Your AgentOS" icon="link" href="/agent-os/connecting-your-os">
    Learn how to connect your local development environment to the platform
  </Card>
</CardGroup>

## Security & Privacy First

AgentOS is designed with enterprise security and data privacy as foundational principles, not afterthoughts.

<Frame>
  <img src="https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=b13db5d4b3c25eb5508752f7d3474b51" alt="AgentOS Security and Privacy Architecture" style={{ borderRadius: "0.5rem" }} data-og-width="3258" width="3258" data-og-height="1938" height="1938" data-path="images/agentos-secure-infra-illustration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?w=280&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=c548641b352030a8fee914cd49919417 280w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?w=560&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=9640bb14a9d22619973e7efb20ab1be5 560w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?w=840&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=82645dfaae8f0155bc3912cdfaf656cc 840w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?w=1100&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=ba5cf9921c1b389d58216ba71ef38515 1100w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?w=1650&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=d7ca28c6e75259c18b08783224c1a2e4 1650w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-secure-infra-illustration.png?w=2500&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=122528a3dc3ecf7789fb1b076be48f08 2500w" />
</Frame>

### Complete Data Ownership

* **Your Infrastructure, Your Data**: AgentOS runs entirely within your cloud environment
* **Zero Data Transmission**: No conversations, logs, or metrics are sent to external services
* **Private by Default**: All processing, storage, and analytics happen locally

To learn more about AgentOS Security, check out the [AgentOS Security](/agent-os/security) page.

<CardGroup cols={2}>
  <Card title="Control Plane" icon="desktop" href="/agent-os/control-plane">
    Learn how to use the AgentOS control plane to manage and monitor your OSs
  </Card>

<Card title="Create Your First OS" icon="rocket" href="/agent-os/creating-your-first-os">
    Get started by creating your first AgentOS instance
  </Card>
</CardGroup>

---

## Resend

**URL:** llms-txt#resend

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/resend

**ResendTools** enable an Agent to send emails using Resend

The following example requires the `resend` library and an API key from [Resend](https://resend.com/).

The following agent will send an email using Resend

| Parameter           | Type   | Default | Description                                                   |
| ------------------- | ------ | ------- | ------------------------------------------------------------- |
| `api_key`           | `str`  | -       | API key for authentication purposes.                          |
| `from_email`        | `str`  | -       | The email address used as the sender in email communications. |
| `enable_send_email` | `bool` | `True`  | Enable the send\_email functionality.                         |
| `all`               | `bool` | `False` | Enable all functionality.                                     |

| Function     | Description                         |
| ------------ | ----------------------------------- |
| `send_email` | Send an email using the Resend API. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/resend.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/resend_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will send an email using Resend
```

---

## SSH Access

**URL:** llms-txt#ssh-access

**Contents:**
- Dev SSH Access
- Production SSH Access

Source: https://docs.agno.com/templates/infra-management/ssh-access

SSH Access is an important part of the developer workflow.

SSH into the dev containers using the `docker exec` command

## Production SSH Access

Your ECS tasks are already enabled with SSH access. SSH into the production containers using:

**Examples:**

Example 1 (unknown):
```unknown
## Production SSH Access

Your ECS tasks are already enabled with SSH access. SSH into the production containers using:
```

---

## Step

**URL:** llms-txt#step

Source: https://docs.agno.com/reference/workflows/step

| Parameter              | Type                     | Default | Description                                                                                       |
| ---------------------- | ------------------------ | ------- | ------------------------------------------------------------------------------------------------- |
| `name`                 | `Optional[str]`          | `None`  | Name of the step for identification                                                               |
| `agent`                | `Optional[Agent]`        | `None`  | Agent to execute for this step                                                                    |
| `team`                 | `Optional[Team]`         | `None`  | Team to execute for this step                                                                     |
| `executor`             | `Optional[StepExecutor]` | `None`  | Custom function to execute for this step                                                          |
| `step_id`              | `Optional[str]`          | `None`  | Unique identifier for the step (auto-generated if not provided)                                   |
| `description`          | `Optional[str]`          | `None`  | Description of the step's purpose                                                                 |
| `max_retries`          | `int`                    | `3`     | Maximum number of retry attempts on failure                                                       |
| `timeout_seconds`      | `Optional[int]`          | `None`  | Timeout for step execution in seconds                                                             |
| `skip_on_failure`      | `bool`                   | `False` | Whether to skip this step if it fails after all retries                                           |
| `add_workflow_history` | `bool`                   | `False` | If True, add the workflow history to the step                                                     |
| `num_history_runs`     | `int`                    | `None`  | Number of runs to include in the workflow history, if not provided, all history runs are included |

---

## Memory Creation

**URL:** llms-txt#memory-creation

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/memory-creation

Create user memories with an Agent by providing a either text or a list of messages.

```python memory-creation.py theme={null}
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager, UserMemory
from agno.models.message import Message
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresDb(db_url=db_url)

memory = MemoryManager(model=OpenAIChat(id="gpt-5-mini"), db=memory_db)

john_doe_id = "john_doe@example.com"
memory.add_user_memory(
    memory=UserMemory(
        memory="""
I enjoy hiking in the mountains on weekends,
reading science fiction novels before bed,
cooking new recipes from different cultures,
playing chess with friends,
and attending live music concerts whenever possible.
Photography has become a recent passion of mine, especially capturing landscapes and street scenes.
I also like to meditate in the mornings and practice yoga to stay centered.
"""
    ),
    user_id=john_doe_id,
)

memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)

jane_doe_id = "jane_doe@example.com"

---

## Reranking Specialist Agent - Specialized in result optimization

**URL:** llms-txt#reranking-specialist-agent---specialized-in-result-optimization

reranking_specialist = Agent(
    name="Reranking Specialist",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Apply advanced reranking to optimize retrieval results",
    knowledge=reranked_knowledge,
    search_knowledge=True,
    instructions=[
        "Apply advanced reranking techniques to optimize result relevance.",
        "Focus on precision and ranking quality over quantity.",
        "Use the Cohere reranker to identify the most relevant content.",
        "Prioritize results that best match the user's specific needs.",
    ],
    markdown=True,
)

---

## ------------------------------------------------------------------------------

**URL:** llms-txt#------------------------------------------------------------------------------

**Contents:**
- Usage

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)

bash  theme={null}
    pip install -U agno weaviate-client openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Weaviate Cloud theme={null}
      # 1. Create account at https://console.weaviate.cloud/
      # 2. Create a cluster and copy the "REST endpoint" and "Admin" API Key
      # 3. Set environment variables:
      export WCD_URL="your-cluster-url" 
      export WCD_API_KEY="your-api-key"
      # 4. Set local=False in the code
      bash Local Development theme={null}
      # 1. Install Docker from https://docs.docker.com/get-docker/
      # 2. Run Weaviate locally:
      docker run -d \
          -p 8080:8080 \
          -p 50051:50051 \
          --name weaviate \
          cr.weaviate.io/semitechnologies/weaviate:1.28.4
      # 3. Set local=True in the code
      bash Mac theme={null}
      python cookbook/knowledge/filters/vector_dbs/filtering_weaviate.py
      bash Windows theme={null}
      python cookbook/knowledge/filters/vector_dbs/filtering_weaviate.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup Weaviate">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Weaviate

**URL:** llms-txt#weaviate

Source: https://docs.agno.com/reference/vector-db/weaviate

<Snippet file="vector-db-weaviate-reference.mdx" />

---

## Selecting Custom Table Names

**URL:** llms-txt#selecting-custom-table-names

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/selecting-tables

Agno allows you to customize table names when using databases, providing flexibility in organizing your data storage.

Specify custom table names when initializing your database connection.

```python selecting_tables.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## stream=True,

**URL:** llms-txt#stream=true,

---

## Initialize Traceloop - must be called before creating agents

**URL:** llms-txt#initialize-traceloop---must-be-called-before-creating-agents

Traceloop.init(app_name="agno_agent")

---

## S3 Content

**URL:** llms-txt#s3-content

Source: https://docs.agno.com/reference/knowledge/remote-content/s3-content

S3Content is a class that allows you to add content from a S3 bucket to the knowledge base.

<Snippet file="s3-remote-content-params.mdx" />

---

## JSON Files as Database

**URL:** llms-txt#json-files-as-database

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/json/overview

Learn to use JSON files as a database for your Agents

Agno supports using local JSON files as a "database" with the `JsonDb` class.
This is a simple way to store your Agent's session data without having to setup a database.

<Warning>
  Using JSON files as a database is not recommended for production applications.
  Use it for demos, testing and any other use case where you don't want to setup a database.
</Warning>

```python json_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.json import JsonDb

---

## Send a history of messages and add memories

**URL:** llms-txt#send-a-history-of-messages-and-add-memories

**Contents:**
- Usage

memory.create_user_memories(
    messages=[
        Message(role="user", content="My name is Jane Doe"),
        Message(role="assistant", content="That is great!"),
        Message(role="user", content="I like to play chess"),
        Message(role="assistant", content="That is great!"),
    ],
    user_id=jane_doe_id,
)

memories = memory.get_user_memories(user_id=jane_doe_id)
print("Jane Doe's memories:")
pprint(memories)

bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python memory-creation.py
      bash Windows theme={null}
      python memory-creation.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Agent databases

**URL:** llms-txt#agent-databases

agent1_db = SqliteDb(db_file="tmp/agent1.db", id="agent1_db")
agent2_db = SqliteDb(db_file="tmp/agent2.db", id="agent2_db")

---

## Sequence of steps

**URL:** llms-txt#sequence-of-steps

**Contents:**
- Pattern: Sequential Named Steps

Source: https://docs.agno.com/basics/workflows/usage/sequence-of-steps

This example demonstrates how to use named steps in a workflow.

This example demonstrates **Workflows** using named Step objects for better tracking
and organization. This pattern provides clear step identification and enhanced logging
while maintaining simple sequential execution.

## Pattern: Sequential Named Steps

**When to use**: Linear processes where you want clear step identification, better logging,
and future platform support. Ideal when you have distinct phases that benefit from naming.

```python sequence_of_steps.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Setup the Firestore database

**URL:** llms-txt#setup-the-firestore-database

db = FirestoreDb(project_id=PROJECT_ID)

---

## E2B

**URL:** llms-txt#e2b

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/e2b

Enable your Agents to run code in a remote, secure sandbox.

**E2BTools** enable an Agent to execute code in a secure sandboxed environment with support for Python, file operations, and web server capabilities.

The E2B tools require the `e2b_code_interpreter` Python package and an E2B API key.

The following example demonstrates how to create an agent that can run Python code in a secure sandbox:

```python cookbook/tools/e2b_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.e2b import E2BTools

e2b_tools = E2BTools(
    timeout=600,  # 10 minutes timeout (in seconds)
)

agent = Agent(
    name="Code Execution Sandbox",
    id="e2b-sandbox",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[e2b_tools],
    markdown=True,
        instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "",
        "You can use these tools:",
        "1. Run Python code (run_python_code)",
        "2. Upload files to the sandbox (upload_file)",
        "3. Download files from the sandbox (download_file_from_sandbox)",
        "4. Generate and add visualizations as image artifacts (download_png_result)",
        "5. List files in the sandbox (list_files)",
        "6. Read and write file content (read_file_content, write_file_content)",
        "7. Start web servers and get public URLs (run_server, get_public_url)",
        "8. Manage the sandbox lifecycle (set_sandbox_timeout, get_sandbox_status, shutdown_sandbox)",
    ],
)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following example demonstrates how to create an agent that can run Python code in a secure sandbox:
```

---

## Create initial agent with expensive model

**URL:** llms-txt#create-initial-agent-with-expensive-model

expensive_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful assistant for technical discussions.",
    db=db,
    add_history_to_context=True,
)

---

## Post-hooks

**URL:** llms-txt#post-hooks

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/hooks/post-hooks

Running a post-hook is handled automatically during the Agent or Team run. These are the parameters that will be injected:

| Parameter       | Type                           | Default  | Description                                                                  |
| --------------- | ------------------------------ | -------- | ---------------------------------------------------------------------------- |
| `agent`         | `Agent`                        | Required | The Agent that is running the post-hook. Only present in Agent runs.         |
| `team`          | `Team`                         | Required | The Team that is running the post-hook. Only present in Team runs.           |
| `run_output`    | `RunOutput` or `TeamRunOutput` | Required | The output of the current Agent or Team run.                                 |
| `session`       | `AgentSession`                 | Required | The `AgentSession` or `TeamSession` object representing the current session. |
| `session_state` | `Optional[Dict[str, Any]]`     | `None`   | The session state of the current session.                                    |
| `dependencies`  | `Optional[Dict[str, Any]]`     | `None`   | The dependencies of the current run.                                         |
| `metadata`      | `Optional[Dict[str, Any]]`     | `None`   | The metadata of the current run.                                             |
| `user_id`       | `Optional[str]`                | `None`   | The contextual user ID, if any.                                              |
| `debug_mode`    | `Optional[bool]`               | `None`   | Whether the debug mode is enabled.                                           |

---

## Create agent with multiple skills

**URL:** llms-txt#create-agent-with-multiple-skills

multi_skill_agent = Agent(
    name="Multi-Skill Document Creator",
    model=Claude(
        id="claude-sonnet-4-5-20250929",
        skills=[
            {"type": "anthropic", "skill_id": "pptx", "version": "latest"},
            {"type": "anthropic", "skill_id": "xlsx", "version": "latest"},
            {"type": "anthropic", "skill_id": "docx", "version": "latest"},
        ],
    ),
    instructions=[
        "You are a comprehensive business document creator.",
        "You have access to PowerPoint, Excel, and Word document skills.",
        "Create professional document packages with consistent information across all files.",
    ],
    markdown=True,
)

---

## Markdown Reader

**URL:** llms-txt#markdown-reader

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/markdown-reader

The **Markdown Reader** processes Markdown files synchronously and converts them into documents that can be used with Agno's knowledge system.

```python examples/basics/knowledge/readers/markdown_reader_sync.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.markdown_reader import MarkdownReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="markdown_documents",
        db_url=db_url,
    ),
)

---

## Define custom AgentQL query for specific data extraction (see https://docs.agentql.com/basics/query-language)

**URL:** llms-txt#define-custom-agentql-query-for-specific-data-extraction-(see-https://docs.agentql.com/basics/query-language)

custom_query = """
{
    title
    text_content[]
}
"""

---

## LangDB Embedder

**URL:** llms-txt#langdb-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/langdb/usage/langdb-embedder

```python  theme={null}

from agno.knowledge.embedder.langdb import LangDBEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = LangDBEmbedder().get_embedding("Embed me")

---

## Create OpenAI agent

**URL:** llms-txt#create-openai-agent

openai_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful assistant.",
    db=db,
    add_history_to_context=True,
)

---

## Audio Generation Tools

**URL:** llms-txt#audio-generation-tools

**Contents:**
- Prerequisites
- Developer Resources

Source: https://docs.agno.com/basics/multimodal/audio/audio_generation

Learn how to use audio generation tools with Agno agents.

The following example demonstrates how to generate an audio using the ElevenLabs tool with an agent. See [Eleven Labs](https://elevenlabs.io/) for more details.

You need to install the `elevenlabs` library and an API key which can be obtained from [Eleven Labs](https://elevenlabs.io/)

Set the `ELEVEN_LABS_API_KEY` environment variable.

## Developer Resources

* See the [Music Generation](/basics/multimodal/audio/usage/generate-music-agent) example.

**Examples:**

Example 1 (unknown):
```unknown
Set the `ELEVEN_LABS_API_KEY` environment variable.
```

Example 2 (unknown):
```unknown

```

---

## Example usage with different types of movie queries

**URL:** llms-txt#example-usage-with-different-types-of-movie-queries

**Contents:**
- More example prompts to explore:
- Usage

movie_recommendation_agent.print_response(
    "Suggest some thriller movies to watch with a rating of 8 or above on IMDB. "
    "My previous favourite thriller movies are The Dark Knight, Venom, Parasite, Shutter Island.",
    stream=True,
)
bash  theme={null}
    pip install openai exa_py agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python movie_recommender.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## More example prompts to explore:

**Genre-specific queries:**

1. "Find me psychological thrillers similar to Black Swan and Gone Girl"
2. "What are the best animated movies from Studio Ghibli?"
3. "Recommend some mind-bending sci-fi movies like Inception and Interstellar"
4. "What are the highest-rated crime documentaries from the last 5 years?"

**International Cinema:**

1. "Suggest Korean movies similar to Parasite and Train to Busan"
2. "What are the must-watch French films from the last decade?"
3. "Recommend Japanese animated movies for adults"
4. "Find me award-winning European drama films"

**Family & Group Watching:**

1. "What are good family movies for kids aged 8-12?"
2. "Suggest comedy movies perfect for a group movie night"
3. "Find educational documentaries suitable for teenagers"
4. "Recommend adventure movies that both adults and children would enjoy"

**Upcoming Releases:**

1. "What are the most anticipated movies coming out next month?"
2. "Show me upcoming superhero movie releases"
3. "What horror movies are releasing this Halloween season?"
4. "List upcoming book-to-movie adaptations"

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Advanced research topics to explore:

**URL:** llms-txt#advanced-research-topics-to-explore:

**Contents:**
- What to Expect
- Usage
- Next Steps

"""
Technology & Innovation:
1. "Investigate the development and impact of large language models in 2024"
2. "Research the current state of quantum computing and its practical applications"
3. "Analyze the evolution and future of edge computing technologies"
4. "Explore the latest advances in brain-computer interface technology"

Environmental & Sustainability:
1. "Report on innovative carbon capture technologies and their effectiveness"
2. "Investigate the global progress in renewable energy adoption"
3. "Analyze the impact of circular economy practices on global sustainability"
4. "Research the development of sustainable aviation technologies"

Healthcare & Biotechnology:
1. "Explore the latest developments in CRISPR gene editing technology"
2. "Analyze the impact of AI on drug discovery and development"
3. "Investigate the evolution of personalized medicine approaches"
4. "Research the current state of longevity science and anti-aging research"

Societal Impact:
1. "Examine the effects of social media on democratic processes"
2. "Analyze the impact of remote work on urban development"
3. "Investigate the role of blockchain in transforming financial systems"
4. "Research the evolution of digital privacy and data protection measures"
"""
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai ddgs newspaper4k lxml_html_clean
    bash Mac theme={null}
      python research_agent.py
      bash Windows theme={null}
      python research_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Modify the research topic to explore different subjects
* Adjust the number of sources searched for more comprehensive coverage
* Customize the `expected_output` template to match your preferred article style
* Explore [DuckDuckGo Tools](/integrations/toolkits/search/duckduckgo) for advanced search options

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The agent will conduct comprehensive research on your query using DuckDuckGo to find multiple authoritative sources. It extracts content from relevant articles using Newspaper4k, cross-references information, and synthesizes findings into a professional NYT-style article.

The output includes an executive summary, background context, key findings with expert quotes and statistics, impact analysis, future outlook, and a complete list of sources. All content follows journalistic standards with balanced perspectives and verified facts.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Groq Claude + DeepSeek R1

**URL:** llms-txt#groq-claude-+-deepseek-r1

Source: https://docs.agno.com/basics/reasoning/usage/models/groq/groq-plus-claude

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Groq and Anthropic API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Groq and Anthropic API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Stripe MCP agent

**URL:** llms-txt#stripe-mcp-agent

Source: https://docs.agno.com/basics/tools/mcp/usage/stripe

Using the [Stripe MCP server](https://github.com/stripe/agent-toolkit/tree/main/modelcontextprotocol) to create an Agent that can interact with the Stripe API:

```python  theme={null}
"""ðŸ’µ Stripe MCP Agent - Manage Your Stripe Operations

This example demonstrates how to create an Agno agent that interacts with the Stripe API via the Model Context Protocol (MCP). This agent can create and manage Stripe objects like customers, products, prices, and payment links using natural language commands.

Setup:
2. Install Python dependencies: `pip install agno mcp-sdk`
3. Set Environment Variable: export STRIPE_SECRET_KEY=***.

Stripe MCP Docs: https://github.com/stripe/agent-toolkit
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from agno.utils.log import log_error, log_exception, log_info

async def run_agent(message: str) -> None:
    """
    Sets up the Stripe MCP server and initialize the Agno agent
    """
    # Verify Stripe API Key is available
    stripe_api_key = os.getenv("STRIPE_SECRET_KEY")
    if not stripe_api_key:
        log_error("STRIPE_SECRET_KEY environment variable not set.")
        return

enabled_tools = "paymentLinks.create,products.create,prices.create,customers.create,customers.read"

# handle different Operating Systems
    npx_command = "npx.cmd" if os.name == "nt" else "npx"

try:
        # Initialize MCP toolkit with Stripe server
        async with MCPTools(
            command=f"{npx_command} -y @stripe/mcp --tools={enabled_tools} --api-key={stripe_api_key}"
        ) as mcp_toolkit:
            agent = Agent(
                name="StripeAgent",
                instructions=dedent("""\
                    You are an AI assistant specialized in managing Stripe operations.
                    You interact with the Stripe API using the available tools.

- Understand user requests to create or list Stripe objects (customers, products, prices, payment links).
                    - Clearly state the results of your actions, including IDs of created objects or lists retrieved.
                    - Ask for clarification if a request is ambiguous.
                    - Use markdown formatting, especially for links or code snippets.
                    - Execute the necessary steps sequentially if a request involves multiple actions (e.g., create product, then price, then link).
                """),
                tools=[mcp_toolkit],
                markdown=True,
                            )

# Run the agent with the provided task
            log_info(f"Running agent with assignment: '{message}'")
            await agent.aprint_response(message, stream=True)

except FileNotFoundError:
        error_msg = f"Error: '{npx_command}' command not found. Please ensure Node.js and npm/npx are installed and in your system's PATH."
        log_error(error_msg)
    except Exception as e:
        log_exception(f"An unexpected error occurred during agent execution: {e}")

if __name__ == "__main__":
    task = "Create a new Stripe product named 'iPhone'. Then create a price of $999.99 USD for it. Finally, create a payment link for that price."
    asyncio.run(run_agent(task))

---

## Create a team with both shared and private state

**URL:** llms-txt#create-a-team-with-both-shared-and-private-state

shopping_team = Team(
    name="Shopping Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[shopping_agent],
    session_state={"shopping_list": [], "chores": []},
    tools=[list_items, add_chore],
    instructions=[
        "You manage a shopping list.",
        "Forward add/remove requests to the Shopping List Agent.",
        "Use list_items to show the current list.",
        "Log completed tasks using add_chore.",
    ],
)

---

## --- File management ---

**URL:** llms-txt#----file-management----

reports_dir = Path(__file__).parent.joinpath("reports", "investment")
if reports_dir.is_dir():
    rmtree(path=reports_dir, ignore_errors=True)
reports_dir.mkdir(parents=True, exist_ok=True)

stock_analyst_report = str(reports_dir.joinpath("stock_analyst_report.md"))
research_analyst_report = str(reports_dir.joinpath("research_analyst_report.md"))
investment_report = str(reports_dir.joinpath("investment_report.md"))

---

## Configure custom tracer provider

**URL:** llms-txt#configure-custom-tracer-provider

trace_provider = TracerProvider()
trace_provider.add_span_processor(
    SimpleSpanProcessor(
        OTLPSpanExporter(endpoint="http://127.0.0.1:4318/v1/traces")
    )
)
trace.set_tracer_provider(trace_provider)

---

## Field Labeled CSV Reader

**URL:** llms-txt#field-labeled-csv-reader

Source: https://docs.agno.com/reference/knowledge/reader/field-labeled-csv

FieldLabeledCSVReader is a reader class that converts CSV rows into field-labeled text documents.

<Snippet file="field-labeled-csv-reader-reference.mdx" />

---

## Add documents with metadata for filtering

**URL:** llms-txt#add-documents-with-metadata-for-filtering

knowledge_base.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    reader=PDFReader(chunk=True),
)

---

## This is the function that the agent will use to retrieve documents

**URL:** llms-txt#this-is-the-function-that-the-agent-will-use-to-retrieve-documents

**Contents:**
- Usage

def knowledge_retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom knowledge retriever function to search the vector database for relevant documents.

Args:
        query (str): The search query string
        agent (Agent): The agent instance making the query
        num_documents (int): Number of documents to retrieve (default: 5)
        **kwargs: Additional keyword arguments

Returns:
        Optional[list[dict]]: List of retrieved documents or None if search fails
    """
    try:
        qdrant_client = QdrantClient(url="http://localhost:6333")
        query_embedding = embedder.get_embedding(query)
        results = qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None

def main():
    """Main function to demonstrate agent usage."""
    # Initialize agent with custom knowledge retriever
    # Remember to set search_knowledge=True to use agentic_rag or add_reference=True for traditional RAG
    # search_knowledge=True is default when you add a knowledge base but is needed here
    agent = Agent(
        knowledge_retriever=knowledge_retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
    )

# Example query
    query = "List down the ingredients to make Massaman Gai"
    agent.print_response(query, markdown=True)

if __name__ == "__main__":
    main()
bash  theme={null}
    pip install -U agno openai qdrant-client
    bash  theme={null}
    docker run -p 6333:6333 qdrant/qdrant
    bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key_here
    bash Mac theme={null}
      python cookbook/knowledge/custom_retriever/retriever.py
      bash Windows theme={null}
      python cookbook/knowledge/custom_retriever/retriever.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Qdrant">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set OpenAI API key">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the example">
    <CodeGroup>
```

---

## In-Memory Storage

**URL:** llms-txt#in-memory-storage

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/in-memory/overview

Learn to use In-Memory storage for your Agents

Agno supports using In-Memory storage with the `InMemoryDb` class. By doing this, you will be able to use all features that depend on having a database, without having to set one up.

<Warning>
  Using the In-Memory storage is not recommended for production applications.
  Use it for demos, testing and any other use case where you don't want to setup a database.
</Warning>

```python  theme={null}
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb

---

## Tweet Analysis Agent

**URL:** llms-txt#tweet-analysis-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/tweet-analysis-agent

An agent that analyzes tweets and provides comprehensive brand monitoring and sentiment analysis.

* Real-time tweet analysis and sentiment classification
* Engagement metrics analysis (likes, retweets, replies)
* Brand health monitoring and competitive intelligence
* Strategic recommendations and response strategies

<Note> Check out the detailed [Social Media Agent](https://github.com/agno-agi/agno/tree/main/cookbook/examples/agents/social_media_agent.py). </Note>

* "Analyze sentiment around our brand on X for the past 10 tweets"
* "Monitor competitor mentions and compare sentiment vs our brand"
* "Generate a brand health report from recent social media activity"
* "Identify trending topics and user sentiment about our product"
* "Create a social media intelligence report for executive review"

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your X credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
<Note> Check out the detailed [Social Media Agent](https://github.com/agno-agi/agno/tree/main/cookbook/examples/agents/social_media_agent.py). </Note>

More prompts to try:

* "Analyze sentiment around our brand on X for the past 10 tweets"
* "Monitor competitor mentions and compare sentiment vs our brand"
* "Generate a brand health report from recent social media activity"
* "Identify trending topics and user sentiment about our product"
* "Create a social media intelligence report for executive review"

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your X credentials">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Meta

**URL:** llms-txt#meta

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/meta

The Meta model provides access to Meta's language models.

| Parameter               | Type            | Default                                     | Description                                                      |
| ----------------------- | --------------- | ------------------------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"meta-llama/Meta-Llama-3.1-405B-Instruct"` | The id of the Meta model to use                                  |
| `name`                  | `str`           | `"MetaLlama"`                               | The name of the model                                            |
| `provider`              | `str`           | `"Meta"`                                    | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                                      | The API key for Meta (defaults to META\_API\_KEY env var)        |
| `base_url`              | `str`           | `"https://api.llama-api.com"`               | The base URL for the Meta API                                    |
| `retries`               | `int`           | `0`                                         | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                                         | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                                     | If True, the delay between retries is doubled each time          |

Meta extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Example 1: Transcription

**URL:** llms-txt#example-1:-transcription

**Contents:**
- Using Multimodal Models

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

local_audio_path = Path("tmp/sample_conversation.wav")
print(f"Downloading file to local path: {local_audio_path}")
download_file(url, local_audio_path)

transcription_agent = Agent(
    tools=[OpenAITools(transcription_model="gpt-4o-transcribe")],
    markdown=True,
)
transcription_agent.print_response(
    f"Transcribe the audio file for this file: {local_audio_path}"
)
python cookbook/agents/multimodal/audio_to_text.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

**Examples:**

Example 1 (unknown):
```unknown
**Best for**: High accuracy, cloud processing

## Using Multimodal Models

Multimodal models like Gemini can transcribe audio directly without additional tools.
```

---

## Blog Post Generator

**URL:** llms-txt#blog-post-generator

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/workflows/blog-post-generator

Build a sophisticated blog post generator that combines web research with professional writing expertise. This workflow uses a multi-stage approach to create high-quality, well-researched blog content with proper citations.

By building this workflow, you'll understand:

* How to structure multi-phase workflows with research, extraction, and writing stages
* How to implement caching strategies for expensive operations like web scraping
* How to coordinate multiple specialized agents for complex content creation
* How to integrate web search and content extraction tools effectively

Build content creation platforms, automated journalism systems, SEO content generators, or research-based article writers.

The workflow follows a three-phase content creation process:

1. **Research**: Research agent searches the web and evaluates 10-15 sources to find the 5-7 best articles
2. **Extract**: Content scraper agent extracts and processes full article content in markdown format
3. **Write**: Writer agent crafts an engaging blog post with proper citations and SEO optimization
4. **Cache**: All intermediate results are cached in session state for efficiency

The workflow uses SQLite for session persistence and caches search results, scraped articles, and final blog posts.

```python blog_post_generator.py theme={null}
"""Blog Post Generator v2.0 - Your AI Content Creation Studio!

This advanced example demonstrates how to build a sophisticated blog post generator using
the new workflow v2.0 architecture. The workflow combines web research capabilities with
professional writing expertise using a multi-stage approach:

1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Key capabilities:
- Advanced web research and source evaluation
- Content scraping and processing
- Professional writing with SEO optimization
- Automatic content caching for efficiency
- Source attribution and fact verification
"""

import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

---

## Webhook

**URL:** llms-txt#webhook

Source: https://docs.agno.com/reference-api/schema/whatsapp/webhook

post /whatsapp/webhook
Handle incoming WhatsApp messages

---

## Search Knowledge

**URL:** llms-txt#search-knowledge

Source: https://docs.agno.com/reference-api/schema/knowledge/search-knowledge

post /knowledge/search
Search the knowledge base for relevant documents using query, filters and search type.

---

## Delete operations

**URL:** llms-txt#delete-operations

vector_db.delete_by_name("Recipes")

---

## Please download a sample audio file to test this Agent and upload using:

**URL:** llms-txt#please-download-a-sample-audio-file-to-test-this-agent-and-upload-using:

**Contents:**
- Usage

audio_path = Path(__file__).parent.joinpath("sample.mp3")

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(filepath=audio_path)],
    stream=True,
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/audio_input_local_file_upload.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/audio_input_local_file_upload.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cheap model for memory operations (60x less expensive)

**URL:** llms-txt#cheap-model-for-memory-operations-(60x-less-expensive)

memory_manager = MemoryManager(
    db=db,
    model=OpenAIChat(id="gpt-4o-mini")
)

---

## External Tool Execution Toolkit

**URL:** llms-txt#external-tool-execution-toolkit

Source: https://docs.agno.com/basics/hitl/usage/external-tool-execution-toolkit

This example demonstrates how to execute toolkit-based tools outside of the agent using external tool execution. It shows how to create a custom toolkit with tools that require external execution.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create memory manager and optimize memories

**URL:** llms-txt#create-memory-manager-and-optimize-memories

memory_manager = MemoryManager(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
)

print("\nOptimizing memories with 'summarize' strategy...")
memory_manager.optimize_memories(
    user_id=user_id,
    strategy=MemoryOptimizationStrategyType.SUMMARIZE,  # Combine all memories into one
    apply=True,  # Apply changes to database
)

---

## Add documentation content

**URL:** llms-txt#add-documentation-content

knowledge.add_contents(urls=["https://docs.agno.com/introduction/agents.md"])

---

## Brave Search

**URL:** llms-txt#brave-search

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/bravesearch

**BraveSearch** enables an Agent to search the web for information using the Brave search engine.

The following examples requires the `brave-search` library.

| Parameter             | Type            | Default | Description                                                                    |
| --------------------- | --------------- | ------- | ------------------------------------------------------------------------------ |
| `api_key`             | `Optional[str]` | `None`  | Brave API key. If not provided, will use BRAVE\_API\_KEY environment variable. |
| `fixed_max_results`   | `Optional[int]` | `None`  | A fixed number of maximum results.                                             |
| `fixed_language`      | `Optional[str]` | `None`  | A fixed language for the search results.                                       |
| `enable_brave_search` | `bool`          | `True`  | Enable or disable the brave\_search function.                                  |
| `all`                 | `bool`          | `False` | Enable all available functions in the toolkit.                                 |

| Function       | Description                                                                                                                                                                                                                                                                                                                                                                       |
| -------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `brave_search` | Searches Brave for a specified query. Parameters include `query` (str) for the search term, `max_results` (int, default=5) for the maximum number of results, `country` (str, default="US") for the country code for search results, and `search_lang` (str, default="en") for the language of the search results. Returns a JSON formatted string containing the search results. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/bravesearch.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/bravesearch_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example
```

---

## Reply to a post

**URL:** llms-txt#reply-to-a-post

agent.print_response(
    "Can you reply to this [post ID] post as a general message as to how great this project is: https://x.com/AgnoAgi",
    markdown=True,
)

---

## Agent with Structured Outputs

**URL:** llms-txt#agent-with-structured-outputs

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/structured-output

```python cookbook/models/openai/chat/structured_output.py theme={null}
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa

class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

---

## Additional agents for multi-step condition

**URL:** llms-txt#additional-agents-for-multi-step-condition

trend_analyzer_agent = Agent(
    name="Trend Analyzer",
    instructions="Analyze trends and patterns from research data",
)

fact_checker_agent = Agent(
    name="Fact Checker",
    instructions="Verify facts and cross-reference information",
)

---

## Delete All Content

**URL:** llms-txt#delete-all-content

Source: https://docs.agno.com/reference-api/schema/knowledge/delete-all-content

delete /knowledge/content
Permanently remove all content from the knowledge base. This is a destructive operation that cannot be undone. Use with extreme caution.

---

## ag infra config

**URL:** llms-txt#ag-infra-config

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/config

Prints active infra config

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

---

## Production Best Practices

**URL:** llms-txt#production-best-practices

**Contents:**
- Quick Reference
- The Agentic Memory Token Trap

Source: https://docs.agno.com/basics/memory/best-practices

Avoid common pitfalls, optimize costs, and ensure reliable memory behavior in production.

Memory is powerful, but without careful configuration, it can lead to unexpected token consumption, behavioral issues, and high costs. This guide shows you what to watch out for and how to optimize your memory usage for production.

* **Default to automatic memory** (`enable_user_memories=True`) unless you have a specific reason for agentic control
* **Always provide user\_id**, don't rely on the default "default" user
* **Use cheaper models** for memory operations when using agentic memory
* **Implement pruning** for long-running applications
* **Monitor token usage** in production to catch memory-related cost spikes
* **Test with realistic data**: 100+ memories behave very differently than 5 memories

## The Agentic Memory Token Trap

**The Problem:** When you use `enable_agentic_memory=True`, every memory operation triggers a **separate, nested LLM call**. This architecture can cause token usage to explode, especially as memories accumulate.

Here's what happens under the hood:

1. User sends a message â†’ Main LLM call processes it
2. Agent decides to update memory â†’ Calls `update_user_memory` tool
3. **Nested LLM call fires** with:
   * Detailed system prompt (\~50 lines)
   * ALL existing user memories loaded into context
   * Memory management instructions and tools
4. Memory LLM makes tool calls (add, update, delete)
5. Control returns to main conversation

**Real-world impact:**

```python  theme={null}

---

## Step with function

**URL:** llms-txt#step-with-function

Source: https://docs.agno.com/basics/workflows/usage/step-with-function

This example demonstrates how to use named steps with custom function executors.

This example demonstrates **Workflows** using named Step objects with custom function
executors. This pattern combines the benefits of named steps with the flexibility of
custom functions, allowing for sophisticated data processing within structured workflow steps.

**When to use**: When you need named step organization but want custom logic that goes
beyond what agents/teams provide. Ideal for complex data processing, multi-step operations,
or when you need to orchestrate multiple agents within a single step.

```python step_with_function.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## Specify the user_id and session_id on run to start or continue the conversation

**URL:** llms-txt#specify-the-user_id-and-session_id-on-run-to-start-or-continue-the-conversation

**Contents:**
- Session Storage Schema
- Next Steps

agent.print_response("Hello!", session_id="session_456", user_id="alice@example.com")
```

## Session Storage Schema

When you configure a database, Agno stores sessions in a structured format. Here's what gets saved for each session:

| Field           | Type   | Description                                     |
| --------------- | ------ | ----------------------------------------------- |
| `session_id`    | `str`  | Unique identifier for this conversation thread  |
| `session_type`  | `str`  | Type of session (agent, team, or workflow)      |
| `agent_id`      | `str`  | The agent ID (if this is an agent session)      |
| `team_id`       | `str`  | The team ID (if this is a team session)         |
| `workflow_id`   | `str`  | The workflow ID (if this is a workflow session) |
| `user_id`       | `str`  | The user this session belongs to                |
| `session_data`  | `dict` | Session-specific data and state                 |
| `agent_data`    | `dict` | Agent configuration and metadata                |
| `team_data`     | `dict` | Team configuration and metadata                 |
| `workflow_data` | `dict` | Workflow configuration and metadata             |
| `metadata`      | `dict` | Additional custom metadata                      |
| `runs`          | `list` | All the runs (interactions) in this session     |
| `summary`       | `dict` | The session summary (if enabled)                |
| `created_at`    | `int`  | Unix timestamp when session was created         |
| `updated_at`    | `int`  | Unix timestamp of last update                   |

<Tip>
  Want to visualize your sessions? Check out the [AgentOS UI sessions page](https://os.agno.com/sessions) for a beautiful interface to view and manage all your conversation threads.
</Tip>

Now that you have persistence configured, explore what you can do with sessions:

* [Storage Control](/basics/sessions/persisting-sessions/storage-control) - Optimize what gets saved
* [History Management](/basics/sessions/history-management) - Control conversation history
* [Session Summaries](/basics/sessions/session-summaries) - Condense long conversations
* [Session Management](/basics/sessions/session-management) - Naming, caching, and more

---

## WorkflowRunOutput

**URL:** llms-txt#workflowrunoutput

**Contents:**
- WorkflowRunOutput Attributes
- WorkflowRunOutputEvent Types and Attributes
  - BaseWorkflowRunOutputEvent Attributes
  - WorkflowStartedEvent Attributes
  - WorkflowCompletedEvent Attributes
  - WorkflowCancelledEvent Attributes
  - StepStartedEvent Attributes
  - StepCompletedEvent Attributes
  - ConditionExecutionStartedEvent Attributes
  - ConditionExecutionCompletedEvent Attributes

Source: https://docs.agno.com/reference/workflows/run-output

## WorkflowRunOutput Attributes

| Parameter            | Type                                                              | Default             | Description                                                           |
| -------------------- | ----------------------------------------------------------------- | ------------------- | --------------------------------------------------------------------- |
| `content`            | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel, Any]]` | `None`              | Main content/output from the workflow execution                       |
| `content_type`       | `str`                                                             | `"str"`             | Type of the content (e.g., "str", "json", etc.)                       |
| `workflow_id`        | `Optional[str]`                                                   | `None`              | Unique identifier of the executed workflow                            |
| `workflow_name`      | `Optional[str]`                                                   | `None`              | Name of the executed workflow                                         |
| `run_id`             | `Optional[str]`                                                   | `None`              | Unique identifier for this specific run                               |
| `session_id`         | `Optional[str]`                                                   | `None`              | Session UUID associated with this run                                 |
| `images`             | `Optional[List[Image]]`                                           | `None`              | List of image artifacts generated                                     |
| `videos`             | `Optional[List[Video]]`                                           | `None`              | List of video artifacts generated                                     |
| `audio`              | `Optional[List[Audio]]`                                           | `None`              | List of audio artifacts generated                                     |
| `response_audio`     | `Optional[Audio]`                                                 | `None`              | Audio response from the workflow                                      |
| `step_results`       | `List[Union[StepOutput, List[StepOutput]]]`                       | `[]`                | Actual step execution results as StepOutput objects                   |
| `step_executor_runs` | `Optional[List[Union[RunOutput, TeamRunOutput]]]`                 | `None`              | Store agent/team responses separately with parent\_run\_id references |
| `events`             | `Optional[List[WorkflowRunOutputEvent]]`                          | `None`              | Events captured during workflow execution                             |
| `metrics`            | `Optional[WorkflowMetrics]`                                       | `None`              | Workflow metrics including duration and step-level data               |
| `metadata`           | `Optional[Dict[str, Any]]`                                        | `None`              | Additional metadata stored with the response                          |
| `created_at`         | `int`                                                             | `int(time())`       | Unix timestamp when the response was created                          |
| `status`             | `RunStatus`                                                       | `RunStatus.pending` | Current status of the workflow run                                    |

## WorkflowRunOutputEvent Types and Attributes

### BaseWorkflowRunOutputEvent Attributes

| Parameter        | Type            | Default       | Description                                              |
| ---------------- | --------------- | ------------- | -------------------------------------------------------- |
| `created_at`     | `int`           | `int(time())` | Unix timestamp when the event was created                |
| `event`          | `str`           | `""`          | Type of the event (e.g., "WorkflowStarted")              |
| `workflow_id`    | `Optional[str]` | `None`        | Unique identifier of the workflow                        |
| `workflow_name`  | `Optional[str]` | `None`        | Name of the workflow                                     |
| `session_id`     | `Optional[str]` | `None`        | Session UUID associated with the workflow                |
| `run_id`         | `Optional[str]` | `None`        | Unique identifier for the workflow run                   |
| `step_id`        | `Optional[str]` | `None`        | Unique identifier for the current step                   |
| `parent_step_id` | `Optional[str]` | `None`        | Unique identifier for the parent step (for nested steps) |

### WorkflowStartedEvent Attributes

| Parameter                                               | Type  | Default                                   | Description           |
| ------------------------------------------------------- | ----- | ----------------------------------------- | --------------------- |
| `event`                                                 | `str` | `WorkflowRunEvent.workflow_started.value` | Event type identifier |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |       |                                           |                       |

### WorkflowCompletedEvent Attributes

<Snippet file="workflow-completed-event.mdx" />

### WorkflowCancelledEvent Attributes

| Parameter                                               | Type                       | Default                                     | Description                                 |
| ------------------------------------------------------- | -------------------------- | ------------------------------------------- | ------------------------------------------- |
| `event`                                                 | `str`                      | `WorkflowRunEvent.workflow_completed.value` | Event type identifier                       |
| `content`                                               | `Optional[Any]`            | `None`                                      | Final output content from the workflow      |
| `content_type`                                          | `str`                      | `"str"`                                     | Type of the content                         |
| `step_results`                                          | `List[StepOutput]`         | `[]`                                        | List of all step execution results          |
| `metadata`                                              | `Optional[Dict[str, Any]]` | `None`                                      | Additional metadata from workflow execution |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                            |                                             |                                             |

### StepStartedEvent Attributes

| Parameter                                               | Type                          | Default                               | Description                    |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------- | ------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.step_started.value` | Event type identifier          |
| `step_name`                                             | `Optional[str]`               | `None`                                | Name of the step being started |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                | Index or position of the step  |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                       |                                |

### StepCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                 | Description                           |
| ------------------------------------------------------- | ----------------------------- | --------------------------------------- | ------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.step_completed.value` | Event type identifier                 |
| `step_name`                                             | `Optional[str]`               | `None`                                  | Name of the step that completed       |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                  | Index or position of the step         |
| `content`                                               | `Optional[Any]`               | `None`                                  | Content output from the step          |
| `content_type`                                          | `str`                         | `"str"`                                 | Type of the content                   |
| `images`                                                | `Optional[List[Image]]`       | `None`                                  | Image artifacts from the step         |
| `videos`                                                | `Optional[List[Video]]`       | `None`                                  | Video artifacts from the step         |
| `audio`                                                 | `Optional[List[Audio]]`       | `None`                                  | Audio artifacts from the step         |
| `response_audio`                                        | `Optional[Audio]`             | `None`                                  | Audio response from the step          |
| `step_response`                                         | `Optional[StepOutput]`        | `None`                                  | Complete step execution result object |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                         |                                       |

### ConditionExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                              | Description                        |
| ------------------------------------------------------- | ----------------------------- | ---------------------------------------------------- | ---------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.condition_execution_started.value` | Event type identifier              |
| `step_name`                                             | `Optional[str]`               | `None`                                               | Name of the condition step         |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                               | Index or position of the condition |
| `condition_result`                                      | `Optional[bool]`              | `None`                                               | Result of the condition evaluation |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                      |                                    |

### ConditionExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                                | Description                                 |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------------ | ------------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.condition_execution_completed.value` | Event type identifier                       |
| `step_name`                                             | `Optional[str]`               | `None`                                                 | Name of the condition step                  |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                                 | Index or position of the condition          |
| `condition_result`                                      | `Optional[bool]`              | `None`                                                 | Result of the condition evaluation          |
| `executed_steps`                                        | `Optional[int]`               | `None`                                                 | Number of steps executed based on condition |
| `step_results`                                          | `List[StepOutput]`            | `[]`                                                   | Results from executed steps                 |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                        |                                             |

### ParallelExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                             | Description                            |
| ------------------------------------------------------- | ----------------------------- | --------------------------------------------------- | -------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.parallel_execution_started.value` | Event type identifier                  |
| `step_name`                                             | `Optional[str]`               | `None`                                              | Name of the parallel step              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                              | Index or position of the parallel step |
| `parallel_step_count`                                   | `Optional[int]`               | `None`                                              | Number of steps to execute in parallel |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                     |                                        |

### ParallelExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                               | Description                            |
| ------------------------------------------------------- | ----------------------------- | ----------------------------------------------------- | -------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.parallel_execution_completed.value` | Event type identifier                  |
| `step_name`                                             | `Optional[str]`               | `None`                                                | Name of the parallel step              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                                | Index or position of the parallel step |
| `parallel_step_count`                                   | `Optional[int]`               | `None`                                                | Number of steps executed in parallel   |
| `step_results`                                          | `List[StepOutput]`            | `field(default_factory=list)`                         | Results from all parallel steps        |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                       |                                        |

### LoopExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                         | Description                          |
| ------------------------------------------------------- | ----------------------------- | ----------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_execution_started.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                          | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                          | Index or position of the loop        |
| `max_iterations`                                        | `Optional[int]`               | `None`                                          | Maximum number of iterations allowed |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                 |                                      |

### LoopIterationStartedEvent Attributes

| Parameter                                               | Type                          | Default                                         | Description                          |
| ------------------------------------------------------- | ----------------------------- | ----------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_iteration_started.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                          | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                          | Index or position of the loop        |
| `iteration`                                             | `int`                         | `0`                                             | Current iteration number             |
| `max_iterations`                                        | `Optional[int]`               | `None`                                          | Maximum number of iterations allowed |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                 |                                      |

### LoopIterationCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                           | Description                          |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_iteration_completed.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                            | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                            | Index or position of the loop        |
| `iteration`                                             | `int`                         | `0`                                               | Current iteration number             |
| `max_iterations`                                        | `Optional[int]`               | `None`                                            | Maximum number of iterations allowed |
| `iteration_results`                                     | `List[StepOutput]`            | `[]`                                              | Results from this iteration          |
| `should_continue`                                       | `bool`                        | `True`                                            | Whether the loop should continue     |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                   |                                      |

### LoopExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                           | Description                          |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_execution_completed.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                            | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                            | Index or position of the loop        |
| `total_iterations`                                      | `int`                         | `0`                                               | Total number of iterations completed |
| `max_iterations`                                        | `Optional[int]`               | `None`                                            | Maximum number of iterations allowed |
| `all_results`                                           | `List[List[StepOutput]]`      | `[]`                                              | Results from all iterations          |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                   |                                      |

### RouterExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                           | Description                           |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------- | ------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.router_execution_started.value` | Event type identifier                 |
| `step_name`                                             | `Optional[str]`               | `None`                                            | Name of the router step               |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                            | Index or position of the router       |
| `selected_steps`                                        | `List[str]`                   | `field(default_factory=list)`                     | Names of steps selected by the router |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                   |                                       |

### RouterExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                             | Description                       |
| ------------------------------------------------------- | ----------------------------- | --------------------------------------------------- | --------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.router_execution_completed.value` | Event type identifier             |
| `step_name`                                             | `Optional[str]`               | `None`                                              | Name of the router step           |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                              | Index or position of the router   |
| `selected_steps`                                        | `List[str]`                   | `field(default_factory=list)`                       | Names of steps that were selected |
| `executed_steps`                                        | `Optional[int]`               | `None`                                              | Number of steps executed          |
| `step_results`                                          | `List[StepOutput]`            | `field(default_factory=list)`                       | Results from executed steps       |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                     |                                   |

### StepsExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                          | Description                          |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------ | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.steps_execution_started.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                           | Name of the steps group              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                           | Index or position of the steps group |
| `steps_count`                                           | `Optional[int]`               | `None`                                           | Number of steps in the group         |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                  |                                      |

### StepsExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                            | Description                          |
| ------------------------------------------------------- | ----------------------------- | -------------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.steps_execution_completed.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                             | Name of the steps group              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                             | Index or position of the steps group |
| `steps_count`                                           | `Optional[int]`               | `None`                                             | Number of steps in the group         |
| `executed_steps`                                        | `Optional[int]`               | `None`                                             | Number of steps actually executed    |
| `step_results`                                          | `List[StepOutput]`            | `field(default_factory=list)`                      | Results from all executed steps      |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                    |                                      |

### StepOutputEvent Attributes

| Parameter                                               | Type                                                              | Default        | Description                                  |
| ------------------------------------------------------- | ----------------------------------------------------------------- | -------------- | -------------------------------------------- |
| `event`                                                 | `str`                                                             | `"StepOutput"` | Event type identifier                        |
| `step_name`                                             | `Optional[str]`                                                   | `None`         | Name of the step that produced output        |
| `step_index`                                            | `Optional[Union[int, tuple]]`                                     | `None`         | Index or position of the step                |
| `step_output`                                           | `Optional[StepOutput]`                                            | `None`         | Complete step execution result               |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                                                                   |                |                                              |
| **Properties (read-only):**                             |                                                                   |                |                                              |
| `content`                                               | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel, Any]]` | -              | Content from the step output                 |
| `images`                                                | `Optional[List[Image]]`                                           | -              | Images from the step output                  |
| `videos`                                                | `Optional[List[Video]]`                                           | -              | Videos from the step output                  |
| `audio`                                                 | `Optional[List[Audio]]`                                           | -              | Audio from the step output                   |
| `success`                                               | `bool`                                                            | -              | Whether the step succeeded                   |
| `error`                                                 | `Optional[str]`                                                   | -              | Error message if step failed                 |
| `stop`                                                  | `bool`                                                            | -              | Whether the step requested early termination |

| Parameter  | Type                     | Default | Description                              |
| ---------- | ------------------------ | ------- | ---------------------------------------- |
| `steps`    | `Dict[str, StepMetrics]` | -       | Step-level metrics mapped by step name   |
| `duration` | `Optional[float]`        | `None`  | Total workflow execution time in seconds |

| Parameter       | Type                | Default | Description                                       |
| --------------- | ------------------- | ------- | ------------------------------------------------- |
| `step_name`     | `str`               | -       | Name of the step                                  |
| `executor_type` | `str`               | -       | Type of executor ("agent", "team", or "function") |
| `executor_name` | `str`               | -       | Name of the executor                              |
| `metrics`       | `Optional[Metrics]` | `None`  | Execution metrics (duration, tokens, model usage) |

---

## Run workflow

**URL:** llms-txt#run-workflow

response = workflow.run(input="AI trends in 2024")

---

## Reddit

**URL:** llms-txt#reddit

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/social/reddit

RedditTools enables agents to interact with Reddit for browsing posts, comments, and subreddit information.

The following agent can browse and analyze Reddit content:

| Parameter         | Type                    | Default              | Description                                                   |
| ----------------- | ----------------------- | -------------------- | ------------------------------------------------------------- |
| `reddit_instance` | `Optional[praw.Reddit]` | `None`               | Existing Reddit instance to use.                              |
| `client_id`       | `Optional[str]`         | `None`               | Reddit client ID. Uses REDDIT\_CLIENT\_ID if not set.         |
| `client_secret`   | `Optional[str]`         | `None`               | Reddit client secret. Uses REDDIT\_CLIENT\_SECRET if not set. |
| `user_agent`      | `Optional[str]`         | `"RedditTools v1.0"` | User agent string for API requests.                           |
| `username`        | `Optional[str]`         | `None`               | Reddit username for authenticated access.                     |
| `password`        | `Optional[str]`         | `None`               | Reddit password for authenticated access.                     |

| Function              | Description                                              |
| --------------------- | -------------------------------------------------------- |
| `get_subreddit_info`  | Get information about a specific subreddit.              |
| `get_subreddit_posts` | Get posts from a subreddit with various sorting options. |
| `search_subreddits`   | Search for subreddits by name or topic.                  |
| `get_post_details`    | Get detailed information about a specific post.          |
| `get_post_comments`   | Get comments from a specific post.                       |
| `search_posts`        | Search for posts across Reddit or within subreddits.     |
| `get_user_info`       | Get information about a Reddit user.                     |
| `get_user_posts`      | Get posts submitted by a specific user.                  |
| `get_user_comments`   | Get comments made by a specific user.                    |
| `create_post`         | Create a new post (requires authentication).             |
| `create_comment`      | Create a comment on a post (requires authentication).    |
| `vote_on_post`        | Vote on a post (requires authentication).                |
| `vote_on_comment`     | Vote on a comment (requires authentication).             |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/reddit.py)
* [Reddit API Documentation](https://www.reddit.com/dev/api/)
* [PRAW Documentation](https://praw.readthedocs.io/)

---

## Infra Settings

**URL:** llms-txt#infra-settings

**Contents:**
  - Infra Name
- Image Repository
- Build Images
- Push Images
- AWS Settings

Source: https://docs.agno.com/templates/infra-management/infra-settings

The `InfraSettings` object in the `infra/settings.py` file defines common settings used by your apps and resources. Here are the settings we recommend updating:

<Note>
  `InfraSettings` can also be updated using environment variables or the `.env` file.

Checkout the `example.env` file for an example.
</Note>

The `infra_name` is used to name your apps and resources. Change it to your project or team name, for example:

* `infra_name="booking-ai"`
* `infra_name="reddit-ai"`
* `infra_name="vantage-ai"`

The `infra_name` is used to name:

* The image for your application
* Apps like db, streamlit app and FastAPI server
* Resources like buckets, secrets and loadbalancers

Checkout the `infra/dev_resources.py` and `infra/prd_resources.py` file to see how its used.

The `image_repo` defines the repo for your image.

* If using dockerhub it would be something like `agno`.
* If using ECR it would be something like `[ACCOUNT_ID].dkr.ecr.us-east-1.amazonaws.com`

Checkout the `dev_image` in `infra/dev_resources.py` and `prd_image` in `infra/prd_resources.py` to see how its used.

Setting `build_images=True` will build images locally when running `ag infra up dev:docker` or `ag infra up prd:docker`.

Checkout the `dev_image` in `infra/dev_resources.py` and `prd_image` in `infra/prd_resources.py` to see how its used.

* [Building your development image](/templates/infra-management/development-app#build-your-development-image)
* [Building your production image](/templates/infra-management/production-app#build-your-production-image)

Setting `push_images=True` will push images after building when running `ag infra up dev:docker` or `ag infra up prd:docker`.

Checkout the `dev_image` in `infra/dev_resources.py` and `prd_image` in `infra/prd_resources.py` to see how its used.

* [Building your development image](/templates/infra-management/development-app#build-your-development-image)
* [Building your production image](/templates/infra-management/production-app#build-your-production-image)

The `aws_region` and `subnet_ids` provide values used for creating production resources. Checkout the `infra/prd_resources.py` file to see how its used.

---

## Local File System

**URL:** llms-txt#local-file-system

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/local/local-file-system

LocalFileSystemTools enables agents to write files to the local file system with automatic directory management.

The following agent can write content to local files:

| Parameter           | Type            | Default | Description                                                  |
| ------------------- | --------------- | ------- | ------------------------------------------------------------ |
| `target_directory`  | `Optional[str]` | `None`  | Default directory to write files to. Uses current directory. |
| `default_extension` | `str`           | `"txt"` | Default file extension to use if none specified.             |
| `enable_write_file` | `bool`          | `True`  | Enable file writing functionality.                           |

| Function     | Description                                              |
| ------------ | -------------------------------------------------------- |
| `write_file` | Write content to a local file with customizable options. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/local_file_system.py)
* [Python pathlib Documentation](https://docs.python.org/3/library/pathlib.html)
* [File I/O Best Practices](https://docs.python.org/3/tutorial/inputoutput.html)

---

## Accessing your Traces

**URL:** llms-txt#accessing-your-traces

**Contents:**
- Trace Functions
  - `db.get_trace()`

Source: https://docs.agno.com/basics/tracing/db-functions

Database convenience functions for querying traces and spans

Agno provides convenience functions on your database instance to query traces and spans. These functions work with any supported database (SQLite, PostgreSQL, etc.).

Get a single trace by identifier.

```python  theme={null}

---

## FastEmbed

**URL:** llms-txt#fastembed

Source: https://docs.agno.com/reference/knowledge/embedder/fastembed

FastEmbed Embedder is a class that allows you to embed documents using FastEmbed's efficient embedding models, with BAAI/bge-small-en-v1.5 as the default model.

<Snippet file="embedder-fastembed-reference.mdx" />

---

## Count tokens before optimization

**URL:** llms-txt#count-tokens-before-optimization

strategy = SummarizeStrategy()
tokens_before = strategy.count_tokens(memories_before)
print(f"  Token count: {tokens_before} tokens")

print("\nIndividual memories:")
for i, memory in enumerate(memories_before, 1):
    print(f"  {i}. {memory.memory}")

---

## Create Git Repo

**URL:** llms-txt#create-git-repo

Source: https://docs.agno.com/templates/infra-management/git-repo

Create a git repository to share your application with your team.

<Steps>
  <Step title="Create a git repository">
    Create a new [git repository](https://github.com/new).
  </Step>

<Step title="Push your code">
    Push your code to the git repository.

<Step title="Ask your team to join">
    Ask your team to follow the [setup steps for new users](/templates/infra-management/new-users) to use this workspace.
  </Step>
</Steps>

---

## Async Reliability Evaluation

**URL:** llms-txt#async-reliability-evaluation

Source: https://docs.agno.com/basics/evals/reliability/usage/reliability-async

Example showing how to run reliability evaluations asynchronously.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Shared knowledge base for the team

**URL:** llms-txt#shared-knowledge-base-for-the-team

knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_team",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

---

## Teams with Knowledge

**URL:** llms-txt#teams-with-knowledge

Source: https://docs.agno.com/basics/knowledge/teams/overview

Learn how to use teams with knowledge bases.

Teams can use a knowledge base to store and retrieve information, just like agents:

```python  theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb

---

## Scenario: User with 100 existing memories

**URL:** llms-txt#scenario:-user-with-100-existing-memories

agent = Agent(
    db=db,
    enable_agentic_memory=True,
    model=OpenAIChat(id="gpt-4o")
)

---

## vLLM

**URL:** llms-txt#vllm

**Contents:**
- Usage

Source: https://docs.agno.com/reference/knowledge/embedder/vllm

The vLLM Embedder provides high-performance embedding inference with support for both local and remote deployment modes. It can load models directly for local inference or connect to a remote vLLM server via an OpenAI-compatible API.

```python  theme={null}
from agno.knowledge.embedder.vllm import VLLMEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## with valid input

**URL:** llms-txt#with-valid-input

print("=== Testing TypedDict Input Schema ===")
hackernews_agent.print_response(
    input={
        "topic": "AI",
        "focus_areas": ["Machine Learning", "LLMs", "Neural Networks"],
        "target_audience": "Developers",
        "sources_required": 5,
    }
)

---

## Response Composer Agent - Specialized in response composition

**URL:** llms-txt#response-composer-agent---specialized-in-response-composition

response_composer = Agent(
    name="Response Composer",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Compose comprehensive responses with proper source attribution",
    instructions=[
        "Combine validated information from all team members.",
        "Create well-structured, comprehensive responses.",
        "Include proper source attribution and data provenance.",
        "Ensure clarity and coherence in the final response.",
        "Format responses for optimal user experience.",
    ],
    markdown=True,
)

---

## --- Simulation tools ---

**URL:** llms-txt#----simulation-tools----

def simulate_zoom_scheduling(
    agent: Agent, candidate_name: str, candidate_email: str
) -> str:
    """Simulate Zoom call scheduling"""
    # Generate a future time slot (1-7 days from now, between 10am-6pm IST)
    base_time = datetime.now() + timedelta(days=random.randint(1, 7))
    hour = random.randint(10, 17)  # 10am to 5pm
    scheduled_time = base_time.replace(hour=hour, minute=0, second=0, microsecond=0)

# Generate fake Zoom URL
    meeting_id = random.randint(100000000, 999999999)
    zoom_url = f"https://zoom.us/j/{meeting_id}"

result = "Zoom call scheduled successfully!\n"
    result += f"Time: {scheduled_time.strftime('%Y-%m-%d %H:%M')} IST\n"
    result += f"Meeting URL: {zoom_url}\n"
    result += f"Participant: {candidate_name} ({candidate_email})"

def simulate_email_sending(agent: Agent, to_email: str, subject: str, body: str) -> str:
    """Simulate email sending"""
    result = "Email sent successfully!\n"
    result += f"To: {to_email}\n"
    result += f"Subject: {subject}\n"
    result += f"Body length: {len(body)} characters\n"
    result += f"Sent at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

---

## AgentOS with dedicated tracing database

**URL:** llms-txt#agentos-with-dedicated-tracing-database

**Contents:**
  - Why Use a Dedicated Tracing Database?
- Using setup\_tracing() with AgentOS

agent_os = AgentOS(
    agents=[hackernews_agent, search_agent],
    tracing=True,
    tracing_db=tracing_db,  # All traces go here
)

app = agent_os.get_app()
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tracing import setup_tracing

**Examples:**

Example 1 (unknown):
```unknown
### Why Use a Dedicated Tracing Database?

Without `tracing_db`, AgentOS will store traces in the **first database it finds** from your agents or teams. This leads to:

* **Fragmented traces**: Traces scattered across different databases
* **Incomplete observability**: Can't query all traces from one place
* **Unpredictable behavior**: Which database gets traces depends on agent order

With a dedicated `tracing_db`:

* **Unified observability**: All traces in one queryable location
* **Cross-agent analysis**: Compare performance across agents
* **Independent scaling**: Traces don't affect agent data storage
* **Predictable behavior**: You control exactly where traces go

## Using setup\_tracing() with AgentOS

You can also use `setup_tracing()` for more control over tracing configuration. In this case, you still need to ensure the tracing database is available to AgentOS:
```

---

## Creating your own tools

**URL:** llms-txt#creating-your-own-tools

**Contents:**
- Learn more

Source: https://docs.agno.com/basics/tools/creating-tools/overview

Learn how to write your own tools and how to use the `@tool` decorator to modify the behavior of a tool.

In most production cases, you will need to write your own tools. Which is why we're focused on provide the best tool-use experience in Agno.

* Any Python function can be used as a tool by an Agent.
* Use the `@tool` decorator to modify what happens before and after this tool is called.

There are two main ways to create tools in Agno:

1. Create a python function (optionally using the `@tool` decorator)
2. Create a Toolkit

<CardGroup>
  <Card title="Python Functions as Tools" href="/basics/tools/creating-tools/python-functions">
    Learn how to create tools with python functions.
  </Card>

<Card title="Toolkits" href="/basics/tools/creating-tools/toolkits">
    Learn how to create tools with toolkits.
  </Card>
</CardGroup>

---

## Continue conversation - history shared via session_id

**URL:** llms-txt#continue-conversation---history-shared-via-session_id

**Contents:**
- Cross-Provider Switching

budget_agent.print_response(
    "Can you summarize our discussion so far?", session_id=session_id, user_id=user_id
)
python  theme={null}
from uuid import uuid4
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.models.google import Gemini

db = SqliteDb(db_file="tmp/cross_provider_demo.db")

session_id = str(uuid4())
user_id = "user@example.com"

**Examples:**

Example 1 (unknown):
```unknown
## Cross-Provider Switching

Switching between providers can cause compatibility issues and is not recommended for production use without thorough testing:
```

---

## Create workflow with loop

**URL:** llms-txt#create-workflow-with-loop

workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    # Test the workflow
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
    )
```

This was a synchronous non-streaming example of this pattern. To checkout async and streaming versions, see the cookbooks-

* [Loop Steps Workflow (sync streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/sync/loop_steps_workflow_stream.py)
* [Loop Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/async/loop_steps_workflow.py)
* [Loop Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/async/loop_steps_workflow_stream.py)

---

## Initialize LanceDB

**URL:** llms-txt#initialize-lancedb

---

## SurrealDB

**URL:** llms-txt#surrealdb

Source: https://docs.agno.com/reference/vector-db/surrealdb

<Snippet file="vector_db_surrealdb_params.mdx" />

---

## Conversational Workflows

**URL:** llms-txt#conversational-workflows

**Contents:**
- Quick Start
- Architecture
- Workflow History for Conversational Workflows:
- Instructions for the WorkflowAgent:
- Usage Example

Source: https://docs.agno.com/basics/workflows/conversational-workflows

Learn about conversational workflows in Agno.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.6" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.6">v2.2.6</Tooltip>
</Badge>

If your users interact directly with a Workflow, it is often useful to make it a **Conversational Workflow**. Users will then be able to chat with the Workflow,
in the same way you'd interact with an Agent or a Team.

This feature allows you to add a `WorkflowAgent` to your workflow that intelligently decides whether to:

1. **Answer directly** based on the current input and past workflow results
2. **Run the workflow** when the input cannot be answered based on past results

<Note>
  What is a **WorkflowAgent**?

`WorkflowAgent` is a restricted version of the `Agent` class specifically designed for workflow orchestration.
</Note>

This is how you can add a `WorkflowAgent` to your workflow:

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=ec5144a1155be8ca93b18d50be13b084" alt="Workflows conversational workflows diagram" style={{ maxWidth: '500px', width: '100%', height: 'auto' }} data-og-width="958" width="958" data-og-height="1362" height="1362" data-path="images/workflow-agent-flow-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=280&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=e5d8ed7fb9cb4d802b194a8aea60d9ca 280w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=560&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=5e3bc26dd499d6ce3e5e262f73988ad2 560w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=840&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=7c24aad031f8caf88fb0b67348140101 840w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=1100&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=1e6f92a0a2dffd2bd99bf60b893fa4b9 1100w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=1650&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=95e73e7791ff58af2342aa8efff4a3f8 1650w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=2500&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=56702e0fdd18e1f8946c2f216b24e1e2 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=a9f050d257d118db37b63c11937397ad" alt="Workflows conversational workflows diagram" style={{ maxWidth: '500px', width: '100%', height: 'auto' }} data-og-width="958" width="958" data-og-height="1332" height="1332" data-path="images/workflow-agent-flow-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=280&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=e0d45e2df7934ac3d654039f6c4e1ccb 280w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=560&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=ae85021963cbde778149b133472b1991 560w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=840&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=f6f984a95ebb6a6bdcf08cf2644d40c9 840w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=1100&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=e7323d14876732545fdde33c6b8c83fb 1100w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=1650&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=685e3d8f72f6b654fbc6f5e0e52da7f7 1650w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=2500&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=0eacd3dc568b6828e48ee8b4f1edf3d1 2500w" />

## Workflow History for Conversational Workflows:

Similar to [workflow history for steps](/basics/chat-history/workflow/overview), the `WorkflowAgent` has access to the full history of workflow runs for the current session.
This makes it possible to answer questions about previous results, compare outputs from multiple runs, and maintain conversation continuity.

<Tip>
  How to control the number of previous runs the workflow agent can see?

The `num_history_runs` parameter controls how many previous workflow runs the agent can see when making decisions. This is crucial for:

* **Context awareness**: The agent needs to see past runs to answer follow-up questions
  * **Memory limits**: Including too many runs can exceed the model context window
  * **Performance**: Fewer runs mean faster processing and lower input tokens
</Tip>

## Instructions for the WorkflowAgent:

You can provide custom instructions to the `WorkflowAgent` to control its behavior. Although default instructions are provided which instruct the agent to answer directly from history
or to run the workflow when new processing is needed, you can override them by providing your own instructions.

<Warning>
  We recommend using the default instructions unless you have a specific use case where you want the agent to answer in a particular way or include some specific information in the response.
  The default instructions should be sufficient for most use cases.
</Warning>

```python  theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.workflow import WorkflowAgent
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

story_writer = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are tasked with writing a 100 word story based on a given topic",
)

story_formatter = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are tasked with breaking down a short story in prelogues, body and epilogue",
)

def add_references(step_input: StepInput):
    """Add references to the story"""

previous_output = step_input.previous_step_content

if isinstance(previous_output, str):
        return previous_output + "\n\nReferences: https://www.agno.com"

**Examples:**

Example 1 (unknown):
```unknown
## Architecture

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=ec5144a1155be8ca93b18d50be13b084" alt="Workflows conversational workflows diagram" style={{ maxWidth: '500px', width: '100%', height: 'auto' }} data-og-width="958" width="958" data-og-height="1362" height="1362" data-path="images/workflow-agent-flow-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=280&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=e5d8ed7fb9cb4d802b194a8aea60d9ca 280w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=560&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=5e3bc26dd499d6ce3e5e262f73988ad2 560w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=840&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=7c24aad031f8caf88fb0b67348140101 840w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=1100&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=1e6f92a0a2dffd2bd99bf60b893fa4b9 1100w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=1650&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=95e73e7791ff58af2342aa8efff4a3f8 1650w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-light.png?w=2500&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=56702e0fdd18e1f8946c2f216b24e1e2 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=a9f050d257d118db37b63c11937397ad" alt="Workflows conversational workflows diagram" style={{ maxWidth: '500px', width: '100%', height: 'auto' }} data-og-width="958" width="958" data-og-height="1332" height="1332" data-path="images/workflow-agent-flow-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=280&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=e0d45e2df7934ac3d654039f6c4e1ccb 280w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=560&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=ae85021963cbde778149b133472b1991 560w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=840&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=f6f984a95ebb6a6bdcf08cf2644d40c9 840w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=1100&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=e7323d14876732545fdde33c6b8c83fb 1100w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=1650&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=685e3d8f72f6b654fbc6f5e0e52da7f7 1650w, https://mintcdn.com/agno-v2/zKmlURgt8K26VBJI/images/workflow-agent-flow-dark.png?w=2500&fit=max&auto=format&n=zKmlURgt8K26VBJI&q=85&s=0eacd3dc568b6828e48ee8b4f1edf3d1 2500w" />

## Workflow History for Conversational Workflows:

Similar to [workflow history for steps](/basics/chat-history/workflow/overview), the `WorkflowAgent` has access to the full history of workflow runs for the current session.
This makes it possible to answer questions about previous results, compare outputs from multiple runs, and maintain conversation continuity.

<Tip>
  How to control the number of previous runs the workflow agent can see?

  The `num_history_runs` parameter controls how many previous workflow runs the agent can see when making decisions. This is crucial for:

  * **Context awareness**: The agent needs to see past runs to answer follow-up questions
  * **Memory limits**: Including too many runs can exceed the model context window
  * **Performance**: Fewer runs mean faster processing and lower input tokens
</Tip>

## Instructions for the WorkflowAgent:

You can provide custom instructions to the `WorkflowAgent` to control its behavior. Although default instructions are provided which instruct the agent to answer directly from history
or to run the workflow when new processing is needed, you can override them by providing your own instructions.

<Warning>
  We recommend using the default instructions unless you have a specific use case where you want the agent to answer in a particular way or include some specific information in the response.
  The default instructions should be sufficient for most use cases.
</Warning>
```

Example 2 (unknown):
```unknown
## Usage Example
```

---

## Load all documents into the vector database

**URL:** llms-txt#load-all-documents-into-the-vector-database

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

---

## Linkup

**URL:** llms-txt#linkup

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/linkup

LinkupTools provides advanced web search capabilities with deep search options and structured results.

The following agent can perform advanced web searches using Linkup:

| Parameter                       | Type            | Default           | Description                                        |
| ------------------------------- | --------------- | ----------------- | -------------------------------------------------- |
| `api_key`                       | `Optional[str]` | `None`            | Linkup API key. Uses LINKUP\_API\_KEY if not set.  |
| `depth`                         | `Literal`       | `"standard"`      | Search depth: "standard" or "deep".                |
| `output_type`                   | `Literal`       | `"searchResults"` | Output format: "searchResults" or "sourcedAnswer". |
| `enable_web_search_with_linkup` | `bool`          | `True`            | Enable web search functionality.                   |

| Function                 | Description                                                       |
| ------------------------ | ----------------------------------------------------------------- |
| `web_search_with_linkup` | Perform advanced web searches with configurable depth and format. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/linkup.py)
* [Linkup SDK Documentation](https://docs.linkup.com/)
* [Linkup API Reference](https://api.linkup.com/docs)

---

## Workflow Patterns

**URL:** llms-txt#workflow-patterns

**Contents:**
- Building Blocks
- Advanced Patterns

Source: https://docs.agno.com/basics/workflows/workflow-patterns/overview

Master deterministic workflow patterns including sequential, parallel, conditional, and looping execution for reliable multi-agent automation.

Build deterministic, production-ready workflows that orchestrate agents, teams, and functions with predictable execution patterns. This comprehensive guide covers all workflow types, from simple sequential processes to complex branching logic with parallel execution and dynamic routing.

Unlike free-form agent interactions, these patterns provide structured automation with consistent, repeatable results ideal for production systems.

The core building blocks of Agno Workflows are:

| Component     | Purpose                         |
| ------------- | ------------------------------- |
| **Step**      | Basic execution unit            |
| **Agent**     | AI assistant with specific role |
| **Team**      | Coordinated group of agents     |
| **Function**  | Custom Python logic             |
| **Parallel**  | Concurrent execution            |
| **Condition** | Conditional execution           |
| **Loop**      | Iterative execution             |
| **Router**    | Dynamic routing                 |

Agno Workflows support multiple execution patterns that can be combined to build sophisticated automation systems.
Each pattern serves specific use cases and can be composed together for complex workflows.

<CardGroup cols={3}>
  <Card title="Sequential Workflows" icon="arrow-right" href="/basics/workflows/workflow-patterns/sequential">
    Linear execution with step-by-step processing
  </Card>

<Card title="Parallel Workflows" icon="arrows-split-up-and-left" href="/basics/workflows/workflow-patterns/parallel-workflow">
    Concurrent execution for independent tasks
  </Card>

<Card title="Conditional Workflows" icon="code-branch" href="/basics/workflows/workflow-patterns/conditional-workflow">
    Branching logic based on conditions
  </Card>

<Card title="Iterative Workflows" icon="rotate" href="/basics/workflows/workflow-patterns/iterative-workflow">
    Loop-based execution with quality controls
  </Card>

<Card title="Branching Workflows" icon="sitemap" href="/basics/workflows/workflow-patterns/branching-workflow">
    Dynamic routing and path selection
  </Card>

<Card title="Grouped Steps" icon="layer-group" href="/basics/workflows/workflow-patterns/grouped-steps-workflow">
    Reusable step sequences and modular design
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Function-Based Workflows" icon="function" href="/basics/workflows/workflow-patterns/custom-function-step-workflow">
    Pure Python workflows with complete control
  </Card>

<Card title="Multi-Pattern Combinations" icon="puzzle-piece" href="/basics/workflows/workflow-patterns/advanced-workflow-patterns">
    Complex workflows combining multiple patterns
  </Card>
</CardGroup>

---

## - "What's the most upvoted story today?"

**URL:** llms-txt#--"what's-the-most-upvoted-story-today?"

**Contents:**
- Usage

agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)
bash  theme={null}
    pip install openai httpx agno
    bash  theme={null}
    python custom_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Sequence of functions and agents

**URL:** llms-txt#sequence-of-functions-and-agents

Source: https://docs.agno.com/basics/workflows/usage/sequence-of-functions-and-agents

This example demonstrates how to use a sequence of functions and agents in a workflow.

This example demonstrates **Workflows** combining custom functions with agents and teams
in a sequential execution pattern. This shows how to mix different component types for
maximum flexibility in your workflow design.

**When to use**: Linear processes where you need custom data preprocessing between AI agents,
or when combining multiple component types (functions, agents, teams) in sequence.

```python sequence_of_functions_and_agents.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## Member-Level History

**URL:** llms-txt#member-level-history

**Contents:**
- Ask question in German
- Follow up in German
- Ask question in Spanish
- Follow up in Spanish
- Usage

Source: https://docs.agno.com/basics/chat-history/team/usage/history-of-members

This example demonstrates a team where each member has access to its own history through `add_history_to_context=True` set on individual agents.

Unlike team-level history, each member only has access to its own conversation history, not the history of other members or the team.

Use member-level history when:

* Each member handles distinct, independent tasks
* You don't need cross-member context sharing
* Members should maintain isolated conversation threads
* You want to minimize context size for each member

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Video Caption Agent

**URL:** llms-txt#video-caption-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/video/usage/video-caption

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## To equipt our Agent with our tool, we simply pass it with the tools parameter

**URL:** llms-txt#to-equipt-our-agent-with-our-tool,-we-simply-pass-it-with-the-tools-parameter

agent = Agent(
    model=OpenAIChat(id="gpt-5-nano"),
    tools=[get_weather],
    markdown=True,
)

---

## Usage

**URL:** llms-txt#usage

```python aws_bedrock_embedder.py theme={null}
import asyncio
from agno.knowledge.embedder.aws_bedrock import AwsBedrockEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Singlestore for Team

**URL:** llms-txt#singlestore-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/singlestore/usage/singlestore-for-team

Agno supports using Singlestore as a storage backend for Teams using the `SingleStoreDb` class.

Obtain the credentials for Singlestore from [here](https://portal.singlestore.com/)

```python singlestore_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""
from os import getenv
from typing import List

from agno.agent import Agent
from agno.db.singlestore.singlestore import SingleStoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## CSV Reader Async

**URL:** llms-txt#csv-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/csv-reader-async

The **CSV Reader** with asynchronous processing allows you to handle CSV files and integrate them with knowledge bases efficiently.

```python examples/basics/knowledge/readers/csv_reader_async.py theme={null}
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
    max_results=5,  # Number of results to return on search
)

---

## Traditional RAG approach

**URL:** llms-txt#traditional-rag-approach

**Contents:**
- Ready to Build?

agent = Agent(
    knowledge=knowledge,
    add_knowledge_to_context=True  # Always includes knowledge
)
```

Now that you understand how Knowledge works in Agno, here's where to go next:

<CardGroup cols={3}>
  <Card title="Getting Started Guide" icon="rocket" href="/basics/knowledge/getting-started">
    Follow our step-by-step tutorial to create your first knowledge base in minutes
  </Card>

<Card title="Performance Quick Wins" icon="gauge" href="/basics/knowledge/performance-tips">
    Optimize search quality, speed, and resource usage for production
  </Card>
</CardGroup>

---

## Tool Decorator

**URL:** llms-txt#tool-decorator

Source: https://docs.agno.com/reference/tools/decorator

Reference for the @tool decorator.

| Parameter               | Type             | Description                                                       |
| ----------------------- | ---------------- | ----------------------------------------------------------------- |
| `name`                  | `str`            | Override for the function name                                    |
| `description`           | `str`            | Override for the function description                             |
| `stop_after_tool_call`  | `bool`           | If True, the agent will stop after the function call              |
| `tool_hooks`            | `list[Callable]` | List of hooks that wrap the function execution                    |
| `pre_hook`              | `Callable`       | Hook to run before the function is executed                       |
| `post_hook`             | `Callable`       | Hook to run after the function is executed                        |
| `requires_confirmation` | `bool`           | If True, requires user confirmation before execution              |
| `requires_user_input`   | `bool`           | If True, requires user input before execution                     |
| `user_input_fields`     | `list[str]`      | List of fields that require user input                            |
| `external_execution`    | `bool`           | If True, the tool will be executed outside of the agent's control |
| `cache_results`         | `bool`           | If True, enable caching of function results                       |
| `cache_dir`             | `str`            | Directory to store cache files                                    |
| `cache_ttl`             | `int`            | Time-to-live for cached results in seconds (default: 3600)        |

---

## Stream Message

**URL:** llms-txt#stream-message

Source: https://docs.agno.com/reference-api/schema/a2a/stream-message

post /a2a/message/stream
Stream a message to an Agno Agent, Team, or Workflow.The Agent, Team or Workflow is identified via the 'agentId' field in params.message or X-Agent-ID header. Optional: Pass user ID via X-User-ID header (recommended) or 'userId' in params.message.metadata. Returns real-time updates as newline-delimited JSON (NDJSON).

---

## Including and excluding tools

**URL:** llms-txt#including-and-excluding-tools

**Contents:**
- Example

Source: https://docs.agno.com/basics/tools/selecting-tools

Learn how to include and exclude tools from a Toolkit.

You can specify which tools to include or exclude from a `Toolkit` by using the `include_tools` and `exclude_tools` parameters. This can be very useful to limit the number of tools that are available to an Agent.

For example, here's how to include only the `get_latest_emails` tool in the `GmailTools` toolkit:

Similarly, here's how to exclude the `create_draft_email` tool from the `GmailTools` toolkit:

Here's an example of how to use the `include_tools` and `exclude_tools` parameters to limit the number of tools that are available to an Agent:

**Examples:**

Example 1 (unknown):
```unknown
Similarly, here's how to exclude the `create_draft_email` tool from the `GmailTools` toolkit:
```

Example 2 (unknown):
```unknown
## Example

Here's an example of how to use the `include_tools` and `exclude_tools` parameters to limit the number of tools that are available to an Agent:
```

---

## Get the final app

**URL:** llms-txt#get-the-final-app

app = agent_os.get_app()

---

## LangDB

**URL:** llms-txt#langdb

**Contents:**
- Integrating Agno with LangDB
- Prerequisites
- Sending Traces to LangDB
  - Example: Basic Agent Setup

Source: https://docs.agno.com/integrations/observability/langdb

Integrate Agno with LangDB to trace agent execution, tool calls, and gain comprehensive observability into your agent's performance.

## Integrating Agno with LangDB

[LangDB](https://langdb.ai/) provides an AI Gateway platform for comprehensive observability and tracing of AI agents and LLM interactions. By integrating Agno with LangDB, you can gain full visibility into your agent's operations, including agent runs, tool calls, team interactions, and performance metrics.

For detailed integration instructions, see the [LangDB Agno documentation](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-agno).

<Frame caption="LangDB Finance Team Trace">
  <img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=12aaf8fd6e3e9ce0dcca4e7bd0da9c43" style={{ borderRadius: '10px', width: '100%', maxWidth: '800px' }} alt="langdb-agno finance team observability" data-og-width="1623" width="1623" data-og-height="900" height="900" data-path="images/langdb-finance-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=44bb9cf4c423a327b5459917cd3562cb 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a2589678cacdac15c3b5c8dc21903189 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b3aeeed6a9e129f4465d41d2e9e75929 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=d803a20ad4a20d1871212d0c23156624 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3f5eb5fa7780f3c740331550747b190b 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a397f1a8bb1d2e1ef366866ec6484a04 2500w" />
</Frame>

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup LangDB Account**

* Sign up for an account at [LangDB](https://app.langdb.ai/signup)
   * Create a new project in the LangDB dashboard
   * Obtain your API key and Project ID from the project settings

3. **Set Environment Variables**

Configure your environment with the LangDB credentials:

## Sending Traces to LangDB

### Example: Basic Agent Setup

This example demonstrates how to instrument your Agno agent with LangDB tracing.

```python  theme={null}
from pylangdb.agno import init

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup LangDB Account**

   * Sign up for an account at [LangDB](https://app.langdb.ai/signup)
   * Create a new project in the LangDB dashboard
   * Obtain your API key and Project ID from the project settings

3. **Set Environment Variables**

   Configure your environment with the LangDB credentials:
```

Example 2 (unknown):
```unknown
## Sending Traces to LangDB

### Example: Basic Agent Setup

This example demonstrates how to instrument your Agno agent with LangDB tracing.
```

---

## Sqlite for Agent

**URL:** llms-txt#sqlite-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/sqlite/usage/sqlite-for-agent

Agno supports using Sqlite as a storage backend for Agents using the `SqliteDb` class.

You need to provide either `db_url`, `db_file` or `db_engine`. The following example uses `db_file`.

```python sqlite_for_agent.py theme={null}

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Stock price and analyst data agent with structured output

**URL:** llms-txt#stock-price-and-analyst-data-agent-with-structured-output

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-5-mini"),
    output_schema=StockAnalysis,
    role="Searches the web for information on a stock.",
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com", "wsj.com"],
            text=False,
            show_results=True,
            highlights=False,
        )
    ],
)

---

## Toolkit

**URL:** llms-txt#toolkit

**Contents:**
- Import
- Constructor
  - Parameters
- Usage Examples
  - Basic Toolkit
  - Toolkit with Instructions

Source: https://docs.agno.com/reference/tools/toolkit

Reference for the Toolkit class.

The `Toolkit` class provides a way to group and manage multiple tools (functions) together. It handles tool registration, filtering, caching, and execution control.

| Parameter                           | Type             | Description                                                                                        |
| ----------------------------------- | ---------------- | -------------------------------------------------------------------------------------------------- |
| `name`                              | `str`            | A descriptive name for the toolkit.                                                                |
| `tools`                             | `List[Callable]` | List of callable functions to include in the toolkit.                                              |
| `instructions`                      | `str`            | Instructions for using the toolkit. Can be added to agent context.                                 |
| `add_instructions`                  | `bool`           | Whether to add toolkit instructions to the agent's context.                                        |
| `include_tools`                     | `list[str]`      | List of tool names to include from the toolkit. If specified, only these tools will be registered. |
| `exclude_tools`                     | `list[str]`      | List of tool names to exclude from the toolkit. These tools will not be registered.                |
| `requires_confirmation_tools`       | `list[str]`      | List of tool names that require user confirmation before execution.                                |
| `external_execution_required_tools` | `list[str]`      | List of tool names that will be executed outside of the agent loop.                                |
| `stop_after_tool_call_tools`        | `List[str]`      | List of tool names that should stop the agent after execution.                                     |
| `show_result_tools`                 | `List[str]`      | List of tool names whose results should be shown to the user.                                      |
| `cache_results`                     | `bool`           | Enable in-memory caching of function results.                                                      |
| `cache_ttl`                         | `int`            | Time-to-live for cached results in seconds (default: 1 hour).                                      |
| `cache_dir`                         | `str`            | Directory to store cache files. Defaults to system temp directory.                                 |
| `auto_register`                     | `bool`           | Whether to automatically register all tools in the toolkit upon initialization.                    |

### Toolkit with Instructions

**Examples:**

Example 1 (unknown):
```unknown
## Constructor

### Parameters

| Parameter                           | Type             | Description                                                                                        |
| ----------------------------------- | ---------------- | -------------------------------------------------------------------------------------------------- |
| `name`                              | `str`            | A descriptive name for the toolkit.                                                                |
| `tools`                             | `List[Callable]` | List of callable functions to include in the toolkit.                                              |
| `instructions`                      | `str`            | Instructions for using the toolkit. Can be added to agent context.                                 |
| `add_instructions`                  | `bool`           | Whether to add toolkit instructions to the agent's context.                                        |
| `include_tools`                     | `list[str]`      | List of tool names to include from the toolkit. If specified, only these tools will be registered. |
| `exclude_tools`                     | `list[str]`      | List of tool names to exclude from the toolkit. These tools will not be registered.                |
| `requires_confirmation_tools`       | `list[str]`      | List of tool names that require user confirmation before execution.                                |
| `external_execution_required_tools` | `list[str]`      | List of tool names that will be executed outside of the agent loop.                                |
| `stop_after_tool_call_tools`        | `List[str]`      | List of tool names that should stop the agent after execution.                                     |
| `show_result_tools`                 | `List[str]`      | List of tool names whose results should be shown to the user.                                      |
| `cache_results`                     | `bool`           | Enable in-memory caching of function results.                                                      |
| `cache_ttl`                         | `int`            | Time-to-live for cached results in seconds (default: 1 hour).                                      |
| `cache_dir`                         | `str`            | Directory to store cache files. Defaults to system temp directory.                                 |
| `auto_register`                     | `bool`           | Whether to automatically register all tools in the toolkit upon initialization.                    |

## Usage Examples

### Basic Toolkit
```

Example 2 (unknown):
```unknown
### Toolkit with Instructions
```

---

## PDF Reader

**URL:** llms-txt#pdf-reader

Source: https://docs.agno.com/reference/knowledge/reader/pdf

PDFReader is a reader class that allows you to read data from PDF files.

<Snippet file="pdf-reader-reference.mdx" />

---

## Audio Sentiment Analysis Agent

**URL:** llms-txt#audio-sentiment-analysis-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/audio/usage/audio-sentiment-analysis

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setup in-memory database

**URL:** llms-txt#setup-in-memory-database

---

## ChromaDB Async

**URL:** llms-txt#chromadb-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/chroma/usage/async-chroma-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize LanceDB vector database

**URL:** llms-txt#initialize-lancedb-vector-database

---

## Single Step Continuous Execution Workflow

**URL:** llms-txt#single-step-continuous-execution-workflow

Source: https://docs.agno.com/basics/chat-history/workflow/usage/single-step-continuous-execution-workflow

This example demonstrates a workflow with a single step that is executed continuously with access to workflow history.

This example shows how to use the `add_workflow_history_to_steps` flag to add workflow history to all the steps in the workflow.

In this case we have a single step workflow with a single agent.

The agent has access to the workflow history and uses it to provide personalized educational support.

---

## Team Input as Messages List

**URL:** llms-txt#team-input-as-messages-list

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/input-as-messages-list

This example demonstrates how to provide input to a team as a list of Message objects, creating a conversation context with multiple user and assistant messages for more complex interactions.

```python cookbook/examples/teams/basic/input_as_messages_list.py theme={null}
from agno.agent import Agent
from agno.models.message import Message
from agno.team import Team

---

## Create the company information gathering team

**URL:** llms-txt#create-the-company-information-gathering-team

**Contents:**
- Usage

company_info_team = Team(
    name="Company Info Team",
    id=id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[AgentQLTools(agentql_query=custom_query)],
    members=[
        wikipedia_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        company_info_team.aprint_response(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
            stream=True,
        )
    )
bash  theme={null}
    pip install agno wikipedia ddgs agentql
    bash  theme={null}
    export OPENAI_API_KEY=****
    export ANTHROPIC_API_KEY=****
    export MISTRAL_API_KEY=****
    export AGENTQL_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/tools/03_async_team_with_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Fast processing for simple content

**URL:** llms-txt#fast-processing-for-simple-content

fast_chunking = FixedSizeChunking(
    chunk_size=800,
    overlap=80
)

---

## MySQL for Agent

**URL:** llms-txt#mysql-for-agent

**Contents:**
- Usage
  - Run MySQL
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/mysql/usage/mysql-for-agent

Agno supports using MySQL as a storage backend for Agents using the `MySQLDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MySQL** on port **3306** using:

<Snippet file="db-mysql-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/mysql/mysql_storage_for_agent.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Remove Content

**URL:** llms-txt#remove-content

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/getting-started/usage/remove-content

```python 09_remove_content.py theme={null}
import asyncio
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## Team Events Monitoring

**URL:** llms-txt#team-events-monitoring

Source: https://docs.agno.com/basics/teams/usage/basic-flows/stream-events

This example demonstrates how to monitor and handle different types of events during team execution, including tool calls, run states, and content generation events.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/streaming" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Enable tracing (call this ONCE at startup)

**URL:** llms-txt#enable-tracing-(call-this-once-at-startup)

setup_tracing(db=tracing_db)

---

## Agent with Knowledge Tracing

**URL:** llms-txt#agent-with-knowledge-tracing

Source: https://docs.agno.com/agent-os/tracing/usage/agent-with-knowledge-tracing

Learn how to trace your agents with knowledge with Agno in AgentOS

This example shows how to trace an agent with a knowledge base in AgentOS. The traces capture knowledge searches, model calls, and all agent operations.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    python
            from agno.agent import Agent
            from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(tools=[DuckDuckGoTools()])

# Perform a web search and capture the response
            response = agent.run("What's happening in France?")
    `
    </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    `
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Start PostgreSQL with pgvector">
    Make sure you have PostgreSQL running with pgvector extension. You can use Docker:

<Step title="Run AgentOS">
    <CodeGroup>

Your AgentOS will be available at `http://localhost:7777`. View traces in the AgentOS dashboard.
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (python):
```python
from agno.agent import Agent
            from agno.tools.duckduckgo import DuckDuckGoTools

            agent = Agent(tools=[DuckDuckGoTools()])

            # Perform a web search and capture the response
            response = agent.run("What's happening in France?")
```

Example 3 (unknown):
```unknown
</Step>

    <Snippet file="create-venv-step.mdx" />

    <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

---

## Cost increase: 8x more expensive!

**URL:** llms-txt#cost-increase:-8x-more-expensive!

**Contents:**
- Mitigation Strategy #1: Use Automatic Memory

**Examples:**

Example 1 (unknown):
```unknown
As memories accumulate, each memory operation gets more expensive. With 200 memories, a single memory update could consume 10,000+ tokens just loading context.

## Mitigation Strategy #1: Use Automatic Memory

For most use cases, automatic memory is your best betâ€”it's significantly more efficient:
```

---

## Verify Webhook

**URL:** llms-txt#verify-webhook

Source: https://docs.agno.com/reference-api/schema/whatsapp/verify-webhook

get /whatsapp/webhook
Handle WhatsApp webhook verification

---

## Setup Redis

**URL:** llms-txt#setup-redis

---

## Filtering on LanceDB

**URL:** llms-txt#filtering-on-lancedb

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-lance-db

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in LanceDB.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## Newspaper4k

**URL:** llms-txt#newspaper4k

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/newspaper4k

**Newspaper4k** enables an Agent to read news articles using the Newspaper4k library.

The following example requires the `newspaper4k` and `lxml_html_clean` libraries.

The following agent will summarize the article: [https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime](https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime).

| Parameter             | Type   | Default | Description                                                                        |
| --------------------- | ------ | ------- | ---------------------------------------------------------------------------------- |
| `enable_read_article` | `bool` | `True`  | Enables the functionality to read the full content of an article.                  |
| `include_summary`     | `bool` | `False` | Specifies whether to include a summary of the article along with the full content. |
| `article_length`      | `int`  | -       | The maximum length of the article or its summary to be processed or returned.      |

| Function           | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| `get_article_data` | This function reads the full content and data of an article. |
| `read_article`     | This function reads the full content of an article.          |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/newspaper4k.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/newspaper4k_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will summarize the article: [https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime](https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime).
```

---

## Run the example

**URL:** llms-txt#run-the-example

**Contents:**
- How MCPToolbox Works

asyncio.run(main())
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## How MCPToolbox Works

MCPToolbox solves the **tool overload problem**. Without filtering, your agent gets overwhelmed with too many database tools:

**Without MCPToolbox (50+ tools):**
```

---

## Workflows Use Cases

**URL:** llms-txt#workflows-use-cases

**Contents:**
- Getting Started
- Featured Examples

Source: https://docs.agno.com/examples/use-cases/workflows/overview

Explore Agno's Workflow use cases showcasing what advanced Workflows have to offer.

Welcome to Agno's Workflow use cases! Here you'll discover practical examples of multi-phase workflows that orchestrate complex business processes. You can either:

* Run the examples individually
* Clone the entire [Agno cookbook](https://github.com/agno-agi/agno/tree/main/cookbook)

Have an interesting example to share? Please consider [contributing](https://github.com/agno-agi/agno-docs) to our growing collection.

If you're just getting started, follow the [Getting Started](/examples/getting-started) guide for a step-by-step tutorial. The examples build on each other, introducing new concepts and capabilities progressively.

Explore these popular workflow examples to see what's possible with Agno:

<CardGroup cols={3}>
  <Card title="Startup Idea Validator" icon="lightbulb" iconType="duotone" href="/examples/use-cases/workflows/startup-idea-validator">
    Evaluate business ideas through market research and competitive analysis.
  </Card>

<Card title="Notion Knowledge Manager" icon="database" iconType="duotone" href="/examples/use-cases/workflows/notion-knowledge-manager">
    Automatically create, update, and organize content in Notion databases.
  </Card>

<Card title="Blog Post Generator" icon="blog" iconType="duotone" href="/examples/use-cases/workflows/blog-post-generator">
    Create high-quality blog posts with web research and professional writing.
  </Card>

<Card title="Investment Report Generator" icon="chart-line" iconType="duotone" href="/examples/use-cases/workflows/investment-report-generator">
    Analyze stocks and generate strategic portfolio allocation recommendations.
  </Card>

<Card title="Employee Recruiter" icon="user-tie" iconType="duotone" href="/examples/use-cases/workflows/employee-recruiter">
    Automate resume screening, interview scheduling, and email communication.
  </Card>

<Card title="Company Description" icon="building" iconType="duotone" href="/examples/use-cases/workflows/company-description">
    Generate comprehensive supplier profiles with web crawling and analysis.
  </Card>
</CardGroup>

---

## Step 2: Configure tools

**URL:** llms-txt#step-2:-configure-tools

tools = [
    XTools(
        include_post_metrics=True,
        wait_on_rate_limit=True,
    ),
    ExaTools(
        num_results=10,
        include_domains=["reddit.com", "news.ycombinator.com", "medium.com"],
        exclude_domains=["spam-site.com"],
    )
]

---

## Example 1: List all Lambda functions

**URL:** llms-txt#example-1:-list-all-lambda-functions

agent.print_response("List all Lambda functions in our AWS account", markdown=True)

---

## Response Coordinator Agent - Specialized in synthesis and coordination

**URL:** llms-txt#response-coordinator-agent---specialized-in-synthesis-and-coordination

response_coordinator = Agent(
    name="Response Coordinator",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Coordinate team findings into comprehensive reasoned response",
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Synthesize all team member contributions into a coherent response.",
        "Ensure logical flow and consistency across the response.",
        "Include proper citations and evidence references.",
        "Present reasoning chains clearly and transparently.",
        "Use reasoning tools to structure the final response.",
    ],
    markdown=True,
)

---

## Debug search quality

**URL:** llms-txt#debug-search-quality

**Contents:**
  - Issue: Content Loading is Slow

results = knowledge.search("your query", max_results=10)
if not results:
    content_list, count = knowledge.get_content()
    print(f"Total content items: {count}")
    
    # Check for failed content
    for content in content_list[:5]:
        status, message = knowledge.get_content_status(content.id)
        print(f"{content.name}: {status}")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Issue: Content Loading is Slow

**What's happening:** Processing large files without batching, or using semantic chunking on huge datasets.

**Quick fixes:**

1. Use `skip_if_exists=True` to avoid reprocessing
2. Switch to fixed-size chunking for faster processing
3. Process in batches instead of all at once
4. Use file filters to only process what you need
```

---

## Database Tables

**URL:** llms-txt#database-tables

**Contents:**
- Table Definition
- Create a database revision
- Migrate dev database
  - Optional: Add test user
- Migrate production database
  - Update the `workspace/prd_resources.py` file

Source: https://docs.agno.com/templates/infra-management/database-tables

Agno templates come pre-configured with [SqlAlchemy](https://www.sqlalchemy.org/) and [Alembic](https://alembic.sqlalchemy.org/en/latest/) to manage databases. You can use these tables to store data for your Agents, Teams and Workflows. The general way to add a table is:

1. Add table definition to the `db/tables` directory.
2. Import the table class in the `db/tables/__init__.py` file.
3. Create a database migration.
4. Run database migration.

Let's create a `UsersTable`, copy the following code to `db/tables/user.py`

Update the `db/tables/__init__.py` file:

## Create a database revision

Run the alembic command to create a database migration in the dev container:

## Migrate dev database

Run the alembic command to migrate the dev database:

### Optional: Add test user

Now lets's add a test user. Copy the following code to `db/tables/test_add_user.py`

Run the script to add a test adding a user:

## Migrate production database

We recommended migrating the production database by setting the environment variable `MIGRATE_DB = True` and restarting the production service. This runs `alembic -c db/alembic.ini upgrade head` from the entrypoint script at container startup.

### Update the `workspace/prd_resources.py` file

```python workspace/prd_resources.py theme={null}
...

**Examples:**

Example 1 (unknown):
```unknown
Update the `db/tables/__init__.py` file:
```

Example 2 (unknown):
```unknown
## Create a database revision

Run the alembic command to create a database migration in the dev container:
```

Example 3 (unknown):
```unknown
## Migrate dev database

Run the alembic command to migrate the dev database:
```

Example 4 (unknown):
```unknown
### Optional: Add test user

Now lets's add a test user. Copy the following code to `db/tables/test_add_user.py`
```

---

## Mem0

**URL:** llms-txt#mem0

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/mem0

Mem0Tools provides intelligent memory management capabilities for agents using the Mem0 memory platform.

The following agent can store and retrieve memories using Mem0:

| Parameter                    | Type             | Default | Description                                     |
| ---------------------------- | ---------------- | ------- | ----------------------------------------------- |
| `config`                     | `Optional[Dict]` | `None`  | Mem0 configuration dictionary.                  |
| `api_key`                    | `Optional[str]`  | `None`  | Mem0 API key. Uses MEM0\_API\_KEY if not set.   |
| `user_id`                    | `Optional[str]`  | `None`  | User ID for memory operations.                  |
| `org_id`                     | `Optional[str]`  | `None`  | Organization ID. Uses MEM0\_ORG\_ID if not set. |
| `project_id`                 | `Optional[str]`  | `None`  | Project ID. Uses MEM0\_PROJECT\_ID if not set.  |
| `infer`                      | `bool`           | `True`  | Enable automatic memory inference.              |
| `enable_add_memory`          | `bool`           | `True`  | Enable memory addition functionality.           |
| `enable_search_memory`       | `bool`           | `True`  | Enable memory search functionality.             |
| `enable_get_all_memories`    | `bool`           | `True`  | Enable retrieving all memories functionality.   |
| `enable_delete_all_memories` | `bool`           | `True`  | Enable memory deletion functionality.           |

| Function              | Description                                   |
| --------------------- | --------------------------------------------- |
| `add_memory`          | Store new memories or information.            |
| `search_memory`       | Search through stored memories using queries. |
| `get_all_memories`    | Retrieve all stored memories for the user.    |
| `delete_all_memories` | Delete all stored memories for the user.      |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/mem0.py)
* [Mem0 Documentation](https://docs.mem0.ai/)
* [Mem0 Platform](https://mem0.ai/)

---

## For async workflow execution

**URL:** llms-txt#for-async-workflow-execution

workflow_tools = WorkflowTools(
    workflow=my_async_workflow,
    async_mode=True,  # This will use async versions of the tools
    enable_run_workflow=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[workflow_tools],
)

await agent.arun(...)
```

---

## Create agent with both Google Search and URL context enabled

**URL:** llms-txt#create-agent-with-both-google-search-and-url-context-enabled

agent = Agent(
    model=Gemini(id="gemini-2.5-flash", search=True, url_context=True),
    markdown=True,
)

---

## Advanced Filtering

**URL:** llms-txt#advanced-filtering

**Contents:**
- Filter Expression Operators
  - Comparison Operators

Source: https://docs.agno.com/basics/knowledge/filters/advanced-filtering

Use filter expressions (EQ, AND, OR, NOT) for complex logical filtering of knowledge base searches.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.12" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.12">v2.2.12</Tooltip>
</Badge>

When basic dictionary filters aren't enough, filter expressions give you powerful logical control over knowledge searches. Use them to combine multiple conditions with AND/OR logic, exclude content with NOT, or perform comparisons like "greater than" and "less than".

For basic filtering with dictionary format, see [Search & Retrieval](/basics/knowledge/search-and-retrieval/overview).

## Filter Expression Operators

Agno provides a rich set of filter expressions that can be combined to create sophisticated search criteria:

### Comparison Operators

These operators let you match against specific values:

Match content where a metadata field equals a specific value.

```python  theme={null}
from agno.filters import EQ

---

## Example 2: Parallel Primitive with Event Storage

**URL:** llms-txt#example-2:-parallel-primitive-with-event-storage

print("=== 2. Parallel Example ===")
parallel_workflow = Workflow(
    name="Parallel Research Workflow",
    steps=[
        Parallel(
            Step(name="News Research", agent=news_agent),
            Step(name="Web Search", agent=search_agent),
            name="Parallel Research",
        ),
        Step(name="Combine Results", agent=analysis_agent),
    ],
    db=SqliteDb(
        session_table="workflow_parallel",
        db_file="tmp/workflow_parallel.db",
    ),
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.parallel_execution_started,
        WorkflowRunEvent.parallel_execution_completed,
    ],
)

print("Running Parallel workflow...")
for event in parallel_workflow.run(
    input="Research machine learning developments",
    stream=True,
    stream_events=True,
):
    # Filter out RunContentEvent from printing
    if not isinstance(event, RunContentEvent):
        print(
            f"Event: {event.event if hasattr(event, 'event') else type(event).__name__}"
        )

run_response = parallel_workflow.get_last_run_output()
print(f"Parallel workflow stored {len(run_response.events)} events")
print_stored_events(run_response, "Parallel Workflow")
print()
```

---

## Agent 2 with its own database

**URL:** llms-txt#agent-2-with-its-own-database

search_agent = Agent(
    name="Search Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    db=agent2_db,
)

---

## Input & Output

**URL:** llms-txt#input-&-output

**Contents:**
- Structured Output
  - Agent Example
  - Team Example
  - Workflow Example
- Structured Input
  - Agent Example
  - Team Example
  - Workflow Example
- Typesafe Patterns
- Advanced Features

Source: https://docs.agno.com/basics/input-output/overview

Learn how to control inputs and outputs with type-safe Pydantic models for reliable, production-ready systems.

Agno Agents, Teams and Workflows support various forms of input and output, from simple string-based interactions to structured data validation using Pydantic models.

Generate validated, type-safe outputs using Pydantic models for reliable production systems. Structured outputs provide consistent, predictable response formats instead of unstructured text.

Common use cases include data extraction, classification, content generation, and validation.

<Info>
  Structured outputs work seamlessly with tools, knowledge bases, and most other Agno features.
</Info>

<Tip>
  For models that don't support structured output, Agno provides JSON mode, which instructs the model to respond in JSON format. While less accurate than native structured output, JSON mode offers a fallback option for compatible models. Set `use_json_mode=True` on your Agent, Team, or Workflow.
</Tip>

Agents, Teams, and Workflows support structured input using Pydantic models or `TypedDict`. Pass them to the `input` parameter in `run()` or `print_response()`.

When you combine both `input_schema` and `output_schema`, you create a **typesafe** component with end-to-end type safetyâ€”a fully validated data pipeline from input to output.

* **[Parser Model](/basics/input-output/agent/usage/parser-model)**: Use a different model to parse and structure output from your primary model
* **[Output Model](/basics/input-output/agent/usage/output-model)**: Use a different model to produce the final output
* **[Input Validation](/basics/input-output/agent/usage/input-schema-on-agent)**: Set `input_schema` to automatically validate dictionary inputs
* **[Streaming](/basics/input-output/team/usage/structured-output-streaming)**: Structured outputs work seamlessly with streaming

<CardGroup cols={3}>
  <Card title="Agent Usage Examples" icon="robot" iconType="duotone" href="/basics/input-output/agent/usage">
    Explore detailed examples for structured input and output with Agents.
  </Card>

<Card title="Team Usage Examples" icon="users" iconType="duotone" href="/basics/input-output/team/usage">
    Explore detailed examples for structured input and output with Teams.
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
### Team Example
```

Example 2 (unknown):
```unknown
### Workflow Example
```

Example 3 (unknown):
```unknown
<Tip>
  For models that don't support structured output, Agno provides JSON mode, which instructs the model to respond in JSON format. While less accurate than native structured output, JSON mode offers a fallback option for compatible models. Set `use_json_mode=True` on your Agent, Team, or Workflow.
</Tip>

## Structured Input

Agents, Teams, and Workflows support structured input using Pydantic models or `TypedDict`. Pass them to the `input` parameter in `run()` or `print_response()`.

### Agent Example
```

Example 4 (unknown):
```unknown
### Team Example
```

---

## Parallel

**URL:** llms-txt#parallel

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/search/parallel

Use Parallel with Agno for AI-optimized web search and content extraction.

**ParallelTools** enable an Agent to perform AI-optimized web search and content extraction using [Parallel's APIs](https://docs.parallel.ai/home).

The following example requires the `parallel-web` library and an API key which can be obtained from [Parallel](https://platform.parallel.ai).

The following agent will search for information on AI agents and autonomous systems, then extract content from specific URLs:

```python cookbook/tools/parallel_tools.py theme={null}
from agno.agent import Agent
from agno.tools.parallel import ParallelTools

agent = Agent(
    tools=[
        ParallelTools(
            enable_search=True,
            enable_extract=True,
            max_results=5,
            max_chars_per_result=8000,
        )
    ],
    markdown=True,
)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search for information on AI agents and autonomous systems, then extract content from specific URLs:
```

---

## "I'd like to start with a soup, then im thinking a thai curry for the main course and finish with a dessert",

**URL:** llms-txt#"i'd-like-to-start-with-a-soup,-then-im-thinking-a-thai-curry-for-the-main-course-and-finish-with-a-dessert",

---

## Create a knowledge retriever from the vector store

**URL:** llms-txt#create-a-knowledge-retriever-from-the-vector-store

**Contents:**
- Usage

knowledge_retriever = db.as_retriever()

knowledge = Knowledge(
    vector_db=LangChainVectorDb(knowledge_retriever=knowledge_retriever)
)

agent = Agent(knowledge=knowledge)

agent.print_response("What did the president say?", markdown=True)
bash  theme={null}
    pip install -U langchain langchain-community langchain-openai langchain-chroma pypdf openai agno
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/knowledge/vector_db/langchain/langchain_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/langchain/langchain_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create a team

**URL:** llms-txt#create-a-team

research_team = Team(
    name="Research Team",
    description="A team of agents that research the web",
    members=[research_agent, simple_agent],
    model=OpenAIChat(id="gpt-5-mini"),
    id="research_team",
    instructions=[
        "You are the lead researcher of a research team! ðŸ”",
    ],
    db=db,
    enable_user_memories=True,
    add_datetime_to_context=True,
    markdown=True,
)

---

## Create an agent with Notion Tools

**URL:** llms-txt#create-an-agent-with-notion-tools

**Contents:**
- Toolkit Functions
- Toolkit Params
- Developer Resources

notion_agent = Agent(
    name="Notion Knowledge Manager",
    instructions=[
        "You are a smart assistant that helps organize information in Notion.",
        "When given content, analyze it and categorize it appropriately.",
        "Available categories: travel, tech, general-blogs, fashion, documents",
        "Always search first to avoid duplicate pages with the same tag.",
        "Be concise and helpful in your responses.",
    ],
    tools=[NotionTools()],
    markdown=True,
)

def demonstrate_tools():
    print("  Notion Tools Demonstration\n")
    print("=" * 60)

# Example 1: Travel Notes
    print("\n Example 1: Organizing Travel Information")
    print("-" * 60)
    prompt = """
    I found this amazing travel guide: 
    'Ha Giang Loop in Vietnam - 4 day motorcycle adventure through stunning mountains.
    Best time to visit: October to March. Must-see spots include Ma Pi Leng Pass.'
    
    Save this to Notion under the travel category.
    """
    notion_agent.print_response(prompt)

# Example 2: Tech Bookmarks
    print("\n Example 2: Saving Tech Articles")
    print("-" * 60)
    prompt = """
    Save this tech article to Notion:
    'The Rise of AI Agents in 2025 - How autonomous agents are revolutionizing software development.
    Key trends include multi-agent systems, agentic workflows, and AI-powered automation.'
    
    Categorize this appropriately and add to Notion.
    """
    notion_agent.print_response(prompt)

# Example 3: Multiple Items
    print("\n Example 3: Batch Processing Multiple Items")
    print("-" * 60)
    prompt = """
    I need to save these items to Notion:
    1. 'Best fashion trends for spring 2025 - Sustainable fabrics and minimalist designs'
    2. 'My updated resume and cover letter for job applications'
    3. 'Quick thoughts on productivity hacks for remote work'
    
    Process each one and save them to the appropriate categories.
    """
    notion_agent.print_response(prompt)

# Example 4: Search and Update
    print("\nðŸ” Example 4: Finding and Updating Existing Content")
    print("-" * 60)
    prompt = """
    Search for any pages tagged 'tech' and let me know what you find.
    Then add this new insight to one of them:
    'Update: AI agents now support structured output with Pydantic models for better type safety.'
    """
    notion_agent.print_response(prompt)

# Example 5: Smart Categorization
    print("\n Example 5: Automatic Smart Categorization")
    print("-" * 60)
    prompt = """
    I have this content but I'm not sure where it belongs:
    'Exploring the ancient temples of Angkor Wat in Cambodia. The sunrise view from Angkor Wat 
    is breathtaking. Best visited during the dry season from November to March.'
    
    Analyze this content, decide the best category, and save it to Notion.
    """
    notion_agent.print_response(prompt)

print("\n" + "=" * 60)
    print(
        "\nYour Notion database now contains organized content across different categories."
    )
    print("Check your Notion workspace to see the results!")

if __name__ == "__main__":
    demonstrate_tools()
```

These are the functions exposed by `NotionTools`:

| Function                           | Description                                                               |
| ---------------------------------- | ------------------------------------------------------------------------- |
| `create_page(title, tag, content)` | Creates a new page in the Notion database with a title, tag, and content. |
| `update_page(page_id, content)`    | Adds content to an existing Notion page by appending a new paragraph.     |
| `search_pages(tag)`                | Searches for pages in the database by tag and returns matching results.   |

These parameters are passed to the `NotionTools` constructor:

| Parameter             | Type            | Default | Description                                                                             |
| --------------------- | --------------- | ------- | --------------------------------------------------------------------------------------- |
| `api_key`             | `Optional[str]` | `None`  | Notion API key (integration token). Uses `NOTION_API_KEY` env var if not provided.      |
| `database_id`         | `Optional[str]` | `None`  | The ID of the database to work with. Uses `NOTION_DATABASE_ID` env var if not provided. |
| `enable_create_page`  | `bool`          | `True`  | Enable the create\_page tool.                                                           |
| `enable_update_page`  | `bool`          | `True`  | Enable the update\_page tool.                                                           |
| `enable_search_pages` | `bool`          | `True`  | Enable the search\_pages tool.                                                          |
| `all`                 | `bool`          | `False` | Enable all tools. Overrides individual enable flags when `True`.                        |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/notion.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/notion_tools.py)

---

## Params

**URL:** llms-txt#params

| Parameter               | Type                       | Default                          | Description                                                                                                                   |
| ----------------------- | -------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| `id`                    | `str`                      | `"cohere.embed-multilingual-v3"` | The model ID to use. You need to enable this model in your AWS Bedrock model catalog.                                         |
| `dimensions`            | `int`                      | `1024`                           | The dimensionality of the embeddings generated by the model(1024 for Cohere models).                                          |
| `input_type`            | `str`                      | `"search_query"`                 | Prepends special tokens to differentiate types. Options: 'search\_document', 'search\_query', 'classification', 'clustering'. |
| `truncate`              | `Optional[str]`            | `None`                           | How to handle inputs longer than the maximum token length. Options: 'NONE', 'START', 'END'.                                   |
| `embedding_types`       | `Optional[List[str]]`      | `None`                           | Types of embeddings to return . Options: 'float', 'int8', 'uint8', 'binary', 'ubinary'.                                       |
| `aws_region`            | `Optional[str]`            | `None`                           | The AWS region to use. If not provided, falls back to AWS\_REGION env variable.                                               |
| `aws_access_key_id`     | `Optional[str]`            | `None`                           | The AWS access key ID. If not provided, falls back to AWS\_ACCESS\_KEY\_ID env variable.                                      |
| `aws_secret_access_key` | `Optional[str]`            | `None`                           | The AWS secret access key. If not provided, falls back to AWS\_SECRET\_ACCESS\_KEY env variable.                              |
| `session`               | `Optional[Session]`        | `None`                           | A boto3 Session object to use for authentication.                                                                             |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`                           | Additional parameters to pass to the API requests.                                                                            |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`                           | Additional parameters to pass to the boto3 client.                                                                            |
| `client`                | `Optional[AwsClient]`      | `None`                           | An instance of the AWS Bedrock client to use for making API requests.                                                         |

---

## from agno.models.groq import Groq

**URL:** llms-txt#from-agno.models.groq-import-groq

**Contents:**
- Usage

from agno.tools.openai import OpenAITools
from agno.utils.media import download_image
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="embed_vision_documents",
        embedder=CohereEmbedder(
            id="embed-v4.0",
        ),
    ),
)

asyncio.run(
    knowledge.add_content_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )
)

agent = Agent(
    name="EmbedVisionRAGAgent",
    model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
    tools=[OpenAITools()],
    knowledge=knowledge,
    instructions=[
        "You are a specialized recipe assistant.",
        "When asked for a recipe:",
        "1. Search the knowledge base to retrieve the relevant recipe details.",
        "2. Analyze the retrieved recipe steps carefully.",
        "3. Use the `generate_image` tool to create a visual, step-by-step image manual for the recipe.",
        "4. Present the recipe text clearly and mention that you have generated an accompanying image manual. Add instructions while generating the image.",
    ],
    markdown=True,
)

agent.print_response(
    "What is the recipe for a Thai curry?",
)
response = agent.get_last_run_output()

if response.images:
    download_image(response.images[0].url, Path("tmp/recipe_image.png"))

bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai groq cohere
    bash Mac theme={null}
      python cookbook/examples/agents/recipe_rag_image.py
      bash Windows theme={null}
      python cookbook/examples/agents/recipe_rag_image.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## - "What are the trending tech discussions on HN right now?"

**URL:** llms-txt#--"what-are-the-trending-tech-discussions-on-hn-right-now?"

---

## Second run - this will use the cached system prompt

**URL:** llms-txt#second-run---this-will-use-the-cached-system-prompt

response = agent.run(
    "What are the key principles of clean code and how do I apply them in Python?"
)
if response and response.metrics:
    print(f"Second run cache read tokens = {response.metrics.cache_read_tokens}")
```

---

## Run team and print response to the terminal

**URL:** llms-txt#run-team-and-print-response-to-the-terminal

**Contents:**
- Interactive CLI

team.print_response("What is the weather in Tokyo?", show_member_responses=True)
python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

news_agent = Agent(name="News Agent", role="Get the latest news")
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file="tmp/data.db"),
    add_history_to_context=True,
    num_history_runs=3,
)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  You can set `debug_level=2` to get even more detailed logs.
</Tip>

## Interactive CLI

Agno also comes with a pre-built interactive CLI that runs your Team as a command-line application. You can use this to test back-and-forth conversations with your team.
```

---

## Running Teams

**URL:** llms-txt#running-teams

**Contents:**
- Basic Execution

Source: https://docs.agno.com/basics/teams/running-teams

Learn how to run Agno Teams.

Run your Team by calling `Team.run()` or `Team.arun()`. Below is a flowchart that explains how the team runs:

<Accordion title="Execution Flow Diagram">
  <img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=620f30536d975bf35fd7a39e44f343bc" alt="Team execution flow" data-og-width="3636" width="3636" data-og-height="5664" height="5664" data-path="images/teams/team-details-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=e3d863ed3a9710f8f9c30c7bf2759349 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=53722384f2acbe892e9b2e6d308f97be 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=7f575c44ea568261de09a60d7a1047a9 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=532005f08da6722a638a4119000922e8 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=d631f4a778e58e2948ba88d60ea16ac1 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=715b5ca74b465e857e331b9eaf6ff2f4 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=bb04b20f1810e17d08f4f3880f7a1615" alt="Team execution flow" data-og-width="3636" width="3636" data-og-height="5664" height="5664" data-path="images/teams/team-details-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=53e911082ca802723c89443fb6e859cc 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=ad31aa1ff5078f01382e5b291725e5a4 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=2e5e07e3dc2e5e4b74b989db695489e7 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=57be6f8c58e62def20fdea637a38e257 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=0914ded32d9a5fcfc6a1a5ad59b661b7 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-details-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=f7fedcdac8556ab81f81c9f605083a1d 2500w" />
</Accordion>

1. **Pre-hooks execute** (if configured) to perform validation or setup before the run starts.
2. **Reasoning agent runs** (if enabled) to plan and break down the task.
3. **Context is built** including system message, user message, chat history, user memories, session state, and other inputs.
4. **Model is invoked** with the prepared context.
5. **Model decides** whether to respond directly, call provided tools, or delegate requests to team members.
6. **If delegation occurs**, member agents execute their tasks concurrently (in `async` mode) and return results to the team leader. The team-leader model processes these results and may delegate further or respond.
7. **Response is processed** and parsed into an [`output_schema`](/basics/input-output/overview#structured-output) if provided.
8. **Post-hooks execute** (if configured) to perform final validation or transformation of the final output.
9. **Session and metrics are stored** in the database (if configured).
10. **TeamRunOutput is returned** to the caller with the final response.

<Note>
  In the case of streaming responses, the team leader will stream responses from the model and from members to the caller. See the [Streaming](/basics/teams/running-teams#streaming) section for more details.
</Note>

The `Team.run()` function runs the team and returns the output â€” either as a `TeamRunOutput` object or as a stream of `TeamRunOutputEvent` and `RunOutputEvent` (for member agents) objects (when `stream=True`). For example:

```python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

news_agent = Agent(
    name="News Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Get the latest news",
    tools=[DuckDuckGoTools()]
)
weather_agent = Agent(
    name="Weather Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Get the weather for the next 7 days",
    tools=[DuckDuckGoTools()]
)

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o")
)

---

## Configure X Tools for social media data

**URL:** llms-txt#configure-x-tools-for-social-media-data

**Contents:**
  - 2b. Add ExaTools for Web Intelligence

x_tools = XTools(
    include_post_metrics=True,    # Critical: gets likes, retweets, replies for influence analysis
    wait_on_rate_limit=True,      # Handles API limits gracefully
)

print("XTools configured with post metrics enabled")
python  theme={null}
from agno.tools.exa import ExaTools

**Examples:**

Example 1 (unknown):
```unknown
**What `include_post_metrics=True` gives you:**

* Like counts (engagement volume)
* Retweet counts (viral spread)
* Reply counts (conversation depth)
* Author verification status (influence weighting)

### 2b. Add ExaTools for Web Intelligence

**Why ExaTools?** Social media discussions often reference broader conversations happening across the web. ExaTools finds this context.
```

---

## Create and configure your Agno agent

**URL:** llms-txt#create-and-configure-your-agno-agent

agent = Agent(
    name="A helpful AI Assistant",
    model=OpenAIChat(id="gpt-5"),
    tools=[],
    instructions="You are a helpful AI Assistant.",
    debug_mode=True,
)

---

## Add from S3 bucket

**URL:** llms-txt#add-from-s3-bucket

**Contents:**
- Usage
- Params

asyncio.run(
    knowledge.add_content_async(
        name="S3 PDF",
        remote_content=S3Content(
            bucket_name="agno-public", key="recipes/ThaiRecipes.pdf"
        ),
        metadata={"remote_content": "S3"},
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What is the best way to make a Thai curry?",
    markdown=True,
)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector boto3
    bash Mac theme={null}
      python cookbook/knowledge/basic_operations/06_from_s3.py
      bash Windows theme={null}
      python cookbook/knowledge/basic_operations/06_from_s3.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="s3-remote-content-params.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Configure AWS credentials">
    Set up your AWS credentials using one of these methods:

    * AWS CLI: `aws configure`
    * Environment variables: `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`
    * IAM roles (if running on AWS infrastructure)
  </Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Agent Input as Dictionary

**URL:** llms-txt#agent-input-as-dictionary

Source: https://docs.agno.com/basics/input-output/agent/usage/input-as-dict

This example demonstrates how to provide input to an agent as a dictionary format, specifically for multimodal inputs like text and images.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## SQLite for development

**URL:** llms-txt#sqlite-for-development

from agno.db.sqlite import SqliteDb
contents_db = SqliteDb(db_file="my_knowledge.db")

---

## Set the endpoint and headers for LangSmith

**URL:** llms-txt#set-the-endpoint-and-headers-for-langsmith

endpoint = "https://eu.api.smith.langchain.com/otel/v1/traces"
headers = {
    "x-api-key": os.getenv("LANGSMITH_API_KEY"),
    "Langsmith-Project": os.getenv("LANGSMITH_PROJECT"),
}

---

## Custom Logging

**URL:** llms-txt#custom-logging

**Contents:**
- Specifying a custom logging configuration

Source: https://docs.agno.com/basics/custom-logging

Learn how to use custom logging in your Agno setup.

You can provide your own logging configuration to Agno, to be used instead of the default ones.

This can be useful if you need your system to log in any specific format.

## Specifying a custom logging configuration

You can configure Agno to use your own logging configuration by using the `configure_agno_logging` function.

```python  theme={null}
import logging

from agno.agent import Agent
from agno.utils.log import configure_agno_logging, log_info

---

## Company Description Workflow

**URL:** llms-txt#company-description-workflow

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code Structure
  - 1. Agents (`agents.py`)
  - 2. Prompts (`prompts.py`)
  - 3. Workflow Implementation (`run_workflow.py`)
- Key Features
- Usage Example

Source: https://docs.agno.com/examples/use-cases/workflows/company-description

Build a comprehensive supplier profile generator that combines web crawling, search engines, Wikipedia, and competitor analysis. This workflow processes company information through specialized agents and delivers detailed markdown reports via email.

By building this workflow, you'll understand:

* How to coordinate multiple agents running in parallel for data gathering
* How to integrate diverse data sources (web crawling, search, Wikipedia)
* How to implement workflow session state caching for expensive operations
* How to automate report generation and email delivery with Resend

Build supplier research platforms, company intelligence systems, vendor evaluation tools, or automated due diligence reports.

The workflow coordinates four specialized agents running in parallel:

1. **Crawl**: Crawler agent uses Firecrawl to extract information from the company website
2. **Search**: Search agent uses DuckDuckGo to find recent news and articles
3. **Research**: Wikipedia agent gathers background and historical information
4. **Analyze**: Competitor agent identifies and analyzes market competitors
5. **Synthesize**: Profile agent combines all findings into a structured markdown report
6. **Deliver**: Report is automatically sent via email using Resend

The workflow caches analysis results in session state - subsequent requests for the same supplier return cached data.

This company description workflow consists of three main files:

### 1. Agents (`agents.py`)

Specialized agents for gathering information from multiple sources:

### 2. Prompts (`prompts.py`)

Detailed instructions for each specialized agent:

### 3. Workflow Implementation (`run_workflow.py`)

Complete workflow with parallel information gathering and email delivery:

* **Parallel Processing**: Four agents gather information simultaneously for maximum efficiency
* **Multi-Source Data**: Combines web crawling, search engines, Wikipedia, and competitor analysis
* **Email Integration**: Automatically sends formatted reports via email using Resend
* **Markdown Formatting**: Generates structured, readable reports in HTML format
* **Modular Design**: Clean separation of agents, prompts, and workflow logic
* **Efficient Execution**: Uses parallel steps to minimize execution time
* **Type Safety**: Pydantic models for structured data validation

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
export OPENAI_API_KEY="your-openai-key"
export RESEND_API_KEY="your-resend-key"
export FIRECRAWL_API_KEY="your-firecrawl-key"
```

Example 2 (unknown):
```unknown
pip install agno openai firecrawl-py resend
```

Example 3 (unknown):
```unknown
### 2. Prompts (`prompts.py`)

Detailed instructions for each specialized agent:
```

Example 4 (unknown):
```unknown
### 3. Workflow Implementation (`run_workflow.py`)

Complete workflow with parallel information gathering and email delivery:
```

---

## Create team with structured output streaming

**URL:** llms-txt#create-team-with-structured-output-streaming

**Contents:**
- Usage

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
    show_members_responses=True,
)

async def test_structured_streaming():
    """Test async structured output streaming."""
    # Run with streaming and consume the async generator to get the final response
    async_stream = team.arun(
        "Give me a stock report for NVDA", stream=True, stream_events=True
    )

# Consume the async streaming events and get the final response
    run_response = None
    async for event_or_response in async_stream:
        # The last item in the stream is the final TeamRunOutput
        run_response = event_or_response

assert isinstance(run_response.content, StockReport)
    print(f"âœ… Stock Symbol: {run_response.content.symbol}")
    print(f"âœ… Company Name: {run_response.content.company_name}")

async def test_structured_streaming_with_arun():
    """Test async structured output streaming using arun() method."""
    await apprint_run_response(
        team.arun(
            input="Give me a stock report for AAPL",
            stream=True,
        )
    )

if __name__ == "__main__":
    asyncio.run(test_structured_streaming())

asyncio.run(test_structured_streaming_with_arun())
bash  theme={null}
    pip install agno exa_py pydantic
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/05_async_structured_output_streaming.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Basic Conversational Workflow

**URL:** llms-txt#basic-conversational-workflow

Source: https://docs.agno.com/basics/workflows/usage/basic-workflow-agent

This example demonstrates a basic conversational workflow with a WorkflowAgent.

This example shows how to use the `WorkflowAgent` to create a conversational workflow.

```python basic_conversational_workflow.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.workflow import WorkflowAgent
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

story_writer = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are tasked with writing a 100 word story based on a given topic",
)

story_formatter = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are tasked with breaking down a short story in prelogues, body and epilogue",
)

def add_references(step_input: StepInput):
    """Add references to the story"""

previous_output = step_input.previous_step_content

if isinstance(previous_output, str):
        return previous_output + "\n\nReferences: https://www.agno.com"

---

## Print stored events in a nice format

**URL:** llms-txt#print-stored-events-in-a-nice-format

print_stored_events(run_response, "Simple Step Workflow")

---

## And automatically recalled here

**URL:** llms-txt#and-automatically-recalled-here

**Contents:**
  - Agentic Memory (`enable_agentic_memory=True`)

agent.print_response("What's the best way to reach me?")
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

**Examples:**

Example 1 (unknown):
```unknown
**Best for:** Customer support, personal assistants, conversational apps where you want consistent memory behavior.

### Agentic Memory (`enable_agentic_memory=True`)

The agent gets full control over memory management through built-in tools. It decides when to create, update, or delete memories based on the conversation context.
```

---

## Example 2: Delete a task

**URL:** llms-txt#example-2:-delete-a-task

print("\n=== Delete a task ===")
todoist_agent.print_response(
    "Delete the todoist task to buy groceries tomorrow at 10am"
)

---

## Navigate to the stagehand directory

**URL:** llms-txt#navigate-to-the-stagehand-directory

cd mcp-server-browserbase/stagehand

---

## Capture Reasoning Content

**URL:** llms-txt#capture-reasoning-content

Source: https://docs.agno.com/basics/reasoning/usage/agents/capture-reasoning-content-cot

Example showing how to access and print the `reasoning_content` when using a Reasoning Agent (with `reasoning=True`) or setting a specific `reasoning_model`.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## OpenAI GPT-4.1

**URL:** llms-txt#openai-gpt-4.1

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/reasoning-model-gpt4-1

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize OpenLIT with custom tracer

**URL:** llms-txt#initialize-openlit-with-custom-tracer

openlit.init(
    tracer=trace.get_tracer(__name__),
    disable_batch=True
)

---

## Initialize and connect to the MCP server

**URL:** llms-txt#initialize-and-connect-to-the-mcp-server

---

## OpenAI Moderation Guardrail for Teams

**URL:** llms-txt#openai-moderation-guardrail-for-teams

Source: https://docs.agno.com/basics/guardrails/usage/team/openai-moderation

This example demonstrates how to use Agno's built-in OpenAI moderation guardrail with a Team to detect and block policy violations.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## OpenAI o4-mini with reasoning summary

**URL:** llms-txt#openai-o4-mini-with-reasoning-summary

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/reasoning-summary

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Config

**URL:** llms-txt#get-config

Source: https://docs.agno.com/reference-api/schema/knowledge/get-config

get /knowledge/config
Retrieve available readers, chunkers, and configuration options for content processing. This endpoint provides metadata about supported file types, processing strategies, and filters.

---

## Class-based Executor

**URL:** llms-txt#class-based-executor

Source: https://docs.agno.com/basics/workflows/usage/class-based-executor

This example demonstrates how to use a class-based executor in a workflow.

This example demonstrates how to use a class-based executor in a workflow.

```python class_based_executor.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## Environment variables

**URL:** llms-txt#environment-variables

Source: https://docs.agno.com/templates/infra-management/env-vars

Environment variables can be added to resources using the `env_vars` parameter or the `env_file` parameter pointing to a `yaml` file. Examples

The apps in your templates are already configured to read environment variables.

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Stdio Transport

**URL:** llms-txt#stdio-transport

Source: https://docs.agno.com/basics/tools/mcp/transports/stdio

The stdio (standard input/output) transport is the default one in Agno's integration. It works best for local integrations.

To use it, simply initialize the `MCPTools` class with the `command` argument.
The command you want to pass is the one used to run the MCP server the agent will have access to.

For example `uvx mcp-server-git`, which runs a [git MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/git):

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

---

## JWT Secret (use environment variable in production)

**URL:** llms-txt#jwt-secret-(use-environment-variable-in-production)

JWT_SECRET = "a-string-secret-at-least-256-bits-long"

---

## How to get the token and chat_id:

**URL:** llms-txt#how-to-get-the-token-and-chat_id:

---

## Cleanup

**URL:** llms-txt#cleanup

**Contents:**
- Speech Generation
- Context Caching

model.delete_file_search_store(store.name)
python  theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from agno.utils.audio import write_wav_audio_to_file

agent = Agent(
    model=Gemini(
        id="gemini-2.5-flash-preview-tts",
        response_modalities=["AUDIO"],
        speech_config={
            "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}}
        },
    )
)

run_output = agent.run("Say cheerfully: Have a wonderful day!")

if run_output.response_audio is not None:
    audio_data = run_output.response_audio.content
    write_wav_audio_to_file("tmp/cheerful_greeting.wav", audio_data)
python  theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from google import genai

client = genai.Client()

**Examples:**

Example 1 (unknown):
```unknown
## Speech Generation

Generate audio responses from the model. See [Google's speech generation documentation](https://ai.google.dev/gemini-api/docs/speech-generation) for available voices and options.
```

Example 2 (unknown):
```unknown
## Context Caching

Cache large contexts to reduce costs and latency. See [Google's context caching documentation](https://ai.google.dev/gemini-api/docs/caching) for more details.
```

---

## Set up the agent

**URL:** llms-txt#set-up-the-agent

web_research_agent = Agent(
    name="Basic Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)

---

## Provide the agent with the audio file and audio configuration and get result as text + audio

**URL:** llms-txt#provide-the-agent-with-the-audio-file-and-audio-configuration-and-get-result-as-text-+-audio

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "sage", "format": "wav"},
    ),
    db=InMemoryDb(),
    add_history_to_context=True,
    markdown=True,
)
run_output: RunOutput = agent.run("Tell me a 5 second scary story")

---

## Contributing to Agno

**URL:** llms-txt#contributing-to-agno

**Contents:**
- ðŸ‘©â€ðŸ’» How to contribute
- Development setup
- Formatting and validation

Source: https://docs.agno.com/how-to/contribute

Learn how to contribute to Agno through our fork and pull request workflow.

Agno is an open-source project and we welcome contributions.

## ðŸ‘©â€ðŸ’» How to contribute

Please follow the [fork and pull request](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) workflow:

* Fork the repository.
* Create a new branch for your feature.
* Add your feature or improvement.
* Send a pull request.
* We appreciate your support & input!

1. Clone the repository.
2. Create a virtual environment:
   * For Unix, use `./scripts/dev_setup.sh`.
   * This setup will:
     * Create a `.venv` virtual environment in the current directory.
     * Install the required packages.
     * Install the `agno` package in editable mode.
3. Activate the virtual environment:
   * On Unix: `source .venv/bin/activate`

> From here on you have to use `uv pip install` to install missing packages

## Formatting and validation

Ensure your code meets our quality standards by running the appropriate formatting and validation script before submitting a pull request:

* For Unix:
  * `./scripts/format.sh`
  * `./scripts/validate.sh`

These scripts will perform code formatting with `ruff` and static type checks with `mypy`.

Read more about the guidelines [here](https://github.com/agno-agi/agno/tree/main/CONTRIBUTING.md)

Message us on [Discord](https://discord.gg/4MtYHHrgA8) or post on [Discourse](https://community.agno.com/) if you have any questions or need help with credits.

---

## team = Team(

**URL:** llms-txt#team-=-team(

---

## Example prompts:

**URL:** llms-txt#example-prompts:

"""
Customer Management:
- "Create a customer. Name: ACME Corp, Email: billing@acme.example.com"
- "List my customers."
- "Find customer by email 'jane.doe@example.com'" # Note: Requires 'customers.retrieve' or search capability

Product and Price Management:
- "Create a new product called 'Basic Plan'."
- "Create a recurring monthly price of $10 USD for product 'Basic Plan'."
- "Create a product 'Ebook Download' and a one-time price of $19.95 USD."
- "List all products." # Note: Requires 'products.list' capability
- "List all prices." # Note: Requires 'prices.list' capability

Payment Links:
- "Create a payment link for the $10 USD monthly 'Basic Plan' price."
- "Generate a payment link for the '$19.95 Ebook Download'."

Combined Tasks:
- "Create a product 'Pro Service', add a price $150 USD (one-time), and give me the payment link."
- "Register a new customer 'support@example.com' named 'Support Team'."
"""

---

## Create agent with database

**URL:** llms-txt#create-agent-with-database

---

## Example 1: Scrape a webpage as Markdown

**URL:** llms-txt#example-1:-scrape-a-webpage-as-markdown

**Contents:**
- Toolkit Params
- Toolkit Functions
- Supported Data Sources
  - E-commerce
  - Professional Networks
  - Social Media
  - Other Platforms
- Developer Resources

agent.print_response(
    "Scrape this webpage as markdown: https://docs.agno.com/introduction",
)
```

| Parameter                | Type            | Default           | Description                                                                                                               |
| ------------------------ | --------------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------- |
| `api_key`                | `Optional[str]` | `None`            | BrightData API key. If not provided, uses BRIGHT\_DATA\_API\_KEY environment variable.                                    |
| `enable_scrape_markdown` | `bool`          | `True`            | Enable the scrape\_as\_markdown function.                                                                                 |
| `enable_screenshot`      | `bool`          | `True`            | Enable the get\_screenshot function.                                                                                      |
| `enable_search_engine`   | `bool`          | `True`            | Enable the search\_engine function.                                                                                       |
| `enable_web_data_feed`   | `bool`          | `True`            | Enable the web\_data\_feed function.                                                                                      |
| `all`                    | `bool`          | `False`           | Enable all available functions. When True, all enable flags are ignored.                                                  |
| `serp_zone`              | `str`           | `"serp_api"`      | SERP zone for search operations. Can be overridden with BRIGHT\_DATA\_SERP\_ZONE environment variable.                    |
| `web_unlocker_zone`      | `str`           | `"web_unlocker1"` | Web unlocker zone for scraping operations. Can be overridden with BRIGHT\_DATA\_WEB\_UNLOCKER\_ZONE environment variable. |
| `verbose`                | `bool`          | `False`           | Enable verbose logging.                                                                                                   |
| `timeout`                | `int`           | `600`             | Timeout in seconds for operations.                                                                                        |

| Function             | Description                                                                                                                                                                                                                           |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scrape_as_markdown` | Scrapes a webpage and returns content in Markdown format. Parameters: `url` (str) - URL to scrape.                                                                                                                                    |
| `get_screenshot`     | Captures a screenshot of a webpage and adds it as an image artifact. Parameters: `url` (str) - URL to screenshot, `output_path` (str, optional) - Output path (default: "screenshot.png").                                            |
| `search_engine`      | Searches using Google, Bing, or Yandex and returns results in Markdown. Parameters: `query` (str), `engine` (str, default: "google"), `num_results` (int, default: 10), `language` (Optional\[str]), `country_code` (Optional\[str]). |
| `web_data_feed`      | Retrieves structured data from various sources like LinkedIn, Amazon, Instagram, etc. Parameters: `source_type` (str), `url` (str), `num_of_reviews` (Optional\[int]).                                                                |

## Supported Data Sources

* `amazon_product` - Amazon product details
* `amazon_product_reviews` - Amazon product reviews
* `amazon_product_search` - Amazon product search results
* `walmart_product` - Walmart product details
* `walmart_seller` - Walmart seller information
* `ebay_product` - eBay product details
* `homedepot_products` - Home Depot products
* `zara_products` - Zara products
* `etsy_products` - Etsy products
* `bestbuy_products` - Best Buy products

### Professional Networks

* `linkedin_person_profile` - LinkedIn person profiles
* `linkedin_company_profile` - LinkedIn company profiles
* `linkedin_job_listings` - LinkedIn job listings
* `linkedin_posts` - LinkedIn posts
* `linkedin_people_search` - LinkedIn people search results

* `instagram_profiles` - Instagram profiles
* `instagram_posts` - Instagram posts
* `instagram_reels` - Instagram reels
* `instagram_comments` - Instagram comments
* `facebook_posts` - Facebook posts
* `facebook_marketplace_listings` - Facebook Marketplace listings
* `facebook_company_reviews` - Facebook company reviews
* `facebook_events` - Facebook events
* `tiktok_profiles` - TikTok profiles
* `tiktok_posts` - TikTok posts
* `tiktok_shop` - TikTok shop
* `tiktok_comments` - TikTok comments
* `x_posts` - X (Twitter) posts

* `google_maps_reviews` - Google Maps reviews
* `google_shopping` - Google Shopping results
* `google_play_store` - Google Play Store apps
* `apple_app_store` - Apple App Store apps
* `youtube_profiles` - YouTube profiles
* `youtube_videos` - YouTube videos
* `youtube_comments` - YouTube comments
* `reddit_posts` - Reddit posts
* `zillow_properties_listing` - Zillow property listings
* `booking_hotel_listings` - Booking.com hotel listings
* `crunchbase_company` - Crunchbase company data
* `zoominfo_company_profile` - ZoomInfo company profiles
* `reuter_news` - Reuters news
* `github_repository_file` - GitHub repository files
* `yahoo_finance_business` - Yahoo Finance business data

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/brightdata.py)
* View [Cookbook Example](https://github.com/agno-agi/agno/tree/main/cookbook/tools/brightdata_tools.py)

---

## ModelsLabs

**URL:** llms-txt#modelslabs

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/models-labs

You need to install the `requests` library.

Set the `MODELS_LAB_API_KEY` environment variable.

The following agent will use ModelsLabs to generate a video based on a text prompt.

```python cookbook/tools/models_labs_tools.py theme={null}
from agno.agent import Agent
from agno.tools.models_labs import ModelsLabsTools

**Examples:**

Example 1 (unknown):
```unknown
Set the `MODELS_LAB_API_KEY` environment variable.
```

Example 2 (unknown):
```unknown
## Example

The following agent will use ModelsLabs to generate a video based on a text prompt.
```

---

## Define and use examples

**URL:** llms-txt#define-and-use-examples

if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation with custom execution options",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
    )

# Run workflow with additional_data
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        additional_data={
            "user_email": "michael@dundermifflin.com",
            "priority": "high",
            "client_type": "enterprise",
        },
        markdown=True,
        stream=True,
    )

print("\n" + "=" * 60 + "\n")
```

To checkout async version, see the cookbook-

* [Step with Function using Additional Data (async)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_01_basic_workflows/_02_step_with_function/async/step_with_function_additional_data.py)

---

## Structured Output Agent

**URL:** llms-txt#structured-output-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/dashscope/usage/structured-output

```python cookbook/models/dashscope/structured_output.py theme={null}
from typing import List

from agno.agent import Agent
from agno.models.dashscope import DashScope
from pydantic import BaseModel, Field

class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

---

## OpenAI gpt-5-mini with Tools

**URL:** llms-txt#openai-gpt-5-mini-with-tools

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/gpt5-mini-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Retrieve our freshly created session

**URL:** llms-txt#retrieve-our-freshly-created-session

**Contents:**
- Benefits of using session storage
- Storage for Teams and Workflows

session_history = agent.get_session(session_id="123")
python  theme={null}
from agno.team import Team
from agno.workflow import Workflow
from agno.db.sqlite import SQLiteDb

**Examples:**

Example 1 (unknown):
```unknown
## Benefits of using session storage

Storage is often an important feature when building production-ready agentic applications. It enables the system to:

* Continue a session: retrieve previous messages and enable users to pick up a conversation where they left off.
* Keep a record of sessions: enable users to inspect their past conversations.
* Data ownership: keeping sessions in your own database gives you full control over the data.

<Warning>
  Storage is a critical part of your Agentic infrastructure. We recommend to
  never offload it to a third-party service. You should almost always use your
  own storage layer for your Agents.
</Warning>

## Storage for Teams and Workflows

Storage also works with Teams and Workflows, providing persistent memory for your more complex agentic applications.

Similarly to Agents, you simply need to provide your Team or Workflow with a database for sessions to be persisted:
```

---

## Create an Agent that maintains state

**URL:** llms-txt#create-an-agent-that-maintains-state

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a counter starting at 0 (this is the default session state for all users)
    session_state={"shopping_list": []},
    db=SqliteDb(db_file="tmp/agents.db"),
    tools=[add_item],
    # You can use variables from the session state in the instructions
    instructions="Current state (shopping list) is: {shopping_list}",
    markdown=True,
)

---

## Human-in-the-Loop in Agents

**URL:** llms-txt#human-in-the-loop-in-agents

**Contents:**
- Types of Human-in-the-Loop Flows
- Resolving Human-in-the-Loop Requirements

Source: https://docs.agno.com/basics/hitl/overview

Learn how to control the flow of an agent's execution in Agno with human oversight and input.

Human-in-the-Loop (HITL) in Agno enables you to implement patterns where human oversight and input are required during agent execution. This is crucial for:

* Validating sensitive operations
* Reviewing tool calls before execution
* Gathering user input for decision-making
* Managing external tool execution

## Types of Human-in-the-Loop Flows

Agno supports four main types of human-in-the-loop flows:

1. **[User Confirmation](/basics/hitl/user-confirmation)**: Require explicit user approval before executing a tool
2. **[User Input](/basics/hitl/user-input)**: Gather specific information from users during execution
3. **[Dynamic User Input](/basics/hitl/dynamic-user-input)**: Have the agent collect user input as it needs it
4. **[External Tool Execution](/basics/hitl/external-execution)**: Execute tools outside of the agent's control

<Note>
  Currently Agno only supports Human-in flows for `Agent`. `Team` and `Workflow` will be supported in the near future!
</Note>

## Resolving Human-in-the-Loop Requirements

During Human-in-the-Loop flows, the agent run will pause to wait for the user to resolve whatever requirement is needed.

This is how you can interact with the requirements in your code:

```python  theme={null}

---

## Multimodal Agent

**URL:** llms-txt#multimodal-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/ollama/usage/multimodal

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Ollama">
    Follow the [installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:

<Step title="Install libraries">
    
  </Step>

<Step title="Add sample image">
    Place a sample image named `sample.jpg` in the same directory as your script, or update the `image_path` to point to your desired image.
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Add sample image">
    Place a sample image named `sample.jpg` in the same directory as your script, or update the `image_path` to point to your desired image.
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Agentic Knowledge Filters

**URL:** llms-txt#agentic-knowledge-filters

**Contents:**
- Step 1: Attach Metadata
- How It Works
- ðŸŒŸ See Agentic Filters in Action!
- When to Use Agentic Filtering
- Try It Out!
- Developer Resources

Agentic filtering lets the Agent automatically extract filter criteria from your query text, making the experience more natural and interactive.

## Step 1: Attach Metadata

There are two ways to attach metadata to your documents:

1. **Attach Metadata When Initializing the Knowledge Base**

2. **Attach Metadata When Loading Documents One by One**

When you enable agentic filtering (`enable_agentic_knowledge_filters=True`), the Agent analyzes your query and applies filters based on the metadata it detects.

In this example, the Agent will automatically use:

* `user_id = "jordan_mitchell"`
* `document_type = "cv"`

## ðŸŒŸ See Agentic Filters in Action!

Experience how agentic filters automatically extract relevant metadata from your query.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2bf046e2fb9607b1db6e8a1b5ee0ead0" alt="Agentic Filters in Action" data-og-width="1740" width="1740" data-og-height="715" height="715" data-path="images/agentic_filters.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0245d20f27f0679692757ba76db108bc 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=24d9bd1af34e8846247939e2d74b27ac 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=04efcadbf12c4616ef040032fb9b33ff 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1a905546407daa3fc8a19a2972bc4153 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b86573d2610c92c819677f366619146e 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/agentic_filters.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=46ae6ac18ae5b147a8c7eee02c45cfff 2500w" />

*The Agent intelligently narrows down results based on your query.*

## When to Use Agentic Filtering

* When you want a more conversational, user-friendly experience.
* When users may not know the exact filter syntax.

* Enable `enable_agentic_knowledge_filters=True` on your Agent.
* Ask questions naturally, including filter info in your query.
* See how the Agent narrows down results automatically!

## Developer Resources

* [Agentic filtering](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/filters/agentic_filtering.py)

**Examples:**

Example 1 (unknown):
```unknown
2. **Attach Metadata When Loading Documents One by One**
```

Example 2 (unknown):
```unknown
***

## How It Works

When you enable agentic filtering (`enable_agentic_knowledge_filters=True`), the Agent analyzes your query and applies filters based on the metadata it detects.

**Example:**
```

---

## === CONDITION EVALUATOR ===

**URL:** llms-txt#===-condition-evaluator-===

def needs_editing(step_input: StepInput) -> bool:
    """Determine if the story needs editing based on length and complexity"""
    story = step_input.previous_step_content or ""

# Check if story is long enough to benefit from editing
    word_count = len(story.split())

# Edit if story is more than 50 words or contains complex punctuation
    return word_count > 50 or any(punct in story for punct in ["!", "?", ";", ":"])

def add_references(step_input: StepInput):
    """Add references to the story"""
    previous_output = step_input.previous_step_content

if isinstance(previous_output, str):
        return previous_output + "\n\nReferences: https://www.agno.com"

---

## ms-marco-MiniLM-L-12-v2

**URL:** llms-txt#ms-marco-minilm-l-12-v2

**Contents:**
- Usage

3. Export API Keys
export ANTHROPIC_API_KEY="your-anthropic-api-key"

4. Run the Example
python cookbook/agent_basics/agentic_search/agentic_rag_infinity_reranker.py

About Infinity Reranker:
- Provides fast, local reranking without external API calls
- Supports multiple state-of-the-art reranking models
- Can be deployed on GPU for better performance
- Offers both sync and async reranking capabilities
- More deployment options: https://michaelfeil.eu/infinity/0.0.76/deploy/
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker import InfinityReranker
from agno.models.anthropic import Claude
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database, store embeddings in the `agno_docs_infinity` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_infinity",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        # Use Infinity reranker for local, fast reranking
        reranker=InfinityReranker(
            model="BAAI/bge-reranker-base",  # You can change this to other models
            host="localhost",
            port=7997,
            top_n=5,  # Return top 5 reranked documents
        ),
    ),
)

asyncio.run(
    knowledge.add_contents(
        urls=[
            "https://docs.agno.com/introduction/agents.md",
            "https://docs.agno.com/agents/tools.md",
            "https://docs.agno.com/agents/knowledge.md",
        ]
    )
)

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
        "Provide detailed and accurate information based on the retrieved documents.",
    ],
    markdown=True,
)

def test_infinity_connection():
    """Test if Infinity server is running and accessible"""
    try:
        from infinity_client import Client

_ = Client(base_url="http://localhost:7997")
        print("âœ… Successfully connected to Infinity server at localhost:7997")
        return True
    except Exception as e:
        print(f"âŒ Failed to connect to Infinity server: {e}")
        print(
            "\nPlease make sure Infinity server is running. See setup instructions above."
        )
        return False

if __name__ == "__main__":
    print("ðŸš€ Agentic RAG with Infinity Reranker Example")
    print("=" * 50)

# Test Infinity connection first
    if not test_infinity_connection():
        exit(1)

print("\nðŸ¤– Starting agent interaction...")
    print("=" * 50)

# Example questions to test the reranking capabilities
    questions = [
        "What are Agents and how do they work?",
        "How do I use tools with agents?",
        "What is the difference between knowledge and tools?",
    ]

for i, question in enumerate(questions, 1):
        print(f"\nðŸ” Question {i}: {question}")
        print("-" * 40)
        agent.print_response(question, stream=True)
        print("\n" + "=" * 50)

print("\nðŸŽ‰ Example completed!")
    print("\nThe Infinity reranker helped improve the relevance of retrieved documents")
    print("by reranking them based on semantic similarity to your queries.")
bash  theme={null}
    pip install -U agno anthropic infinity-client lancedb "infinity-emb[all]"
    bash  theme={null}
    # Run infinity server with reranking model
    infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
    bash Mac/Linux theme={null}
        export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
      bash Windows theme={null}
        $Env:ANTHROPIC_API_KEY="your_anthropic_api_key_here"
      bash  theme={null}
    touch agentic_rag_infinity_reranker.py
    bash Mac theme={null}
      python agentic_rag_infinity_reranker.py
      bash Windows theme={null}
      python agentic_rag_infinity_reranker.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/agentic_search" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Infinity Server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your ANTHROPIC API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Singlestore for Workflow

**URL:** llms-txt#singlestore-for-workflow

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/singlestore/usage/singlestore-for-workflow

Agno supports using Singlestore as a storage backend for Workflows using the `SingleStoreDb` class.

Obtain the credentials for Singlestore from [here](https://portal.singlestore.com/)

```python singlestore_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.singlestore import SingleStoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Content Analyzer Agent - Specialized in analyzing retrieved content

**URL:** llms-txt#content-analyzer-agent---specialized-in-analyzing-retrieved-content

content_analyzer = Agent(
    name="Content Analyzer",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Analyze and extract key insights from retrieved content",
    instructions=[
        "Analyze the content provided by the Knowledge Searcher.",
        "Extract key concepts, relationships, and important details.",
        "Identify gaps or areas needing additional clarification.",
        "Organize information logically for synthesis.",
    ],
    markdown=True,
)

---

## Run Agent Infra Railway Locally

**URL:** llms-txt#run-agent-infra-railway-locally

Source: https://docs.agno.com/templates/agent-infra-railway/run-local

<Snippet file="run-agent-infra-railway-local.mdx" />

---

## Browser Search Agent

**URL:** llms-txt#browser-search-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/groq/usage/browser-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Reasoning O3 Mini

**URL:** llms-txt#reasoning-o3-mini

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/reasoning-o3-mini

```python cookbook/models/openai/responses/reasoning_o3_mini.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## Slack

**URL:** llms-txt#slack

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/social/slack

The following example requires the `slack-sdk` library.

Get a Slack token from [here](https://api.slack.com/tutorials/tracks/getting-a-token).

The following agent will use Slack to send a message to a channel, list all channels, and get the message history of a specific channel.

```python cookbook/tools/slack_tools.py theme={null}
import os

from agno.agent import Agent
from agno.tools.slack import SlackTools

slack_tools = SlackTools()

agent = Agent(tools=[slack_tools])

**Examples:**

Example 1 (unknown):
```unknown
Get a Slack token from [here](https://api.slack.com/tutorials/tracks/getting-a-token).
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Slack to send a message to a channel, list all channels, and get the message history of a specific channel.
```

---

## Create individual steps

**URL:** llms-txt#create-individual-steps

research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

---

## Setup the database

**URL:** llms-txt#setup-the-database

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini"),
    user_id="test_user",
    session_id="test_session",
    # Pass the database to the Agent
    db=db,
    # Enable user memories
    enable_user_memories=True,
    # Enable session summaries
    enable_session_summaries=True,
    # Show debug logs so, you can see the memory being created
)

---

## Initialize the Agent with various configurations including the knowledge base and storage

**URL:** llms-txt#initialize-the-agent-with-various-configurations-including-the-knowledge-base-and-storage

agent = Agent(
    session_id="session_id",  # use any unique identifier to identify the run
    user_id="user",  # user identifier to identify the user
    model=model,
    knowledge=knowledge,
    db=db,
)

---

## Initialize hybrid vector DB

**URL:** llms-txt#initialize-hybrid-vector-db

hybrid_db = PgVector(
    table_name="recipes",
    db_url=db_url,
    search_type=SearchType.hybrid  # Hybrid Search
)

---

## Weaviate Hybrid Search

**URL:** llms-txt#weaviate-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/weaviate/usage/weaviate-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Setup Weaviate">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Weaviate">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Set environment variables">
```

---

## Initialize Redis Vector DB

**URL:** llms-txt#initialize-redis-vector-db

**Contents:**
- Usage

vector_db = RedisVectorDb(
    index_name=INDEX_NAME,
    redis_url=REDIS_URL,
    search_type=SearchType.vector,  # try SearchType.hybrid for hybrid search
)

contents_db = PostgresDb(
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    knowledge_table="knowledge_contents",
)

knowledge = Knowledge(
    name="My Qdrant Vector Knowledge Base",
    description="This is a knowledge base that uses a Qdrant Vector DB",
    vector_db=vector_db,
    contents_db=contents_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

agent = Agent(knowledge=knowledge)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

vector_db.delete_by_name("Recipes")

vector_db.delete_by_metadata({"doc_type": "recipe_book"})
bash  theme={null}
    pip install -U redis redisvl pypdf openai agno
    bash  theme={null}
    docker run -d --name my-redis -p 6379:6379 -p 8001:8001 redis/redis-stack:latest
    bash Mac theme={null}
      python redis_db.py
      bash Windows theme={null}
      python redis_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Redis">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run the agent

**URL:** llms-txt#run-the-agent

**Contents:**
- Usage

structured_output_agent.print_response("New York")

bash  theme={null}
    ollama pull llama3.2
    bash  theme={null}
    pip install -U ollama agno
    bash Mac theme={null}
      python cookbook/models/ollama/structured_output.py
      bash Windows theme={null}
      python cookbook/models/ollama/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Simple async function to run an Agent.

**URL:** llms-txt#simple-async-function-to-run-an-agent.

async def arun_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        system_message="Be concise, reply with one sentence.",
    )
    response = await agent.arun("What is the capital of France?")
    return response

performance_eval = PerformanceEval(func=arun_agent, num_iterations=10)

---

## Async Basic Agent

**URL:** llms-txt#async-basic-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/xai/usage/basic-async

```python cookbook/models/xai/basic_async.py theme={null}
import asyncio

from agno.agent import Agent, RunOutput
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-3"), markdown=True)

---

## Load the knowledge

**URL:** llms-txt#load-the-knowledge

knowledge.add_content(
    path="data/pptx_files",
    reader=PPTXReader(),
)

---

## PostgreSQL

**URL:** llms-txt#postgresql

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/postgres/overview

Learn to use PostgreSQL as a database for your Agents

Agno supports using [PostgreSQL](https://www.postgresql.org/) as a database with the `PostgresDb` class.

```python postgres_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai" # Replace with your own connection string

---

## Postgres

**URL:** llms-txt#postgres

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/database/postgres

The PostgresTools toolkit enables an Agent to interact with a PostgreSQL database.

**PostgresTools** enable an Agent to interact with a PostgreSQL database.

The following example requires the `psycopg2` library.

You will also need a database. The following example uses a Postgres database running in a Docker container.

The following agent will list all tables in the database.

```python cookbook/tools/postgres.py theme={null}
from agno.agent import Agent
from agno.tools.postgres import PostgresTools

**Examples:**

Example 1 (unknown):
```unknown
You will also need a database. The following example uses a Postgres database running in a Docker container.
```

Example 2 (unknown):
```unknown
## Example

The following agent will list all tables in the database.
```

---

## Business trends

**URL:** llms-txt#business-trends

business_prompt = """\
Analyze media trends for:
Keywords: remote work, digital transformation, sustainability
Sources: forbes.com, bloomberg.com, hbr.org
"""

---

## Create an agent that manages the shopping list

**URL:** llms-txt#create-an-agent-that-manages-the-shopping-list

shopping_agent = Agent(
    name="Shopping List Agent",
    role="Manage the shopping list",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[add_item, remove_item],
)

---

## Example 2: Using NOT operator with teams

**URL:** llms-txt#example-2:-using-not-operator-with-teams

**Contents:**
- Usage

print("Using NOT operator")
team_with_knowledge.print_response(
    "Tell me about the candidate's work and experience",
    knowledge_filters=[
        AND(
            IN("user_id", ["jordan_mitchell", "taylor_brooks"]),
            NOT(IN("user_id", ["morgan_lee", "casey_jordan", "alex_rivera"])),
        )
    ],
    markdown=True,
)
bash  theme={null}
    pip install -U agno openai pgvector psycopg
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/knowledge/filters/filter_expressions.py
      python cookbook/knowledge/filters/filter_expressions_teams.py
      bash Windows theme={null}
      python cookbook/knowledge/filters/filter_expressions.py
      python cookbook/knowledge/filters/filter_expressions_teams.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the examples">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Audio to Text Transcription

**URL:** llms-txt#audio-to-text-transcription

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/agent/usage/audio-to-text

This example demonstrates how to create an agent that can transcribe audio conversations, identifying different speakers and providing accurate transcriptions.

```python audio_to_text.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

---

## Set up named loggers BEFORE creating agents/teams/workflows

**URL:** llms-txt#set-up-named-loggers-before-creating-agents/teams/workflows

logger_configs = [
    ("agno", "agent.log"),
    ("agno-team", "team.log"),
    ("agno-workflow", "workflow.log"),
]

for logger_name, log_file in logger_configs:
    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.INFO)
    handler = logging.FileHandler(log_file)
    handler.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
    logger.addHandler(handler)
    logger.propagate = False

---

## News Agency Team

**URL:** llms-txt#news-agency-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/news_agency_team

This example shows how to create a news agency team that can search the web, write an article, and edit it.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Exclude archived content

**URL:** llms-txt#exclude-archived-content

active_filter = NOT(EQ("status", "archived"))

---

## Mistral

**URL:** llms-txt#mistral

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/mistral

The Mistral model provides access to Mistral's language models.

| Parameter               | Type            | Default                       | Description                                                      |
| ----------------------- | --------------- | ----------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"mistral-large-latest"`      | The id of the Mistral model to use                               |
| `name`                  | `str`           | `"Mistral"`                   | The name of the model                                            |
| `provider`              | `str`           | `"Mistral"`                   | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                        | The API key for Mistral (defaults to MISTRAL\_API\_KEY env var)  |
| `base_url`              | `str`           | `"https://api.mistral.ai/v1"` | The base URL for the Mistral API                                 |
| `retries`               | `int`           | `0`                           | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                           | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                       | If True, the delay between retries is doubled each time          |

Mistral extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## MySQL for Team

**URL:** llms-txt#mysql-for-team

**Contents:**
- Usage
  - Run MySQL

Source: https://docs.agno.com/integrations/database/mysql/usage/mysql-for-team

Agno supports using MySQL as a storage backend for Teams using the `MySQLDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MySQL** on port **3306** using:

```python mysql_for_team.py theme={null}
from typing import List

from agno.agent import Agent
from agno.db.mysql import MySQLDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

---

## MySQL connection settings

**URL:** llms-txt#mysql-connection-settings

**Contents:**
- Params
- Developer Resources

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"
db = MySQLDb(db_url=db_url)

class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

<Snippet file="db-mysql-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/mysql/mysql_storage_for_team.py)

---

## === Rate Limiting Middleware ===

**URL:** llms-txt#===-rate-limiting-middleware-===

class RateLimitMiddleware(BaseHTTPMiddleware):
    """
    Rate limiting middleware that limits requests per IP address.
    """

def __init__(self, app, requests_per_minute: int = 60, window_size: int = 60):
        super().__init__(app)
        self.requests_per_minute = requests_per_minute
        self.window_size = window_size
        # Store request timestamps per IP
        self.request_history: Dict[str, deque] = defaultdict(lambda: deque())

async def dispatch(self, request: Request, call_next) -> Response:
        # Get client IP
        client_ip = request.client.host if request.client else "unknown"
        current_time = time.time()

# Clean old requests outside the window
        history = self.request_history[client_ip]
        while history and current_time - history[0] > self.window_size:
            history.popleft()

# Check if rate limit exceeded
        if len(history) >= self.requests_per_minute:
            return JSONResponse(
                status_code=429,
                content={
                    "detail": f"Rate limit exceeded. Max {self.requests_per_minute} requests per minute."
                },
            )

# Add current request to history
        history.append(current_time)

# Add rate limit headers
        response = await call_next(request)
        response.headers["X-RateLimit-Limit"] = str(self.requests_per_minute)
        response.headers["X-RateLimit-Remaining"] = str(
            self.requests_per_minute - len(history)
        )
        response.headers["X-RateLimit-Reset"] = str(
            int(current_time + self.window_size)
        )

---

## AgentOS manages MCP lifespan

**URL:** llms-txt#agentos-manages-mcp-lifespan

agent_os = AgentOS(
    description="AgentOS with MCP Tools",
    agents=[agent],
)

app = agent_os.get_app()

if __name__ == "__main__":
    # Don't use reload=True with MCP tools to avoid lifespan issues
    agent_os.serve(app="mcp_tools_example:app")
```

<Note>
  Refreshing the connection to MCP servers is **not** automatically handled. You can use [`refresh_connection`](/basics/tools/mcp/overview#connection-refresh) to manually refresh a connection.
</Note>

See here for a [full example](/agent-os/usage/mcp/mcp-tools-example).

---

## Knowledge Base

**URL:** llms-txt#knowledge-base

**Contents:**
- LanceDb Params
- Developer Resources

knowledge_base = Knowledge(
    vector_db=vector_db,
)

def lancedb_agent(user: str = "user"):
    agent = Agent(
        knowledge=knowledge_base,
        debug_mode=True,
    )

while True:
        message = Prompt.ask(f"[bold] :sunglasses: {user} [/bold]")
        if message in ("exit", "bye"):
            break
        agent.print_response(message, session_id=f"{user}_session")

if __name__ == "__main__":
    # Comment out after first run
    knowledge_base.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

typer.run(lancedb_agent)
python async_lance_db.py theme={null}
    # install lancedb - `pip install lancedb`
    import asyncio

from agno.agent import Agent
    from agno.knowledge.knowledge import Knowledge
    from agno.vectordb.lancedb import LanceDb

# Initialize LanceDB
    vector_db = LanceDb(
        table_name="recipes",
        uri="tmp/lancedb",  # You can change this path to store data elsewhere
    )

# Create knowledge base
    knowledge_base = Knowledge(
        vector_db=vector_db,
    )
    agent = Agent(knowledge=knowledge_base, debug_mode=True)

if __name__ == "__main__":
        # Load knowledge base asynchronously
        asyncio.run(knowledge_base.add_content_async(
                url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
            )
        )

# Create and use the agent asynchronously
        asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
    ```

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_lancedb_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/lance_db/lance_db.py)

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      LanceDB also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Agentic RAG with Infinity Reranker

**URL:** llms-txt#agentic-rag-with-infinity-reranker

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag-infinity-reranker

This example demonstrates how to implement Agentic RAG using Infinity Reranker, which provides high-performance, local reranking capabilities for improved document retrieval without external API calls.

```python agentic_rag_infinity_reranker.py theme={null}
"""This cookbook shows how to implement Agentic RAG using Infinity Reranker.

Infinity is a high-performance inference server for text-embeddings, reranking, and classification models.
It provides fast and efficient reranking capabilities for RAG applications.

1. Install Dependencies
Run: pip install agno anthropic infinity-client lancedb

2. Set up Infinity Server
You have several options to deploy Infinity:

---

## Create your agents

**URL:** llms-txt#create-your-agents

agno_agent = Agent(
    name="Agno Agent",
    model=OpenAIChat(id="gpt-4.1"),
    tools=[MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")],
    db=db,
    enable_user_memories=True,
    knowledge=knowledge,
    markdown=True,
)

simple_agent = Agent(
    name="Simple Agent",
    role="Simple agent",
    id="simple_agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=["You are a simple agent"],
    db=db,
    enable_user_memories=True,
)

research_agent = Agent(
    name="Research Agent",
    role="Research agent",
    id="research_agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=["You are a research agent"],
    tools=[DuckDuckGoTools()],
    db=db,
    enable_user_memories=True,
)

---

## Use Pydantic model as structured input

**URL:** llms-txt#use-pydantic-model-as-structured-input

research_request = ResearchTopic(
    topic="AI Agent Frameworks",
    focus_areas=["AI Agents", "Framework Design", "Developer Tools", "Open Source"],
    target_audience="Software Developers and AI Engineers",
    sources_required=7,
)

---

## Create knowledge base

**URL:** llms-txt#create-knowledge-base

**Contents:**
- Usage

knowledge = Knowledge(
    vector_db=vector_db,
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            name="Recipes",
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        )
    )

# Create and use the agent
    asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
bash  theme={null}
    pip install -U weaviate-client pypdf openai agno
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Weaviate Cloud theme={null}
      # 1. Create account at https://console.weaviate.cloud/
      # 2. Create a cluster and copy the "REST endpoint" and "Admin" API Key
      # 3. Set environment variables:
      export WCD_URL="your-cluster-url" 
      export WCD_API_KEY="your-api-key"
      # 4. Set local=False in the code
      bash Local Development theme={null}
      # 1. Install Docker from https://docs.docker.com/get-docker/
      # 2. Run Weaviate locally:
      docker run -d \
          -p 8080:8080 \
          -p 50051:50051 \
          --name weaviate \
          cr.weaviate.io/semitechnologies/weaviate:1.28.4
      # 3. Set local=True in the code
      bash Mac theme={null}
      python cookbook/knowledge/vector_db/weaviate_db/async_weaviate_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/weaviate_db/async_weaviate_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup Weaviate">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Step 2: Query the knowledge base with different filter combinations

**URL:** llms-txt#step-2:-query-the-knowledge-base-with-different-filter-combinations

---

## Create an Agent with the AWSLambdaTool

**URL:** llms-txt#create-an-agent-with-the-awslambdatool

agent = Agent(
    tools=[AWSLambdaTools(region_name="us-east-1")],
    name="AWS Lambda Agent",
    )

---

## -*- Print a response to the cli

**URL:** llms-txt#-*--print-a-response-to-the-cli

**Contents:**
- Usage

asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))

bash  theme={null}
    ollama pull llama3.1:8b
    bash  theme={null}
    pip install -U ollama agno
    bash Mac theme={null}
      python cookbook/models/ollama/async_basic.py
      bash Windows theme={null}
      python cookbook/models/ollama/async_basic.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Common events you might want to skip

**URL:** llms-txt#common-events-you-might-want-to-skip

events_to_skip = [
    WorkflowRunEvent.workflow_started,
    WorkflowRunEvent.workflow_completed,
    WorkflowRunEvent.workflow_cancelled,
    WorkflowRunEvent.step_started,
    WorkflowRunEvent.step_completed,
    WorkflowRunEvent.parallel_execution_started,
    WorkflowRunEvent.parallel_execution_completed,
    WorkflowRunEvent.condition_execution_started,
    WorkflowRunEvent.condition_execution_completed,
    WorkflowRunEvent.loop_execution_started,
    WorkflowRunEvent.loop_execution_completed,
    WorkflowRunEvent.router_execution_started,
    WorkflowRunEvent.router_execution_completed,
]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Use Cases**

* **Debugging**: Store all events to analyze workflow execution flow
* **Audit Trails**: Keep records of all workflow activities for compliance
* **Performance Analysis**: Analyze timing and execution patterns
* **Error Investigation**: Review event sequences leading to failures
* **Noise Reduction**: Skip verbose events like `step_started` to focus on results

**Configuration Examples**
```

---

## SingleStore Vector Database

**URL:** llms-txt#singlestore-vector-database

**Contents:**
- Setup
- Example
- SingleStore Params
- Developer Resources

Source: https://docs.agno.com/integrations/vectordb/singlestore/overview

Learn how to use SingleStore as a vector database for your Knowledge Base

After running the container, set the environment variables:

SingleStore supports both cloud-based and local deployments. For step-by-step guidance on setting up your cloud deployment, please refer to the [SingleStore Setup Guide](https://docs.singlestore.com/cloud/connect-to-singlestore/connect-with-mysql/connect-with-mysql-client/connect-to-singlestore-helios-using-tls-ssl/).

## SingleStore Params

<Snippet file="vectordb_singlestore_params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/singlestore_db/singlestore_db.py)

**Examples:**

Example 1 (unknown):
```unknown
After running the container, set the environment variables:
```

Example 2 (unknown):
```unknown
SingleStore supports both cloud-based and local deployments. For step-by-step guidance on setting up your cloud deployment, please refer to the [SingleStore Setup Guide](https://docs.singlestore.com/cloud/connect-to-singlestore/connect-with-mysql/connect-with-mysql-client/connect-to-singlestore-helios-using-tls-ssl/).

## Example
```

---

## Pinecone

**URL:** llms-txt#pinecone

Source: https://docs.agno.com/reference/vector-db/pinecone

<Snippet file="vector-db-pinecone-reference.mdx" />

---

## JSON for Agent

**URL:** llms-txt#json-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/json/usage/json-for-agent

Agno supports using local JSON files as a storage backend for Agents using the `JsonDb` class.

```python json_for_agent.py theme={null}
"""Run `pip install ddgs openai` to install dependencies."""

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

---

## GCS for Team

**URL:** llms-txt#gcs-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/gcs/usage/gcs-for-team

Agno supports using Google Cloud Storage (GCS) as a storage backend for Teams using the `GcsJsonDb` class. This storage backend stores session data as JSON blobs in a GCS bucket.

Configure your team with GCS storage to enable cloud-based session persistence.

```python gcs_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

import uuid
import google.auth
from typing import List

from agno.agent import Agent
from agno.db.gcs_json import GcsJsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Pipedream Slack

**URL:** llms-txt#pipedream-slack

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/mcp/usage/pipedream-slack

This example shows how to use the Slack Pipedream MCP server with Agno Agents.

---

## Run agent and return the response as a stream

**URL:** llms-txt#run-agent-and-return-the-response-as-a-stream

**Contents:**
- Streaming Internal Events
- Handling Events
- Event Types
  - Core Events
  - Control Flow Events
  - Tool Events
  - Reasoning Events
  - Memory Events
  - Session Summary Events
  - Pre-Hook Events

stream: Iterator[RunOutputEvent] = agent.run("Trending products", stream=True)
for chunk in stream:
    if chunk.event == RunEvent.run_content:
        print(chunk.content)
python  theme={null}
response_stream: Iterator[RunOutputEvent] = agent.run(
    "Trending products",
    stream=True,
    stream_events=True
)
python  theme={null}
from agno.agent import Agent, RunEvent
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-5"),
    tools=[HackerNewsTools()],
    instructions="Write a report on the topic. Output only the report.",
    markdown=True,
)

stream = agent.run("Trending products", stream=True, stream_events=True)

for chunk in stream:
    if chunk.event == RunEvent.run_content:
        print(f"Content: {chunk.content}")
    elif chunk.event == RunEvent.tool_call_started:
        print(f"Tool call started: {chunk.tool.tool_name}")
    elif chunk.event == RunEvent.reasoning_step:
        print(f"Reasoning step: {chunk.reasoning_content}")
python  theme={null}
from dataclasses import dataclass
from agno.run.agent import CustomEvent
from typing import Optional

@dataclass
class CustomerProfileEvent(CustomEvent):
    """CustomEvent for customer profile."""

customer_name: Optional[str] = None
    customer_email: Optional[str] = None
    customer_phone: Optional[str] = None
python  theme={null}
from agno.tools import tool

@tool()
async def get_customer_profile():
    """Example custom tool that simply yields a custom event."""

yield CustomerProfileEvent(
        customer_name="John Doe",
        customer_email="john.doe@example.com",
        customer_phone="1234567890",
    )
python  theme={null}
agent.run("Tell me a 5 second short story about a robot", user_id="john@example.com", session_id="session_123")
python  theme={null}
agent.run("Tell me a 5 second short story about this image", images=[Image(url="https://example.com/image.jpg")])
python  theme={null}
from pydantic import BaseModel
from agno.agent import Agent
from agno.models.openai import OpenAIChat

class TVShow(BaseModel):
    title: str
    episodes: int

agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))
agent.run("Create a TV show", output_schema=TVShow)
```

For more information see the [Input & Output](/basics/input-output/overview) documentation.

## Pausing and Continuing a Run

An agent run can be paused when a human-in-the-loop flow is initiated. You can then continue the execution of the agent by calling the `Agent.continue_run()` method.

See more details in the [Human-in-the-Loop](/basics/hitl/overview) documentation.

A run can be cancelled by calling the `Agent.cancel_run()` method.

See more details in the [Cancelling a Run](/execution-control/run-cancellation/overview) documentation.

## Developer Resources

* View the [Agent reference](/reference/agents/agent)
* View the [RunOutput schema](/reference/agents/run-response)
* View [Agent Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/README.md)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  For asynchronous streaming, see this [example](/basics/agents/usage/streaming).
</Tip>

## Streaming Internal Events

By default, when you stream a response, only the events that contain a response from the model (i.e. `RunContent` events) are streamed.

But there are numerous other events that can be emitted during an agent run, like tool calling, reasoning, memory updates, etc.

You can stream all run events by setting `stream_events=True` in the `run()` method. This will emit all the events related to the agent's internal processes:
```

Example 2 (unknown):
```unknown
## Handling Events

You can process events as they arrive by iterating over the response stream:
```

Example 3 (unknown):
```unknown
<Check>
  `RunEvents` make it possible to build exceptional agent experiences, by giving you complete information about the agent's internal processes.
</Check>

## Event Types

The following events can be yielded by the `Agent.run()` and `Agent.arun()` functions, depending on the agent's configuration:

### Core Events

| Event Type               | Description                                                                                                    |
| ------------------------ | -------------------------------------------------------------------------------------------------------------- |
| `RunStarted`             | Indicates the start of a run                                                                                   |
| `RunContent`             | Contains the model's response text as individual chunks                                                        |
| `RunContentCompleted`    | Signals completion of content streaming                                                                        |
| `RunIntermediateContent` | Contains the model's intermediate response text as individual chunks. This is used when `output_model` is set. |
| `RunCompleted`           | Signals successful completion of the run                                                                       |
| `RunError`               | Indicates an error occurred during the run                                                                     |
| `RunCancelled`           | Signals that the run was cancelled                                                                             |

### Control Flow Events

| Event Type     | Description                                  |
| -------------- | -------------------------------------------- |
| `RunPaused`    | Indicates the run has been paused            |
| `RunContinued` | Signals that a paused run has been continued |

### Tool Events

| Event Type          | Description                                                    |
| ------------------- | -------------------------------------------------------------- |
| `ToolCallStarted`   | Indicates the start of a tool call                             |
| `ToolCallCompleted` | Signals completion of a tool call, including tool call results |

### Reasoning Events

| Event Type           | Description                                          |
| -------------------- | ---------------------------------------------------- |
| `ReasoningStarted`   | Indicates the start of the agent's reasoning process |
| `ReasoningStep`      | Contains a single step in the reasoning process      |
| `ReasoningCompleted` | Signals completion of the reasoning process          |

### Memory Events

| Event Type              | Description                                     |
| ----------------------- | ----------------------------------------------- |
| `MemoryUpdateStarted`   | Indicates that the agent is updating its memory |
| `MemoryUpdateCompleted` | Signals completion of a memory update           |

### Session Summary Events

| Event Type                | Description                                       |
| ------------------------- | ------------------------------------------------- |
| `SessionSummaryStarted`   | Indicates the start of session summary generation |
| `SessionSummaryCompleted` | Signals completion of session summary generation  |

### Pre-Hook Events

| Event Type         | Description                                    |
| ------------------ | ---------------------------------------------- |
| `PreHookStarted`   | Indicates the start of a pre-run hook          |
| `PreHookCompleted` | Signals completion of a pre-run hook execution |

### Post-Hook Events

| Event Type          | Description                                     |
| ------------------- | ----------------------------------------------- |
| `PostHookStarted`   | Indicates the start of a post-run hook          |
| `PostHookCompleted` | Signals completion of a post-run hook execution |

### Parser Model events

| Event Type                     | Description                                      |
| ------------------------------ | ------------------------------------------------ |
| `ParserModelResponseStarted`   | Indicates the start of the parser model response |
| `ParserModelResponseCompleted` | Signals completion of the parser model response  |

### Output Model events

| Event Type                     | Description                                      |
| ------------------------------ | ------------------------------------------------ |
| `OutputModelResponseStarted`   | Indicates the start of the output model response |
| `OutputModelResponseCompleted` | Signals completion of the output model response  |

### Custom Events

If you are using your own custom tools, you can yield custom events along with the rest of the Agno events.

You can create your custom event class by extending the `CustomEvent` class:
```

Example 4 (unknown):
```unknown
You can then yield your custom event from your tool. The event will be handled internally as an Agno event, and you will be able to access it in the same way you would access any other Agno event:
```

---

## Setup your Agent with Culture

**URL:** llms-txt#setup-your-agent-with-culture

**Contents:**
- Three Approaches to Culture Management
  - 1. Automatic Culture (`update_cultural_knowledge=True`)

agent = Agent(
    db=db,
    add_culture_to_context=True,  # Agent reads cultural knowledge
    update_cultural_knowledge=True,  # Agent updates culture after runs
)
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="agno.db")

agent = Agent(
    db=db,
    add_culture_to_context=True,  # Read culture
    update_cultural_knowledge=True,  # Update culture automatically
)

**Examples:**

Example 1 (unknown):
```unknown
With these flags enabled, your agent automatically:

1. Loads relevant cultural knowledge when starting a task
2. Applies that knowledge during reasoning and response generation
3. Reflects on the interaction afterward
4. Updates or adds cultural knowledge based on what it learned

## Three Approaches to Culture Management

Agno gives you three ways to manage cultural knowledge, depending on your needs:

### 1. Automatic Culture (`update_cultural_knowledge=True`)

After each agent run, the system automatically reflects on the interaction and updates cultural knowledge. This is the recommended approach for most production use cases.
```

---

## Add a simple endpoint to clear the JWT authentication cookie

**URL:** llms-txt#add-a-simple-endpoint-to-clear-the-jwt-authentication-cookie

@app.get("/clear-auth-cookie")
async def clear_auth_cookie(response: Response):
    """Endpoint to clear the JWT authentication cookie (logout)."""
    response.delete_cookie(key="auth_token")
    return {"message": "Authentication cookie cleared successfully"}

---

## Async Basic Stream

**URL:** llms-txt#async-basic-stream

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/async-basic-stream

```python cookbook/models/openai/responses/async_basic_stream.py theme={null}
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-5-mini"), markdown=True)

---

## Add Agno JWT middleware to your custom FastAPI app

**URL:** llms-txt#add-agno-jwt-middleware-to-your-custom-fastapi-app

app.add_middleware(
    JWTMiddleware,
    secret_key=JWT_SECRET,
    excluded_route_paths=[
        "/auth/login"
    ],  # We don't want to validate the token for the login endpoint
    validate=True,  # Set validate to False to skip token validation
)

---

## Image Agent With Memory

**URL:** llms-txt#image-agent-with-memory

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/image-agent-with-memory

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Step-Based Workflows

**URL:** llms-txt#step-based-workflows

**Contents:**
- Example

Source: https://docs.agno.com/basics/workflows/workflow-patterns/step-based-workflow

Named steps for better logging and support on the AgentOS chat page

**You can name your steps** for better logging and future support on the Agno platform.
This also changes the name of a step when accessing that step's output inside a `StepInput` object.

```python  theme={null}
from agno.workflow import Step, Workflow

---

## model=OpenAIChat(id="gpt-5-mini"),

**URL:** llms-txt#model=openaichat(id="gpt-5-mini"),

---

## run: RunOutput = agent.run("write a two sentence horror story")

**URL:** llms-txt#run:-runoutput-=-agent.run("write-a-two-sentence-horror-story")

---

## Confirmation Required with History

**URL:** llms-txt#confirmation-required-with-history

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-with-history

This example demonstrates human-in-the-loop functionality while maintaining conversation history. It shows how user confirmation works when the agent has access to previous conversation context.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The first tool call will be performed. The second one will fail gracefully.

**URL:** llms-txt#the-first-tool-call-will-be-performed.-the-second-one-will-fail-gracefully.

agent.print_response(
    "Find me the current price of TSLA, then after that find me the latest news about Tesla.",
    stream=True,
)

<Tip>
  Important to consider:

* If the Agent tries to run a number of tool calls that exceeds the limit **all at once**, the limit will remain effective. Only as many tool calls as allowed will be performed.
  * The limit is enforced **across a full run**, and not per individual requests triggered by the Agent.
</Tip>

---

## Slack Research Workflow

**URL:** llms-txt#slack-research-workflow

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/interfaces/slack/research-workflow

Integrate a research and writing workflow with Slack for structured AI-powered content creation

```python research_workflow.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.interfaces.slack import Slack
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

workflow_db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

---

## Basic Async Agent Usage

**URL:** llms-txt#basic-async-agent-usage

Source: https://docs.agno.com/basics/agents/usage/basic-async

Learn how to run an agent asynchronously

This example demonstrates how to use `Agent.arun()` or `Agent.aprint_response()` to run an agent asynchronously:

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## AI/ML API

**URL:** llms-txt#ai/ml-api

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/aimlapi

The **AI/ML API** provider gives unified access to over **300+ AI models**, including **Deepseek**, **Gemini**, **ChatGPT**, and others, via a single standardized interface.

The models run with **enterprise-grade rate limits and uptime**, and are ideal for production use.

You can sign up at [aimlapi.com](https://aimlapi.com/?utm_source=agno\&utm_medium=integration\&utm_campaign=aimlapi) and view full provider documentation at [docs.aimlapi.com](https://docs.aimlapi.com/?utm_source=agno\&utm_medium=github\&utm_campaign=integration).

| Parameter               | Type            | Default                        | Description                                                       |
| ----------------------- | --------------- | ------------------------------ | ----------------------------------------------------------------- |
| `id`                    | `str`           | `"gpt-4o-mini"`                | The id of the model to use                                        |
| `name`                  | `str`           | `"AIMLAPI"`                    | The name of the model                                             |
| `provider`              | `str`           | `"AIMLAPI"`                    | The provider of the model                                         |
| `api_key`               | `Optional[str]` | `None`                         | The API key for AI/ML API (defaults to AIMLAPI\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.aimlapi.com/v1"` | The base URL for the AI/ML API                                    |
| `max_tokens`            | `int`           | `4096`                         | Maximum number of tokens to generate                              |
| `retries`               | `int`           | `0`                            | Number of retries to attempt before raising a ModelProviderError  |
| `delay_between_retries` | `int`           | `1`                            | Delay between retries, in seconds                                 |
| `exponential_backoff`   | `bool`          | `False`                        | If True, the delay between retries is doubled each time           |

AIMLAPI extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Knowledge Content Types

**URL:** llms-txt#knowledge-content-types

**Contents:**
- Next Steps

Source: https://docs.agno.com/basics/knowledge/content-types

Agno Knowledge uses `content` as the building block of any piece of knowledge.
Content can be added to knowledge from different sources.

| Content Origin | Description                                                           |
| -------------- | --------------------------------------------------------------------- |
| Path           | Local files or directories containing files                           |
| Url            | Direct links to files or other sites                                  |
| Text           | Raw text content                                                      |
| Topic          | Search topics from repositories like Arxiv or Wikipedia               |
| Remote Content | Content stored in remote repositories like S3 or Google Cloud Storage |

Knowledge content needs to be read and chunked before it can be passed to any VectorDB for embedding, storage and ultimately, retrieval.
When content is added to Knowledge, a default reader is selected. Readers are used to parse content from the origin and then chunk it into smaller
pieces that will then be embedded by the VectorDB.

Custom readers or an override to the default reader and/or its settings can be passed when adding the content. In the below example, an instance of the standard `PDFReader` class is created
but we update the chunk\_size. Similarly, we can update the `chunking_strategy` and other parameters that will influence how content is ingested and processed.

For more information about the different readers and their capabilities checkout the [Readers](/basics/knowledge/readers) page.

<CardGroup cols={3}>
  <Card title="Search & Retrieval" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Learn how agents search and find information in your knowledge base
  </Card>

<Card title="Readers" icon="book-open" href="/basics/knowledge/readers">
    Explore content parsing and ingestion options in detail
  </Card>

<Card title="Chunking Strategies" icon="scissors" href="/basics/knowledge/chunking/overview">
    Optimize how content is broken down for better search results
  </Card>

<Card title="Vector Databases" icon="database" href="/basics/vectordb/overview">
    Choose the right storage solution for your knowledge base
  </Card>
</CardGroup>

---

## Agent that returns a structured output

**URL:** llms-txt#agent-that-returns-a-structured-output

structured_output_agent = Agent(
    model=xAI(id="grok-2-latest"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

---

## Run workflow with additional_data

**URL:** llms-txt#run-workflow-with-additional_data

**Contents:**
- Developer Resources

workflow.print_response(
    input="AI trends in 2024",
    additional_data={
        "user_email": "michael@dundermifflin.com",
        "priority": "high",
        "client_type": "enterprise",
        "budget": "$50000",
        "deadline": "2024-12-15"
    },
    markdown=True,
    stream=True
)
```

## Developer Resources

* [Step with Function and Additional Data](/basics/workflows/usage/step-with-function-additional-data)

---

## Run agent and print response to the terminal

**URL:** llms-txt#run-agent-and-print-response-to-the-terminal

**Contents:**
- Interactive CLI

agent.print_response("Trending startups and products.")
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-5"),
    tools=[HackerNewsTools()],
    db=SqliteDb(db_file="tmp/data.db"),
    add_history_to_context=True,
    num_history_runs=3,
    markdown=True,
)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  You can set `debug_level=2` to get even more detailed logs.
</Tip>

Here's how it looks:

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/Xc0-_OHxxYe_vtGw/videos/debug_mode.mp4?fit=max&auto=format&n=Xc0-_OHxxYe_vtGw&q=85&s=67b080deec475663e285c22130987541" type="video/mp4" data-path="videos/debug_mode.mp4" />
  </video>
</Frame>

## Interactive CLI

Agno also comes with a pre-built interactive CLI that runs your Agent as a command-line application.

You can use this to test back-and-forth conversations with your agent:
```

---

## Get user timeline

**URL:** llms-txt#get-user-timeline

agent.print_response("Get my timeline", markdown=True)

---

## Print the aggregated metrics for the whole run

**URL:** llms-txt#print-the-aggregated-metrics-for-the-whole-run

print("---" * 5, "Run Metrics", "---" * 5)
pprint(run_response.metrics.to_dict())

---

## Find everything except draft documents

**URL:** llms-txt#find-everything-except-draft-documents

**Contents:**
- Using Filters with Agents
  - Basic Agent Filtering

NOT(EQ("status", "draft"))
python  theme={null}
from agno.agent import Agent
from agno.filters import EQ, IN, AND
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

**Examples:**

Example 1 (unknown):
```unknown
## Using Filters with Agents

Here's how to apply filters when running agents with knowledge.

<Note>
  You need a [contents database](/basics/knowledge/content-db) with your Knowledge base to use agentic filtering.
</Note>

### Basic Agent Filtering
```

---

## Basic Accuracy

**URL:** llms-txt#basic-accuracy

Source: https://docs.agno.com/basics/evals/accuracy/usage/basic

Example showing how to check how complete, correct and accurate an Agno Agent's response is.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Save the audio response if available

**URL:** llms-txt#save-the-audio-response-if-available

if run_response.response_audio is not None:
    write_audio_to_file(
        audio=run_response.response_audio.content, filename="tmp/response.wav"
    )

---

## Agent 1 with its own database

**URL:** llms-txt#agent-1-with-its-own-database

hackernews_agent = Agent(
    name="HackerNews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    db=agent1_db,
)

---

## Run infinity server with reranking model

**URL:** llms-txt#run-infinity-server-with-reranking-model

infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997

Wait for the engine to start.

For better performance, you can use larger models:

---

## Delete specific content

**URL:** llms-txt#delete-specific-content

knowledge.remove_content_by_id(content_id)

---

## AgentUI

**URL:** llms-txt#agentui

**Contents:**
- Get Started with Agent UI
- Connect your AgentOS
- View the AgentUI
- Learn more

Source: https://docs.agno.com/basics/agent-ui/overview

An Open Source AgentUI for your AgentOS

<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=72cd1f0888dea4f1ec60a67bff5664c4" style={{ borderRadius: '8px' }} data-og-width="5364" data-og-height="2808" data-path="images/agent-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?w=280&fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=8a962c7d75c6fd40d37b696f258b69fc 280w, https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?w=560&fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=729e6c42c46d47f9c56c66451576c53a 560w, https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?w=840&fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=cabb3ed5cb4c1934bd3a5a1cba70a2d1 840w, https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?w=1100&fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=d880656a6c120ed2ef06879bb522b840 1100w, https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?w=1650&fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=55b22efc72db2bbb9e26079d46aea5b5 1650w, https://mintcdn.com/agno-v2/QfHdyhk-tu-JEw8s/images/agent-ui.png?w=2500&fit=max&auto=format&n=QfHdyhk-tu-JEw8s&q=85&s=5331541ccf7abdb289f0e213f65c9649 2500w" />
</Frame>

Agno provides a beautiful UI for interacting with your agents, completely open source, free to use and build on top of. It's a simple interface that allows you to chat with your agents, view their memory, knowledge, and more.

<Note>
  The AgentOS only uses data in your database. No data is sent to Agno.
</Note>

Built with Next.js and TypeScript, the Open Source Agent UI was developed in response to community requests for a self-hosted alternative following the success of [AgentOS](https://github.com/agent-os/introduction).

## Get Started with Agent UI

To clone the Agent UI, run the following command in your terminal:

Enter `y` to create a new project, install dependencies, then run the agent-ui using:

Open [http://localhost:3000](http://localhost:3000) to view the Agent UI, but remember to connect to your local agents.

<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=02ec5ad7d40581f565548d64fd228fcc" style={{ borderRadius: '8px' }} data-og-width="3452" data-og-height="1910" data-path="images/agent-ui-homepage.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=280&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=09872fba3043aac4475f9d695e485f4d 280w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=560&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=71e8e367452dc8a47c043a3f23237711 560w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=840&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=bb3309860689377a150fae0c90b4837a 840w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=1100&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=68709e16d4ee79f7a0b91df6aab06612 1100w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=1650&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=7c8a19d650dea910381443f87f1f2e24 1650w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=2500&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=14b29401ceb62992fed4584ecef540af 2500w" />
</Frame>

<Accordion title="Clone the repository manually" icon="github">
  You can also clone the repository manually

And run the agent-ui using

## Connect your AgentOS

The Agent UI needs to connect to a AgentOS server, which you can run locally or on any cloud provider.

Let's start with a local AgentOS server. Create a file `agentos.py`

In another terminal, run the AgentOS server:

<Steps>
  <Step title="Setup your virtual environment">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Install dependencies">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Export your OpenAI key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run the AgentOS">
    
  </Step>
</Steps>

<Tip>Make sure the module path in `agent_os.serve()` matches your filename (e.g., `"agentos:app"` for `agentos.py`).</Tip>

* Open [http://localhost:3000](http://localhost:3000) to view the Agent UI
* Enter the `localhost:7777` endpoint on the left sidebar and start chatting with your agents and teams!

<video autoPlay muted controls className="w-full aspect-video" src="https://mintcdn.com/agno-v2/ZS8LtGag_F9zcKU4/videos/agent-ui-demo.mp4?fit=max&auto=format&n=ZS8LtGag_F9zcKU4&q=85&s=e4d7d59efdd156a677a403baeaa1d1a1" data-path="videos/agent-ui-demo.mp4" />

<CardGroup cols={2}>
  <Card title="AgentOS Introduction" icon="server" href="/agent-os/introduction">
    Learn about AgentOS
  </Card>

<Card title="Building Agents" icon="robot" href="/basics/agents/building-agents">
    Build your own agents
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
Enter `y` to create a new project, install dependencies, then run the agent-ui using:
```

Example 2 (unknown):
```unknown
Open [http://localhost:3000](http://localhost:3000) to view the Agent UI, but remember to connect to your local agents.

<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=02ec5ad7d40581f565548d64fd228fcc" style={{ borderRadius: '8px' }} data-og-width="3452" data-og-height="1910" data-path="images/agent-ui-homepage.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=280&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=09872fba3043aac4475f9d695e485f4d 280w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=560&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=71e8e367452dc8a47c043a3f23237711 560w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=840&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=bb3309860689377a150fae0c90b4837a 840w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=1100&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=68709e16d4ee79f7a0b91df6aab06612 1100w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=1650&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=7c8a19d650dea910381443f87f1f2e24 1650w, https://mintcdn.com/agno-v2/JcEPRFr_oNRDYYcC/images/agent-ui-homepage.png?w=2500&fit=max&auto=format&n=JcEPRFr_oNRDYYcC&q=85&s=14b29401ceb62992fed4584ecef540af 2500w" />
</Frame>

<br />

<Accordion title="Clone the repository manually" icon="github">
  You can also clone the repository manually
```

Example 3 (unknown):
```unknown
And run the agent-ui using
```

Example 4 (unknown):
```unknown
</Accordion>

## Connect your AgentOS

The Agent UI needs to connect to a AgentOS server, which you can run locally or on any cloud provider.

Let's start with a local AgentOS server. Create a file `agentos.py`
```

---

## Invalid JSON

**URL:** llms-txt#invalid-json

curl ... -F 'knowledge_filters={invalid json}'

---

## Agent Extra Metrics

**URL:** llms-txt#agent-extra-metrics

Source: https://docs.agno.com/basics/sessions/metrics/usage/agent-extra-metrics

This example demonstrates how to collect special token metrics including audio, cached, and reasoning tokens. It shows different types of advanced metrics available when working with various OpenAI models.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create web search agent for fallback

**URL:** llms-txt#create-web-search-agent-for-fallback

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## SingleStoreDb

**URL:** llms-txt#singlestoredb

Source: https://docs.agno.com/reference/storage/singlestore

`SingleStoreDb` is a class that implements the Db interface using SingleStore  as the backend storage system. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-singlestore-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Google Calendar

**URL:** llms-txt#google-calendar

**Contents:**
- Prerequisites
  - Install dependencies
  - Setup Google Project and OAuth
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/googlecalendar

Enable an Agent to work with Google Calendar to view and schedule meetings.

### Install dependencies

### Setup Google Project and OAuth

Reference: [https://developers.google.com/calendar/api/quickstart/python](https://developers.google.com/calendar/api/quickstart/python)

1. Enable Google Calender API

* Go to [Google Cloud Console](https://console.cloud.google.com/apis/enableflow?apiid=calendar-json.googleapis.com).
   * Select Project and Enable.

2. Go To API & Service -> OAuth Consent Screen

* If you are a Google Workspace user, select Internal.
   * Otherwise, select External.

4. Fill in the app details (App name, logo, support email, etc).

* Click on Add or Remove Scope.
   * Search for Google Calender API (Make sure you've enabled Google calender API otherwise scopes wont be visible).
   * Select scopes accordingly
     * From the dropdown check on `/auth/calendar` scope
   * Save and continue.

* Click Add Users and enter the email addresses of the users you want to allow during testing.
   * NOTE : Only these users can access the app's OAuth functionality when the app is in "Testing" mode.
     Any other users will receive access denied errors.
   * To make the app available to all users, you'll need to move the app's status to "In Production".
     Before doing so, ensure the app is fully verified by Google if it uses sensitive or restricted scopes.
   * Click on Go back to Dashboard.

7. Generate OAuth 2.0 Client ID

* Go to Credentials.
   * Click on Create Credentials -> OAuth Client ID
   * Select Application Type as Desktop app.
   * Download JSON.

8. Using Google Calender Tool
   * Pass the path of downloaded credentials as credentials\_path to Google Calender tool.
   * Optional: Set the `token_path` parameter to specify where the tool should create the `token.json` file.
   * The `token.json` file is used to store the user's access and refresh tokens and is automatically created during the authorization flow if it doesn't already exist.
   * If `token_path` is not explicitly provided, the file will be created in the default location which is your current working directory.
   * If you choose to specify `token_path`, please ensure that the directory you provide has write access, as the application needs to create or update this file during the authentication process.

The following agent will use GoogleCalendarTools to find today's events.

| Parameter          | Type        | Default      | Description                                                                   |
| ------------------ | ----------- | ------------ | ----------------------------------------------------------------------------- |
| `scopes`           | `List[str]` | `None`       | List of OAuth scopes for Google Calendar API access                           |
| `credentials_path` | `str`       | `None`       | Path of the file credentials.json file which contains OAuth 2.0 Client ID     |
| `token_path`       | `str`       | `token.json` | Path of the file token.json which stores the user's access and refresh tokens |
| `access_token`     | `str`       | `None`       | Direct access token for authentication (alternative to OAuth flow)            |
| `calendar_id`      | `str`       | `primary`    | The calendar ID to use for operations                                         |
| `oauth_port`       | `int`       | `8080`       | Port number for OAuth callback server                                         |
| `allow_update`     | `bool`      | `False`      | Whether to allow write operations (create/update/delete events)               |

| Function       | Description                                        |
| -------------- | -------------------------------------------------- |
| `list_events`  | List events from the user's primary calendar.      |
| `create_event` | Create a new event in the user's primary calendar. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/googlecalendar.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/googlecalendar_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
### Setup Google Project and OAuth

Reference: [https://developers.google.com/calendar/api/quickstart/python](https://developers.google.com/calendar/api/quickstart/python)

1. Enable Google Calender API

   * Go to [Google Cloud Console](https://console.cloud.google.com/apis/enableflow?apiid=calendar-json.googleapis.com).
   * Select Project and Enable.

2. Go To API & Service -> OAuth Consent Screen

3. Select User Type

   * If you are a Google Workspace user, select Internal.
   * Otherwise, select External.

4. Fill in the app details (App name, logo, support email, etc).

5. Select Scope

   * Click on Add or Remove Scope.
   * Search for Google Calender API (Make sure you've enabled Google calender API otherwise scopes wont be visible).
   * Select scopes accordingly
     * From the dropdown check on `/auth/calendar` scope
   * Save and continue.

6. Adding Test User

   * Click Add Users and enter the email addresses of the users you want to allow during testing.
   * NOTE : Only these users can access the app's OAuth functionality when the app is in "Testing" mode.
     Any other users will receive access denied errors.
   * To make the app available to all users, you'll need to move the app's status to "In Production".
     Before doing so, ensure the app is fully verified by Google if it uses sensitive or restricted scopes.
   * Click on Go back to Dashboard.

7. Generate OAuth 2.0 Client ID

   * Go to Credentials.
   * Click on Create Credentials -> OAuth Client ID
   * Select Application Type as Desktop app.
   * Download JSON.

8. Using Google Calender Tool
   * Pass the path of downloaded credentials as credentials\_path to Google Calender tool.
   * Optional: Set the `token_path` parameter to specify where the tool should create the `token.json` file.
   * The `token.json` file is used to store the user's access and refresh tokens and is automatically created during the authorization flow if it doesn't already exist.
   * If `token_path` is not explicitly provided, the file will be created in the default location which is your current working directory.
   * If you choose to specify `token_path`, please ensure that the directory you provide has write access, as the application needs to create or update this file during the authentication process.

## Example

The following agent will use GoogleCalendarTools to find today's events.
```

---

## Analyze session-level metrics

**URL:** llms-txt#analyze-session-level-metrics

print("=" * 50)
print("SESSION METRICS")
print("=" * 50)
pprint(team.get_session_metrics(session_id="team_metrics_demo"))

---

## Manual Knowledge Filters

**URL:** llms-txt#manual-knowledge-filters

**Contents:**
- Step 1: Attach Metadata
- Step 2: Query with Filters
  - 1. On the Agent (applies to all queries)
  - 2. On Each Query (overrides Agent filters for that run)
- Combining Multiple Filters
- Try It Yourself!
- Developer Resources

Manual filtering gives you full control over which documents are searched by specifying filters directly in your code.

## Step 1: Attach Metadata

There are two ways to attach metadata to your documents:

1. **Attach Metadata When Initializing the Knowledge Base**

2. **Attach Metadata When Loading Documents One by One**

> ðŸ’¡ **Tips:**\
> â€¢ Use **Option 1** if you have all your documents and metadata ready at once.\
> â€¢ Use **Option 2** if you want to add documents incrementally or as they become available.

## Step 2: Query with Filters

You can pass filters in two ways:

### 1. On the Agent (applies to all queries)

### 2. On Each Query (overrides Agent filters for that run)

<Note>If you pass filters both on the Agent and on the query, the query-level filters take precedence.</Note>

## Combining Multiple Filters

You can filter by multiple fields:

* Load documents with different metadata.
* Query with different filter combinations.
* Observe how the results change!

## Developer Resources

* [Manual filtering](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/filters/filtering.py)
* [Manual filtering on load](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/filters/filtering_on_load.py)

**Examples:**

Example 1 (unknown):
```unknown
2. **Attach Metadata When Loading Documents One by One**
```

Example 2 (unknown):
```unknown
***

> ðŸ’¡ **Tips:**\
> â€¢ Use **Option 1** if you have all your documents and metadata ready at once.\
> â€¢ Use **Option 2** if you want to add documents incrementally or as they become available.

## Step 2: Query with Filters

You can pass filters in two ways:

### 1. On the Agent (applies to all queries)
```

Example 3 (unknown):
```unknown
### 2. On Each Query (overrides Agent filters for that run)
```

Example 4 (unknown):
```unknown
<Note>If you pass filters both on the Agent and on the query, the query-level filters take precedence.</Note>

## Combining Multiple Filters

You can filter by multiple fields:
```

---

## 5. Create agent with knowledge search enabled

**URL:** llms-txt#5.-create-agent-with-knowledge-search-enabled

**Contents:**
  - What Happens When You Add Content
  - What Happens During a Conversation
- Key Components Working Together
- Choosing Your Chunking Strategy
- Managing Your Knowledge Base

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,  # Required for automatic search
    knowledge_filters={"type": "policy"}  # Optional filtering
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  **Smart Defaults**: Agno provides sensible defaults to get you started quickly:

  * **Embedder**: If no embedder is specified, Agno automatically uses `OpenAIEmbedder` with default settings
  * **Chunking**: If no chunking strategy is provided to readers, Agno defaults to `FixedSizeChunking(chunk_size=5000)`
  * **Search Type**: Vector databases default to `SearchType.vector` for semantic search

  This means you can start with minimal configuration and customize as needed!
</Note>

### What Happens When You Add Content

When you call `knowledge.add_content()`, here's what happens:

1. **A reader parses your file** - Agno picks the right reader (PDFReader, CSVReader, WebsiteReader, etc.) based on your file type and extracts the text
2. **Content gets chunked** - Your chosen chunking strategy breaks the text into digestible pieces, whether by semantic boundaries, fixed sizes, or document structure
3. **Embeddings are created** - Each chunk is converted into a vector embedding using your embedder (OpenAI, SentenceTransformer, etc.)
4. **Status is tracked** - Content moves through states: PROCESSING â†’ COMPLETED or FAILED
5. **Everything is stored** - Chunks, embeddings, and metadata all land in your vector database, ready for search

### What Happens During a Conversation

When your agent receives a question:

1. **The agent decides** - Should I search for more context or answer from what I already know?
2. **Query gets embedded** - If searching, your question becomes a vector using the same embedder
3. **Similar chunks are found** - `knowledge.search()` or `knowledge.async_search()` finds chunks with vectors close to your question
4. **Filters are applied** - Any metadata filters you configured narrow down the results
5. **Agent synthesizes the answer** - Retrieved context + your question = accurate, grounded response

## Key Components Working Together

* **Readers** - Agno's reader factory provides specialized parsers: PDFReader, CSVReader, WebsiteReader, MarkdownReader, and more for different content types.

* **Chunking Strategies** - Choose from FixedSizeChunking, SemanticChunking, or RecursiveChunking to optimize how documents are broken down for search.

* **Embedders** - Support for OpenAIEmbedder, SentenceTransformerEmbedder, and other embedding models to convert text into searchable vectors.

* **Contents Database** - A table in your database that keeps track of what content you've added to your Knowledge base.

* **Vector Databases** - PgVector for production, LanceDB for development, or PineconeDB for managed services - each with hybrid search capabilities.

## Choosing Your Chunking Strategy

How you split content dramatically affects search quality. Agno gives you several strategies to match your content type:

* **Fixed Size** - Splits at consistent character counts. Fast and predictable, great for uniform content
* **Semantic** - Uses embeddings to find natural topic boundaries. Best for complex docs where meaning matters
* **Recursive** - Respects document structure (paragraphs, sections). Good balance of speed and context
* **Document** - Preserves natural document divisions. Perfect for well-structured content
* **CSV Row** - Treats each row as a unit. Essential for tabular data
* **Markdown** - Honors heading hierarchy. Ideal for documentation

Learn more about [choosing the right chunking strategy](/basics/knowledge/chunking/overview) for your use case.

## Managing Your Knowledge Base

Once content is loaded, you'll want to check status, search, and manage what's there:
```

---

## Answer Synthesizer Agent - Specialized in synthesis

**URL:** llms-txt#answer-synthesizer-agent---specialized-in-synthesis

answer_synthesizer = Agent(
    name="Answer Synthesizer",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Synthesize retrieved information into comprehensive answers",
    instructions=[
        "Combine information from the Primary Retriever and Context Expander.",
        "Create a comprehensive, well-structured response.",
        "Ensure logical flow and coherence in the final answer.",
        "Include relevant details while maintaining clarity.",
        "Organize information in a user-friendly format.",
    ],
    markdown=True,
)

---

## Context Analyzer Agent - Specialized in context analysis

**URL:** llms-txt#context-analyzer-agent---specialized-in-context-analysis

context_analyzer = Agent(
    name="Context Analyzer",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Analyze context and relevance of reranked results",
    knowledge=validation_knowledge,
    search_knowledge=True,
    instructions=[
        "Analyze the context and relevance of reranked results.",
        "Cross-validate information against the validation knowledge base.",
        "Assess the quality and accuracy of retrieved content.",
        "Identify the most contextually appropriate information.",
    ],
    markdown=True,
)

---

## Configuration for the Chat page

**URL:** llms-txt#configuration-for-the-chat-page

chat:
  quick_prompts:
    <AGENT_ID>:
      - <PROMPT_1>
      - <PROMPT_2>
      - <PROMPT_3>
      ...
    ...

---

## Slack Reasoning Finance Agent

**URL:** llms-txt#slack-reasoning-finance-agent

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/slack/reasoning-agent

Slack agent with advanced reasoning and financial analysis capabilities

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Advanced Reasoning**: ThinkingTools for step-by-step financial analysis
* **Real-time Data**: Stock prices, analyst recommendations, company news
* **Claude Powered**: Superior analytical and reasoning capabilities
* **Slack Integration**: Works in channels and direct messages
* **Structured Output**: Well-formatted tables and financial insights

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize the X toolkit

**URL:** llms-txt#initialize-the-x-toolkit

x_tools = XTools(
    wait_on_rate_limit=True # Retry when rate limits are reached
)

---

## Scenario Testing

**URL:** llms-txt#scenario-testing

**Contents:**
- Code

Source: https://docs.agno.com/integrations/testing/usage/basic

This example demonstrates how to use the [Scenario](https://github.com/langwatch/scenario) framework for agentic simulation-based testing. Scenario enables you to simulate conversations between agents, user simulators, and judges, making it easy to test and evaluate agent behaviors in a controlled environment.

> **Tip:** Want to see a more advanced scenario? Check out the [Customer support scenario example](https://github.com/langwatch/create-agent-app/tree/main/agno_example) for a more complex agent, including tool calls and advanced scenario features.

```python cookbook/agent_basics/other/scenario_testing.py theme={null}
import pytest
import scenario
from agno.agent import Agent
from agno.models.openai import OpenAIChat

---

## Agentic Filtering

**URL:** llms-txt#agentic-filtering

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/agentic-filtering

```python cookbook/knowledge/filters/agentic_filtering.py theme={null}
import asyncio
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## Create research steps

**URL:** llms-txt#create-research-steps

research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

---

## wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4

**URL:** llms-txt#wget-https://storage.googleapis.com/generativeai-downloads/images/greatredspot.mp4

**Contents:**
- Developer Resources

video_path = Path(__file__).parent.joinpath("GreatRedSpot.mp4")

agent.print_response("Tell me about this video", videos=[Video(filepath=video_path)])
```

## Developer Resources

View more [Examples](/basics/multimodal/video/usage/video-caption)

---

## Run with streaming and consume the generator to get the final response

**URL:** llms-txt#run-with-streaming-and-consume-the-generator-to-get-the-final-response

stream_generator = team.run(
    "Give me a stock report for NVDA",
    stream=True,
    stream_events=True,
)

---

## Example 3: Using AND operator

**URL:** llms-txt#example-3:-using-and-operator

print("Using AND operator")
sales_agent.print_response(
    "Describe revenue performance for the region",
    knowledge_filters=[
        AND(EQ("data_type", "sales"), NOT(EQ("region", "north_america")))
    ],
    markdown=True,
)
python filter_expressions_teams.py theme={null}
from agno.agent import Agent
from agno.filters import AND, IN, NOT
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pgvector import PgVector

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Initialize LangDB tracing - must be called before creating agents

**URL:** llms-txt#initialize-langdb-tracing---must-be-called-before-creating-agents

from agno.agent import Agent
from agno.models.langdb import LangDB
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Example: Create a dataframe with sample data and get the first 5 rows

**URL:** llms-txt#example:-create-a-dataframe-with-sample-data-and-get-the-first-5-rows

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("""
Please perform these tasks:
1. Create a pandas dataframe named 'sales_data' using DataFrame() with this sample data:
   {'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],
    'product': ['Widget A', 'Widget B', 'Widget A', 'Widget C', 'Widget B'],
    'quantity': [10, 15, 8, 12, 20],
    'price': [9.99, 15.99, 9.99, 12.99, 15.99]}
2. Show me the first 5 rows of the sales_data dataframe
""")
```

| Parameter                        | Type   | Default | Description                                        |
| -------------------------------- | ------ | ------- | -------------------------------------------------- |
| `enable_create_pandas_dataframe` | `bool` | `True`  | Enables functionality to create pandas DataFrames. |
| `enable_run_dataframe_operation` | `bool` | `True`  | Enables functionality to run DataFrame operations. |
| `all`                            | `bool` | `False` | Enables all functionality when set to True.        |

| Function                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `create_pandas_dataframe` | Creates a Pandas DataFrame named `dataframe_name` by using the specified function `create_using_function` with parameters `function_parameters`. Parameters include 'dataframe\_name' for the name of the DataFrame, 'create\_using\_function' for the function to create it (e.g., 'read\_csv'), and 'function\_parameters' for the arguments required by the function. Returns the name of the created DataFrame if successful, otherwise returns an error message. |
| `run_dataframe_operation` | Runs a specified operation `operation` on a DataFrame `dataframe_name` with the parameters `operation_parameters`. Parameters include 'dataframe\_name' for the DataFrame to operate on, 'operation' for the operation to perform (e.g., 'head', 'tail'), and 'operation\_parameters' for the arguments required by the operation. Returns the result of the operation if successful, otherwise returns an error message.                                             |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/pandas.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/pandas_tools.py)

---

## ------------------------------------------------------------------------------------------------

**URL:** llms-txt#------------------------------------------------------------------------------------------------

---

## WhatsApp Image Generation Agent (Tool-based)

**URL:** llms-txt#whatsapp-image-generation-agent-(tool-based)

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/whatsapp/image-generation-tools

WhatsApp agent that generates images using OpenAI's image generation tools

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Tool-based Generation**: OpenAI's GPT Image-1 model via external tools
* **High-Quality Images**: Professional-grade image generation
* **Conversational Interface**: Natural language interaction for image requests
* **History Context**: Remembers previous images and conversations
* **GPT-4o Orchestration**: Intelligent conversation and tool management

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize Searxng with your Searxng instance URL

**URL:** llms-txt#initialize-searxng-with-your-searxng-instance-url

searxng = SearxngTools(
    host="http://localhost:53153",
    engines=[],
    fixed_max_results=5,
    news=True,
    science=True
)

---

## Pinecone Async

**URL:** llms-txt#pinecone-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/pinecone/usage/async-pinecone-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## User Memories and Session Summaries

**URL:** llms-txt#user-memories-and-session-summaries

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/getting-started/10-user-memories-and-summaries

This example shows how to create an agent with persistent memory that stores:

1. Personalized user memories - facts and preferences learned about specific users
2. Session summaries - key points and context from conversations
3. Chat history - stored in SQLite for persistence

* Stores user-specific memories in SQLite database
* Maintains session summaries for context
* Continues conversations across sessions with memory
* References previous context and user information in responses

Examples:
User: "My name is John and I live in NYC"
Agent: *Creates memory about John's location*

User: "What do you remember about me?"
Agent: *Recalls previous memories about John*

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Traditional RAG with PgVector

**URL:** llms-txt#traditional-rag-with-pgvector

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/traditional-rag-pgvector

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Snippet file="run-pgvector-step.mdx" />

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Query with an Agent

**URL:** llms-txt#query-with-an-agent

**Contents:**
- Redis Params
- Developer Resources

agent = Agent(knowledge=knowledge)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)
```

<Snippet file="vectordb_redis_params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/redis_db/redis_db.py)

---

## Create a knowledge base with the PPTX documents

**URL:** llms-txt#create-a-knowledge-base-with-the-pptx-documents

knowledge = Knowledge(
    # Table name: ai.pptx_documents
    vector_db=PgVector(
        table_name="pptx_documents",
        db_url=db_url,
    ),
)

---

## Visualization

**URL:** llms-txt#visualization

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/visualization

VisualizationTools enables agents to create various types of charts and plots using matplotlib.

The following agent can create various types of data visualizations:

| Parameter                    | Type   | Default    | Description                         |
| ---------------------------- | ------ | ---------- | ----------------------------------- |
| `output_dir`                 | `str`  | `"charts"` | Directory to save generated charts. |
| `enable_create_bar_chart`    | `bool` | `True`     | Enable bar chart creation.          |
| `enable_create_line_chart`   | `bool` | `True`     | Enable line chart creation.         |
| `enable_create_pie_chart`    | `bool` | `True`     | Enable pie chart creation.          |
| `enable_create_scatter_plot` | `bool` | `True`     | Enable scatter plot creation.       |
| `enable_create_histogram`    | `bool` | `True`     | Enable histogram creation.          |

| Function              | Description                                                 |
| --------------------- | ----------------------------------------------------------- |
| `create_bar_chart`    | Create bar charts for categorical data comparison.          |
| `create_line_chart`   | Create line charts for time series and trend visualization. |
| `create_pie_chart`    | Create pie charts for proportional data representation.     |
| `create_scatter_plot` | Create scatter plots for correlation analysis.              |
| `create_histogram`    | Create histograms for data distribution visualization.      |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/visualization.py)
* [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)

---

## Zendesk

**URL:** llms-txt#zendesk

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/zendesk

**ZendeskTools** enable an Agent to access Zendesk API to search for articles.

The following example requires the `requests` library and auth credentials.

The following agent will run seach Zendesk for "How do I login?" and print the response.

| Parameter               | Type   | Default | Description                                                             |
| ----------------------- | ------ | ------- | ----------------------------------------------------------------------- |
| `username`              | `str`  | -       | The username used for authentication or identification purposes.        |
| `password`              | `str`  | -       | The password associated with the username for authentication purposes.  |
| `company_name`          | `str`  | -       | The name of the company related to the user or the data being accessed. |
| `enable_search_zendesk` | `bool` | `True`  | Enable the search Zendesk functionality.                                |
| `all`                   | `bool` | `False` | Enable all functionality.                                               |

| Function         | Description                                                                                    |
| ---------------- | ---------------------------------------------------------------------------------------------- |
| `search_zendesk` | This function searches for articles in Zendesk Help Center that match the given search string. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/zendesk.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/zendesk_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will run seach Zendesk for "How do I login?" and print the response.
```

---

## 1. Create memories by setting `enable_user_memories=True` in the Agent

**URL:** llms-txt#1.-create-memories-by-setting-`enable_user_memories=true`-in-the-agent

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
)
team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    members=[agent],
    db=db,
    enable_user_memories=True,
)

team.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
    session_id=session_id,
)

team.print_response(
    "What are my hobbies?", stream=True, user_id=john_doe_id, session_id=session_id
)

---

## Ollama Embedder

**URL:** llms-txt#ollama-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/ollama/usage/ollama-embedder

```python  theme={null}
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = OllamaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Get your Neon database URL

**URL:** llms-txt#get-your-neon-database-url

NEON_DB_URL = getenv("NEON_DB_URL")

---

## Find recent documents

**URL:** llms-txt#find-recent-documents

---

## Agent with Reasoning Tools Tracing

**URL:** llms-txt#agent-with-reasoning-tools-tracing

Source: https://docs.agno.com/agent-os/tracing/usage/agent-with-reasoning-tools-tracing

Learn how to trace your agents with reasoning tools with Agno in AgentOS

This example shows how to trace an agent that uses reasoning tools in AgentOS. The traces capture the agent's reasoning process, including all intermediate steps.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run AgentOS">
    <CodeGroup>

Your AgentOS will be available at `http://localhost:7777`. View traces in the AgentOS dashboard.
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cancel Team Run

**URL:** llms-txt#cancel-team-run

Source: https://docs.agno.com/reference-api/schema/teams/cancel-team-run

post /teams/{team_id}/runs/{run_id}/cancel
Cancel a currently executing team run. This will attempt to stop the team's execution gracefully.

**Note:** Cancellation may not be immediate for all operations.

---

## 2. Set a custom MemoryManager on the agent

**URL:** llms-txt#2.-set-a-custom-memorymanager-on-the-agent

---

## Dedicated database for traces (separate from agent databases)

**URL:** llms-txt#dedicated-database-for-traces-(separate-from-agent-databases)

tracing_db = SqliteDb(db_file="tmp/traces.db", id="traces_db")

---

## Pandas

**URL:** llms-txt#pandas

Source: https://docs.agno.com/integrations/toolkits/database/pandas

The PandasTools toolkit enables an Agent to perform data manipulation tasks using the Pandas library.

**PandasTools** enable an Agent to perform data manipulation tasks using the Pandas library.

```python cookbook/tools/pandas_tool.py theme={null}
from agno.agent import Agent
from agno.tools.pandas import PandasTools

---

## Create an Agent with the Sleep tool

**URL:** llms-txt#create-an-agent-with-the-sleep-tool

agent = Agent(tools=[SleepTools()], name="Sleep Agent")

---

## Chat Interface

**URL:** llms-txt#chat-interface

**Contents:**
- Overview
- Chat Interfaces
  - Chat with an Agent
  - Work with a Team
  - Run a Workflow
- Troubleshooting
- Related Examples

Source: https://docs.agno.com/agent-os/features/chat-interface

Use AgentOS chat to talk to agents, collaborate with teams, and run workflows

The AgentOS chat is the home for dayâ€‘toâ€‘day work with your AI system. From one screen you can:

* Chat with individual agents
* Collaborate with agent teams
* Trigger and monitor workflows
* Review sessions, knowledge, memory, and metrics

Itâ€™s designed to feel familiarâ€”type a message, attach files, and get live, streaming responses. Each agent, team, and workflow maintains its own context so you can switch between tasks without losing your place.

### Chat with an Agent

* Select an agent from the right panel.
* Ask a question like â€œWhat tools do you have access to?â€
* Agents keep their own history, tools, and instructions; switching agents wonâ€™t mix contexts.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/MMgohmDbM-qeNPya/videos/agentos-agent-chat.mp4?fit=max&auto=format&n=MMgohmDbM-qeNPya&q=85&s=45ae6af616b33280bc431ff63f77cabb" type="video/mp4" data-path="videos/agentos-agent-chat.mp4" />
  </video>
</Frame>

<Info>
  **Learn more about Agents**: Dive deeper into agent configuration, tools,
  memory, and advanced features in our [Agents
  Documentation](/basics/agents/overview).
</Info>

* Switch the top toggle to Teams and pick a team.
* A team delegates tasks to its members and synthesizes their responses into a cohesive response.
* Use the chat stream to watch how the team divides and solves the task.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/CnjZpOWVs1q9bnAO/videos/agentos-teams-chat.mp4?fit=max&auto=format&n=CnjZpOWVs1q9bnAO&q=85&s=b9b6e4ba67bcf79396cef64c58ff7e9a" type="video/mp4" data-path="videos/agentos-teams-chat.mp4" />
  </video>
</Frame>

<Info>
  **Learn more about Teams**: Explore team modes, coordination strategies, and
  multi-agent collaboration in our [Teams
  Documentation](/basics/teams/overview).
</Info>

* Switch to Workflows and choose one.
* Provide the input (plain text or structured, depending on the workflow).
* Watch execution live: steps stream as they start, produce output, and finish.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/CnjZpOWVs1q9bnAO/videos/agentos-workflows-chat.mp4?fit=max&auto=format&n=CnjZpOWVs1q9bnAO&q=85&s=6bfc2fbab99d64e53129e80b49a6be8e" type="video/mp4" data-path="videos/agentos-workflows-chat.mp4" />
  </video>
</Frame>

<Info>
  **Learn more about Workflows**: Discover workflow types, advanced patterns,
  and automation strategies in our [Workflows
  Documentation](/basics/workflows/overview).
</Info>

* The page loads but nothing responds: verify your AgentOS app is running.
* Canâ€™t see previous chats: you may be in a new sessionâ€”open the Sessions panel and pick an older one.
* File didnâ€™t attach: try a common format (png, jpg, pdf, csv, docx, txt, mp3, mp4) and keep size reasonable.

<CardGroup cols={3}>
  <Card title="Demo AgentOS" icon="play" href="/agent-os/usage/demo">
    Comprehensive demo with agents, knowledge, and evaluation system
  </Card>

<Card title="Advanced Demo" icon="rocket" href="/agent-os/usage/demo">
    Advanced demo with knowledge, storage, and multiple agents
  </Card>

<Card title="Slack Interface" icon="slack" href="/agent-os/usage/interfaces/slack/basic">
    Deploy agents to Slack channels
  </Card>

<Card title="WhatsApp Interface" icon="message" href="/agent-os/usage/interfaces/whatsapp/basic">
    Connect agents to WhatsApp messaging
  </Card>
</CardGroup>

---

## Agentic RAG with LanceDB

**URL:** llms-txt#agentic-rag-with-lancedb

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag-lancedb

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Async content loading for better performance

**URL:** llms-txt#async-content-loading-for-better-performance

await knowledge.add_content_async(path="large_dataset/")

---

## -*- Secrets for production application

**URL:** llms-txt#-*--secrets-for-production-application

prd_secret = SecretsManager(
    ...
    # Create secret from workspace/secrets/prd_app_secrets.yml
    secret_files=[
        infra_settings.infra_root.joinpath("infra/secrets/prd_app_secrets.yml")
    ],
)

---

## Example showing economic indicators

**URL:** llms-txt#example-showing-economic-indicators

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "Show me the latest GDP growth rate and inflation numbers for the US"
)
```

| Parameter                      | Type   | Default      | Description                                                                                                              |
| ------------------------------ | ------ | ------------ | ------------------------------------------------------------------------------------------------------------------------ |
| `obb`                          | `Any`  | `None`       | OpenBB app instance. If not provided, uses default.                                                                      |
| `openbb_pat`                   | `str`  | `None`       | Personal Access Token for OpenBB API authentication.                                                                     |
| `provider`                     | `str`  | `"yfinance"` | Data provider for financial information. Options: "benzinga", "fmp", "intrinio", "polygon", "tiingo", "tmx", "yfinance". |
| `enable_get_stock_price`       | `bool` | `True`       | Enable the stock price retrieval function.                                                                               |
| `enable_search_company_symbol` | `bool` | `False`      | Enable the company symbol search function.                                                                               |
| `enable_get_company_news`      | `bool` | `False`      | Enable the company news retrieval function.                                                                              |
| `enable_get_company_profile`   | `bool` | `False`      | Enable the company profile retrieval function.                                                                           |
| `enable_get_price_targets`     | `bool` | `False`      | Enable the price targets retrieval function.                                                                             |
| `all`                          | `bool` | `False`      | Enable all available functions. When True, all enable flags are ignored.                                                 |

| Function                | Description                                                                       |
| ----------------------- | --------------------------------------------------------------------------------- |
| `get_stock_price`       | This function gets the current stock price for a stock symbol or list of symbols. |
| `search_company_symbol` | This function searches for the stock symbol of a company.                         |
| `get_price_targets`     | This function gets the price targets for a stock symbol or list of symbols.       |
| `get_company_news`      | This function gets the latest news for a stock symbol or list of symbols.         |
| `get_company_profile`   | This function gets the company profile for a stock symbol or list of symbols.     |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/openbb.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/openbb_tools.py)

---

## Function instead of steps

**URL:** llms-txt#function-instead-of-steps

Source: https://docs.agno.com/basics/workflows/usage/function-instead-of-steps

This example demonstrates how to use just a single function instead of steps in a workflow.

This example demonstrates **Workflows** using a single custom execution function instead of
discrete steps. This pattern gives you complete control over the orchestration logic while still
benefiting from workflow features like storage, streaming, and session management.

**When to use**: When you need maximum flexibility and control over the execution flow, similar
to Workflows 1.0 approach but with a better structured approach.

```python function_instead_of_steps.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow

---

## Coordinated Agentic RAG Team

**URL:** llms-txt#coordinated-agentic-rag-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/coordinated-agentic-rag

This example demonstrates how multiple specialized agents can coordinate to provide comprehensive RAG (Retrieval-Augmented Generation) responses by dividing search and analysis tasks across team members.

```python cookbook/examples/teams/search_coordination/01_coordinated_agentic_rag.py theme={null}
"""
This example demonstrates how multiple specialized agents can coordinate to provide
comprehensive RAG (Retrieval-Augmented Generation) responses by dividing search
and analysis tasks across team members.

Team Composition:
- Knowledge Searcher: Searches knowledge base for relevant information
- Content Analyzer: Analyzes and synthesizes retrieved content
- Response Synthesizer: Creates final comprehensive response with sources

Setup:
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy`
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run this script to see coordinated RAG in action
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.vectordb.lancedb import LanceDb, SearchType

---

## Create a team for collaborative audio-to-text processing

**URL:** llms-txt#create-a-team-for-collaborative-audio-to-text-processing

**Contents:**
- Usage

audio_team = Team(
    name="Audio Analysis Team",
    model=Gemini(id="gemini-2.0-flash-exp"),
    members=[transcription_specialist, content_analyzer],
    instructions=[
        "Work together to transcribe and analyze audio content.",
        "Transcription Specialist: First convert audio to accurate text with speaker identification.",
        "Content Analyzer: Analyze transcription for insights and key themes.",
    ],
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

audio_team.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)
bash  theme={null}
    pip install agno requests google-generativeai
    bash  theme={null}
    export GOOGLE_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/audio_to_text.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Image Agent with File Upload

**URL:** llms-txt#image-agent-with-file-upload

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/image-input-file-upload

```python cookbook/models/google/gemini/image_input_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from google.generativeai import upload_file
from google.generativeai.types import file_types

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## Semantic Chunking

**URL:** llms-txt#semantic-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/semantic

Semantic chunking is a method of splitting documents into smaller chunks by analyzing semantic similarity between text segments using embeddings.
It uses the chonkie library to identify natural breakpoints where the semantic meaning changes significantly, based on a configurable similarity threshold.
This helps preserve context and meaning better than fixed-size chunking by ensuring semantically related content stays together in the same chunk, while splitting occurs at meaningful topic transitions.

<Snippet file="chunking-semantic.mdx" />

---

## Get embedding with usage information

**URL:** llms-txt#get-embedding-with-usage-information

embedding, usage = custom_embedder.get_embedding_and_usage(
    "Advanced text processing with Jina embeddings and late chunking."
)
print(f"Embedding dimensions: {len(embedding)}")
if usage:
    print(f"Usage info: {usage}")

---

## - Replace the values below or use environment variables

**URL:** llms-txt#--replace-the-values-below-or-use-environment-variables

vector_db = UpstashVectorDb(
    url=os.getenv("UPSTASH_VECTOR_REST_URL"),
    token=os.getenv("UPSTASH_VECTOR_REST_TOKEN"),
)

---

## Setup your Agent using a reasoning model with high reasoning effort

**URL:** llms-txt#setup-your-agent-using-a-reasoning-model-with-high-reasoning-effort

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini", reasoning_effort="high"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## Serialize to JSON

**URL:** llms-txt#serialize-to-json

filter_json = json.dumps(filter_expr.to_dict())

---

## Create agent with knowledge

**URL:** llms-txt#create-agent-with-knowledge

sales_agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

---

## Create an Creative AI Artist Agent

**URL:** llms-txt#create-an-creative-ai-artist-agent

image_agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DalleTools()],
    description=dedent("""\
        You are an experienced AI artist with expertise in various artistic styles,
        from photorealism to abstract art. You have a deep understanding of composition,
        color theory, and visual storytelling.\
    """),
    instructions=dedent("""\
        As an AI artist, follow these guidelines:
        1. Analyze the user's request carefully to understand the desired style and mood
        2. Before generating, enhance the prompt with artistic details like lighting, perspective, and atmosphere
        3. Use the `create_image` tool with detailed, well-crafted prompts
        4. Provide a brief explanation of the artistic choices made
        5. If the request is unclear, ask for clarification about style preferences

Always aim to create visually striking and meaningful images that capture the user's vision!\
    """),
    markdown=True,
    db=SqliteDb(session_table="test_agent", db_file="tmp/test.db"),
)

---

## Test the team

**URL:** llms-txt#test-the-team

team.print_response("What is the capital of France?", stream=True)

---

## YouTube Reader Async

**URL:** llms-txt#youtube-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/youtube-reader-async

The **YouTube Reader** allows you to extract transcripts from YouTube videos and convert them into vector embeddings for your knowledge base.

```python examples/basics/knowledge/readers/youtube_reader.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.youtube_reader import YouTubeReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Agent using our URLGuardrail

**URL:** llms-txt#agent-using-our-urlguardrail

agent = Agent(
    name="URL-Protected Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    # Provide the Guardrails to be used with the pre_hooks parameter
    pre_hooks=[URLGuardrail()],
)

---

## Async agent responses

**URL:** llms-txt#async-agent-responses

**Contents:**
  - Knowledge Filtering

response = await agent.arun("What's in the dataset?")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Knowledge Filtering

Control what information agents can access:
```

---

## Better quality for complex content (but slower)

**URL:** llms-txt#better-quality-for-complex-content-(but-slower)

**Contents:**
  - 5. Use Async for Batch Operations
- Common Performance Pitfalls
  - Issue: Search Returns Irrelevant Results

quality_chunking = SemanticChunking(
    chunk_size=1200,
    similarity_threshold=0.5
)
python  theme={null}
import asyncio

async def load_knowledge_efficiently():
    # Load multiple content sources in parallel
    tasks = [
        knowledge.add_content_async(path="docs/hr/"),
        knowledge.add_content_async(path="docs/engineering/"),
        knowledge.add_content_async(url="https://company.com/api-docs"),
    ]
    await asyncio.gather(*tasks)

asyncio.run(load_knowledge_efficiently())
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Learn more about [choosing chunking strategies](/basics/knowledge/chunking/overview).

### 5. Use Async for Batch Operations

Process multiple items concurrently:
```

Example 2 (unknown):
```unknown
## Common Performance Pitfalls

### Issue: Search Returns Irrelevant Results

**What's happening:** Chunks are too large, too small, or chunking strategy doesn't match your content.

**Quick fixes:**

1. Check your chunking strategy - try semantic chunking for better context
2. Verify content actually loaded: `knowledge.get_content_status(content_id)`
3. Increase `max_results` to see if relevant results are just ranked lower
4. Add metadata filters to narrow the search scope
```

---

## Create the research steps

**URL:** llms-txt#create-the-research-steps

research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

publish_content = Step(
    name="publish_content",
    agent=content_agent,
    description="Create and format final content for publication",
)

---

## 10-message conversation where agent updates memory 7 times:

**URL:** llms-txt#10-message-conversation-where-agent-updates-memory-7-times:

---

## Setup the SQLite database

**URL:** llms-txt#setup-the-sqlite-database

db = SqliteDb(db_file="tmp/data.db")

---

## PPTX Reader Async

**URL:** llms-txt#pptx-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/pptx-reader-async

The **PPTX Reader** with asynchronous processing allows you to read and extract text content from PowerPoint (.pptx) files with better performance for concurrent operations.

```python pptx_reader_async.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pptx_reader import PPTXReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    # Table name: ai.pptx_documents
    vector_db=PgVector(
        table_name="pptx_documents",
        db_url=db_url,
    ),
)

---

## IBM WatsonX

**URL:** llms-txt#ibm-watsonx

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/ibm-watsonx

The IBM WatsonX model provides access to IBM's language models.

| Parameter    | Type            | Default                                               | Description                                                               |
| ------------ | --------------- | ----------------------------------------------------- | ------------------------------------------------------------------------- |
| `id`         | `str`           | `"meta-llama/llama-3-1-70b-instruct"`                 | The id of the IBM WatsonX model to use                                    |
| `name`       | `str`           | `"IBMWatsonx"`                                        | The name of the model                                                     |
| `provider`   | `str`           | `"IBM"`                                               | The provider of the model                                                 |
| `api_key`    | `Optional[str]` | `None`                                                | The API key for IBM WatsonX (defaults to WATSONX\_API\_KEY env var)       |
| `base_url`   | `str`           | `"https://us-south.ml.cloud.ibm.com/ml/v1/text/chat"` | The base URL for the IBM WatsonX API                                      |
| `project_id` | `Optional[str]` | `None`                                                | The project ID for IBM WatsonX (defaults to WATSONX\_PROJECT\_ID env var) |

\| `retries`    | `int`              | `0`                            | Number of retries to attempt before raising a ModelProviderError      |
\| `delay_between_retries` | `int`    | `1`                            | Delay between retries, in seconds                                     |
\| `exponential_backoff` | `bool`     | `False`                        | If True, the delay between retries is doubled each time               |

IBM WatsonX extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Vercel with Reasoning Tools

**URL:** llms-txt#vercel-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/vercel-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your V0 API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your V0 API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create Knowledge Instance

**URL:** llms-txt#create-knowledge-instance

knowledge = Knowledge(
    name="YouTube Knowledge Base",
    description="Knowledge base from YouTube video transcripts",
    vector_db=PgVector(
        table_name="youtube_vectors", 
        db_url=db_url
    ),
)

---

## Update Memory

**URL:** llms-txt#update-memory

Source: https://docs.agno.com/reference-api/schema/memory/update-memory

patch /memories/{memory_id}
Update an existing user memory's content and topics. Replaces the entire memory content and topic list with the provided values.

---

## Create your custom FastAPI app

**URL:** llms-txt#create-your-custom-fastapi-app

app = FastAPI(title="My Custom App")

---

## AgentOS Configuration

**URL:** llms-txt#agentos-configuration

**Contents:**
- Configuration file
- Code

Source: https://docs.agno.com/agent-os/usage/extra-configuration

Passing extra configuration to your AgentOS

## Configuration file

We will first create a YAML file with the extra configuration we want to pass to our AgentOS:

```python cookbook/agent_os/os_config/yaml_config.py theme={null}
"""Example showing how to pass extra configuration to your AgentOS."""

from pathlib import Path

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.vectordb.pgvector import PgVector
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

**Examples:**

Example 1 (unknown):
```unknown
## Code
```

---

## Configure Agno to use the custom logger

**URL:** llms-txt#configure-agno-to-use-the-custom-logger

configure_agno_logging(custom_default_logger=custom_logger)

---

## We run the Agent and get the run response

**URL:** llms-txt#we-run-the-agent-and-get-the-run-response

run_response = agent.run("Perform sensitive operation")

---

## Apple Metal (macOS)

**URL:** llms-txt#apple-metal-(macos)

---

## Share Member Interactions

**URL:** llms-txt#share-member-interactions

Source: https://docs.agno.com/basics/state/team/usage/share-member-interactions

This example demonstrates how to enable sharing of member interactions within a team. When `share_member_interactions` is set to True, team members can see and build upon each other's responses, creating a collaborative workflow.

<Steps>
  <Step title="Create a Python file">
    Create a file `share_member_interactions.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the team">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Agent with Tools Stream

**URL:** llms-txt#agent-with-tools-stream

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/llama-cpp/usage/tool-use-stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install LlamaCpp">
    Follow the [LlamaCpp installation guide](https://github.com/ggerganov/llama.cpp) and start the server:

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install LlamaCpp">
    Follow the [LlamaCpp installation guide](https://github.com/ggerganov/llama.cpp) and start the server:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Traditional RAG with LanceDB

**URL:** llms-txt#traditional-rag-with-lancedb

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/traditional-rag-lancedb

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create a Steps sequence with a Condition containing Parallel steps

**URL:** llms-txt#create-a-steps-sequence-with-a-condition-containing-parallel-steps

article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        initial_research_step,
        # Condition with Parallel steps inside
        Condition(
            name="TechResearchCondition",
            description="If topic is tech-related, do specialized parallel research",
            evaluator=is_tech_topic,
            steps=[
                Parallel(
                    tech_research_step,
                    news_research_step,
                    name="SpecializedResearch",
                    description="Parallel tech and news research",
                ),
                content_prep_step,
            ],
        ),
        writing_step,
        editing_step,
    ],
)

---

## Image to Fiction Story Team

**URL:** llms-txt#image-to-fiction-story-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/image-to-text

This example demonstrates how a team can collaborate to analyze images and create engaging fiction stories using an image analyst and creative writer.

```python cookbook/examples/teams/multimodal/image_to_text.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.team import Team

image_analyzer = Agent(
    name="Image Analyst",
    role="Analyze and describe images in detail",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Analyze images carefully and provide detailed descriptions",
        "Focus on visual elements, composition, and key details",
    ],
)

creative_writer = Agent(
    name="Creative Writer",
    role="Create engaging stories and narratives",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Transform image descriptions into compelling fiction stories",
        "Use vivid language and creative storytelling techniques",
    ],
)

---

## Create analysis steps

**URL:** llms-txt#create-analysis-steps

trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

---

## Please download "GreatRedSpot.mp4" using

**URL:** llms-txt#please-download-"greatredspot.mp4"-using

---

## Oxylabs

**URL:** llms-txt#oxylabs

**Contents:**
- Prerequisites
- Example
- Amazon Product Search
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/oxylabs

**OxylabsTools** provide Agents with access to Oxylabs' powerful web scraping capabilities, including SERP, Amazon product data, and universal web scraping endpoints.

The following examples require the `oxylabs-sdk` library:

Set your credentials as environment variables (recommended):

## Amazon Product Search

| Parameter  | Type  | Default | Description                                                                             |
| ---------- | ----- | ------- | --------------------------------------------------------------------------------------- |
| `username` | `str` | `None`  | Oxylabs dashboard username. If not provided, it defaults to `OXYLABS_USERNAME` env var. |
| `password` | `str` | `None`  | Oxylabs dashboard password. If not provided, it defaults to `OXYLABS_PASSWORD` env var. |

| Function                 | Description                                                                                            |
| ------------------------ | ------------------------------------------------------------------------------------------------------ |
| `search_google`          | Performs a Google SERP search. Accepts all the standard Oxylabs params (e.g. `query`, `geo_location`). |
| `get_amazon_product`     | Retrieves the details of Amazon product(s). Accepts ASIN code or full product URL.                     |
| `search_amazon_products` | Searches for Amazon product(s) using a search term.                                                    |
| `scrape_website`         | Scrapes a webpage URL.                                                                                 |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/oxylabs.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/oxylabs_tools.py)
* View [Oxylabs MCP Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/mcp/oxylabs.py)

**Examples:**

Example 1 (unknown):
```unknown
Set your credentials as environment variables (recommended):
```

Example 2 (unknown):
```unknown
## Example
```

Example 3 (unknown):
```unknown
## Amazon Product Search
```

---

## High Fidelity Image Input

**URL:** llms-txt#high-fidelity-image-input

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/image-input-high-fidelity

This example demonstrates how to use high fidelity image analysis with an AI agent by setting the detail parameter.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Agno

**URL:** llms-txt#agno

python cookbook/evals/performance/instantiate_agent_with_tool.py

---

## Evidence Evaluator Agent - Specialized in evidence assessment

**URL:** llms-txt#evidence-evaluator-agent---specialized-in-evidence-assessment

evidence_evaluator = Agent(
    name="Evidence Evaluator",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Evaluate evidence quality and identify information gaps",
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Evaluate the quality and reliability of gathered evidence.",
        "Identify gaps in information or reasoning.",
        "Assess the strength of logical connections.",
        "Highlight areas needing additional clarification.",
        "Use reasoning tools to structure your evaluation.",
    ],
    markdown=True,
)

---

## Step with Function using Additional Data

**URL:** llms-txt#step-with-function-using-additional-data

**Contents:**
- Key Features:

Source: https://docs.agno.com/basics/workflows/usage/step-with-function-additional-data

This example demonstrates **Workflows 2.0** support for passing metadata and contextual information to steps via `additional_data`.

This example shows how to pass metadata and contextual information to steps via `additional_data`. This allows separation of workflow logic from configuration, enabling dynamic behavior based on external context.

* **Context-Aware Steps**: Access `step_input.additional_data` in custom functions
* **Flexible Metadata**: Pass user info, priorities, settings, etc.
* **Clean Separation**: Keep workflow logic focused while enriching steps with context

```python step_with_function_additional_data.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## Filter with different data types

**URL:** llms-txt#filter-with-different-data-types

**Contents:**
  - Filter Expressions (Advanced Approach)
  - Multiple Filter Expressions
- Error Handling
  - Invalid Filter Structure

{"active": True, "priority": 1, "department": "engineering"}
python Python Client theme={null}
  import requests
  import json
  from agno.filters import EQ

# Create filter expression
  filter_expr = EQ("category", "technology")

# Serialize to JSON
  filter_json = json.dumps(filter_expr.to_dict())

# Send request
  response = requests.post(
      "http://localhost:7777/agents/my-agent/runs",
      data={
          "message": "What are the latest tech articles?",
          "stream": "false",
          "knowledge_filters": filter_json,
      }
  )

result = response.json()
  bash cURL theme={null}
  curl -X 'POST' \
    'http://localhost:7777/agents/my-agent/runs' \
    -H 'Content-Type: multipart/form-data' \
    -F 'message=What are the latest tech articles?' \
    -F 'stream=false' \
    -F 'knowledge_filters={"op": "EQ", "key": "category", "value": "technology"}'
  python Python Client theme={null}
  from agno.filters import EQ, GT

# Create multiple filters
  filters = [
      EQ("status", "published"),
      GT("date", "2024-01-01")
  ]

# Serialize list to JSON
  filters_json = json.dumps([f.to_dict() for f in filters])

response = requests.post(
      "http://localhost:7777/agents/my-agent/runs",
      data={
          "message": "Show recent published articles",
          "stream": "false",
          "knowledge_filters": filters_json,
      }
  )
  bash cURL theme={null}
  curl -X 'POST' \
    'http://localhost:7777/agents/my-agent/runs' \
    -H 'Content-Type: multipart/form-data' \
    -F 'message=Show recent published articles' \
    -F 'stream=false' \
    -F 'knowledge_filters=[{"op": "EQ", "key": "status", "value": "published"}, {"op": "GT", "key": "date", "value": "2024-01-01"}]'
  bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Filter Expressions (Advanced Approach)

For complex filtering with logical operators and comparisons:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

### Multiple Filter Expressions

Send multiple filter expressions as a JSON array:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MCPTools within AgentOS

**URL:** llms-txt#mcptools-within-agentos

**Contents:**
- Example

Source: https://docs.agno.com/agent-os/mcp/tools

Learn how to use MCPTools in the Agents, Teams and Workflows within your AgentOS

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) enables Agents to interact with external systems through a standardized interface.

You can give your Agents access to MCP tools using the `MCPTools` class. Read more about using MCP tools [here](/basics/tools/mcp/overview).

Your `MCPTools` will work normally within AgentOS. Their lifecycle is automatically handled, you don't need to handle their connection and disconnection.

<Note>
  If you are using `MCPTools` within AgentOS, you should not use `reload=True` when serving your AgentOS.
  This can break the MCP connection during the FastAPI lifecycle.
</Note>

```python mcp_tools_example.py theme={null}
from agno.agent import Agent
from agno.os import AgentOS
from agno.tools.mcp import MCPTools

---

## Web Search Reader

**URL:** llms-txt#web-search-reader

Source: https://docs.agno.com/reference/knowledge/reader/web-search

WebSearchReader is a reader class that allows you to read data from web search results.

<Snippet file="web-search-reader-reference.mdx" />

---

## Execute Evaluation

**URL:** llms-txt#execute-evaluation

Source: https://docs.agno.com/reference-api/schema/evals/execute-evaluation

post /eval-runs
Run evaluation tests on agents or teams. Supports accuracy, performance, and reliability evaluations. Requires either agent_id or team_id, but not both.

---

## Example usage:

**URL:** llms-txt#example-usage:

analysis_prompt = """\
Analyze media trends for:
Keywords: ai agents
Sources: verge.com ,linkedin.com, x.com
"""

agent.print_response(analysis_prompt, stream=True)

---

## Remove Vectors

**URL:** llms-txt#remove-vectors

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/getting-started/usage/remove-vectors

```python 10_remove_vectors.py theme={null}
import asyncio
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## Basic Agent Usage

**URL:** llms-txt#basic-agent-usage

Source: https://docs.agno.com/basics/agents/usage/basic

Learn how to initialize and run a very simple agent

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The response will have status: "paused" with tools awaiting confirmation

**URL:** llms-txt#the-response-will-have-status:-"paused"-with-tools-awaiting-confirmation

---

## Setup the beta features we want to use

**URL:** llms-txt#setup-the-beta-features-we-want-to-use

betas = ["context-1m-2025-08-07"]
model = Claude(betas=betas)

---

## LanceDb

**URL:** llms-txt#lancedb

Source: https://docs.agno.com/reference/vector-db/lancedb

<Snippet file="vector-db-lancedb-reference.mdx" />

---

## Deploy your AgentOS

**URL:** llms-txt#deploy-your-agentos

**Contents:**
- Preparing your Python project
- Adding a production ready Dockerfile
- Testing locally
- Deployment options
  - AWS
  - Cloud providers
  - Railway
  - Hosting Platforms
- Next steps

Source: https://docs.agno.com/deploy/overview

How to take your AgentOS to production

Once you have built and configured your AgentOS locally, the next step is taking it to production. AgentOS exposes a FastAPI application under the hood, which means you are free to deploy it to any infrastructure you prefer, including container services, serverless architectures, Kubernetes, or a simple virtual machine.

The most reliable and convenient way to deploy a FastAPI application to production is with Docker. To make this easy, we provide ready-to-use templates for popular platforms including AWS ECS and Railway with more coming soon. If you prefer complete control, you can also turn your AgentOS into a minimal production repository with just a few files.

Here, we'll walk you through the entire process: starting from a plain Python codebase, converting it into a production-ready FastAPI service, containerizing it with Docker, and deploying it to the platform of your choice.

## Preparing your Python project

A minimal AgentOS project usually looks like this:

Your agents, teams and workflows are automatically mounted and ready to serve. As long as they are added to your AgentOS instance.

## Adding a production ready Dockerfile

Here is a minimal Dockerfile for deploying AgentOS:

This creates a clean, production-friendly container image.

Build and run the container using the following commands from the root of your project:

* [Docker Desktop](https://docs.docker.com/get-docker/)
* [Python 3.12](https://www.python.org/downloads/)
* [Agno](https://github.com/agno-agi/agno)

Visit `http://localhost:8000/docs` to see your AgentOS documentation. Here you can see the endpoints for your AgentOS.

## Deployment options

You can deploy AgentOS anywhere that supports Docker containers.\
To get started quickly, we recommend using one of our deployment templates.\
Learn more about them [here](/deploy/templates).

A production-ready configuration running entirely on AWS:

* Multi-architecture Docker builds
* Amazon ECR for container registry
* ECS Fargate service for running your container
* Application Load Balancer with HTTPS
* Amazon RDS (PostgreSQL)

Check out our [AWS template](/templates/agent-infra-aws/introduction) for more information.

AgentOS can run on any cloud provider that supports Docker containers. For example:

* Google Cloud Run
* AWS App Runner
* Azure Container Apps

A simple and fast deployment method. Railway:

* Builds your Dockerfile
* Manages environment variables
* Provides a public HTTPS domain automatically

Check out our [Railway template](/templates/agent-infra-railway/introduction) for more information.

### Hosting Platforms

Similar to Railway, you can use other platforms that accept a Dockerfile, manage environment variables, and host your service with minimal configuration.\
Templates for these are coming soon:

* [Modal](https://modal.com/)
* [Render](https://render.com/)
* [DigitalOcean](https://www.digitalocean.com/)
* [Fly.io](https://fly.io/)

Ready to deploy?\
Choose one of our templates to get started quickly and run AgentOS in production with minimal setup.

<Card title="Explore all Templates" icon="robot" iconType="duotone" href="/deploy/templates">
  Explore all templates and get started quickly.
</Card>

**Examples:**

Example 1 (unknown):
```unknown
Your agents, teams and workflows are automatically mounted and ready to serve. As long as they are added to your AgentOS instance.

## Adding a production ready Dockerfile

Here is a minimal Dockerfile for deploying AgentOS:
```

Example 2 (unknown):
```unknown
This creates a clean, production-friendly container image.

## Testing locally

Build and run the container using the following commands from the root of your project:

Prerequisites:

* [Docker Desktop](https://docs.docker.com/get-docker/)
* [Python 3.12](https://www.python.org/downloads/)
* [Agno](https://github.com/agno-agi/agno)
```

Example 3 (unknown):
```unknown

```

---

## Agent Flex Tier

**URL:** llms-txt#agent-flex-tier

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/agent-flex-tier

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Python Functions as Tools

**URL:** llms-txt#python-functions-as-tools

**Contents:**
- Magic of the @tool decorator

Source: https://docs.agno.com/basics/tools/creating-tools/python-functions

Learn how to create tools with python functions.

Any python function can be used as a tool by an Agent.

For example, here's how to use a `get_top_hackernews_stories` function as a tool:

## Magic of the @tool decorator

To modify the behavior of a tool, use the `@tool` decorator. Some notable features:

* `requires_confirmation=True`: Requires user confirmation before execution.
* `requires_user_input=True`: Requires user input before execution. Use `user_input_fields` to specify which fields require user input.
* `external_execution=True`: The tool will be executed outside of the agent's control.
* `show_result=True`: Show the output of the tool call in the Agent's response, `True` by default. Without this flag, the result of the tool call is sent to the model for further processing.
* `stop_after_tool_call=True`: Stop the agent run after the tool call.
* `tool_hooks`: Run custom logic before and after this tool call.
* `cache_results=True`: Cache the tool result to avoid repeating the same call. Use `cache_dir` and `cache_ttl` to configure the cache.

Here's an example that uses many possible parameters on the `@tool` decorator.

See the [@tool Decorator Reference](/reference/tools/decorator) for more details.

**Examples:**

Example 1 (unknown):
```unknown
## Magic of the @tool decorator

To modify the behavior of a tool, use the `@tool` decorator. Some notable features:

* `requires_confirmation=True`: Requires user confirmation before execution.
* `requires_user_input=True`: Requires user input before execution. Use `user_input_fields` to specify which fields require user input.
* `external_execution=True`: The tool will be executed outside of the agent's control.
* `show_result=True`: Show the output of the tool call in the Agent's response, `True` by default. Without this flag, the result of the tool call is sent to the model for further processing.
* `stop_after_tool_call=True`: Stop the agent run after the tool call.
* `tool_hooks`: Run custom logic before and after this tool call.
* `cache_results=True`: Cache the tool result to avoid repeating the same call. Use `cache_dir` and `cache_ttl` to configure the cache.

Here's an example that uses many possible parameters on the `@tool` decorator.
```

---

## Voyage AI Embedder

**URL:** llms-txt#voyage-ai-embedder

**Contents:**
- Usage

Source: https://docs.agno.com/basics/knowledge/embedder/voyageai/overview

The `VoyageAIEmbedder` class is used to embed text data into vectors using the Voyage AI API. Get your key from [here](https://dash.voyageai.com/api-keys).

```python voyageai_embedder.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.voyageai import VoyageAIEmbedder

---

## Agent

**URL:** llms-txt#agent

**Contents:**
- Parameters
- Functions
  - `run`
  - `arun`
  - `continue_run`
  - `acontinue_run`
  - `print_response`
  - `aprint_response`
  - `cli_app`
  - `acli_app`

Source: https://docs.agno.com/reference/agents/agent

| Parameter                          | Type                                                             | Default    | Description                                                                                                                                                                                                                      |
| ---------------------------------- | ---------------------------------------------------------------- | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`                            | `Optional[Union[Model, str]]`                                    | `None`     | Model to use for this Agent. Can be a Model object or a model string (`provider:model_id`)                                                                                                                                       |
| `name`                             | `Optional[str]`                                                  | `None`     | Agent name                                                                                                                                                                                                                       |
| `id`                               | `Optional[str]`                                                  | `None`     | Agent ID (autogenerated UUID if not set)                                                                                                                                                                                         |
| `user_id`                          | `Optional[str]`                                                  | `None`     | Default user\_id to use for this agent                                                                                                                                                                                           |
| `session_id`                       | `Optional[str]`                                                  | `None`     | Default session\_id to use for this agent (autogenerated if not set)                                                                                                                                                             |
| `session_state`                    | `Optional[Dict[str, Any]]`                                       | `None`     | Default session state (stored in the database to persist across runs)                                                                                                                                                            |
| `add_session_state_to_context`     | `bool`                                                           | `False`    | Set to True to add the session\_state to the context                                                                                                                                                                             |
| `enable_agentic_state`             | `bool`                                                           | `False`    | Set to True to give the agent tools to update the session\_state dynamically                                                                                                                                                     |
| `overwrite_db_session_state`       | `bool`                                                           | `False`    | Set to True to overwrite the session state in the database with the session state provided in the run                                                                                                                            |
| `cache_session`                    | `bool`                                                           | `False`    | If True, cache the current Agent session in memory for faster access                                                                                                                                                             |
| `search_session_history`           | `Optional[bool]`                                                 | `False`    | Set this to `True` to allow searching through previous sessions.                                                                                                                                                                 |
| `num_history_sessions`             | `Optional[int]`                                                  | `None`     | Specify the number of past sessions to include in the search. It's advisable to keep this number to 2 or 3 for now, as a larger number might fill up the context length of the model, potentially leading to performance issues. |
| `dependencies`                     | `Optional[Dict[str, Any]]`                                       | `None`     | Dependencies available for tools and prompt functions                                                                                                                                                                            |
| `add_dependencies_to_context`      | `bool`                                                           | `False`    | If True, add the dependencies to the user prompt                                                                                                                                                                                 |
| `db`                               | `Optional[BaseDb]`                                               | `None`     | Database to use for this agent                                                                                                                                                                                                   |
| `memory_manager`                   | `Optional[MemoryManager]`                                        | `None`     | Memory manager to use for this agent                                                                                                                                                                                             |
| `enable_agentic_memory`            | `bool`                                                           | `False`    | Enable the agent to manage memories of the user                                                                                                                                                                                  |
| `enable_user_memories`             | `bool`                                                           | `False`    | If True, the agent creates/updates user memories at the end of runs                                                                                                                                                              |
| `add_memories_to_context`          | `Optional[bool]`                                                 | `None`     | If True, the agent adds a reference to the user memories in the response                                                                                                                                                         |
| `enable_session_summaries`         | `bool`                                                           | `False`    | If True, the agent creates/updates session summaries at the end of runs                                                                                                                                                          |
| `add_session_summary_to_context`   | `Optional[bool]`                                                 | `None`     | If True, the agent adds session summaries to the context                                                                                                                                                                         |
| `session_summary_manager`          | `Optional[SessionSummaryManager]`                                | `None`     | Session summary manager                                                                                                                                                                                                          |
| `compress_tool_results`            | `bool`                                                           | `False`    | If True, compress tool call results to save context space                                                                                                                                                                        |
| `compression_manager`              | `Optional[CompressionManager]`                                   | `None`     | Custom compression manager for compressing tool call results                                                                                                                                                                     |
| `add_history_to_context`           | `bool`                                                           | `False`    | Add the chat history of the current session to the messages sent to the Model                                                                                                                                                    |
| `num_history_runs`                 | `Optional[int]`                                                  | `None`     | Number of historical runs to include in the messages.                                                                                                                                                                            |
| `num_history_messages`             | `Optional[int]`                                                  | `None`     | Number of historical messages to include messages list sent to the Model.                                                                                                                                                        |
| `knowledge`                        | `Optional[Knowledge]`                                            | `None`     | Agent Knowledge                                                                                                                                                                                                                  |
| `knowledge_filters`                | `Optional[Dict[str, Any]]`                                       | `None`     | Knowledge filters to apply to the knowledge base                                                                                                                                                                                 |
| `enable_agentic_knowledge_filters` | `Optional[bool]`                                                 | `None`     | Let the agent choose the knowledge filters                                                                                                                                                                                       |
| `add_knowledge_to_context`         | `bool`                                                           | `False`    | Enable RAG by adding references from Knowledge to the user prompt                                                                                                                                                                |
| `knowledge_retriever`              | `Optional[Callable[..., Optional[List[Union[Dict, str]]]]]`      | `None`     | Function to get references to add to the user\_message                                                                                                                                                                           |
| `references_format`                | `Literal["json", "yaml"]`                                        | `"json"`   | Format of the references                                                                                                                                                                                                         |
| `metadata`                         | `Optional[Dict[str, Any]]`                                       | `None`     | Metadata stored with this agent                                                                                                                                                                                                  |
| `tools`                            | `Optional[List[Union[Toolkit, Callable, Function, Dict]]]`       | `None`     | A list of tools provided to the Model                                                                                                                                                                                            |
| `tool_call_limit`                  | `Optional[int]`                                                  | `None`     | Maximum number of tool calls allowed for a single run                                                                                                                                                                            |
| `tool_choice`                      | `Optional[Union[str, Dict[str, Any]]]`                           | `None`     | Controls which (if any) tool is called by the model                                                                                                                                                                              |
| `max_tool_calls_from_history`      | `Optional[int]`                                                  | `None`     | Maximum number of tool calls from history to keep in context. If None, all tool calls from history are included. If set to N, only the last N tool calls from history are added to the context for memory management             |
| `tool_hooks`                       | `Optional[List[Callable]]`                                       | `None`     | Functions that will run between tool calls                                                                                                                                                                                       |
| `pre_hooks`                        | `Optional[Union[List[Callable[..., Any]], List[BaseGuardrail]]]` | `None`     | Functions called right after agent-session is loaded, before processing starts                                                                                                                                                   |
| `post_hooks`                       | `Optional[Union[List[Callable[..., Any]], List[BaseGuardrail]]]` | `None`     | Functions called after output is generated but before the response is returned                                                                                                                                                   |
| `reasoning`                        | `bool`                                                           | `False`    | Enable reasoning by working through the problem step by step                                                                                                                                                                     |
| `reasoning_model`                  | `Optional[Union[Model, str]]`                                    | `None`     | Model to use for reasoning. Can be a Model object or a model string (`provider:model_id`)                                                                                                                                        |
| `reasoning_agent`                  | `Optional[Agent]`                                                | `None`     | Agent to use for reasoning                                                                                                                                                                                                       |
| `reasoning_min_steps`              | `int`                                                            | `1`        | Minimum number of reasoning steps                                                                                                                                                                                                |
| `reasoning_max_steps`              | `int`                                                            | `10`       | Maximum number of reasoning steps                                                                                                                                                                                                |
| `read_chat_history`                | `bool`                                                           | `False`    | Add a tool that allows the Model to read the chat history                                                                                                                                                                        |
| `search_knowledge`                 | `bool`                                                           | `True`     | Add a tool that allows the Model to search the knowledge base                                                                                                                                                                    |
| `update_knowledge`                 | `bool`                                                           | `False`    | Add a tool that allows the Model to update the knowledge base                                                                                                                                                                    |
| `read_tool_call_history`           | `bool`                                                           | `False`    | Add a tool that allows the Model to get the tool call history                                                                                                                                                                    |
| `send_media_to_model`              | `bool`                                                           | `True`     | If False, media (images, videos, audio, files) is only available to tools and not sent to the LLM                                                                                                                                |
| `store_media`                      | `bool`                                                           | `True`     | If True, store media in the database                                                                                                                                                                                             |
| `store_tool_messages`              | `bool`                                                           | `True`     | If True, store tool results in the database                                                                                                                                                                                      |
| `store_history_messages`           | `bool`                                                           | `True`     | If True, store history messages in the database                                                                                                                                                                                  |
| `system_message`                   | `Optional[Union[str, Callable, Message]]`                        | `None`     | Provide the system message as a string or function                                                                                                                                                                               |
| `system_message_role`              | `str`                                                            | `"system"` | Role for the system message                                                                                                                                                                                                      |
| `build_context`                    | `bool`                                                           | `True`     | Set to False to skip context building                                                                                                                                                                                            |
| `description`                      | `Optional[str]`                                                  | `None`     | A description of the Agent that is added to the start of the system message                                                                                                                                                      |
| `instructions`                     | `Optional[Union[str, List[str], Callable]]`                      | `None`     | List of instructions for the agent                                                                                                                                                                                               |
| `expected_output`                  | `Optional[str]`                                                  | `None`     | Provide the expected output from the Agent                                                                                                                                                                                       |
| `additional_context`               | `Optional[str]`                                                  | `None`     | Additional context added to the end of the system message                                                                                                                                                                        |
| `markdown`                         | `bool`                                                           | `False`    | If markdown=true, add instructions to format the output using markdown                                                                                                                                                           |
| `add_name_to_context`              | `bool`                                                           | `False`    | If True, add the agent name to the instructions                                                                                                                                                                                  |
| `add_datetime_to_context`          | `bool`                                                           | `False`    | If True, add the current datetime to the instructions to give the agent a sense of time                                                                                                                                          |
| `add_location_to_context`          | `bool`                                                           | `False`    | If True, add the current location to the instructions to give the agent a sense of place                                                                                                                                         |
| `timezone_identifier`              | `Optional[str]`                                                  | `None`     | Allows for custom timezone for datetime instructions following the TZ Database format (e.g. "Etc/UTC")                                                                                                                           |
| `resolve_in_context`               | `bool`                                                           | `True`     | If True, resolve session\_state, dependencies, and metadata in the user and system messages                                                                                                                                      |
| `additional_input`                 | `Optional[List[Union[str, Dict, BaseModel, Message]]]`           | `None`     | A list of extra messages added after the system message and before the user message                                                                                                                                              |
| `user_message_role`                | `str`                                                            | `"user"`   | Role for the user message                                                                                                                                                                                                        |
| `build_user_context`               | `bool`                                                           | `True`     | Set to False to skip building the user context                                                                                                                                                                                   |
| `retries`                          | `int`                                                            | `0`        | Number of retries to attempt when running the Agent                                                                                                                                                                              |
| `delay_between_retries`            | `int`                                                            | `1`        | Delay between retries (in seconds)                                                                                                                                                                                               |
| `exponential_backoff`              | `bool`                                                           | `False`    | If True, the delay between retries is doubled each time                                                                                                                                                                          |
| `input_schema`                     | `Optional[Type[BaseModel]]`                                      | `None`     | Provide an input schema to validate the input                                                                                                                                                                                    |
| `output_schema`                    | `Optional[Type[BaseModel]]`                                      | `None`     | Provide a response model to get the response as a Pydantic model                                                                                                                                                                 |
| `parser_model`                     | `Optional[Union[Model, str]]`                                    | `None`     | Provide a secondary model to parse the response from the primary model. Can be a Model object or a model string (`provider:model_id`)                                                                                            |
| `parser_model_prompt`              | `Optional[str]`                                                  | `None`     | Provide a prompt for the parser model                                                                                                                                                                                            |
| `output_model`                     | `Optional[Union[Model, str]]`                                    | `None`     | Provide an output model to structure the response from the main model. Can be a Model object or a model string (`provider:model_id`)                                                                                             |
| `output_model_prompt`              | `Optional[str]`                                                  | `None`     | Provide a prompt for the output model                                                                                                                                                                                            |
| `parse_response`                   | `bool`                                                           | `True`     | If True, the response from the Model is converted into the output\_schema                                                                                                                                                        |
| `structured_outputs`               | `Optional[bool]`                                                 | `None`     | Use model enforced structured\_outputs if supported (e.g. OpenAIChat)                                                                                                                                                            |
| `use_json_mode`                    | `bool`                                                           | `False`    | If `output_schema` is set, sets the response mode of the model, i.e. if the model should explicitly respond with a JSON object instead of a Pydantic model                                                                       |
| `save_response_to_file`            | `Optional[str]`                                                  | `None`     | Save the response to a file                                                                                                                                                                                                      |
| `stream`                           | `Optional[bool]`                                                 | `None`     | Stream the response from the Agent                                                                                                                                                                                               |
| `stream_events`                    | `bool`                                                           | `False`    | Stream the intermediate steps from the Agent                                                                                                                                                                                     |
| `store_events`                     | `bool`                                                           | `False`    | Persist the events on the run response                                                                                                                                                                                           |
| `events_to_skip`                   | `Optional[List[RunEvent]]`                                       | `None`     | Specify which event types to skip when storing events on the RunOutput                                                                                                                                                           |
| `role`                             | `Optional[str]`                                                  | `None`     | If this Agent is part of a team, this is the role of the agent in the team                                                                                                                                                       |
| `debug_mode`                       | `bool`                                                           | `False`    | Enable debug logs                                                                                                                                                                                                                |
| `debug_level`                      | `Literal[1, 2]`                                                  | `1`        | Debug level for logging                                                                                                                                                                                                          |
| `telemetry`                        | `bool`                                                           | `True`     | Log minimal telemetry for analytics                                                                                                                                                                                              |

* `input` (Union\[str, List, Dict, Message, BaseModel, List\[Message]]): The input to send to the agent
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `retries` (Optional\[int]): Number of retries to attempt
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `output_schema` (Optional\[Type\[BaseModel]]): Output schema to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

Run the agent asynchronously.

* `input` (Union\[str, List, Dict, Message, BaseModel, List\[Message]]): The input to send to the agent
* `stream` (Optional\[bool]): Whether to stream the response
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `retries` (Optional\[int]): Number of retries to attempt
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `output_schema` (Optional\[Type\[BaseModel]]): Output schema to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

* `Union[RunOutput, AsyncIterator[RunOutputEvent]]`: Either a RunOutput or an iterator of RunOutputEvents, depending on the `stream` parameter

* `run_response` (Optional\[RunOutput]): The run response to continue
* `run_id` (Optional\[str]): The run ID to continue
* `updated_tools` (Optional\[List\[ToolExecution]]): Updated tools to use, required if the run is resumed using `run_id`
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `retries` (Optional\[int]): Number of retries to attempt
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

* `Union[RunOutput, Iterator[RunOutputEvent]]`: Either a RunOutput or an iterator of RunOutputEvents, depending on the `stream` parameter

Continue a run asynchronously.

* `run_response` (Optional\[RunOutput]): The run response to continue
* `run_id` (Optional\[str]): The run ID to continue
* `updated_tools` (Optional\[List\[ToolExecution]]): Updated tools to use, required if the run is resumed using `run_id`
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `retries` (Optional\[int]): Number of retries to attempt
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

* `Union[RunOutput, AsyncIterator[Union[RunOutputEvent, RunOutput]]]`: Either a RunOutput or an iterator of RunOutputEvents, depending on the `stream` parameter

Run the agent and print the response.

* `input` (Union\[List, Dict, str, Message, BaseModel, List\[Message]]): The input to send to the agent
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `stream` (Optional\[bool]): Whether to stream the response
* `markdown` (Optional\[bool]): Whether to format output as markdown
* `show_message` (bool): Whether to show the input message
* `show_reasoning` (bool): Whether to show reasoning steps
* `show_full_reasoning` (bool): Whether to show full reasoning information
* `console` (Optional\[Any]): Console to use for output
* `tags_to_include_in_markdown` (Optional\[Set\[str]]): Tags to include in markdown content
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

### `aprint_response`

Run the agent and print the response asynchronously.

* `input` (Union\[List, Dict, str, Message, BaseModel, List\[Message]]): The input to send to the agent
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `stream` (Optional\[bool]): Whether to stream the response
* `markdown` (Optional\[bool]): Whether to format output as markdown
* `show_message` (bool): Whether to show the message
* `show_reasoning` (bool): Whether to show reasoning
* `show_full_reasoning` (bool): Whether to show full reasoning
* `console` (Optional\[Any]): Console to use for output
* `tags_to_include_in_markdown` (Optional\[Set\[str]]): Tags to include in markdown content
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

Run an interactive command-line interface to interact with the agent.

* `input` (Optional\[str]): The input to send to the agent
* `session_id` (Optional\[str]): Session ID to use
* `user_id` (Optional\[str]): User ID to use
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":sunglasses:")
* `stream` (bool): Whether to stream the response (default: False)
* `markdown` (bool): Whether to format output as markdown (default: False)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

Run an interactive command-line interface to interact with the agent asynchronously.

* `input` (Optional\[str]): The input to send to the agent
* `session_id` (Optional\[str]): Session ID to use
* `user_id` (Optional\[str]): User ID to use
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":sunglasses:")
* `stream` (bool): Whether to stream the response (default: False)
* `markdown` (bool): Whether to format output as markdown (default: False)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

Cancel a run by run ID.

* `run_id` (str): The run ID to cancel

* `bool`: True if the run was successfully cancelled

Get the run output for the given run ID.

* `run_id` (str): The run ID
* `session_id` (str): Session ID to use

* `Optional[RunOutput]`: The run output

### get\_last\_run\_output

Get the last run output for the session.

* `session_id` (str): Session ID to use

* `Optional[RunOutput]`: The last run output

Get the session for the given session ID.

* `session_id` (str): Session ID to use

* `Optional[AgentSession]`: The agent session

### get\_session\_summary

Get the session summary for the given session ID.

* `session_id` (str): Session ID to use

* Session summary for the given session

### get\_user\_memories

Get the user memories for the given user ID.

* `user_id` (str): User ID to use

* `Optional[List[UserMemory]]`: The user memories

### aget\_user\_memories

Get the user memories for the given user ID asynchronously.

* `user_id` (str): User ID to use

* `Optional[List[UserMemory]]`: The user memories

### get\_session\_state

Get the session state for the given session ID.

* `session_id` (str): Session ID to use

* `Dict[str, Any]`: The session state

### update\_session\_state

Update the session state for the given session ID.

* `session_id` (str): Session ID to use
* `session_state_updates` (Dict\[str, Any]): The session state keys and values to update. Overwrites the existing session state.

* `Dict[str, Any]`: The updated session state

### get\_session\_metrics

Get the session metrics for the given session ID.

* `session_id` (str): Session ID to use

* `Optional[Metrics]`: The session metrics

* `session_id` (str): Session ID to delete

Save a session to the database.

* `session` (AgentSession): The session to save

Save a session to the database asynchronously.

* `session` (AgentSession): The session to save

Rename the agent and update the session.

* `name` (str): The new name for the agent
* `session_id` (str): Session ID to use

### get\_session\_name

Get the session name for the given session ID.

* `session_id` (str): Session ID to use

* `str`: The session name

### set\_session\_name

Set the session name.

* `session_id` (str): Session ID to use
* `autogenerate` (bool): Whether to autogenerate the name
* `session_name` (Optional\[str]): The name to set

* `AgentSession`: The updated session

### get\_session\_messages

Get the messages for the given session ID.

* `session_id` (Optional\[str]): The session ID to get the messages for. If not provided, the latest used session ID is used.
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs.
* `limit` (Optional\[int]): The number of messages to return, counting from the latest. Defaults to all messages.
* `skip_roles` (Optional\[List\[str]]): Skip messages with these roles.
* `skip_statuses` (Optional\[List\[RunStatus]]): Skip messages with these statuses.
* `skip_history_messages` (bool): Skip messages that were tagged as history in previous runs. Defaults to True.

* `List[Message]`: The messages for the session

### get\_chat\_history

Get the chat history for the given session ID.

* `session_id` (Optional\[str]): The session ID to get the chat history for. If not provided, the latest used session ID is used.
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs.

* `List[Message]`: The chat history

Add a tool to the agent.

* `tool` (Union\[Toolkit, Callable, Function, Dict]): The tool to add

Replace the tools of the agent.

* `tools` (List\[Union\[Toolkit, Callable, Function, Dict]]): The tools to set

---

## Multiple MCP Servers

**URL:** llms-txt#multiple-mcp-servers

**Contents:**
- Using multiple `MCPTools` instances

Source: https://docs.agno.com/basics/tools/mcp/multiple-servers

Understanding how to connect to multiple MCP servers with Agno

Agno's MCP integration also supports handling connections to multiple servers, specifying server parameters and using your own MCP servers

There are two approaches to this:

1. Using multiple `MCPTools` instances
2. Using a single `MultiMCPTools` instance

## Using multiple `MCPTools` instances

```python multiple_mcp_servers.py theme={null}
import asyncio
import os

from agno.agent import Agent
from agno.tools.mcp import MCPTools

async def run_agent(message: str) -> None:
    """Run the Airbnb and Google Maps agent with the given message."""

env = {
        **os.environ,
        "GOOGLE_MAPS_API_KEY": os.getenv("GOOGLE_MAPS_API_KEY"),
    }

# Initialize and connect to multiple MCP servers
    airbnb_tools = MCPTools(command="npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt")
    google_maps_tools = MCPTools(command="npx -y @modelcontextprotocol/server-google-maps", env=env)
    await airbnb_tools.connect()
    await google_maps_tools.connect()

try:
        agent = Agent(
            tools=[airbnb_tools, google_maps_tools],
            markdown=True,
        )

await agent.aprint_response(message, stream=True)
    finally:
        await airbnb_tools.close()
        await google_maps_tools.close()

---

## Third call - will run the workflow (new topic)

**URL:** llms-txt#third-call---will-run-the-workflow-(new-topic)

workflow.print_response(
    "Now tell me a story about a cat named Luna", stream=True
)

---

## Image to Image Generation Agent

**URL:** llms-txt#image-to-image-generation-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/image-to-image-agent

This example demonstrates how to create an AI agent that generates images from existing images using the Fal AI API.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Vector-focused knowledge base for similarity search

**URL:** llms-txt#vector-focused-knowledge-base-for-similarity-search

vector_knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes_vector",
        db_url=db_url,
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## Capturing Team Responses as Variables

**URL:** llms-txt#capturing-team-responses-as-variables

Source: https://docs.agno.com/basics/teams/usage/basic-flows/response-as-variable

This example demonstrates how to capture team responses as variables and validate them using Pydantic models.

It shows a routing team that analyzes stocks and company news, with structured responses for different types of queries.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom routes that use JWT

**URL:** llms-txt#custom-routes-that-use-jwt

@app.post("/auth/login")
async def login(username: str = Form(...), password: str = Form(...)):
    """Login endpoint that returns JWT token"""
    if username == "demo" and password == "password":
        payload = {
            "sub": "user_123",
            "username": username,
            "exp": datetime.now(UTC) + timedelta(hours=24),
            "iat": datetime.now(UTC),
        }
        token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")
        return {"access_token": token, "token_type": "bearer"}

raise HTTPException(status_code=401, detail="Invalid credentials")

---

## movie_agent: RunOutput = movie_agent.run("New York")

**URL:** llms-txt#movie_agent:-runoutput-=-movie_agent.run("new-york")

---

## AgentOS

**URL:** llms-txt#agentos

**Contents:**
- Parameters
- Functions
  - `get_app`
  - `get_routes`
  - `serve`

Source: https://docs.agno.com/reference/agent-os/agent-os

| Parameter           | Type                                                        | Default              | Description                                                                                               |
| ------------------- | ----------------------------------------------------------- | -------------------- | --------------------------------------------------------------------------------------------------------- |
| `id`                | `Optional[str]`                                             | Autogenerated UUID   | AgentOS ID                                                                                                |
| `name`              | `Optional[str]`                                             | `None`               | AgentOS name                                                                                              |
| `description`       | `Optional[str]`                                             | `None`               | AgentOS description                                                                                       |
| `version`           | `Optional[str]`                                             | `None`               | AgentOS version                                                                                           |
| `agents`            | `Optional[List[Agent]]`                                     | `None`               | List of agents available in the AgentOS                                                                   |
| `teams`             | `Optional[List[Team]]`                                      | `None`               | List of teams available in the AgentOS                                                                    |
| `workflows`         | `Optional[List[Workflow]]`                                  | `None`               | List of workflows available in the AgentOS                                                                |
| `knowledge`         | `Optional[List[Knowledge]]`                                 | `None`               | List of standalone knowledge instances available in the AgentOS                                           |
| `interfaces`        | `Optional[List[BaseInterface]]`                             | `None`               | List of interfaces available in the AgentOS                                                               |
| `config`            | `Optional[Union[str, AgentOSConfig]]`                       | `None`               | User-provided configuration for the AgentOS. Either a path to a YAML file or an `AgentOSConfig` instance. |
| `settings`          | `Optional[AgnoAPISettings]`                                 | `None`               | Settings for the AgentOS API                                                                              |
| `base_app`          | `Optional[FastAPI]`                                         | `None`               | Custom FastAPI APP to use for the AgentOS                                                                 |
| `lifespan`          | `Optional[Any]`                                             | `None`               | Lifespan context manager for the FastAPI app                                                              |
| `enable_mcp_server` | `bool`                                                      | `False`              | Whether to enable MCP (Model Context Protocol)                                                            |
| `on_route_conflict` | `Literal["preserve_agentos", "preserve_base_app", "error"]` | `"preserve_agentos"` | What to do when a route conflict is detected in case a custom base\_app is provided.                      |
| `telemetry`         | `bool`                                                      | `True`               | Log minimal telemetry for analytics                                                                       |

Get the FastAPI APP configured for the AgentOS.

Get the routes configured for the AgentOS.

Run the app, effectively starting the AgentOS.

* `app` (Union\[str, FastAPI]): FastAPI APP instance
* `host` (str): Host to bind. Defaults to `localhost`
* `port` (int): Port to bind. Defaults to `7777`
* `workers` (Optional\[int]): Number of workers to use. Defaults to `None`
* `reload` (bool): Enable auto-reload for development. Defaults to `False`

---

## Create agent with HITL tools

**URL:** llms-txt#create-agent-with-hitl-tools

agent = Agent(
    name="Data Manager",
    id="data_manager",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[delete_records, send_notification],
    instructions=["You help users manage data operations"],
    db=db,
    markdown=True,
)

---

## Create your workflows...

**URL:** llms-txt#create-your-workflows...

workflow_tools = WorkflowTools(
    workflow=blog_post_workflow,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[workflow_tools],
)

agent.print_response("Create a blog post on the topic: AI trends in 2024", stream=True)
```

See the [Workflow Tools](/basics/tools/reasoning_tools/workflow-tools) documentation for more details.

---

## OpenAIModerationGuardrail

**URL:** llms-txt#openaimoderationguardrail

**Contents:**
- Parameters
- Moderation categories

Source: https://docs.agno.com/reference/hooks/openai-moderation-guardrail

| Parameter              | Type        | Default                    | Description                                                                               |
| ---------------------- | ----------- | -------------------------- | ----------------------------------------------------------------------------------------- |
| `moderation_model`     | `str`       | `"omni-moderation-latest"` | The model to use for moderation.                                                          |
| `raise_for_categories` | `List[str]` | `None`                     | The categories to raise for.                                                              |
| `api_key`              | `str`       | `None`                     | The API key to use for moderation. Defaults to the OPENAI\_API\_KEY environment variable. |

## Moderation categories

You can check the current list of moderation categories in [OpenAI's docs](https://platform.openai.com/docs/guides/moderation#content-classifications).

---

## Dynamic Session State

**URL:** llms-txt#dynamic-session-state

Source: https://docs.agno.com/basics/state/agent/usage/dynamic-session-state

This example demonstrates how to use tool hooks to dynamically manage session state. It shows how to create a customer management system that updates session state through tool interactions rather than direct modification.

<Steps>
  <Step title="Create a Python file">
    Create a file called `dynamic_session_state.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Define agents for different tasks

**URL:** llms-txt#define-agents-for-different-tasks

researcher = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research the given topic and provide key facts and insights.",
)

tech_researcher = Agent(
    name="Tech Research Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools()],
    instructions="Research tech-related topics from Hacker News and provide latest developments.",
)

news_researcher = Agent(
    name="News Research Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[ExaTools()],
    instructions="Research current news and trends using Exa search.",
)

writer = Agent(
    name="Writing Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="Write a comprehensive article based on the research provided. Make it engaging and well-structured.",
)

editor = Agent(
    name="Editor Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="Review and edit the article for clarity, grammar, and flow. Provide a polished final version.",
)

content_agent = Agent(
    name="Content Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="Prepare and format content for writing based on research inputs.",
)

---

## Ollama Tools

**URL:** llms-txt#ollama-tools

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/ollama-tools

The Ollama Tools model provides access to the Ollama models and passes tools in XML format to the model.

| Parameter               | Type                          | Default                    | Description                                                      |
| ----------------------- | ----------------------------- | -------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`                         | `"llama3.2"`               | The name of the Ollama model to use                              |
| `name`                  | `str`                         | `"OllamaTools"`            | The name of the model                                            |
| `provider`              | `str`                         | `"Ollama"`                 | The provider of the model                                        |
| `host`                  | `str`                         | `"http://localhost:11434"` | The host URL for the Ollama server                               |
| `timeout`               | `Optional[int]`               | `None`                     | Request timeout in seconds                                       |
| `format`                | `Optional[str]`               | `None`                     | The format to return the response in (e.g., "json")              |
| `options`               | `Optional[Dict[str, Any]]`    | `None`                     | Additional model options (temperature, top\_p, etc.)             |
| `keep_alive`            | `Optional[Union[float, str]]` | `None`                     | How long to keep the model loaded (e.g., "5m", 3600 seconds)     |
| `template`              | `Optional[str]`               | `None`                     | The prompt template to use                                       |
| `system`                | `Optional[str]`               | `None`                     | System message to use                                            |
| `raw`                   | `Optional[bool]`              | `None`                     | Whether to return raw response without formatting                |
| `stream`                | `bool`                        | `True`                     | Whether to stream the response                                   |
| `retries`               | `int`                         | `0`                        | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`                         | `1`                        | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`                        | `False`                    | If True, the delay between retries is doubled each time          |

This model passes tools in XML format instead of JSON for better compatibility with certain models.

---

## Run the agent - Agno auto-generates session_id and run_id

**URL:** llms-txt#run-the-agent---agno-auto-generates-session_id-and-run_id

**Contents:**
- Multi-User Conversations
- Learn more

response = agent.run("Tell me a 5 second short story about a robot")
print(response.content)
print(f"Run ID: {response.run_id}")        # Auto-generated UUID
print(f"Session ID: {response.session_id}") # Auto-generated UUID
```

This creates a new session with a single run. But here's the catch: without a database configured, there's no persistence. The `session_id` exists for this single run, but you can't continue the conversation later because nothing is saved. To actually use sessions for multi-turn conversations, you need to configure a database (even an `InMemoryDb` works).

## Multi-User Conversations

In production, multiple users often talk to the same agent or team simultaneously. Sessions keep those threads isolated:

* `user_id` distinguishes the person using your product.
* `session_id` distinguishes conversation threads for that user (think "chat tabs").
* Conversation history only flows into the run when you enable it via [`add_history_to_context`](/basics/chat-history/agent/overview#add-history-to-the-agent-context).

For a full walkthrough that includes persistence, history, and per-user session IDs, follow the [Persisting Sessions guide](/basics/sessions/persisting-sessions#multi-user-sessions) or the [Chat History cookbook example](/examples/basics/agent/session/05_chat_history).

<CardGroup cols={3}>
  <Card title="Session Management" icon="tag" iconType="duotone" href="/basics/sessions/session-management">
    Learn how to manage session IDs, names, and optimize session performance for agents and teams.
  </Card>

<Card title="History Management" icon="clock-rotate-left" iconType="duotone" href="/basics/sessions/history-management">
    Control how conversation history is accessed and used in your sessions.
  </Card>

<Card title="Session Summaries" icon="file-lines" iconType="duotone" href="/basics/sessions/session-summaries">
    Automatically condense long conversations into concise summaries to reduce token costs.
  </Card>
</CardGroup>

---

## Nebius Token Factory

**URL:** llms-txt#nebius-token-factory

**Contents:**
- Authentication
- Example
- Params

Source: https://docs.agno.com/integrations/models/gateways/nebius/overview

Learn how to use Nebius models in Agno.

Nebius Token Factory is a platform from Nebius that simplifies the process of building applications using AI models. It provides a suite of tools and services for developers to easily test, integrate and fine-tune various AI models, including those for text and image generation.
You can checkout the list of available models [here](https://tokenfactory.nebius.com/).

We recommend experimenting to find the best-suited-model for your use-case.

Set your `NEBIUS_API_KEY` environment variable. Get your key [from Nebius Token Factory here](https://tokenfactory.nebius.com/?modals=create-api-key).

Use `Nebius` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/gateways/nebius/usage/basic-stream). </Note>

| Parameter  | Type            | Default                                    | Description                                                   |
| ---------- | --------------- | ------------------------------------------ | ------------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Meta-Llama-3.1-70B-Instruct"` | The id of the Nebius model to use                             |
| `name`     | `str`           | `"Nebius"`                                 | The name of the model                                         |
| `provider` | `str`           | `"Nebius"`                                 | The provider of the model                                     |
| `api_key`  | `Optional[str]` | `None`                                     | The API key for Nebius (defaults to NEBIUS\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.tokenfactory.nebius.com/v1"` | The base URL for the Nebius API                               |

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Nebius` with your `Agent`:

<CodeGroup>
```

---

## Hybrid Search

**URL:** llms-txt#hybrid-search

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/hybrid-search

```python hybrid_search.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Test agentic knowledge filtering

**URL:** llms-txt#test-agentic-knowledge-filtering

**Contents:**
- Usage

team_with_knowledge.print_response(
    "Tell me about Jordan Mitchell's work and experience with user_id as jordan_mitchell"
)
bash  theme={null}
    pip install agno openai lancedb
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/knowledge/03_team_with_agentic_knowledge_filters.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Streaming Agent

**URL:** llms-txt#streaming-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/vercel/usage/basic-stream

```python cookbook/models/vercel/basic_stream.py theme={null}
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutput  # noqa
from agno.models.vercel import V0

agent = Agent(model=V0(id="v0-1.0-md"), markdown=True)

---

## Create knowledge search agent

**URL:** llms-txt#create-knowledge-search-agent

web_agent = Agent(
    name="Knowledge Search Agent",
    role="Handle knowledge search",
    knowledge=knowledge_base,
    model=OpenAIChat(id="gpt-5-mini"),
)

---

## Agno Telemetry

**URL:** llms-txt#agno-telemetry

**Contents:**
- What Data is Collected?
  - Example Telemetry Payload
- How to Disable Telemetry
  - Environment Variable
  - Per-Instance Configuration

Source: https://docs.agno.com/basics/telemetry

Control what usage data Agno collects

Agno collects anonymous usage data about agents, teams, workflows, and AgentOS configurations to help improve the platform and provide better support.

<Note>
  **Privacy First:** No sensitive data (prompts, responses, user data, or API keys) is ever collected. All telemetry is anonymous and aggregated.
</Note>

## What Data is Collected?

Agno collects basic usage metrics for:

* **Agent runs** - Model providers, database types, feature usage
* **Team runs** - Multi-agent coordination patterns
* **Workflow runs** - Orchestration and execution patterns
* **AgentOS launches** - Platform usage and configurations

### Example Telemetry Payload

Here's what an agent run telemetry payload looks like:

## How to Disable Telemetry

You can disable telemetry in two ways:

### Environment Variable

Set the `AGNO_TELEMETRY` environment variable to `false`:

### Per-Instance Configuration

Disable telemetry for specific agents, teams, workflows, or AgentOS instances:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
## How to Disable Telemetry

You can disable telemetry in two ways:

### Environment Variable

Set the `AGNO_TELEMETRY` environment variable to `false`:
```

Example 2 (unknown):
```unknown
### Per-Instance Configuration

Disable telemetry for specific agents, teams, workflows, or AgentOS instances:
```

---

## Send through API

**URL:** llms-txt#send-through-api

**Contents:**
- Next Steps

response = requests.post(
    "http://localhost:7777/agents/my-agent/runs",
    data={
        "message": "Find popular published articles",
        "stream": "false",
        "knowledge_filters": filter_json,
    }
)
```

<Note>
  FilterExpressions use a dictionary format with an `"op"` key (e.g., `{"op": "EQ", "key": "status", "value": "published"}`) which tells the API to deserialize them as FilterExpr objects. Regular dict filters without the `"op"` key continue to work for backward compatibility.
</Note>

For detailed examples, API-specific patterns, and troubleshooting, see the [API Filtering Guide](/agent-os/knowledge/filter-knowledge).

<CardGroup cols={2}>
  <Card title="API Filtering Guide" icon="code" href="/agent-os/knowledge/filter-knowledge">
    Use filter expressions through the Agent OS API
  </Card>

<Card title="Search & Retrieval" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Learn about different search strategies and optimization
  </Card>

<Card title="Content Database" icon="database" href="/basics/knowledge/content-db">
    Understand how content and metadata are stored and managed
  </Card>

<Card title="Knowledge Bases" icon="book-open" href="/basics/knowledge/knowledge-bases">
    Deep dive into knowledge base architecture and design
  </Card>

<Card title="Performance Tips" icon="gauge" href="/basics/knowledge/performance-tips">
    Optimize your filtered searches for speed and accuracy
  </Card>
</CardGroup>

---

## Dependencies

**URL:** llms-txt#dependencies

**Contents:**
- Basic usage
- How dependencies work
- Learn more

Source: https://docs.agno.com/basics/dependencies/overview

Learn how to use dependencies to add context to your agents and teams.

**Dependencies** are a way to inject variables into your Agent or Team context. The `dependencies` parameter accepts a dictionary containing functions or static variables that are automatically resolved before the agent or team runs.

<Note>
  You can use dependencies to inject memories, dynamic few-shot examples, "retrieved" documents, etc.
</Note>

You can reference the dependencies in your agent instructions or user message.

<Tip>
  You can set `dependencies` on `Agent`/`Team` initialization, or pass it to the `run()` and `arun()` methods.
</Tip>

## How dependencies work

Dependencies are resolved at runtime, just before your agent or team executes. Here's the flow:

1. **Define dependencies**: Provide a dictionary of key-value pairs where values can be static data or callable functions
2. **Resolution**: When the agent/team runs, Agno calls all callable dependencies and replaces them with their return values
3. **Template substitution**: Resolved dependencies are available in your instructions using `{dependency_name}` syntax
4. **Context injection**: When `add_dependencies_to_context=True`, dependencies are automatically added to the user message

<CardGroup cols={2}>
  <Card title="Dependencies with Agents" icon="robot" href="/basics/dependencies/agent">
    Learn how to use dependencies with agents
  </Card>

<Card title="Dependencies with Teams" icon="users" href="/basics/dependencies/team">
    Learn how to use dependencies with teams
  </Card>

<Card title="AgentOS API" icon="code-branch" href="/agent-os/api/usage">
    Pass dependencies via the AgentOS API
  </Card>

<Card title="Agent Schema" icon="book" href="/reference/agents/agent">
    View the full Agent schema reference
  </Card>
</CardGroup>

---

## Both (default)

**URL:** llms-txt#both-(default)

ReasoningTools(enable_think=True, enable_analyze=True)

---

## Initialize the Agent with the knowledge

**URL:** llms-txt#initialize-the-agent-with-the-knowledge

**Contents:**
- Usage
- Params

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    # Comment out after first run
    asyncio.run(
        knowledge.add_content_async(
            topics=["web3 latest trends 2025"],
            reader=WebSearchReader(
                max_results=3,
                search_engine="duckduckgo",
                chunk=True,
            ),
        )
    )

# Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What are the latest AI trends according to the search results?",
            markdown=True,
        )
    )

bash  theme={null}
    pip install -U requests beautifulsoup4 agno openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python examples/basics/knowledge/readers/web_search_reader_async.py
      bash Windows theme={null}
      python examples/basics/knowledge/readers/web_search_reader_async.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="web-search-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Accuracy with Tools

**URL:** llms-txt#accuracy-with-tools

Source: https://docs.agno.com/basics/evals/accuracy/usage/accuracy-with-tools

Exmaple showing an evaluation that runs the provided agent with the provided input and then evaluates the answer that the agent gives.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Reasoning Analyst Agent - Specialized in logical analysis

**URL:** llms-txt#reasoning-analyst-agent---specialized-in-logical-analysis

reasoning_analyst = Agent(
    name="Reasoning Analyst",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Apply logical reasoning to analyze gathered information",
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Analyze information using structured reasoning approaches.",
        "Identify logical connections and relationships.",
        "Apply deductive and inductive reasoning where appropriate.",
        "Break down complex topics into logical components.",
        "Use reasoning tools to structure your analysis.",
    ],
    markdown=True,
)

---

## Initialize OpenLIT instrumentation

**URL:** llms-txt#initialize-openlit-instrumentation

openlit.init(otlp_endpoint="http://127.0.0.1:4318")

---

## TeamRunOutput

**URL:** llms-txt#teamrunoutput

**Contents:**
- TeamRunOutput Attributes
- TeamRunOutputEvent Types
  - Core Events
  - Pre-Hook Events
  - Post-Hook Events
  - Tool Events
  - Reasoning Events
  - Memory Events
  - Session Summary Events
- Event Attributes

Source: https://docs.agno.com/reference/teams/team-response

The `TeamRunOutput` class represents the response from a team run, containing both the team's overall response and individual member responses. It supports streaming and provides real-time events throughout the execution of a team.

## TeamRunOutput Attributes

| Attribute             | Type                                              | Default             | Description                                 |
| --------------------- | ------------------------------------------------- | ------------------- | ------------------------------------------- |
| `content`             | `Any`                                             | `None`              | Content of the response                     |
| `content_type`        | `str`                                             | `"str"`             | Specifies the data type of the content      |
| `messages`            | `List[Message]`                                   | `None`              | A list of messages included in the response |
| `metrics`             | `Metrics`                                         | `None`              | Usage metrics of the run                    |
| `model`               | `str`                                             | `None`              | The model used in the run                   |
| `model_provider`      | `str`                                             | `None`              | The model provider used in the run          |
| `member_responses`    | `List[Union[TeamRunOutput, RunOutput]]`           | `[]`                | Responses from individual team members      |
| `run_id`              | `str`                                             | `None`              | Run Id                                      |
| `team_id`             | `str`                                             | `None`              | Team Id for the run                         |
| `team_name`           | `str`                                             | `None`              | Name of the team                            |
| `session_id`          | `str`                                             | `None`              | Session Id for the run                      |
| `parent_run_id`       | `str`                                             | `None`              | Parent run ID if this is a nested run       |
| `tools`               | `List[ToolExecution]`                             | `None`              | List of tools provided to the model         |
| `images`              | `List[Image]`                                     | `None`              | List of images from member runs             |
| `videos`              | `List[Video]`                                     | `None`              | List of videos from member runs             |
| `audio`               | `List[Audio]`                                     | `None`              | List of audio snippets from member runs     |
| `files`               | `List[File]`                                      | `None`              | List of files from member runs              |
| `response_audio`      | `Audio`                                           | `None`              | The model's raw response in audio           |
| `input`               | `TeamRunInput`                                    | `None`              | Input media and messages from user          |
| `reasoning_content`   | `str`                                             | `None`              | Any reasoning content the model produced    |
| `citations`           | `Citations`                                       | `None`              | Any citations used in the response          |
| `model_provider_data` | `Any`                                             | `None`              | Model provider specific metadata            |
| `metadata`            | `Dict[str, Any]`                                  | `None`              | Additional metadata for the run             |
| `references`          | `List[MessageReferences]`                         | `None`              | Message references                          |
| `additional_input`    | `List[Message]`                                   | `None`              | Additional input messages                   |
| `reasoning_steps`     | `List[ReasoningStep]`                             | `None`              | Reasoning steps taken during execution      |
| `reasoning_messages`  | `List[Message]`                                   | `None`              | Messages related to reasoning               |
| `created_at`          | `int`                                             | Current timestamp   | Unix timestamp of the response creation     |
| `events`              | `List[Union[RunOutputEvent, TeamRunOutputEvent]]` | `None`              | List of events that occurred during the run |
| `status`              | `RunStatus`                                       | `RunStatus.running` | Current status of the run                   |
| `workflow_step_id`    | `str`                                             | `None`              | FK: Points to StepOutput.step\_id           |

## TeamRunOutputEvent Types

The following events are sent by the `Team.run()` function depending on the team's configuration:

| Event Type                   | Description                                             |
| ---------------------------- | ------------------------------------------------------- |
| `TeamRunStarted`             | Indicates the start of a team run                       |
| `TeamRunContent`             | Contains the model's response text as individual chunks |
| `TeamRunContentCompleted`    | Signals completion of content streaming                 |
| `TeamRunIntermediateContent` | Contains intermediate content during the run            |
| `TeamRunCompleted`           | Signals successful completion of the team run           |
| `TeamRunError`               | Indicates an error occurred during the team run         |
| `TeamRunCancelled`           | Signals that the team run was cancelled                 |

| Event Type             | Description                                    |
| ---------------------- | ---------------------------------------------- |
| `TeamPreHookStarted`   | Indicates the start of a pre-run hook          |
| `TeamPreHookCompleted` | Signals completion of a pre-run hook execution |

| Event Type              | Description                                     |
| ----------------------- | ----------------------------------------------- |
| `TeamPostHookStarted`   | Indicates the start of a post-run hook          |
| `TeamPostHookCompleted` | Signals completion of a post-run hook execution |

| Event Type              | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| `TeamToolCallStarted`   | Indicates the start of a tool call                             |
| `TeamToolCallCompleted` | Signals completion of a tool call, including tool call results |

| Event Type               | Description                                         |
| ------------------------ | --------------------------------------------------- |
| `TeamReasoningStarted`   | Indicates the start of the team's reasoning process |
| `TeamReasoningStep`      | Contains a single step in the reasoning process     |
| `TeamReasoningCompleted` | Signals completion of the reasoning process         |

| Event Type                  | Description                                    |
| --------------------------- | ---------------------------------------------- |
| `TeamMemoryUpdateStarted`   | Indicates that the team is updating its memory |
| `TeamMemoryUpdateCompleted` | Signals completion of a memory update          |

### Session Summary Events

| Event Type                    | Description                                       |
| ----------------------------- | ------------------------------------------------- |
| `TeamSessionSummaryStarted`   | Indicates the start of session summary generation |
| `TeamSessionSummaryCompleted` | Signals completion of session summary generation  |

### Base TeamRunOutputEvent

All events inherit from `BaseTeamRunEvent` which provides these common attributes:

| Attribute         | Type            | Default           | Description                           |
| ----------------- | --------------- | ----------------- | ------------------------------------- |
| `created_at`      | `int`           | Current timestamp | Unix timestamp of the event creation  |
| `event`           | `str`           | `""`              | The type of event                     |
| `team_id`         | `str`           | `""`              | ID of the team generating the event   |
| `team_name`       | `str`           | `""`              | Name of the team generating the event |
| `run_id`          | `Optional[str]` | `None`            | ID of the current run                 |
| `session_id`      | `Optional[str]` | `None`            | ID of the current session             |
| `workflow_id`     | `Optional[str]` | `None`            | ID of the workflow                    |
| `workflow_run_id` | `Optional[str]` | `None`            | ID of the workflow's run              |
| `step_id`         | `Optional[str]` | `None`            | ID of the workflow step               |
| `step_name`       | `Optional[str]` | `None`            | Name of the workflow step             |
| `step_index`      | `Optional[int]` | `None`            | Index of the workflow step            |
| `content`         | `Optional[Any]` | `None`            | For backwards compatibility           |

| Attribute        | Type  | Default            | Description               |
| ---------------- | ----- | ------------------ | ------------------------- |
| `event`          | `str` | `"TeamRunStarted"` | Event type                |
| `model`          | `str` | `""`               | The model being used      |
| `model_provider` | `str` | `""`               | The provider of the model |

### IntermediateRunContentEvent

| Attribute      | Type            | Default                        | Description                          |
| -------------- | --------------- | ------------------------------ | ------------------------------------ |
| `event`        | `str`           | `"TeamRunIntermediateContent"` | Event type                           |
| `content`      | `Optional[Any]` | `None`                         | Intermediate content of the response |
| `content_type` | `str`           | `"str"`                        | Type of the content                  |

### RunContentCompletedEvent

| Attribute | Type  | Default                     | Description |
| --------- | ----- | --------------------------- | ----------- |
| `event`   | `str` | `"TeamRunContentCompleted"` | Event type  |

| Attribute             | Type                                | Default            | Description                      |
| --------------------- | ----------------------------------- | ------------------ | -------------------------------- |
| `event`               | `str`                               | `"TeamRunContent"` | Event type                       |
| `content`             | `Optional[Any]`                     | `None`             | The content of the response      |
| `content_type`        | `str`                               | `"str"`            | Type of the content              |
| `reasoning_content`   | `Optional[str]`                     | `None`             | Reasoning content produced       |
| `citations`           | `Optional[Citations]`               | `None`             | Citations used in the response   |
| `model_provider_data` | `Optional[Any]`                     | `None`             | Model provider specific metadata |
| `response_audio`      | `Optional[Audio]`                   | `None`             | Model's audio response           |
| `image`               | `Optional[Image]`                   | `None`             | Image attached to the response   |
| `references`          | `Optional[List[MessageReferences]]` | `None`             | Message references               |
| `additional_input`    | `Optional[List[Message]]`           | `None`             | Additional input messages        |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`             | Reasoning steps                  |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`             | Reasoning messages               |

### RunCompletedEvent

| Attribute             | Type                                    | Default              | Description                             |
| --------------------- | --------------------------------------- | -------------------- | --------------------------------------- |
| `event`               | `str`                                   | `"TeamRunCompleted"` | Event type                              |
| `content`             | `Optional[Any]`                         | `None`               | Final content of the response           |
| `content_type`        | `str`                                   | `"str"`              | Type of the content                     |
| `reasoning_content`   | `Optional[str]`                         | `None`               | Reasoning content produced              |
| `citations`           | `Optional[Citations]`                   | `None`               | Citations used in the response          |
| `model_provider_data` | `Optional[Any]`                         | `None`               | Model provider specific metadata        |
| `images`              | `Optional[List[Image]]`                 | `None`               | Images attached to the response         |
| `videos`              | `Optional[List[Video]]`                 | `None`               | Videos attached to the response         |
| `audio`               | `Optional[List[Audio]]`                 | `None`               | Audio snippets attached to the response |
| `response_audio`      | `Optional[Audio]`                       | `None`               | Model's audio response                  |
| `references`          | `Optional[List[MessageReferences]]`     | `None`               | Message references                      |
| `additional_input`    | `Optional[List[Message]]`               | `None`               | Additional input messages               |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`         | `None`               | Reasoning steps                         |
| `reasoning_messages`  | `Optional[List[Message]]`               | `None`               | Reasoning messages                      |
| `member_responses`    | `List[Union[TeamRunOutput, RunOutput]]` | `[]`                 | Responses from individual team members  |
| `metadata`            | `Optional[Dict[str, Any]]`              | `None`               | Additional metadata                     |
| `metrics`             | `Optional[Metrics]`                     | `None`               | Usage metrics                           |

| Attribute | Type            | Default          | Description   |
| --------- | --------------- | ---------------- | ------------- |
| `event`   | `str`           | `"TeamRunError"` | Event type    |
| `content` | `Optional[str]` | `None`           | Error message |

### RunCancelledEvent

| Attribute | Type            | Default              | Description             |
| --------- | --------------- | -------------------- | ----------------------- |
| `event`   | `str`           | `"TeamRunCancelled"` | Event type              |
| `reason`  | `Optional[str]` | `None`               | Reason for cancellation |

### PreHookStartedEvent

| Attribute       | Type                     | Default                | Description                         |
| --------------- | ------------------------ | ---------------------- | ----------------------------------- |
| `event`         | `str`                    | `"TeamPreHookStarted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`          | `None`                 | Name of the pre-hook being executed |
| `run_input`     | `Optional[TeamRunInput]` | `None`                 | The run input passed to the hook    |

### PreHookCompletedEvent

| Attribute       | Type                     | Default                  | Description                         |
| --------------- | ------------------------ | ------------------------ | ----------------------------------- |
| `event`         | `str`                    | `"TeamPreHookCompleted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`          | `None`                   | Name of the pre-hook that completed |
| `run_input`     | `Optional[TeamRunInput]` | `None`                   | The run input passed to the hook    |

### PostHookStartedEvent

| Attribute        | Type            | Default                 | Description                          |
| ---------------- | --------------- | ----------------------- | ------------------------------------ |
| `event`          | `str`           | `"TeamPostHookStarted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`                  | Name of the post-hook being executed |

### PostHookCompletedEvent

| Attribute        | Type            | Default                   | Description                          |
| ---------------- | --------------- | ------------------------- | ------------------------------------ |
| `event`          | `str`           | `"TeamPostHookCompleted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`                    | Name of the post-hook that completed |

### ToolCallStartedEvent

| Attribute | Type                      | Default                 | Description           |
| --------- | ------------------------- | ----------------------- | --------------------- |
| `event`   | `str`                     | `"TeamToolCallStarted"` | Event type            |
| `tool`    | `Optional[ToolExecution]` | `None`                  | The tool being called |

### ToolCallCompletedEvent

| Attribute | Type                      | Default                   | Description                 |
| --------- | ------------------------- | ------------------------- | --------------------------- |
| `event`   | `str`                     | `"TeamToolCallCompleted"` | Event type                  |
| `tool`    | `Optional[ToolExecution]` | `None`                    | The tool that was called    |
| `content` | `Optional[Any]`           | `None`                    | Result of the tool call     |
| `images`  | `Optional[List[Image]]`   | `None`                    | Images produced by the tool |
| `videos`  | `Optional[List[Video]]`   | `None`                    | Videos produced by the tool |
| `audio`   | `Optional[List[Audio]]`   | `None`                    | Audio produced by the tool  |

### ReasoningStartedEvent

| Attribute | Type  | Default                  | Description |
| --------- | ----- | ------------------------ | ----------- |
| `event`   | `str` | `"TeamReasoningStarted"` | Event type  |

### ReasoningStepEvent

| Attribute           | Type            | Default               | Description                   |
| ------------------- | --------------- | --------------------- | ----------------------------- |
| `event`             | `str`           | `"TeamReasoningStep"` | Event type                    |
| `content`           | `Optional[Any]` | `None`                | Content of the reasoning step |
| `content_type`      | `str`           | `"str"`               | Type of the content           |
| `reasoning_content` | `str`           | `""`                  | Detailed reasoning content    |

### ReasoningCompletedEvent

| Attribute      | Type            | Default                    | Description                   |
| -------------- | --------------- | -------------------------- | ----------------------------- |
| `event`        | `str`           | `"TeamReasoningCompleted"` | Event type                    |
| `content`      | `Optional[Any]` | `None`                     | Content of the reasoning step |
| `content_type` | `str`           | `"str"`                    | Type of the content           |

### MemoryUpdateStartedEvent

| Attribute | Type  | Default                     | Description |
| --------- | ----- | --------------------------- | ----------- |
| `event`   | `str` | `"TeamMemoryUpdateStarted"` | Event type  |

### MemoryUpdateCompletedEvent

| Attribute | Type  | Default                       | Description |
| --------- | ----- | ----------------------------- | ----------- |
| `event`   | `str` | `"TeamMemoryUpdateCompleted"` | Event type  |

### SessionSummaryStartedEvent

| Attribute | Type  | Default                       | Description |
| --------- | ----- | ----------------------------- | ----------- |
| `event`   | `str` | `"TeamSessionSummaryStarted"` | Event type  |

### SessionSummaryCompletedEvent

| Attribute         | Type            | Default                         | Description                   |
| ----------------- | --------------- | ------------------------------- | ----------------------------- |
| `event`           | `str`           | `"TeamSessionSummaryCompleted"` | Event type                    |
| `session_summary` | `Optional[Any]` | `None`                          | The generated session summary |

### ParserModelResponseStartedEvent

| Attribute | Type  | Default                            | Description |
| --------- | ----- | ---------------------------------- | ----------- |
| `event`   | `str` | `"TeamParserModelResponseStarted"` | Event type  |

### ParserModelResponseCompletedEvent

| Attribute | Type  | Default                              | Description |
| --------- | ----- | ------------------------------------ | ----------- |
| `event`   | `str` | `"TeamParserModelResponseCompleted"` | Event type  |

### OutputModelResponseStartedEvent

| Attribute | Type  | Default                            | Description |
| --------- | ----- | ---------------------------------- | ----------- |
| `event`   | `str` | `"TeamOutputModelResponseStarted"` | Event type  |

### OutputModelResponseCompletedEvent

| Attribute | Type  | Default                              | Description |
| --------- | ----- | ------------------------------------ | ----------- |
| `event`   | `str` | `"TeamOutputModelResponseCompleted"` | Event type  |

| Attribute | Type  | Default         | Description |
| --------- | ----- | --------------- | ----------- |
| `event`   | `str` | `"CustomEvent"` | Event type  |

---

## Run async example

**URL:** llms-txt#run-async-example

**Contents:**
- Params
- Features
- Developer Resources

asyncio.run(embed_texts())
```

| Parameter        | Type                                 | Default                               | Description                                                       |
| ---------------- | ------------------------------------ | ------------------------------------- | ----------------------------------------------------------------- |
| `id`             | `str`                                | `"jina-embeddings-v3"`                | The model ID to use for generating embeddings.                    |
| `dimensions`     | `int`                                | `1024`                                | The number of dimensions for the embedding vectors.               |
| `embedding_type` | `Literal["float", "base64", "int8"]` | `"float"`                             | The format type of the returned embeddings.                       |
| `late_chunking`  | `bool`                               | `False`                               | Whether to enable late chunking optimization.                     |
| `user`           | `Optional[str]`                      | `None`                                | User identifier for tracking purposes. Optional.                  |
| `api_key`        | `Optional[str]`                      | `JINA_API_KEY` env var                | The Jina AI API key. Can be set via environment variable.         |
| `base_url`       | `str`                                | `"https://api.jina.ai/v1/embeddings"` | The base URL for the Jina API.                                    |
| `headers`        | `Optional[Dict[str, str]]`           | `None`                                | Additional headers to include in API requests. Optional.          |
| `request_params` | `Optional[Dict[str, Any]]`           | `None`                                | Additional parameters to include in the API request. Optional.    |
| `timeout`        | `Optional[float]`                    | `None`                                | Timeout in seconds for API requests. Optional.                    |
| `enable_batch`   | `bool`                               | `False`                               | Enable batch processing to reduce API calls and avoid rate limits |
| `batch_size`     | `int`                                | `100`                                 | Number of texts to process in each API call for batch operations. |

* **Async Support**: Full async/await support for better performance in concurrent applications
* **Batch Processing**: Efficient batch processing of multiple texts with configurable batch size
* **Late Chunking**: Support for Jina's late chunking optimization technique
* **Flexible Output**: Multiple embedding formats (float, base64, int8)
* **Usage Tracking**: Get detailed usage information for API calls
* **Error Handling**: Robust error handling with fallback mechanisms

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/embedders/jina_embedder.py)
* [Jina AI Documentation](https://jina.ai/embeddings/)

---

## run: RunOutput = agent.run("Share a 2 sentence horror story")

**URL:** llms-txt#run:-runoutput-=-agent.run("share-a-2-sentence-horror-story")

---

## Get by trace_id

**URL:** llms-txt#get-by-trace_id

trace = db.get_trace(trace_id="abc123...")

---

## Create team with input_schema for automatic validation

**URL:** llms-txt#create-team-with-input_schema-for-automatic-validation

research_team = Team(
    name="Research Team with Input Validation",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[hackernews_agent, web_researcher],
    input_schema=ResearchProject,
    instructions=[
        "Conduct thorough research based on the validated input",
        "Coordinate between team members to avoid duplicate work",
        "Ensure research depth matches the specified level",
        "Respect the maximum sources limit",
        "Focus on recent sources if requested",
    ],
)

print("=== Example 1: Valid Dictionary Input (will be auto-validated) ===")

---

## JWT Middleware with Authorization Headers

**URL:** llms-txt#jwt-middleware-with-authorization-headers

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/middleware/jwt-middleware

Complete AgentOS setup with JWT middleware for authentication and parameter injection using Authorization headers

This example demonstrates how to use JWT middleware with AgentOS for authentication and automatic parameter injection using Authorization headers.

```python jwt_middleware.py theme={null}
from datetime import UTC, datetime, timedelta

import jwt
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.middleware import JWTMiddleware

---

## Reddit Post Generator

**URL:** llms-txt#reddit-post-generator

**Contents:**
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/reddit-post-generator

**Reddit Post Generator** is a team of agents that can research topics on the web and make posts for a subreddit on Reddit.

Create a file `reddit_post_generator_team.py` with the following code:

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Alternative example with different topic

**URL:** llms-txt#alternative-example-with-different-topic

**Contents:**
- Usage

alternative_research = ResearchTopic(
    topic="Distributed Systems",
    focus_areas=["Microservices", "Event-Driven Architecture", "Scalability"],
    target_audience="Backend Engineers",
    sources_required=5,
)

team.print_response(input=alternative_research)
bash  theme={null}
    pip install agno pydantic
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/01_pydantic_model_as_input.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## The database you want to migrate

**URL:** llms-txt#the-database-you-want-to-migrate

**Contents:**
- Troubleshooting
- Migrating from Agno v1 to v2

db = AsyncPostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

async run_revert_migrations():
    await MigrationManager(db).down(target_version="2.0.0")

if __name__ == "__main__":
    asyncio.run(run_revert_migrations())
python  theme={null}
      await MigrationManager(db).up(
          target_version="2.3.0",
          table_type="memory",
          force=True,
      )
    ```
  </Tab>

<Tab title="Invalid Column Errors">
    Ensure you have run the migration before using the updated `agno` code in production.
  </Tab>

<Tab title="SQL INSERT Errors">
    If you are facing SQL INSERT errors when using your database, ensure that you have run the latest migrations AND you restarted your AgentOS instance.
  </Tab>
</Tabs>

## Migrating from Agno v1 to v2

If you started using Agno during its v1 and want to move to v2, we have a migration script that can help you update your database tables.

You can find the script in the [`libs/agno/migrations/v1_to_v2/migrate_to_v2.py`](https://github.com/agno-agi/agno/blob/main/libs/agno/migrations/v1_to_v2/migrate_to_v2.py) file.

You can find more information about migrating from v1 to v2 in the [Migrating to Agno v2 guide](/how-to/v2-migration).

**Examples:**

Example 1 (unknown):
```unknown
## Troubleshooting

<Tabs>
  <Tab title="Schema mismatch errors">
    If you continue to see errors and are not able to read or write to the database, it's likely due to a mismatch between the schema version and the actual schema of the table.

    Please set the `force` parameter to `True` to force the migration for a specific table.
```

---

## Team Streaming Responses

**URL:** llms-txt#team-streaming-responses

Source: https://docs.agno.com/basics/teams/usage/basic-flows/basic-streaming

This example demonstrates streaming responses from a team using specialized agents with financial tools to provide real-time stock information with streaming output.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/streaming" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setup our Agent with the reasoning tools

**URL:** llms-txt#setup-our-agent-with-the-reasoning-tools

reasoning_agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20241022"),
    tools=[
        ReasoningTools(add_instructions=True),
    ],
    instructions="Use tables where possible",
    markdown=True,
)

---

## Demo Gemma

**URL:** llms-txt#demo-gemma

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/ollama/usage/demo-gemma

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Production Application

**URL:** llms-txt#production-application

**Contents:**
- Workspace Settings
- Build your production image
  - Create an ECR Repository
  - Update the `InfraSettings`
  - Build a new image
- ECS Task Definition
- ECS Service

Source: https://docs.agno.com/templates/infra-management/production-app

Your production application runs on AWS and its resources are defined in the `infra/prd_resources.py` file. This guide shows how to:

1. [Build a production image](#build-your-production-image)
2. [Update ECS Task Definitions](#ecs-task-definition)
3. [Update ECS Services](#ecs-service)

## Workspace Settings

The `InfraSettings` object in the `infra/settings.py` file defines common settings used by your workspace apps and resources.

## Build your production image

Your application uses the `agno` images by default. To use your own image:

* Create a Repository in `ECR` and authenticate or use `Dockerhub`.
* Open `infra/settings.py` file
* Update the `image_repo` to your image repository
* Set `build_images=True` and `push_images=True`
* Optional - Set `build_images=False` and `push_images=False` to use an existing image in the repository

### Create an ECR Repository

To use ECR, **create the image repo and authenticate with ECR** before pushing images.

**1. Create the image repository in ECR**

The repo name should match the `infra_name`. Meaning if you're using the default infra name, the repo name would be `ai`.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c68ceb3a9b6784fd519cc04b0e38caf1" alt="create-ecr-image" data-og-width="1389" width="1389" data-og-height="408" height="408" data-path="images/create-ecr-image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ce02d48da7e53a6c335736a17ebec6e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f0b4d1687849a637c0a595c4a8d0690a 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=26b1f13eb8b6f9b09a06b9e6bb1eeb27 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e53f084201341a7c92738fa62efdb64c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c9e2477e1befaf12f81d4d345dac5a26 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a22af7830053ba9139cfb6d0d4017d4a 2500w" />

**2. Authenticate with ECR**

You can also use a helper script to avoid running the full command

<Note>
  Update the script with your ECR repo before running.
</Note>

<CodeGroup>
  
</CodeGroup>

### Update the `InfraSettings`

<Note>
  The `image_repo` defines the repo for your image.

* If using dockerhub it would be something like `agno`.
  * If using ECR it would be something like `[ACCOUNT_ID].dkr.ecr.us-east-1.amazonaws.com`
</Note>

### Build a new image

Build the production image using:

To `force` rebuild images, use the `--force` or `-f` flag

Because the only docker resources in the production env are docker images, you can also use:

## ECS Task Definition

If you updated the Image, CPU, Memory or Environment Variables, update the Task Definition using:

To redeploy the production application, update the ECS Service using:

<Note>
  If you **ONLY** rebuilt the image, you do not need to update the task definition and can just patch the service to pickup the new image.
</Note>

**Examples:**

Example 1 (unknown):
```unknown
You can also use a helper script to avoid running the full command

<Note>
  Update the script with your ECR repo before running.
</Note>

<CodeGroup>
```

Example 2 (unknown):
```unknown
</CodeGroup>

### Update the `InfraSettings`
```

Example 3 (unknown):
```unknown
<Note>
  The `image_repo` defines the repo for your image.

  * If using dockerhub it would be something like `agno`.
  * If using ECR it would be something like `[ACCOUNT_ID].dkr.ecr.us-east-1.amazonaws.com`
</Note>

### Build a new image

Build the production image using:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Chat History

**URL:** llms-txt#chat-history

**Contents:**
- Learn more

Source: https://docs.agno.com/basics/chat-history/overview

Learn about how to manage history in Agno.

Chat History enables your agents, teams, and workflows to remember and reference previous conversations, creating intelligent and context-aware interactions.

Instead of starting fresh with each interaction, Chat History allows you to:

* **Maintain conversation continuity** - Build on previous exchanges within a session
* **Provide personalized responses** - Reference past interactions to tailor outputs
* **Avoid repetitive questions** - Access previously provided information
* **Enable long-running conversations** - Support multi-turn dialogues with persistent memory

<Note>
  All history features require a database configured on your agent, team, or workflow. See [Database](/basics/database/overview) for setup details.
</Note>

<CardGroup cols={3}>
  <Card title="Chat History in Agents" icon="robot" iconType="duotone" href="/basics/chat-history/agent">
    Learn about managing chat history in agents.
  </Card>

<Card title="Chat History in Teams" icon="users" iconType="duotone" href="/basics/chat-history/team">
    Learn about managing chat history in teams.
  </Card>

<Card title="Chat History in Workflows" icon="diagram-project" iconType="duotone" href="/basics/chat-history/workflow">
    Learn about managing chat history in workflows.
  </Card>
</CardGroup>

---

## LlamaCpp

**URL:** llms-txt#llamacpp

**Contents:**
  - Google Gemma Models
  - Meta Llama Models
  - Default Options
- Set up LlamaCpp
  - Install LlamaCpp

Source: https://docs.agno.com/integrations/models/local/llama-cpp/overview

Learn how to use LlamaCpp with Agno.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.0.7" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.0.7">v2.0.7</Tooltip>
</Badge>

Run Large Language Models locally with LLaMA CPP

[LlamaCpp](https://github.com/ggerganov/llama.cpp) is a powerful tool for running large language models locally with efficient inference. LlamaCpp supports multiple open-source models and provides an OpenAI-compatible API server.

LlamaCpp supports a wide variety of models in GGML format. You can find models on HuggingFace, including the default `ggml-org/gpt-oss-20b-GGUF` used in the examples below.

We recommend experimenting to find the best model for your use case. Here are some popular model recommendations:

### Google Gemma Models

* `google/gemma-2b-it-GGUF` - Lightweight 2B parameter model, great for resource-constrained environments
* `google/gemma-7b-it-GGUF` - Balanced 7B model with strong performance for general tasks
* `ggml-org/gemma-3-1b-it-GGUF` - Latest Gemma 3 series, efficient for everyday use

### Meta Llama Models

* `Meta-Llama-3-8B-Instruct` - Popular 8B parameter model with excellent instruction following
* `Meta-Llama-3.1-8B-Instruct` - Enhanced version with improved capabilities and 128K context
* `Meta-Llama-3.2-3B-Instruct` - Compact 3B model for faster inference

* `ggml-org/gpt-oss-20b-GGUF` - Default model for general use cases
* Models with different quantizations (Q4\_K\_M, Q8\_0, etc.) for different speed/quality tradeoffs
* Choose models based on your hardware constraints and performance requirements

First, install LlamaCpp following the [official installation guide](https://github.com/ggerganov/llama.cpp):

Or using package managers:

```bash brew install theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Or using package managers:
```

---

## Create conversational meal planning workflow

**URL:** llms-txt#create-conversational-meal-planning-workflow

meal_workflow = Workflow(
    name="Conversational Meal Planner",
    description="Smart meal planning with conversation awareness and preference learning",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/meal_workflow.db",
    ),
    steps=[suggestion_step, preference_analysis_step, recipe_step],
    add_workflow_history_to_steps=True,
)

def demonstrate_conversational_meal_planning():
    """Demonstrate natural conversational meal planning"""
    session_id = "meal_planning_demo"

print("ðŸ½ï¸  Conversational Meal Planning Demo")
    print("=" * 60)

# First interaction
    print("\nðŸ‘¤ User: What should I cook for dinner tonight?")
    meal_workflow.print_response(
        input="What should I cook for dinner tonight?",
        session_id=session_id,
        markdown=True,
    )

# Second interaction - user provides preferences
    print(
        "\nðŸ‘¤ User: I had Italian yesterday, and I'm trying to eat healthier these days"
    )
    meal_workflow.print_response(
        input="I had Italian yesterday, and I'm trying to eat healthier these days",
        session_id=session_id,
        markdown=True,
    )

# Third interaction - more specific request
    print(
        "\nðŸ‘¤ User: Actually, do you have something with fish? I love Asian flavors too"
    )
    meal_workflow.print_response(
        input="Actually, do you have something with fish? I love Asian flavors too",
        session_id=session_id,
        markdown=True,
    )

if __name__ == "__main__":
    demonstrate_conversational_meal_planning()
```

---

## LlamaIndex Async

**URL:** llms-txt#llamaindex-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/llamaindex/usage/async-llamaindex-db

```python cookbook/knowledge/vector_db/llamaindex_db/llamaindex_db.py theme={null}
import asyncio
from pathlib import Path
from shutil import rmtree

import httpx
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.llamaindex import LlamaIndexVectorDb
from llama_index.core import (
    SimpleDirectoryReader,
    StorageContext,
    VectorStoreIndex,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import VectorIndexRetriever

data_dir = Path(__file__).parent.parent.parent.joinpath("wip", "data", "paul_graham")
if data_dir.is_dir():
    rmtree(path=data_dir, ignore_errors=True)
data_dir.mkdir(parents=True, exist_ok=True)

url = "https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt"
file_path = data_dir.joinpath("paul_graham_essay.txt")
response = httpx.get(url)
if response.status_code == 200:
    with open(file_path, "wb") as file:
        file.write(response.content)
    print(f"File downloaded and saved as {file_path}")
else:
    print("Failed to download the file")

documents = SimpleDirectoryReader(str(data_dir)).load_data()

splitter = SentenceSplitter(chunk_size=1024)

nodes = splitter.get_nodes_from_documents(documents)

storage_context = StorageContext.from_defaults()

index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)

knowledge_retriever = VectorIndexRetriever(index)

knowledge = Knowledge(
    vector_db=LlamaIndexVectorDb(knowledge_retriever=knowledge_retriever)
)

---

## Create agent with Redis db

**URL:** llms-txt#create-agent-with-redis-db

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")

---

## Use FileHandler to write to file

**URL:** llms-txt#use-filehandler-to-write-to-file

handler = logging.FileHandler(log_file_path)
formatter = logging.Formatter("%(levelname)s: %(message)s")
handler.setFormatter(formatter)
custom_logger.addHandler(handler)
custom_logger.setLevel(logging.INFO)
custom_logger.propagate = False

---

## Development: Fast, local, zero setup

**URL:** llms-txt#development:-fast,-local,-zero-setup

dev_db = LanceDb(
    table_name="dev_knowledge",
    uri="./local_db"
)

---

## Add content with metadata

**URL:** llms-txt#add-content-with-metadata

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

---

## FirestoreDb

**URL:** llms-txt#firestoredb

Source: https://docs.agno.com/reference/storage/firestore

`FirestoreDb` is a class that implements the Db interface using Google Firestore as the backend storage system. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-firestore-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Agentic RAG with Hybrid Search and Reranking

**URL:** llms-txt#agentic-rag-with-hybrid-search-and-reranking

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag

This example demonstrates how to implement Agentic RAG using Hybrid Search and Reranking with LanceDB, Cohere embeddings, and Cohere reranking for enhanced document retrieval and response generation.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your ANTHROPIC API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/agentic_search" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your ANTHROPIC API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Structured Output

**URL:** llms-txt#structured-output

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/xai/usage/structured-output

```python cookbook/models/xai/structured_output.py theme={null}
from typing import List

from agno.agent import Agent
from agno.models.xai.xai import xAI
from agno.run.agent import RunOutput
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa

class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

---

## More examples to try:

**URL:** llms-txt#more-examples-to-try:

"""
Sample prompts to explore:
1. "What's the historical significance of this location?"
2. "How has this place changed over time?"
3. "What cultural events happen here?"
4. "What's the architectural style and influence?"
5. "What recent developments affect this area?"

Sample image URLs to analyze:
1. Eiffel Tower: "https://upload.wikimedia.org/wikipedia/commons/8/85/Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg"
2. Taj Mahal: "https://upload.wikimedia.org/wikipedia/commons/b/bd/Taj_Mahal%2C_Agra%2C_India_edit3.jpg"
3. Golden Gate Bridge: "https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
"""

---

## Create an agent equipped with X toolkit

**URL:** llms-txt#create-an-agent-equipped-with-x-toolkit

agent = Agent(
    instructions=[
        "Use X tools to interact as the authorized user",
        "Generate appropriate content when asked to create posts",
        "Only post content when explicitly instructed",
        "Respect X's usage policies and rate limits",
    ],
    tools=[x_tools],
    )

---

## Document Chunking

**URL:** llms-txt#document-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/document

Document chunking is a method of splitting documents into smaller chunks based on document structure like paragraphs and sections.
It analyzes natural document boundaries rather than splitting at fixed character counts. This is useful when you want to process large documents while preserving semantic meaning and context.

<Snippet file="chunking-document.mdx" />

---

## Create an agent with Zoom capabilities

**URL:** llms-txt#create-an-agent-with-zoom-capabilities

agent = Agent(tools=[zoom_tools])

---

## EVM (Ethereum Virtual Machine)

**URL:** llms-txt#evm-(ethereum-virtual-machine)

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/evm

EvmTools enables agents to interact with Ethereum and EVM-compatible blockchains for transactions and smart contract operations.

The following agent can interact with Ethereum blockchain:

| Parameter                 | Type            | Default | Description                                                   |
| ------------------------- | --------------- | ------- | ------------------------------------------------------------- |
| `private_key`             | `Optional[str]` | `None`  | Private key for signing transactions. Uses EVM\_PRIVATE\_KEY. |
| `rpc_url`                 | `Optional[str]` | `None`  | RPC URL for blockchain connection. Uses EVM\_RPC\_URL.        |
| `enable_send_transaction` | `bool`          | `True`  | Enable transaction sending functionality.                     |

| Function           | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| `send_transaction` | Send ETH or interact with smart contracts on the blockchain. |
| `get_balance`      | Get ETH balance for an address.                              |
| `get_transaction`  | Get transaction details by hash.                             |
| `estimate_gas`     | Estimate gas cost for a transaction.                         |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/evm.py)
* [Web3.py Documentation](https://web3py.readthedocs.io/)
* [Ethereum Documentation](https://ethereum.org/developers/)

---

## This section loads the knowledge base. Skip if your knowledge base was populated elsewhere.

**URL:** llms-txt#this-section-loads-the-knowledge-base.-skip-if-your-knowledge-base-was-populated-elsewhere.

---

## Ask the agent about the knowledge

**URL:** llms-txt#ask-the-agent-about-the-knowledge

**Contents:**
- Usage
- Params

agent.print_response(
    "What are the latest AI trends according to the search results?", markdown=True
)
bash  theme={null}
    pip install -U requests beautifulsoup4 agno openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python examples/basics/knowledge/readers/web_search_reader.py
      bash Windows theme={null}
      python examples/basics/knowledge/readers/web_search_reader.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="web-search-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Milvus Vector Database

**URL:** llms-txt#milvus-vector-database

**Contents:**
- Setup
- Initialize Milvus
- Example

Source: https://docs.agno.com/integrations/vectordb/milvus/overview

Learn how to use Milvus as a vector database for your Knowledge Base

Set the uri and token for your Milvus server.

* If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
* If you have large scale data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md).
  In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use `your_username:your_password` as the token, otherwise don't set the token.
* If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.

```python agent_with_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.milvus import Milvus

vector_db = Milvus(
    collection="recipes",
    uri="./milvus.db",
)

**Examples:**

Example 1 (unknown):
```unknown
## Initialize Milvus

Set the uri and token for your Milvus server.

* If you only need a local vector database for small scale data or prototyping, setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.
* If you have large scale data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md).
  In this setup, please use the server address and port as your uri, e.g.`http://localhost:19530`. If you enable the authentication feature on Milvus, use `your_username:your_password` as the token, otherwise don't set the token.
* If you use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.

## Example
```

---

## --- Helper Functions ---

**URL:** llms-txt#----helper-functions----

def get_cached_blog_post(session_state, topic: str) -> Optional[str]:
    """Get cached blog post from workflow session state"""
    logger.info("Checking if cached blog post exists")
    return session_state.get("blog_posts", {}).get(topic)

def cache_blog_post(session_state, topic: str, blog_post: str):
    """Cache blog post in workflow session state"""
    logger.info(f"Saving blog post for topic: {topic}")
    if "blog_posts" not in session_state:
        session_state["blog_posts"] = {}
    session_state["blog_posts"][topic] = blog_post

def get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:
    """Get cached search results from workflow session state"""
    logger.info("Checking if cached search results exist")
    search_results = session_state.get("search_results", {}).get(topic)
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"Could not validate cached search results: {e}")
    return search_results if isinstance(search_results, SearchResults) else None

def cache_search_results(session_state, topic: str, search_results: SearchResults):
    """Cache search results in workflow session state"""
    logger.info(f"Saving search results for topic: {topic}")
    if "search_results" not in session_state:
        session_state["search_results"] = {}
    session_state["search_results"][topic] = search_results.model_dump()

def get_cached_scraped_articles(
    session_state, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """Get cached scraped articles from workflow session state"""
    logger.info("Checking if cached scraped articles exist")
    scraped_articles = session_state.get("scraped_articles", {}).get(topic)
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"Could not validate cached scraped articles: {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None

def cache_scraped_articles(
    session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """Cache scraped articles in workflow session state"""
    logger.info(f"Saving scraped articles for topic: {topic}")
    if "scraped_articles" not in session_state:
        session_state["scraped_articles"] = {}
    session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }

async def get_search_results(
    session_state, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """Get search results with caching support"""

# Check cache first
    if use_cache:
        cached_results = get_cached_search_results(session_state, topic)
        if cached_results:
            logger.info(f"Found {len(cached_results.articles)} articles in cache.")
            return cached_results

# Search for new results
    for attempt in range(num_attempts):
        try:
            print(
                f"ðŸ” Searching for articles about: {topic} (attempt {attempt + 1}/{num_attempts})"
            )
            response = await research_agent.arun(topic)

if (
                response
                and response.content
                and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"Found {article_count} articles on attempt {attempt + 1}")
                print(f"âœ… Found {article_count} relevant articles")

# Cache the results
                cache_search_results(session_state, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                )

except Exception as e:
            logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

logger.error(f"Failed to get search results after {num_attempts} attempts")
    return None

async def scrape_articles(
    session_state,
    topic: str,
    search_results: SearchResults,
    use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """Scrape articles with caching support"""

# Check cache first
    if use_cache:
        cached_articles = get_cached_scraped_articles(session_state, topic)
        if cached_articles:
            logger.info(f"Found {len(cached_articles)} scraped articles in cache.")
            return cached_articles

scraped_articles: Dict[str, ScrapedArticle] = {}

print(f"ðŸ“„ Scraping {len(search_results.articles)} articles...")

for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f"ðŸ“– Scraping article {i}/{len(search_results.articles)}: {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

if (
                response
                and response.content
                and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"Scraped article: {response.content.url}")
                print(f"âœ… Successfully scraped: {response.content.title[:50]}...")
            else:
                print(f"âŒ Failed to scrape: {article.title[:50]}...")

except Exception as e:
            logger.warning(f"Failed to scrape {article.url}: {str(e)}")
            print(f"âŒ Error scraping: {article.title[:50]}...")

# Cache the scraped articles
    cache_scraped_articles(session_state, topic, scraped_articles)
    return scraped_articles

---

## This should route to the stock_searcher

**URL:** llms-txt#this-should-route-to-the-stock_searcher

**Contents:**
- Usage

response = team.run("What is the current stock price of NVDA?")
assert isinstance(response.content, StockReport)
pprint_run_response(response)
bash  theme={null}
    pip install agno openai ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/00_pydantic_model_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Add content to knowledge base

**URL:** llms-txt#add-content-to-knowledge-base

agno_docs_knowledge.add_content(url="https://docs.agno.com/llms-full.txt")

---

## Initialize AsyncSqliteDb with a database file

**URL:** llms-txt#initialize-asyncsqlitedb-with-a-database-file

**Contents:**
- Params
- Developer Resources

db = AsyncSqliteDb(db_file="workflow_storage.db")

hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)
web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web for the latest news and trends",
)
content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

research_step = Step(
    name="Research Step",
    team=research_team,
)
content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)
content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=db,
    steps=[research_step, content_planning_step],
)

if __name__ == "__main__":
    asyncio.run(
        content_creation_workflow.aprint_response(
            input="AI trends in 2024",
            markdown=True,
        )
    )

<Snippet file="db-async-sqlite-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/sqlite/async_sqlite/async_sqlite_for_workflow.py)

---

## Knowledge base with advanced reranking

**URL:** llms-txt#knowledge-base-with-advanced-reranking

reranked_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_reranked",
        uri="tmp/lancedb",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

---

## Accuracy with Teams

**URL:** llms-txt#accuracy-with-teams

Source: https://docs.agno.com/basics/evals/accuracy/usage/accuracy-with-teams

Example showing how to evaluate the accuracy of an Agno Team.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Sets the session state for the session with the id "user_2_session_1"

**URL:** llms-txt#sets-the-session-state-for-the-session-with-the-id-"user_2_session_1"

team.print_response("What is my name?", session_id="user_2_session_1", user_id="user_2", session_state={"user_name": "Jane", "age": 25})

---

## Development Application

**URL:** llms-txt#development-application

**Contents:**
- Infra Settings
- Build your development image
  - Build a new image
- Restart all containers
- Recreate development resources

Source: https://docs.agno.com/templates/infra-management/development-app

Your development application runs locally on docker and its resources are defined in the `infra/dev_resources.py` file. This guide shows how to:

1. [Build a development image](#build-your-development-image)
2. [Restart all docker containers](#restart-all-containers)
3. [Recreate development resources](#recreate-development-resources)

The `InfraSettings` object in the `infra/settings.py` file defines common settings used by your Agno Infra apps and resources.

## Build your development image

Your application uses the `agno` docker images by default. To use your own image:

* Open `infra/settings.py` file
* Update the `image_repo` to your image repository
* Set `build_images=True`

### Build a new image

Build the development image using:

To `force` rebuild images, use the `--force` or `-f` flag

## Restart all containers

Restart all docker containers using:

## Recreate development resources

To recreate all dev resources, use the `--force` flag:

**Examples:**

Example 1 (unknown):
```unknown
### Build a new image

Build the development image using:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

To `force` rebuild images, use the `--force` or `-f` flag

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Fully Python Workflow

**URL:** llms-txt#fully-python-workflow

Source: https://docs.agno.com/basics/workflows/workflow-patterns/fully-python-workflow

Keep it Simple with Pure Python, in v1 workflows style

**Keep it Simple with Pure Python**: If you prefer the Workflows 1.0 approach or need maximum flexibility, you can still use a single Python function to handle everything.
This approach gives you complete control over the execution flow while still benefiting from workflow features like storage, streaming, and session management.

Replace all the steps in the workflow with a single executable function where you can control everything.

* [Function-Based Workflow](/basics/workflows/usage/function-instead-of-steps) - Complete function-based workflow

For migration from 1.0 style workflows, refer to the page for [Migrating to Workflows 2.0](/how-to/v2-migration)

---

## Webex

**URL:** llms-txt#webex

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/social/webex

**WebexTools** enable an Agent to interact with Cisco Webex, allowing it to send messages and list rooms.

The following example requires the `webexpythonsdk` library and a Webex access token which can be obtained from [Webex Developer Portal](https://developer.webex.com/docs/bots).

To get started with Webex:

1. **Create a Webex Bot:**
   * Go to the [Developer Portal](https://developer.webex.com/)
   * Navigate to My Webex Apps â†’ Create a Bot
   * Fill in the bot details and click Add Bot

2. **Get your access token:**
   * Copy the token shown after bot creation
   * Or regenerate via My Webex Apps â†’ Edit Bot
   * Set as WEBEX\_ACCESS\_TOKEN environment variable

3. **Add the bot to Webex:**
   * Launch Webex and add the bot to a space
   * Use the bot's email (e.g. [test@webex.bot](mailto:test@webex.bot))

The following agent will list all spaces and send a message using Webex:

```python cookbook/tools/webex_tool.py theme={null}
from agno.agent import Agent
from agno.tools.webex import WebexTools

agent = Agent(tools=[WebexTools()])

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will list all spaces and send a message using Webex:
```

---

## Audio Multi Turn

**URL:** llms-txt#audio-multi-turn

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/audio-multi-turn

This example demonstrates how to create an agent that can handle multi-turn audio conversations, maintaining context between audio interactions while generating both text and audio responses.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Perplexity

**URL:** llms-txt#perplexity

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/perplexity

The Perplexity model provides access to Perplexity's language models.

| Parameter               | Type            | Default                               | Description                                                           |
| ----------------------- | --------------- | ------------------------------------- | --------------------------------------------------------------------- |
| `id`                    | `str`           | `"llama-3.1-sonar-small-128k-online"` | The id of the Perplexity model to use                                 |
| `name`                  | `str`           | `"Perplexity"`                        | The name of the model                                                 |
| `provider`              | `str`           | `"Perplexity"`                        | The provider of the model                                             |
| `api_key`               | `Optional[str]` | `None`                                | The API key for Perplexity (defaults to PERPLEXITY\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.perplexity.ai"`         | The base URL for the Perplexity API                                   |
| `retries`               | `int`           | `0`                                   | Number of retries to attempt before raising a ModelProviderError      |
| `delay_between_retries` | `int`           | `1`                                   | Delay between retries, in seconds                                     |
| `exponential_backoff`   | `bool`          | `False`                               | If True, the delay between retries is doubled each time               |

Perplexity extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Add a memory for the default user

**URL:** llms-txt#add-a-memory-for-the-default-user

memory.add_user_memory(
    memory=UserMemory(memory="The user's name is John Doe", topics=["name"]),
)
print("Memories:")
pprint(memory.get_user_memories())

---

## Reasoning Agents

**URL:** llms-txt#reasoning-agents

**Contents:**
- How It Works
  - The Reasoning Framework
  - How It Differs by Model Type
- Basic Example

Source: https://docs.agno.com/basics/reasoning/reasoning-agents

Transform any model into a reasoning system through structured chain-of-thought processing, perfect for complex problems that require multiple steps, tool use, and self-validation.

**The problem:** Regular models often rush to answers on complex problems, missing steps or making logical errors.

**The solution:** Enable `reasoning=True` and watch your model break down the problem, explore multiple approaches, validate results, and deliver thoroughly vetted solutions.

**The beauty?** It works with any model, from GPT-4o to Claude to local models via Ollama. You're not limited to specialized reasoning models.

Enable reasoning on any agent by setting `reasoning=True`:

Behind the scenes, Agno creates a **separate reasoning agent instance** that uses your same model but with specialized prompting that guides it through a rigorous 6-step reasoning framework:

### The Reasoning Framework

1. **Problem Analysis**

* Restate the task to ensure full comprehension
   * Identify required information and necessary tools

2. **Decompose and Strategize**

* Break down the problem into subtasks
   * Develop multiple distinct approaches

3. **Intent Clarification and Planning**

* Articulate the user's intent
   * Select the best strategy with clear justification
   * Create a detailed action plan

4. **Execute the Action Plan**

* For each step: document title, action, result, reasoning, next action, and confidence score
   * Call tools as needed to gather information
   * Self-correct if errors are detected

5. **Validation (Mandatory)**

* Cross-verify with alternative approaches
   * Use additional tools to confirm accuracy
   * Reset and revise if validation fails

6. **Final Answer**
   * Deliver the thoroughly validated solution
   * Explain how it addresses the original task

The reasoning agent works through these steps iteratively (up to 10 by default), building on previous results, calling tools, and self-correcting until it reaches a confident solution. Once complete, it hands the full reasoning back to your main agent for the final response.

### How It Differs by Model Type

**With regular models** (gpt-4o, Claude Sonnet, Gemini):

* Forces structured chain-of-thought through the 6-step framework
* Creates detailed reasoning steps with confidence scores
* **This is where reasoning agents shine**: transforming any model into a reasoning system

**With native reasoning models** (gpt-5-mini, DeepSeek-R1, o3-mini):

* Uses the model's built-in reasoning capabilities
* Adds a validation pass from your main agent
* Useful for critical tasks but often unnecessary overhead for simpler problems

Let's transform a regular GPT-4o model into a reasoning system:

```python reasoning_agent.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
Behind the scenes, Agno creates a **separate reasoning agent instance** that uses your same model but with specialized prompting that guides it through a rigorous 6-step reasoning framework:

### The Reasoning Framework

1. **Problem Analysis**

   * Restate the task to ensure full comprehension
   * Identify required information and necessary tools

2. **Decompose and Strategize**

   * Break down the problem into subtasks
   * Develop multiple distinct approaches

3. **Intent Clarification and Planning**

   * Articulate the user's intent
   * Select the best strategy with clear justification
   * Create a detailed action plan

4. **Execute the Action Plan**

   * For each step: document title, action, result, reasoning, next action, and confidence score
   * Call tools as needed to gather information
   * Self-correct if errors are detected

5. **Validation (Mandatory)**

   * Cross-verify with alternative approaches
   * Use additional tools to confirm accuracy
   * Reset and revise if validation fails

6. **Final Answer**
   * Deliver the thoroughly validated solution
   * Explain how it addresses the original task

The reasoning agent works through these steps iteratively (up to 10 by default), building on previous results, calling tools, and self-correcting until it reaches a confident solution. Once complete, it hands the full reasoning back to your main agent for the final response.

### How It Differs by Model Type

**With regular models** (gpt-4o, Claude Sonnet, Gemini):

* Forces structured chain-of-thought through the 6-step framework
* Creates detailed reasoning steps with confidence scores
* **This is where reasoning agents shine**: transforming any model into a reasoning system

**With native reasoning models** (gpt-5-mini, DeepSeek-R1, o3-mini):

* Uses the model's built-in reasoning capabilities
* Adds a validation pass from your main agent
* Useful for critical tasks but often unnecessary overhead for simpler problems

## Basic Example

Let's transform a regular GPT-4o model into a reasoning system:
```

---

## Use in workflow

**URL:** llms-txt#use-in-workflow

**Contents:**
- Developer Resources

workflow = Workflow(
    name="Enhanced Research Workflow",
    steps=[
        Step(name="research_hackernews", agent=hackernews_agent),
        Step(name="research_web", agent=web_agent),
        Step(name="comprehensive_report", executor=create_comprehensive_report),  # Accesses both previous steps
        Step(name="final_reasoning", agent=reasoning_agent),
    ],
)
python  theme={null}
  parallel_step_output = step_input.get_step_content("parallel_step_name")
  python  theme={null}
  {
      "individual_step_name_1": "output_from_individual_step_1",
      "individual_step_name_2": "output_from_individual_step_2",
  }
  ```
</Note>

## Developer Resources

* [Access Multiple Previous Steps Output](/basics/workflows/usage/access-multiple-previous-steps-output)

**Examples:**

Example 1 (unknown):
```unknown
**Available Methods**

* `step_input.get_step_content("step_name")` - Get content from specific step by name
* `step_input.get_all_previous_content()` - Get all previous step content combined
* `step_input.workflow_message` - Access the original workflow input message
* `step_input.previous_step_content` - Get content from immediate previous step

<Note>
  In case of `Parallel` step, when you do `step_input.get_step_content("parallel_step_name")`, it will return a dict with each key as `individual_step_name` for all the outputs from the steps defined in parallel.
  Example:
```

Example 2 (unknown):
```unknown
`parallel_step_output` will be a dict with each key as `individual_step_name` for all the outputs from the steps defined in parallel.
```

---

## Cursor Rules for Building Agents

**URL:** llms-txt#cursor-rules-for-building-agents

**Contents:**
- What is .cursorrules?
- Why Use It?
- How to Use .cursorrules

Source: https://docs.agno.com/how-to/cursor-rules

Use .cursorrules to improve AI coding assistant suggestions when building agents with Agno

A [`.cursorrules`](https://docs.cursor.com/context/rules-for-ai) file teaches AI coding assistants (like Cursor, Windsurf) how to build better agents with Agno.

## What is .cursorrules?

`.cursorrules` is a configuration file that provides your AI coding assistant with instructions on how to generate specific code.
Agno's recommended `.cursorrules` file contains:

* Agno-specific patterns and best practices
* Correct parameter names and syntax
* Common mistakes to avoid
* When to use Agent vs Team vs Workflow

Think of it as a reference guide that helps your AI assistant build agents correctly with Agno.

Without `.cursorrules`, AI assistants might suggest:

* Wrong parameter names (like `agents=` instead of `members=` for Teams)
* Outdated patterns or incorrect syntax
* Performance anti-patterns (creating agents in loops)
* Non-existent methods or features

With `.cursorrules`, your AI will:

* Suggest correct Agno patterns automatically
* Follow performance best practices
* Use the right approach for your use case
* Catch common mistakes before you make them

## How to Use .cursorrules

Copy the Agno `.cursorrules` file to your project root:

```bash  theme={null}

---

## Chat with your agent

**URL:** llms-txt#chat-with-your-agent

**Contents:**
- What Just Happened?
- Next Steps: Explore Advanced Features
- Troubleshooting

if __name__ == "__main__":
    agent.print_response("What is Agno Knowledge?", stream=True)
bash  theme={null}
python knowledge_agent.py
```

## What Just Happened?

When you ran the code, here's what occurred behind the scenes:

1. **Content Processing**: Your text was chunked into smaller pieces and converted to vector embeddings
2. **Intelligent Search**: The agent analyzed your question and searched for relevant information
3. **Contextual Response**: The agent combined the retrieved knowledge with your question to provide an accurate answer
4. **Source Attribution**: The response is based on your specific content, not generic training data

## Next Steps: Explore Advanced Features

<CardGroup cols={3}>
  <Card title="Content Types" icon="file-lines" href="/basics/knowledge/content-types">
    Learn about different ways to add content: files, URLs, databases, and more.
  </Card>

<Card title="Chunking Strategies" icon="scissors" href="/basics/knowledge/chunking/overview">
    Optimize how your content is broken down for better search results.
  </Card>

<Card title="Vector Databases" icon="database" href="/basics/vectordb/overview">
    Choose the right storage solution for your needs and scale.
  </Card>

<Card title="Search Types" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Explore different search strategies: vector, keyword, and hybrid search.
  </Card>
</CardGroup>

<AccordionGroup>
  <Accordion title="Agent isn't using knowledge in responses">
    Make sure you set `search_knowledge=True` when creating your agent and consider adding explicit instructions to search the knowledge base.
  </Accordion>

<Accordion title="Vector database connection errors">
    For local development, try LanceDB instead of PostgreSQL. For production, ensure your database connection string is correct.
  </Accordion>

<Accordion title="Content not being found in searches">
    Your content might need better chunking. Try different chunking strategies or smaller chunk sizes for more precise retrieval.
  </Accordion>
</AccordionGroup>

<Card title="Ready for Core Concepts?" icon="graduation-cap" href="/basics/knowledge/knowledge-bases">
  Dive deeper into understanding knowledge bases and how they power intelligent agents
</Card>

**Examples:**

Example 1 (unknown):
```unknown
Run it:
```

---

## Early Stop a Workflow

**URL:** llms-txt#early-stop-a-workflow

Source: https://docs.agno.com/basics/workflows/usage/early-stop-workflow

This example demonstrates **Workflows 2.0** early termination of a running workflow.

This example shows how to create workflows that can terminate
gracefully when quality conditions aren't met, preventing downstream processing of
invalid or unsafe data.

**When to use**: When you need safety mechanisms, quality gates, or validation checkpoints
that should prevent downstream processing if conditions aren't met. Ideal for data
validation pipelines, security checks, quality assurance workflows, or any process where
continuing with invalid inputs could cause problems.

```python early_stop_workflow_with_agents.py theme={null}
from agno.agent import Agent
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.workflow import Workflow
from agno.workflow.types import StepInput, StepOutput

---

## By default, it stores data in /tmp/lancedb

**URL:** llms-txt#by-default,-it-stores-data-in-/tmp/lancedb

vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

---

## Your AgentOS API is running on localhost:7777

**URL:** llms-txt#your-agentos-api-is-running-on-localhost:7777

**Contents:**
- Next Steps
- Conclusion

curl http://localhost:7777/v1/agents
bash  theme={null}
curl -X POST http://localhost:7777/v1/agents/social_media_agent/runs \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Analyze social media sentiment for: Tesla OR @elonmusk",
    "stream": false
  }'
json  theme={null}
{
  "run_id": "run_123",
  "content": "### Executive Dashboard\n- **Brand Health Score**: 7.2/10...",
  "metrics": {
    "tokens_used": 1250,
    "tools_called": ["x_tools", "exa_tools"],
    "analysis_time": "23.4s"
  }
}
```

Your social media intelligence system is now live with a production-ready API! Consider these as possible next steps to extend this system:

* **Specialized Agents**: Create focused agents for crisis detection, competitive analysis, or influencer identification
* **Alert Integration**: Connect webhooks to Slack, email, or your existing monitoring systems
* **Visual Analytics**: Build dashboards that consume the API for executive reporting
* **Multi-Brand Monitoring**: Scale to monitor multiple brands or competitors simultaneously

You've built a comprehensive social media intelligence system that:

* Combines direct social data with broader web intelligence
* Provides weighted sentiment analysis with strategic recommendations
* Serves insights via production-ready AgentOS API
* Scales from development through enterprise deployment

This demonstrates Agno's infrastructure-first approach, where your AI agents become immediately deployable services with proper monitoring, scaling, and integration capabilities built-in.

**Examples:**

Example 1 (unknown):
```unknown
**Test with Postman or curl:**
```

Example 2 (unknown):
```unknown
**Expected response structure:**
```

---

## Response: Your Route -> A -> B -> C

**URL:** llms-txt#response:-your-route-->-a-->-b-->-c

**Contents:**
- Developer Resources
  - Examples
  - External Resources

**Best Practice:** Add middleware in logical order:

1. **Security middleware first** (CORS, security headers)
2. **Authentication middleware** (JWT, session validation)
3. **Monitoring middleware** (logging, metrics)
4. **Business logic middleware** (rate limiting, custom logic)

## Developer Resources

<CardGroup cols={2}>
  <Card title="JWT with Headers" icon="shield" href="/agent-os/usage/middleware/jwt-middleware">
    JWT authentication using Authorization headers for API clients.
  </Card>

<Card title="JWT with Cookies" icon="cookie" href="/agent-os/usage/middleware/jwt-cookies">
    JWT authentication using HTTP-only cookies for web applications.
  </Card>

<Card title="Custom Middleware" icon="gear" href="/agent-os/usage/middleware/custom-middleware">
    Rate limiting and request logging middleware implementation.
  </Card>

<Card title="Custom FastAPI + JWT" icon="code" href="/agent-os/usage/middleware/custom-fastapi-jwt">
    Custom FastAPI app with JWT middleware and AgentOS integration.
  </Card>
</CardGroup>

### External Resources

<CardGroup cols={2}>
  <Card title="FastAPI Middleware" icon="book" href="https://fastapi.tiangolo.com/tutorial/middleware/">
    Official FastAPI middleware documentation and examples.
  </Card>

<Card title="Starlette Middleware" icon="book" href="https://www.starlette.io/middleware/">
    Starlette middleware reference and implementation guides.
  </Card>
</CardGroup>

---

## Create a team for collaborative image generation

**URL:** llms-txt#create-a-team-for-collaborative-image-generation

**Contents:**
- Usage

image_team = Team(
    name="Image Generation Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[prompt_engineer, image_generator],
    instructions=[
        "Generate high-quality images from user prompts.",
        "Prompt Engineer: First enhance and optimize the user's prompt.",
        "Image Creator: Generate images using the enhanced prompt with DALL-E.",
    ],
    markdown=True,
)

run_stream: Iterator[RunOutputEvent] = image_team.run(
    "Create an image of a yellow siamese cat",
    stream=True,
    stream_events=True,
)
for chunk in run_stream:
    pprint(dataclass_to_dict(chunk, exclude={"messages"}))
    print("---" * 20)
bash  theme={null}
    pip install agno rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/generate_image_with_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Run the evaluation calling the arun method.

**URL:** llms-txt#run-the-evaluation-calling-the-arun-method.

**Contents:**
- Accuracy with Teams

result: Optional[AccuracyResult] = asyncio.run(evaluation.arun(print_results=True))
assert result is not None and result.avg_score >= 8

python accuracy_with_team.py theme={null}
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.team.team import Team

**Examples:**

Example 1 (unknown):
```unknown
## Accuracy with Teams

Evaluate accuracy with a team:
```

---

## Custom Memory Instructions

**URL:** llms-txt#custom-memory-instructions

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/custom-memory-instructions

Create user memories with an Agent by providing a either text or a list of messages.

```python custom-memory-instructions.py theme={null}
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager
from agno.models.anthropic.claude import Claude
from agno.models.message import Message
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory_db = PostgresDb(db_url=db_url)

memory = MemoryManager(
    model=OpenAIChat(id="gpt-5-mini"),
    memory_capture_instructions="""\
                    Memories should only include details about the user's academic interests.
                    Only include which subjects they are interested in.
                    Ignore names, hobbies, and personal interests.
                    """,
    db=memory_db,
)

john_doe_id = "john_doe@example.com"

memory.create_user_memories(
    input="""\
My name is John Doe.

I enjoy hiking in the mountains on weekends,
reading science fiction novels before bed,
cooking new recipes from different cultures,
playing chess with friends.

I am interested to learn about the history of the universe and other astronomical topics.
""",
    user_id=john_doe_id,
)

memories = memory.get_user_memories(user_id=john_doe_id)
print("John Doe's memories:")
pprint(memories)

---

## Multi-DB Tracing with setup_tracing()

**URL:** llms-txt#multi-db-tracing-with-setup_tracing()

Source: https://docs.agno.com/agent-os/tracing/usage/tracing-with-multi-db-scenario

Learn how to trace agents with multiple databases using setup_tracing() in AgentOS

This example shows how to configure tracing when agents have separate databases using `setup_tracing()`. A dedicated tracing database ensures all traces are stored in one central location.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run AgentOS">
    <CodeGroup>

Your AgentOS will be available at `http://localhost:7777`. View traces in the AgentOS dashboard.
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Python

**URL:** llms-txt#python

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/local/python

**PythonTools** enable an Agent to write and run python code.

The following agent will write a python script that creates the fibonacci series, save it to a file, run it and return the result.

| Parameter      | Type   | Default | Description                                                                                            |
| -------------- | ------ | ------- | ------------------------------------------------------------------------------------------------------ |
| `base_dir`     | `Path` | `None`  | Specifies the base directory for operations. Default is None, indicating the current working directory |
| `safe_globals` | `dict` | `None`  | Dictionary of global variables that are considered safe to use during execution                        |
| `safe_locals`  | `dict` | `None`  | Dictionary of local variables that are considered safe to use during execution                         |

| Function                          | Description                                                                                                                                                                                                                                                            |
| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `save_to_file_and_run`            | This function saves Python code to a file called `file_name` and then runs it. If successful, returns the value of `variable_to_return` if provided otherwise returns a success message. If failed, returns an error message. Make sure the file\_name ends with `.py` |
| `run_python_file_return_variable` | This function runs code in a Python file. If successful, returns the value of `variable_to_return` if provided otherwise returns a success message. If failed, returns an error message.                                                                               |
| `read_file`                       | Reads the contents of the file `file_name` and returns the contents if successful.                                                                                                                                                                                     |
| `list_files`                      | Returns a list of files in the base directory                                                                                                                                                                                                                          |
| `run_python_code`                 | This function runs Python code in the current environment. If successful, returns the value of `variable_to_return` if provided otherwise returns a success message. If failed, returns an error message.                                                              |
| `pip_install_package`             | This function installs a package using pip in the current environment. If successful, returns a success message. If failed, returns an error message.                                                                                                                  |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/python.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/python_tools.py)

---

## Intent Routing with Workflow History

**URL:** llms-txt#intent-routing-with-workflow-history

Source: https://docs.agno.com/basics/chat-history/workflow/usage/intent-routing-with-history

This example demonstrates how to use workflow history in intent routing.

This example demonstrates:

1. A simple Router that routes to different specialist agents
2. All agents share the same conversation history for context continuity
3. The power of shared context across different agents

The router uses basic intent detection, but the real value is in the shared history.

```python 06_intent_routing_with_history.py theme={null}
from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

---

## Pass your app to AgentOS

**URL:** llms-txt#pass-your-app-to-agentos

agent_os = AgentOS(
    agents=[Agent(id="basic-agent", model=OpenAIChat(id="gpt-5-mini"))],
    base_app=app  # Your custom FastAPI app
)

---

## Send Message

**URL:** llms-txt#send-message

Source: https://docs.agno.com/reference-api/schema/a2a/send-message

post /a2a/message/send
Send a message to an Agno Agent, Team, or Workflow. The Agent, Team or Workflow is identified via the 'agentId' field in params.message or X-Agent-ID header. Optional: Pass user ID via X-User-ID header (recommended) or 'userId' in params.message.metadata.

---

## Create a boto3 session with your preferred authentication

**URL:** llms-txt#create-a-boto3-session-with-your-preferred-authentication

**Contents:**
- Example
- Parameters

session = Session(
    aws_access_key_id="your-access-key",
    aws_secret_access_key="your-secret-key",
    region_name="us-east-1"
)

agent = Agent(
    model=Claude(
        id="us.anthropic.claude-sonnet-4-20250514-v1:0",
        session=session
    )
)
python agent.py theme={null}
  from agno.agent import Agent
  from agno.models.aws import Claude

agent = Agent(
      model=Claude(id="us.anthropic.claude-sonnet-4-20250514-v1:0"),
      markdown=True
  )

# Print the response on the terminal
  agent.print_response("Share a 2 sentence horror story.")
  ```
</CodeGroup>

<Note> View more examples [here](/integrations/models/cloud/aws-claude/usage/basic-stream). </Note>

| Parameter        | Type                       | Default                                     | Description                                                                             |
| ---------------- | -------------------------- | ------------------------------------------- | --------------------------------------------------------------------------------------- |
| `id`             | `str`                      | `"anthropic.claude-sonnet-4-20250514-v1:0"` | The specific AWS Bedrock Claude model ID to use                                         |
| `name`           | `str`                      | `"AwsBedrockAnthropicClaude"`               | The name identifier for the AWS Bedrock Claude model                                    |
| `provider`       | `str`                      | `"AwsBedrock"`                              | The provider of the model                                                               |
| `api_key`        | `Optional[str]`            | `None`                                      | The AWS Bedrock API key for authentication (defaults to AWS\_BEDROCK\_API\_KEY env var) |
| `aws_access_key` | `Optional[str]`            | `None`                                      | The AWS access key to use (defaults to AWS\_ACCESS\_KEY env var)                        |
| `aws_secret_key` | `Optional[str]`            | `None`                                      | The AWS secret key to use (defaults to AWS\_SECRET\_KEY env var)                        |
| `aws_region`     | `Optional[str]`            | `None`                                      | The AWS region to use (defaults to AWS\_REGION env var)                                 |
| `session`        | `Optional[Session]`        | `None`                                      | A boto3 Session object to use for authentication                                        |
| `max_tokens`     | `int`                      | `4096`                                      | Maximum number of tokens to generate in the chat completion                             |
| `temperature`    | `Optional[float]`          | `None`                                      | Controls randomness in the model's output                                               |
| `top_p`          | `Optional[float]`          | `None`                                      | Controls diversity via nucleus sampling                                                 |
| `top_k`          | `Optional[int]`            | `None`                                      | Controls diversity via top-k sampling                                                   |
| `stop_sequences` | `Optional[List[str]]`      | `None`                                      | A list of strings that the model should stop generating text at                         |
| `request_params` | `Optional[Dict[str, Any]]` | `None`                                      | Additional parameters to include in the request                                         |
| `client_params`  | `Optional[Dict[str, Any]]` | `None`                                      | Additional parameters for client configuration                                          |

`Claude` (AWS) extends the [Anthropic Claude](/integrations/models/native/anthropic/overview) model with AWS Bedrock integration and has access to most of the same parameters.

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  The authentication methods are checked in this order: Session â†’ API Key â†’ Access Key/Secret Key. The first available method will be used.
</Note>

## Example

Use `Claude` with your `Agent`:

<CodeGroup>
```

---

## Together Embedder

**URL:** llms-txt#together-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/together/usage/together-embedder

```python  theme={null}
from agno.knowledge.embedder.together import TogetherEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = TogetherEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Selector for Image Video Generation Pipelines

**URL:** llms-txt#selector-for-image-video-generation-pipelines

**Contents:**
- Key Features:
- Key Features:

Source: https://docs.agno.com/basics/workflows/usage/selector-for-image-video-generation-pipelines

This example demonstrates **Workflows 2.0** router pattern for dynamically selecting between image and video generation pipelines.

This example demonstrates **Workflows 2.0** router pattern for dynamically selecting between image and video generation pipelines. It uses `Steps` to encapsulate each media type's workflow and a `Router` to intelligently choose the pipeline based on input analysis.

* **Dynamic Routing**: Selects pipelines (`Steps`) based on input keywords (e.g., "image" or "video").
* **Modular Pipelines**: Encapsulates image/video workflows as reusable `Steps` objects.
* **Structured Inputs**: Uses Pydantic models for type-safe configuration (e.g., resolution, style).

* **Nested Logic**: Embeds `Condition` and `Parallel` within a `Steps` sequence.
* **Topic-Specialized Research**: Uses `Condition` to trigger parallel tech/news research for tech topics.
* **Modular Design**: Encapsulates the entire workflow as a reusable `Steps` object.

```python selector_for_image_video_generation_pipelines.py theme={null}
from typing import List, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models.gemini import GeminiTools
from agno.tools.openai import OpenAITools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel

---

## Cross-Reference Validator Agent - Validates across sources

**URL:** llms-txt#cross-reference-validator-agent---validates-across-sources

cross_reference_validator = Agent(
    name="Cross-Reference Validator",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Validate information consistency across different search results",
    instructions=[
        "Compare and validate information from both searchers.",
        "Identify consistencies and discrepancies in the results.",
        "Highlight areas where information aligns or conflicts.",
        "Assess the reliability of different information sources.",
    ],
    markdown=True,
)

---

## Provide the agent with the audio file and get result as text

**URL:** llms-txt#provide-the-agent-with-the-audio-file-and-get-result-as-text

**Contents:**
- Usage

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini-audio-preview", modalities=["text"]),
    markdown=True,
)
agent.print_response(
    "What is in this audio?", audio=[Audio(content=wav_data, format="wav")]
)
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai requests agno
    bash Mac theme={null}
      python cookbook/models/openai/chat/audio_input_agent.py
      bash Windows theme={null}
      python cookbook/models/openai/chat/audio_input_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Generate unique IDs

**URL:** llms-txt#generate-unique-ids

user_id = str(uuid4())
id = str(uuid4())

---

## Test basic generation

**URL:** llms-txt#test-basic-generation

agent.print_response(
    "Generate an image of a futuristic city with flying cars",
    markdown=True,
)

---

## Find documents within a date range

**URL:** llms-txt#find-documents-within-a-date-range

**Contents:**
  - Logical Operators

LT("year", 2025)
python  theme={null}
from agno.filters import AND, EQ

**Examples:**

Example 1 (unknown):
```unknown
### Logical Operators

Combine multiple conditions using logical operators:

#### AND

All conditions must be true.
```

---

## Agent with Instructions

**URL:** llms-txt#agent-with-instructions

Source: https://docs.agno.com/basics/agents/usage/instructions

Learn how to give instructions to your agent

In this example, we will see how giving elaborate instructions to an agent can help it perform better.

We will create a news reporter agent that can report on news stories happening in New York City:

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Tool Confirmation Required

**URL:** llms-txt#tool-confirmation-required

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required

This example demonstrates how to implement human-in-the-loop functionality by requiring user confirmation before executing sensitive tool operations.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## DynamoDB

**URL:** llms-txt#dynamodb

Source: https://docs.agno.com/reference/storage/dynamodb

DynamoDB is a class that implements the Db interface using Amazon DynamoDB as the backend storage system. It provides scalable, managed storage for agent sessions with support for indexing and efficient querying.

<Snippet file="db-dynamodb-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## FireCrawl Reader

**URL:** llms-txt#firecrawl-reader

Source: https://docs.agno.com/reference/knowledge/reader/firecrawl

FireCrawlReader is a reader class that allows you to read data from websites using Firecrawl.

<Snippet file="firecrawl-reader-reference.mdx" />

---

## Gemini with Reasoning Tools

**URL:** llms-txt#gemini-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/gemini-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Google API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Google API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Tokens-per-minute rate limiting

**URL:** llms-txt#tokens-per-minute-rate-limiting

Source: https://docs.agno.com/faq/tpm-issues

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=6c4a68b6662597c61b761f782dbe5f65" alt="Chat with pdf" data-og-width="698" width="698" data-og-height="179" height="179" data-path="images/tpm_issues.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=9d57cc77b1620f3ebc6ed5ac2348cd53 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=62e01716372bec142658c779175b6d1e 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=0840a597cda52c0c883f722e8f0cf13c 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=a3f1e2f454802a9143f82e893eb45af0 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=f5556252c255a94e36218a003e929a40 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=ac5204f428d2ee70c268d6ec9e68b44b 2500w" />

If you face any problems with proprietary models (like OpenAI models) where you are rate limited, we provide the option to set `exponential_backoff=True` and to change `delay_between_retries` to a value in seconds (defaults to 1 second).

See our [models documentation](/basics/models) for specific information about rate limiting.

In the case of OpenAI, they have tier based rate limits. See the [docs](https://platform.openai.com/docs/guides/rate-limits/usage-tiers) for more information.

---

## Early Stopping

**URL:** llms-txt#early-stopping

**Contents:**
- Example

Source: https://docs.agno.com/basics/workflows/early-stop

How to early stop workflows

Workflows support early termination when specific conditions are met, preventing unnecessary processing and implementing safety gates. Any step can trigger early termination by returning `StepOutput(stop=True)`, immediately halting the entire workflow execution.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=8d2d411a2853e475b18e4a195a9d65df" alt="Workflows early stop diagram" data-og-width="7281" width="7281" data-og-height="1179" height="1179" data-path="images/workflows-early-stop-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c0dcd798df8113cf9c146b8d6946f2e4 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=230326e06f4d0d607e4fff253910b7d0 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=7ea8b5e50c4303af888b0bab7fcdb6f9 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=28702a9fcd250dd3f678f84b40b4f207 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=9cddb518531d6dc748a4d7a9da2308a4 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=1c1b977a426432c9e08df94a524a4d2f 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=6c2609a237ba9a60fde24fb6ef3c25dc" alt="Workflows early stop diagram" data-og-width="7281" width="7281" data-og-height="1179" height="1179" data-path="images/workflows-early-stop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=8ab6aca3447a781d230858aefc525613 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5ad070c303d4203799af900d5bf15dae 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c29866bad79a0fa0215962c59605e93f 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5b498d3d0faacc133474650fb236e29a 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=8b8e0f5a7070635380fdf23217e5fda3 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=97cf31c8418685c1b28185e594c11e44 2500w" />

```python  theme={null}
from agno.workflow import Step, Workflow, StepInput, StepOutput

def security_gate(step_input: StepInput) -> StepOutput:
    """Security gate that stops deployment if vulnerabilities found"""
    security_result = step_input.previous_step_content or ""
    
    if "VULNERABLE" in security_result.upper():
        return StepOutput(
            content="ðŸš¨ SECURITY ALERT: Critical vulnerabilities detected. Deployment blocked.",
            stop=True  # Stop the entire workflow
        )
    else:
        return StepOutput(
            content="âœ… Security check passed. Proceeding with deployment...",
            stop=False
        )

---

## Trafilatura

**URL:** llms-txt#trafilatura

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/trafilatura

TrafilaturaTools provides advanced web scraping and text extraction capabilities with support for crawling and content analysis.

The following agent can extract and analyze web content:

| Parameter                 | Type            | Default | Description                                                  |
| ------------------------- | --------------- | ------- | ------------------------------------------------------------ |
| `output_format`           | `str`           | `"txt"` | Default output format (txt, json, xml, markdown, csv, html). |
| `include_comments`        | `bool`          | `False` | Whether to extract comments along with main text.            |
| `include_tables`          | `bool`          | `False` | Whether to include table content.                            |
| `include_images`          | `bool`          | `False` | Whether to include image information (experimental).         |
| `include_formatting`      | `bool`          | `False` | Whether to preserve text formatting.                         |
| `include_links`           | `bool`          | `False` | Whether to preserve links (experimental).                    |
| `with_metadata`           | `bool`          | `False` | Whether to include metadata in extractions.                  |
| `favor_precision`         | `bool`          | `False` | Whether to prefer precision over recall.                     |
| `favor_recall`            | `bool`          | `False` | Whether to prefer recall over precision.                     |
| `target_language`         | `Optional[str]` | `None`  | Target language filter (ISO 639-1 format).                   |
| `deduplicate`             | `bool`          | `True`  | Whether to remove duplicate segments.                        |
| `max_crawl_urls`          | `int`           | `100`   | Maximum number of URLs to crawl per website.                 |
| `max_known_urls`          | `int`           | `1000`  | Maximum number of known URLs during crawling.                |
| `enable_extract_text`     | `bool`          | `True`  | Whether to extract text content.                             |
| `enable_extract_metadata` | `bool`          | `True`  | Whether to extract metadata information.                     |
| `enable_html_to_text`     | `bool`          | `True`  | Whether to convert HTML content to clean text.               |
| `enable_batch_extract`    | `bool`          | `True`  | Whether to extract content from multiple URLs in batch.      |

| Function           | Description                                              |
| ------------------ | -------------------------------------------------------- |
| `extract_text`     | Extract clean text content from a URL or HTML.           |
| `extract_metadata` | Extract metadata information from web pages.             |
| `html_to_text`     | Convert HTML content to clean text.                      |
| `crawl_website`    | Crawl a website and extract content from multiple pages. |
| `batch_extract`    | Extract content from multiple URLs in batch.             |
| `get_page_info`    | Get comprehensive page information including metadata.   |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/trafilatura.py)
* [Trafilatura Documentation](https://trafilatura.readthedocs.io/)
* [Web Scraping Best Practices](https://trafilatura.readthedocs.io/en/latest/corefunctions.html)

---

## Initialize Upstash DB

**URL:** llms-txt#initialize-upstash-db

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with Upstash Vector DB",
    vector_db=vector_db,
)

---

## Agno Assist Agent

**URL:** llms-txt#agno-assist-agent

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/agents/agno-assist

Build an AI assistant that answers questions using your own documentation. This example uses retrieval-augmented generation (RAG) to ensure accurate, documentation-grounded responses instead of hallucinations.

By building this agent, you'll understand:

* How to create a vector database to store and search documentation
* Why hybrid search (combining semantic and keyword matching) improves retrieval accuracy
* How to maintain conversation history across multiple interactions
* How to ensure agents search knowledge bases instead of relying solely on training data

Build documentation assistants, customer support agents, help desk systems, or educational tutors that need to reference specific knowledge bases.

The agent uses retrieval-augmented generation (RAG) to answer questions:

1. **Search**: Queries the vector database using hybrid search (semantic + keyword matching)
2. **Retrieve**: Gets relevant documentation chunks from LanceDB
3. **Context**: Combines retrieved docs with conversation history from SQLite
4. **Generate**: LLM creates an answer grounded in the documentation

This ensures responses are based on your actual documentation, not just the model's training data.

```python agno_assist.py theme={null}
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.vectordb.lancedb import LanceDb, SearchType

---

## Get OS Configuration

**URL:** llms-txt#get-os-configuration

Source: https://docs.agno.com/reference-api/schema/core/get-os-configuration

get /config
Retrieve the complete configuration of the AgentOS instance, including:

- Available models and databases
- Registered agents, teams, and workflows
- Chat, session, memory, knowledge, and evaluation configurations
- Available interfaces and their routes

---

## Lumalabs

**URL:** llms-txt#lumalabs

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/lumalabs

**LumaLabTools** enables an Agent to generate media using the [Lumalabs platform](https://lumalabs.ai/dream-machine).

The following example requires the `lumaai` library. To install the Lumalabs client, run the following command:

The following agent will use Lumalabs to generate any video requested by the user.

| Parameter               | Type   | Default | Description                                          |
| ----------------------- | ------ | ------- | ---------------------------------------------------- |
| `api_key`               | `str`  | `None`  | If you want to manually supply the Lumalabs API key. |
| `enable_generate_video` | `bool` | `True`  | Enable the generate\_video functionality.            |
| `enable_image_to_video` | `bool` | `True`  | Enable the image\_to\_video functionality.           |
| `all`                   | `bool` | `False` | Enable all functionality.                            |

| Function         | Description                                                           |
| ---------------- | --------------------------------------------------------------------- |
| `generate_video` | Generate a video from a prompt.                                       |
| `image_to_video` | Generate a video from a prompt, a starting image and an ending image. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/lumalabs.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/lumalabs_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
The following example requires the `lumaai` library. To install the Lumalabs client, run the following command:
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Lumalabs to generate any video requested by the user.
```

---

## Agent with Memory

**URL:** llms-txt#agent-with-memory

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/perplexity/usage/memory

```python cookbook/models/perplexity/memory.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.perplexity import Perplexity
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
agent = Agent(
    model=Perplexity(id="sonar-pro"),
    # Store the memories and summary in a database
    db=PostgresDb(db_url=db_url),
    enable_user_memories=True,
    enable_session_summaries=True,
)

---

## Example 2: Custom aspect ratio generator

**URL:** llms-txt#example-2:-custom-aspect-ratio-generator

portrait_agent = Agent(
    tools=[
        NanoBananaTools(
            aspect_ratio="2:3"  # Portrait orientation
        )
    ],
    name="Portrait NanoBanana Generator",
)

---

## Option 1: Filters on the Agent

**URL:** llms-txt#option-1:-filters-on-the-agent

---

## Every reader implements these core methods

**URL:** llms-txt#every-reader-implements-these-core-methods

**Contents:**
  - The Reading Process
  - Content Types and Specialization
- Reader Configuration
  - Chunking Control
  - Content Processing Options
  - Encoding Control
  - Metadata and Naming
- The Document Output
- Chunking Integration
  - Automatic Chunking

class Reader:
    def read(self, obj, name=None) -> List[Document]:
        """Synchronously read and process content"""
        pass

async def async_read(self, obj, name=None) -> List[Document]:
        """Asynchronously read and process content"""
        pass
python  theme={null}
@classmethod
def get_supported_content_types(cls) -> List[ContentType]:
    """Returns the content types this reader can handle"""
    return [ContentType.PDF]  # Example for PDFReader
python  theme={null}
reader = PDFReader(
    chunk=True,                    # Enable/disable chunking
    chunk_size=1000,              # Size of each chunk
    chunking_strategy=MyStrategy() # Custom chunking logic
)
python  theme={null}
reader = PDFReader(
    split_on_pages=True,          # Create separate documents per page
    password="secret123",         # Handle encrypted PDFs
    read_images=True             # Extract text from images via OCR
)
python  theme={null}
reader = TextReader(
    encoding="utf-8"              # Override default encoding
)

reader = CSVReader(
    encoding="latin-1"            # Handle files with specific encodings
)

reader = MarkdownReader(
    encoding="cp1252"             # Windows-specific encoding
)
python  theme={null}
documents = reader.read(
    file_path,
    name="custom_document_name",  # Override default naming
    password="file_password"      # Runtime password override
)
python  theme={null}
Document(
    content="The extracted text content...",
    id="unique_document_identifier",
    name="document_name",
    meta_data={
        "page": 1,                # Page number for PDFs
        "url": "https://...",     # Source URL for web content
        "author": "...",          # Document metadata
    },
    size=len(content)             # Content size in characters
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### The Reading Process

When a reader processes content, it follows these steps:

1. **Content Ingestion**: The reader receives raw content (file, URL, text, etc.)
2. **Parsing**: Extract text and metadata using format-specific logic
3. **Document Creation**: Convert parsed content into `Document` objects
4. **Chunking**: Apply chunking strategies to break content into smaller pieces
5. **Return**: Provide a list of processed documents ready for embedding

### Content Types and Specialization

Each reader specializes in handling specific content types:
```

Example 2 (unknown):
```unknown
This specialization allows each reader to:

* Use format-specific parsing libraries
* Extract relevant metadata
* Handle format-specific challenges (encryption, encoding, etc.)
* Optimize processing for that content type

## Reader Configuration

Readers are highly configurable to meet different processing needs:

### Chunking Control
```

Example 3 (unknown):
```unknown
### Content Processing Options
```

Example 4 (unknown):
```unknown
### Encoding Control

For text-based readers, the file encoding can be overridden:
```

---

## List All Agents

**URL:** llms-txt#list-all-agents

Source: https://docs.agno.com/reference-api/schema/agents/list-all-agents

get /agents
Retrieve a comprehensive list of all agents configured in this OS instance.

**Returns:**
- Agent metadata (ID, name, description)
- Model configuration and capabilities
- Available tools and their configurations
- Session, knowledge, memory, and reasoning settings
- Only meaningful (non-default) configurations are included

---

## Supabase

**URL:** llms-txt#supabase

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/supabase/overview

Learn to use Supabase as a database provider for your Agents

Agno supports using [Supabase](https://supabase.com/) with the `PostgresDb` class.

You can get started with Supabase following their [Get Started guide](https://supabase.com/docs/guides/getting-started).

You can read more about the [`PostgresDb` class](/integrations/database/postgres/overview) in its section.

```python supabase_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from os import getenv

---

## Setup the database for the Agent Session to be stored

**URL:** llms-txt#setup-the-database-for-the-agent-session-to-be-stored

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini"),
    db=db,
    tools=[{"type": "file_search"}, {"type": "web_search_preview"}],
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file and search the web for more information.",
    files=[File(url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf")],
)

---

## Fixed Size Chunking

**URL:** llms-txt#fixed-size-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/fixed-size

Fixed size chunking is a method of splitting documents into smaller chunks of a specified size, with optional overlap between chunks.
This is useful when you want to process large documents in smaller, manageable pieces.

<Snippet file="chunking-fixed-size.mdx" />

---

## Agents Use Cases

**URL:** llms-txt#agents-use-cases

**Contents:**
- Getting Started
- Featured Examples

Source: https://docs.agno.com/examples/use-cases/agents/overview

Explore Agno's Agent use cases showcasing what Agents have to offer.

Welcome to Agno's Agent use cases! Here you'll discover practical examples of single-agent applications that solve real-world problems. You can either:

* Run the examples individually
* Clone the entire [Agno cookbook](https://github.com/agno-agi/agno/tree/main/cookbook)

Have an interesting example to share? Please consider [contributing](https://github.com/agno-agi/agno-docs) to our growing collection.

If you're just getting started, follow the [Getting Started](/examples/getting-started) guide for a step-by-step tutorial. The examples build on each other, introducing new concepts and capabilities progressively.

Explore these popular agent examples to see what's possible with Agno:

<CardGroup cols={3}>
  <Card title="Agno Assist" icon="book" iconType="duotone" href="/examples/use-cases/agents/agno-assist">
    Build a documentation assistant using RAG to answer questions from your knowledge base.
  </Card>

<Card title="Competitor Analysis Agent" icon="chart-line" iconType="duotone" href="/examples/use-cases/agents/competitor-analysis-agent">
    Perform competitive intelligence with web search, content scraping, and strategic analysis.
  </Card>

<Card title="Research Agent" icon="magnifying-glass" iconType="duotone" href="/examples/use-cases/agents/research-agent">
    Create professional research articles with web search, fact-checking, and journalism.
  </Card>

<Card title="Web Extraction Agent" icon="globe" iconType="duotone" href="/examples/use-cases/agents/web-extraction-agent">
    Transform unstructured web content into organized, structured data.
  </Card>

<Card title="YouTube Agent" icon="video" iconType="duotone" href="/examples/use-cases/agents/youtube-agent">
    Analyze YouTube videos and create structured summaries with accurate timestamps.
  </Card>
</CardGroup>

---

## MySQL

**URL:** llms-txt#mysql

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/mysql/overview

Learn to use MySQL as a database for your Agents

Agno supports using [MySQL](https://www.mysql.com/) as a database with the `MySQLDb` class.

```python mysql_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.mysql import MySQLDb

---

## Example 1: Send a message to a Slack channel

**URL:** llms-txt#example-1:-send-a-message-to-a-slack-channel

agent.print_response("Send a message 'Hello from Agno!' to the channel #general", markdown=True)

---

## Get Content Status

**URL:** llms-txt#get-content-status

Source: https://docs.agno.com/reference-api/schema/knowledge/get-content-status

get /knowledge/content/{content_id}/status
Retrieve the current processing status of a content item. Useful for monitoring asynchronous content processing progress and identifying any processing errors.

---

## structured_output_response: RunOutput = structured_output_agent.run("New York")

**URL:** llms-txt#structured_output_response:-runoutput-=-structured_output_agent.run("new-york")

---

## Step-level: override for specific steps

**URL:** llms-txt#step-level:-override-for-specific-steps

**Contents:**
- Developer Resources

Step("Analysis", agent=analysis_agent, 
     add_workflow_history=True,
     num_history_runs=3  # Only last 3 runs for this step
)
```

## Developer Resources

<CardGroup cols={2}>
  <Card title="Single Step Continuous Execution" icon="rotate" iconType="duotone" href="/basics/chat-history/workflow/usage/single-step-continuous-execution-workflow">
    Single step workflow with continuous execution and history awareness.
  </Card>

<Card title="Workflow History for Steps" icon="list-check" iconType="duotone" href="/basics/chat-history/workflow/usage/workflow-with-history-enabled-for-steps">
    Add workflow history to all steps in the workflow.
  </Card>

<Card title="Enable History for Specific Step" icon="toggle-on" iconType="duotone" href="/basics/chat-history/workflow/usage/enable-history-for-step">
    Enable workflow history for a specific step only.
  </Card>

<Card title="Get History in Function" icon="code" iconType="duotone" href="/basics/chat-history/workflow/usage/get-history-in-function">
    Access workflow history in custom functions for analysis.
  </Card>
</CardGroup>

---

## Use default memory manager

**URL:** llms-txt#use-default-memory-manager

memory = MemoryManager(model=Claude(id="claude-3-5-sonnet-latest"), db=memory_db)
jane_doe_id = "jane_doe@example.com"

---

## dependencies={"user_profile": get_user_profile},

**URL:** llms-txt#dependencies={"user_profile":-get_user_profile},

---

## Example usage with different types of recipe queries

**URL:** llms-txt#example-usage-with-different-types-of-recipe-queries

recipe_agent.print_response(
    "I have chicken breast, broccoli, garlic, and rice. Need a healthy dinner recipe that takes less than 45 minutes.",
    stream=True,
)

---

## Session State in Instructions

**URL:** llms-txt#session-state-in-instructions

Source: https://docs.agno.com/basics/state/team/usage/session-state-in-instructions

This example demonstrates how to use session state variables directly in team instructions using template syntax. The session state values are automatically injected into the instructions, making them available to the team during execution.

<Steps>
  <Step title="Create a Python file">
    Create a file `session_state_in_instructions.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the team">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Delete Multiple Memories

**URL:** llms-txt#delete-multiple-memories

Source: https://docs.agno.com/reference-api/schema/memory/delete-multiple-memories

delete /memories
Delete multiple user memories by their IDs in a single operation. This action cannot be undone and all specified memories will be permanently removed.

---

## RAG with LanceDB and SQLite Storage

**URL:** llms-txt#rag-with-lancedb-and-sqlite-storage

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/agents/usage/rag-with-lance-db-and-sqlite

This example demonstrates how to implement RAG using LanceDB vector database with Ollama embeddings and SQLite for agent data storage, providing a complete local setup for document retrieval.

```python rag_with_lance_db_and_sqlite.py theme={null}
from agno.agent import Agent
from agno.db.sqlite.sqlite import SqliteDb
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.ollama import Ollama
from agno.vectordb.lancedb import LanceDb

---

## Create the AgentOS

**URL:** llms-txt#create-the-agentos

agent_os = AgentOS(agents=[agno_agent])

---

## === BASIC AGENTS ===

**URL:** llms-txt#===-basic-agents-===

researcher = Agent(
    name="Researcher",
    instructions="Research the given topic and provide detailed findings.",
    tools=[DuckDuckGoTools()],
)

summarizer = Agent(
    name="Summarizer",
    instructions="Create a clear summary of the research findings.",
)

fact_checker = Agent(
    name="Fact Checker",
    instructions="Verify facts and check for accuracy in the research.",
    tools=[DuckDuckGoTools()],
)

writer = Agent(
    name="Writer",
    instructions="Write a comprehensive article based on all available research and verification.",
)

---

## Initialize Knowledge

**URL:** llms-txt#initialize-knowledge

knowledge = Knowledge(
    vector_db=vector_db,
    max_results=5,
    contents_db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        knowledge_table="knowledge_contents",
    ),
)

knowledge.add_content(
    path=downloaded_csv_paths[0],
    metadata={
        "data_type": "sales",
        "quarter": "Q1",
        "year": 2024,
        "region": "north_america",
        "currency": "USD",
    },
)

knowledge.add_content(
    path=downloaded_csv_paths[1],
    metadata={
        "data_type": "sales",
        "year": 2024,
        "region": "europe",
        "currency": "EUR",
    },
)

knowledge.add_content(
    path=downloaded_csv_paths[2],
    metadata={
        "data_type": "survey",
        "survey_type": "customer_satisfaction",
        "year": 2024,
        "target_demographic": "mixed",
    },
)

knowledge.add_content(
    path=downloaded_csv_paths[3],
    metadata={
        "data_type": "financial",
        "sector": "technology",
        "year": 2024,
        "report_type": "quarterly_earnings",
    },
)

---

## Agent Run Cancellation

**URL:** llms-txt#agent-run-cancellation

**Contents:**
- Example
- API Endpoint

Source: https://docs.agno.com/execution-control/run-cancellation/agent-cancel-run

Learn how to cancel a running agent execution by starting an agent run in a separate thread and cancelling it from another thread.

This example demonstrates how to cancel a running agent execution by starting an agent run in a separate thread and cancelling it from another thread. It shows proper handling of cancelled responses and thread management.

Agent runs can be cancelled via the AgentOS API:

**Reference:** [Cancel Agent Run API](/reference-api/schema/agents/cancel-agent-run)

**Examples:**

Example 1 (unknown):
```unknown
## API Endpoint

Agent runs can be cancelled via the AgentOS API:
```

Example 2 (unknown):
```unknown
**Example:**
```

---

## Create an agent with the knowledge

**URL:** llms-txt#create-an-agent-with-the-knowledge

**Contents:**
- Usage
- Params

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

async def main():
    # Add YouTube video content
    await knowledge.add_content_async(
        metadata={"source": "youtube", "type": "educational"},
        urls=[
            "https://www.youtube.com/watch?v=dQw4w9WgXcQ",  # Replace with actual educational video
            "https://www.youtube.com/watch?v=example123"   # Replace with actual video URL
        ],
        reader=YouTubeReader(),
    )

# Query the knowledge base
    await agent.aprint_response(
        "What are the main topics discussed in the videos?",
        markdown=True
    )

if __name__ == "__main__":
    asyncio.run(main())
bash  theme={null}
    pip install -U youtube-transcript-api pytube sqlalchemy psycopg pgvector agno openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python examples/basics/knowledge/readers/youtube_reader.py
      bash Windows theme={null}
      python examples/basics/knowledge/readers/youtube_reader.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="youtube-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Please download the image using

**URL:** llms-txt#please-download-the-image-using

---

## Save the response audio to a file

**URL:** llms-txt#save-the-response-audio-to-a-file

**Contents:**
- Usage

if run_output.response_audio:
    write_audio_to_file(
        audio=run_output.response_audio.content,
        filename="tmp/scary_story_sequal.wav",
    )
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/openai/chat/audio_output_agent.py
      bash Windows theme={null}
      python cookbook/models/openai/chat/audio_output_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MongoDb

**URL:** llms-txt#mongodb

Source: https://docs.agno.com/reference/vector-db/mongodb

<Snippet file="vector-db-mongodb-reference.mdx" />

---

## 4. Get the chat_id by going to the URL:

**URL:** llms-txt#4.-get-the-chat_id-by-going-to-the-url:

---

## Initialize Agent with Cartesia tools

**URL:** llms-txt#initialize-agent-with-cartesia-tools

agent = Agent(
    name="Cartesia TTS Agent",
    description="An agent that uses Cartesia for text-to-speech.",
    tools=[CartesiaTools()],
)

response = agent.run(
    """Generate a simple greeting using Text-to-Speech:

Say "Welcome to Cartesia, the advanced  speech synthesis platform. This speech is generated by an agent."
    """
)

---

## DuckDb

**URL:** llms-txt#duckdb

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/database/duckdb

The DuckDbTools toolkit enables an Agent to run SQL and analyze data using DuckDb.

**DuckDbTools** enable an Agent to run SQL and analyze data using DuckDb.

The following example requires DuckDB library. To install DuckDB, run the following command:

For more installation options, please refer to [DuckDB documentation](https://duckdb.org/docs/installation).

The following agent will analyze the movies file using SQL and return the result.

| Parameter       | Type                 | Default | Description                                                   |
| --------------- | -------------------- | ------- | ------------------------------------------------------------- |
| `db_path`       | `str`                | `None`  | Specifies the path to the database file.                      |
| `connection`    | `DuckDBPyConnection` | `None`  | Provides an existing DuckDB connection object.                |
| `init_commands` | `List`               | `None`  | A list of initial SQL commands to run on database connection. |
| `read_only`     | `bool`               | `False` | Configures the database connection to be read-only.           |
| `config`        | `dict`               | `None`  | Configuration options for the database connection.            |

| Function                   | Description                                                                                                                                                                                                                                    |
| -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `show_tables`              | Function to show tables in the database                                                                                                                                                                                                        |
| `describe_table`           | Function to describe a table                                                                                                                                                                                                                   |
| `inspect_query`            | Function to inspect a query and return the query plan. Always inspect your query before running them.                                                                                                                                          |
| `run_query`                | Function that runs a query and returns the result.                                                                                                                                                                                             |
| `summarize_table`          | Function to compute a number of aggregates over a table. The function launches a query that computes a number of aggregates over all columns, including min, max, avg, std and approx\_unique.                                                 |
| `get_table_name_from_path` | Get the table name from a path                                                                                                                                                                                                                 |
| `create_table_from_path`   | Creates a table from a path                                                                                                                                                                                                                    |
| `export_table_to_path`     | Save a table in a desired format (default: parquet). If the path is provided, the table will be saved under that path. Eg: If path is /tmp, the table will be saved as /tmp/table.parquet. Otherwise it will be saved in the current directory |
| `load_local_path_to_table` | Load a local file into duckdb                                                                                                                                                                                                                  |
| `load_local_csv_to_table`  | Load a local CSV file into duckdb                                                                                                                                                                                                              |
| `load_s3_path_to_table`    | Load a file from S3 into duckdb                                                                                                                                                                                                                |
| `load_s3_csv_to_table`     | Load a CSV file from S3 into duckdb                                                                                                                                                                                                            |
| `create_fts_index`         | Create a full text search index on a table                                                                                                                                                                                                     |
| `full_text_search`         | Full text Search in a table column for a specific text/keyword                                                                                                                                                                                 |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/duckdb.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/duckdb_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
For more installation options, please refer to [DuckDB documentation](https://duckdb.org/docs/installation).

## Example

The following agent will analyze the movies file using SQL and return the result.
```

---

## Query the documents

**URL:** llms-txt#query-the-documents

run = agent.run("What are the key points in the document?")
print(run.content)

---

## Optionally pass the dependencies to the print_response method

**URL:** llms-txt#optionally-pass-the-dependencies-to-the-print_response-method

---

## Define parallel research steps

**URL:** llms-txt#define-parallel-research-steps

tech_research_step = Step(
    name="TechResearch",
    agent=tech_researcher,
    description="Research tech developments from Hacker News",
)

news_research_step = Step(
    name="NewsResearch",
    agent=news_researcher,
    description="Research current news and trends",
)

---

## Setup knowledge with metadata

**URL:** llms-txt#setup-knowledge-with-metadata

knowledge = Knowledge(
    contents_db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        knowledge_table="knowledge_contents",
    ),
    vector_db=PgVector(
        table_name="filtered_knowledge",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    )
)

---

## Filtering on Load

**URL:** llms-txt#filtering-on-load

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/filtering-on-load

```python  theme={null}
from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## "My name is John Doe and I like to hike in the mountains on weekends.",

**URL:** llms-txt#"my-name-is-john-doe-and-i-like-to-hike-in-the-mountains-on-weekends.",

---

## Only analysis, no thinking

**URL:** llms-txt#only-analysis,-no-thinking

ReasoningTools(enable_think=False, enable_analyze=True)

---

## Live Search Agent

**URL:** llms-txt#live-search-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/live-search-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run the evaluation

**URL:** llms-txt#run-the-evaluation

**Contents:**
- Best Practices
- Next Steps

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

* **Start Simple:** Begin with basic accuracy tests before progressing to complex performance and reliability evaluations
* **Use Multiple Test Cases:** Don't rely on a single test caseâ€”build comprehensive test suites that cover edge cases
* **Track Over Time:** Monitor your eval metrics continuously as you iterate on your agents
* **Combine Dimensions:** Evaluate across all three dimensions for a holistic view of agent quality

Dive deeper into each evaluation dimension:

1. **[Accuracy Evals](/basics/evals/accuracy)** - Learn LLM-as-a-judge techniques and multiple test case strategies
2. **[Performance Evals](/basics/evals/performance)** - Measure latency, memory usage, and compare different configurations
3. **[Reliability Evals](/basics/evals/reliability)** - Test tool calls, error handling, and rate limiting behavior

---

## Define the database URL where the vector database will be stored

**URL:** llms-txt#define-the-database-url-where-the-vector-database-will-be-stored

db_url = "/tmp/lancedb"

---

## Setup our AgentOS with MCP enabled

**URL:** llms-txt#setup-our-agentos-with-mcp-enabled

**Contents:**
- Define a local test client

agent_os = AgentOS(
    description="Example app with MCP enabled",
    agents=[web_research_agent],
    enable_mcp_server=True,  # This enables a LLM-friendly MCP server at /mcp
)

app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

You can see view your LLM-friendly MCP server at:
    http://localhost:7777/mcp

"""
    agent_os.serve(app="enable_mcp_example:app")
python test_client.py theme={null}
import asyncio

from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.mcp import MCPTools

**Examples:**

Example 1 (unknown):
```unknown
## Define a local test client
```

---

## Running Agents

**URL:** llms-txt#running-agents

**Contents:**
- Basic Execution

Source: https://docs.agno.com/basics/agents/running-agents

Learn how to run your Agents and process their output.

Run your Agent by calling `Agent.run()` or `Agent.arun()`. Here's how they work:

1. The agent builds the context to send to the model (system message, user message, chat history, user memories, session state and other relevant inputs).
2. The agent sends this context to the model.
3. The model processes the input and responds with either a message or a tool call.
4. If the model makes a tool call, the agent executes it and returns the results to the model.
5. The model processes the updated context, repeating this loop until it produces a final message without any tool calls.
6. The agent returns this final response to the caller.

The `Agent.run()` function runs the agent and returns the output â€” either as a `RunOutput` object or as a stream of `RunOutputEvent` objects (when `stream=True`). For example:

```python  theme={null}
from agno.agent import Agent, RunOutput
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response

agent = Agent(
    model=Claude(id="claude-sonnet-4-5"),
    tools=[HackerNewsTools()],
    instructions="Write a report on the topic. Output only the report.",
    markdown=True,
)

---

## Pydantic model for classification output

**URL:** llms-txt#pydantic-model-for-classification-output

class ClassificationResult(BaseModel):
    query: str
    tag: str
    message: str

---

## Retrieve and display generated images using get_last_run_output

**URL:** llms-txt#retrieve-and-display-generated-images-using-get_last_run_output

**Contents:**
- Usage

run_response = agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput) and run_response.images:
    for image_response in run_response.images:
        image_bytes = image_response.content
        if image_bytes:
            image = PILImage.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")
else:
    print("No images found in run response")
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai pillow agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/image_editing.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/image_editing.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Prepare your image">
    Place an image file at `tmp/test_photo.png` or update the filepath in the code to point to your image.
  </Step>

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How Knowledge Works

**URL:** llms-txt#how-knowledge-works

**Contents:**
- The Knowledge Pipeline: Three Simple Steps
- Vector Embeddings and Search
- Setting Up Knowledge in Code

Source: https://docs.agno.com/basics/knowledge/how-it-works

Learn the Knowledge pipeline and technical architecture that powers intelligent knowledge retrieval in Agno agents.

At its core, Agno's Knowledge system is **Retrieval Augmented Generation (RAG)** made simple. Instead of cramming everything into a prompt, you store information in a searchable knowledge base and let agents pull exactly what they need, when they need it.

## The Knowledge Pipeline: Three Simple Steps

<Steps>
  <Step title="Store: Break Down and Index Information">
    Your documents, files, and data are processed by specialized readers, broken into chunks using configurable strategies, and stored in a vector database with their meanings captured as embeddings.

**Example:** A 50-page employee handbook is processed by Agno's PDFReader, chunked using SemanticChunking strategy, and becomes 200 searchable chunks with topics like "vacation policy," "remote work guidelines," or "expense procedures."
  </Step>

<Step title="Search: Find Relevant Information">
    When a user asks a question, the agent automatically searches the knowledge base using Agno's search methods to find the most relevant information chunks.

**Example:** User asks "How many vacation days do I get?" â†’ Agent calls `knowledge.search()` and finds chunks about vacation policies, PTO accrual, and holiday schedules.
  </Step>

<Step title="Generate: Create Contextual Responses">
    The agent combines the retrieved information with the user's question to generate an accurate, contextual response, with sources tracked through Agno's content management system.

**Example:** "Based on your employee handbook, full-time employees receive 15 vacation days per year, accrued monthly at 1.25 days per month..."
  </Step>
</Steps>

## Vector Embeddings and Search

Think of embeddings as a way to capture meaning in numbers. When you ask "What's our refund policy?", the system doesn't just match the word "refund"â€”it understands you're asking about returns, money back, and customer satisfaction.

That's because text gets converted into **vectors** (lists of numbers) where similar meanings cluster together. "Refund policy" and "return procedures" end up close in vector space, even though they don't share exact words. This is what enables semantic search beyond simple keyword matching.

## Setting Up Knowledge in Code

Here's how you connect the pieces to build a knowledge-powered agent:

```python  theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.chunking.semantic import SemanticChunking
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.agent import Agent

---

## Confirmation Required with Async Streaming

**URL:** llms-txt#confirmation-required-with-async-streaming

Source: https://docs.agno.com/basics/hitl/usage/confirmation-required-stream-async

This example demonstrates human-in-the-loop functionality with asynchronous streaming responses. It shows how to handle user confirmation during tool execution in an async environment while maintaining real-time streaming.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create agent with memory enabled

**URL:** llms-txt#create-agent-with-memory-enabled

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    enable_user_memories=True,
)

---

## Workflow will use custom_workflow_logger

**URL:** llms-txt#workflow-will-use-custom_workflow_logger

**Contents:**
- Using Named Loggers

workflow = Workflow(
    debug_mode=True,
    steps=[Step(name="step1", agent=agent)]
)
workflow.print_response("Tell me a fun fact")
python  theme={null}
import logging
from agno.agent import Agent
from agno.team import Team
from agno.workflow import Workflow
from agno.workflow.step import Step

**Examples:**

Example 1 (unknown):
```unknown
## Using Named Loggers

As it's conventional in Python, you can also provide custom loggers just by setting loggers with specific names. This is useful if you want to set them up using configuration files.

Agno automatically recognizes and uses these logger names:

* `agno` will be used for all Agent logs
* `agno-team` will be used for all Team logs
* `agno-workflow` will be used for all Workflow logs
```

---

## WhatsApp Image Generation Agent (Model-based)

**URL:** llms-txt#whatsapp-image-generation-agent-(model-based)

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/whatsapp/image-generation-model

WhatsApp agent that generates images using Gemini's built-in capabilities

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Direct Image Generation**: Gemini 2.0 Flash experimental image generation
* **Text-to-Image**: Converts descriptions into visual content
* **Multimodal Responses**: Generates both text and images
* **WhatsApp Integration**: Sends images directly through WhatsApp
* **Debug Mode**: Enhanced logging for troubleshooting

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Filter Knowledge

**URL:** llms-txt#filter-knowledge

**Contents:**
- Two Approaches to Filtering
  - 1. Dictionary Filters (Simple)
  - 2. Filter Expressions (Advanced)
- Filter Operators
  - Comparison Operators
  - Logical Operators
- Serialization Format

Source: https://docs.agno.com/agent-os/knowledge/filter-knowledge

Learn how to use advanced filter expressions through the Agno API for precise knowledge base filtering.

When using the AgentOS API, you can apply filters to precisely control which knowledge base documents your agents search, without changing your agent code.
Filter expressions serialize to JSON and are automatically reconstructed server-side for powerful, programmatic filtering.

## Two Approaches to Filtering

Agno supports two ways to filter knowledge through the API:

* Use **dictionary filters** for simple "field = value" lookups
* Use **filter expressions** when you need OR/NOT logic or ranges

### 1. Dictionary Filters (Simple)

Best for straightforward equality matching. Send a JSON object with key-value pairs:

### 2. Filter Expressions (Advanced)

Best for complex filtering with full logical control. Send structured filter objects:

<Tip>
  **When to use which:**

* Use **dict filters** for simple queries like filtering by category or status
  * Use **filter expressions** when you need OR logic, exclusions (NOT), or range queries (GT/LT)
</Tip>

Filter expressions support a range of comparison and logical operators:

### Comparison Operators

* **`EQ(key, value)`** - Equality: field equals value
* **`GT(key, value)`** - Greater than: field > value
* **`LT(key, value)`** - Less than: field \< value
* **`IN(key, [values])`** - Inclusion: field in list of values

### Logical Operators

* **`AND(*filters)`** - All conditions must be true
* **`OR(*filters)`** - At least one condition must be true
* **`NOT(filter)`** - Negate a condition

## Serialization Format

Filter expression objects use a dictionary format with an `"op"` key that distinguishes them from regular dict filters:

```python  theme={null}
from agno.filters import EQ, GT, AND

**Examples:**

Example 1 (unknown):
```unknown
### 2. Filter Expressions (Advanced)

Best for complex filtering with full logical control. Send structured filter objects:
```

Example 2 (unknown):
```unknown
<Tip>
  **When to use which:**

  * Use **dict filters** for simple queries like filtering by category or status
  * Use **filter expressions** when you need OR logic, exclusions (NOT), or range queries (GT/LT)
</Tip>

## Filter Operators

Filter expressions support a range of comparison and logical operators:

### Comparison Operators

* **`EQ(key, value)`** - Equality: field equals value
* **`GT(key, value)`** - Greater than: field > value
* **`LT(key, value)`** - Less than: field \< value
* **`IN(key, [values])`** - Inclusion: field in list of values

### Logical Operators

* **`AND(*filters)`** - All conditions must be true
* **`OR(*filters)`** - At least one condition must be true
* **`NOT(filter)`** - Negate a condition

## Serialization Format

Filter expression objects use a dictionary format with an `"op"` key that distinguishes them from regular dict filters:
```

---

## Get specific content by ID

**URL:** llms-txt#get-specific-content-by-id

content = knowledge.get_content_by_id(content_id)

---

## ************* Create Agent *************

**URL:** llms-txt#*************-create-agent-*************

agno_agent = Agent(
    name="Agno Agent",
    model=Claude(id="claude-sonnet-4-5"),
    db=SqliteDb(db_file="agno.db"),
    tools=[MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")],
    add_history_to_context=True,
    markdown=True,
)

---

## DeepInfra

**URL:** llms-txt#deepinfra

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/deepinfra

The DeepInfra model provides access to DeepInfra's hosted language models.

| Parameter               | Type            | Default                                 | Description                                                         |
| ----------------------- | --------------- | --------------------------------------- | ------------------------------------------------------------------- |
| `id`                    | `str`           | `"meta-llama/Llama-2-70b-chat-hf"`      | The id of the DeepInfra model to use                                |
| `name`                  | `str`           | `"DeepInfra"`                           | The name of the model                                               |
| `provider`              | `str`           | `"DeepInfra"`                           | The provider of the model                                           |
| `api_key`               | `Optional[str]` | `None`                                  | The API key for DeepInfra (defaults to DEEPINFRA\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.deepinfra.com/v1/openai"` | The base URL for the DeepInfra API                                  |
| `retries`               | `int`           | `0`                                     | Number of retries to attempt before raising a ModelProviderError    |
| `delay_between_retries` | `int`           | `1`                                     | Delay between retries, in seconds                                   |
| `exponential_backoff`   | `bool`          | `False`                                 | If True, the delay between retries is doubled each time             |

DeepInfra extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## How to connect to an Upstash Vector index

**URL:** llms-txt#how-to-connect-to-an-upstash-vector-index

---

## Schedule a meeting

**URL:** llms-txt#schedule-a-meeting

**Contents:**
- Toolkit Params
- Toolkit Functions
- Rate Limits
- Developer Resources

response = agent.print_response("""
Schedule a team meeting with the following details:
- Topic: Weekly Team Sync
- Time: Tomorrow at 2 PM UTC
- Duration: 45 minutes
""", markdown=True)
```

| Parameter       | Type            | Default | Description                                                                          |
| --------------- | --------------- | ------- | ------------------------------------------------------------------------------------ |
| `account_id`    | `Optional[str]` | `None`  | Zoom account ID. If not provided, uses ZOOM\_ACCOUNT\_ID environment variable.       |
| `client_id`     | `Optional[str]` | `None`  | Zoom client ID. If not provided, uses ZOOM\_CLIENT\_ID environment variable.         |
| `client_secret` | `Optional[str]` | `None`  | Zoom client secret. If not provided, uses ZOOM\_CLIENT\_SECRET environment variable. |

| Function                 | Description                                       |
| ------------------------ | ------------------------------------------------- |
| `schedule_meeting`       | Schedule a new Zoom meeting                       |
| `get_upcoming_meetings`  | Get a list of upcoming meetings                   |
| `list_meetings`          | List all meetings based on type                   |
| `get_meeting_recordings` | Get recordings for a specific meeting             |
| `delete_meeting`         | Delete a scheduled meeting                        |
| `get_meeting`            | Get detailed information about a specific meeting |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

The Zoom API has rate limits that vary by endpoint and account type:

* Server-to-Server OAuth apps: 100 requests/second
* Meeting endpoints: Specific limits apply based on account type
* Recording endpoints: Lower rate limits, check Zoom documentation

For detailed rate limits, refer to [Zoom API Rate Limits](https://developers.zoom.us/docs/api/#rate-limits).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/zoom.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/zoom_tools.py)

---

## Basic Stream

**URL:** llms-txt#basic-stream

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/xai/usage/basic-stream

```python cookbook/models/xai/basic_stream.py theme={null}
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

---

## Set Temperature

**URL:** llms-txt#set-temperature

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/local/ollama/usage/set-temperature

```python cookbook/models/ollama/set_temperature.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.2", options={"temperature": 0.5}), markdown=True)

---

## Filtering with Invalid Keys

**URL:** llms-txt#filtering-with-invalid-keys

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/filtering-with-invalid-keys

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## When to use a Workflow vs a Team in Agno

**URL:** llms-txt#when-to-use-a-workflow-vs-a-team-in-agno

**Contents:**
- Use a Workflow when:
- Use an Agent Team when:
- ðŸ’¡ Pro Tip

Source: https://docs.agno.com/faq/When-to-use-a-Workflow-vs-a-Team-in-Agno

Agno offers two powerful ways to build multi-agent systems: **Workflows** and **Teams**. Each is suited for different kinds of use-cases.

## Use a Workflow when:

You need **orchestrated, multi-step execution** with flexible control flow and a predictable outcome.

Workflows are ideal for:

* **Sequential processes** - Step-by-step agent executions with dependencies
* **Parallel execution** - Running independent tasks simultaneously
* **Conditional logic** - Dynamic routing based on content analysis
* **Quality assurance** - Iterative loops with end conditions
* **Complex pipelines** - Mixed components (agents, teams, functions) with branching
* **Structured processes** - Data transformation with predictable patterns

[Learn more about Workflows](/basics/workflows/overview)

## Use an Agent Team when:

Your task requires reasoning, collaboration, or multi-tool decision-making.

Agent Teams are best for:

* Research and planning
* Tasks where agents divide responsibilities

[Learn more about Agent Teams](/basics/teams/overview)

> Think of **Workflows** as assembly lines for known tasks,
> and **Agent Teams** as collaborative task forces for solving open-ended problems.

---

## Llama Essay Writer

**URL:** llms-txt#llama-essay-writer

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/huggingface/usage/llama-essay-writer

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Postgres for Workflows

**URL:** llms-txt#postgres-for-workflows

**Contents:**
- Usage
  - Run PgVector

Source: https://docs.agno.com/integrations/database/postgres/usage/postgres-for-workflow

Agno supports using PostgreSQL as a storage backend for Workflows using the `PostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

```python postgres_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Example 2: Sleep for a longer duration

**URL:** llms-txt#example-2:-sleep-for-a-longer-duration

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Sleep for 5 seconds")
```

| Parameter      | Type   | Default | Description                                |
| -------------- | ------ | ------- | ------------------------------------------ |
| `enable_sleep` | `bool` | `True`  | Enables sleep functionality                |
| `all`          | `bool` | `False` | Enables all functionality when set to True |

| Function | Description                                        |
| -------- | -------------------------------------------------- |
| `sleep`  | Pauses execution for a specified number of seconds |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/sleep.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/sleep_tools.py)

---

## Create knowledge base with documents and metadata

**URL:** llms-txt#create-knowledge-base-with-documents-and-metadata

knowledge = Knowledge(
    name="CSV Knowledge Base",
    description="A knowledge base for CSV files",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_csv_paths[0],
            "metadata": {
                "data_type": "sales",
                "quarter": "Q1",
                "year": 2024,
                "region": "north_america",
                "currency": "USD",
            },
        },
        {
            "path": downloaded_csv_paths[1],
            "metadata": {
                "data_type": "sales",
                "year": 2024,
                "region": "europe",
                "currency": "EUR",
            },
        },
        {
            "path": downloaded_csv_paths[2],
            "metadata": {
                "data_type": "survey",
                "survey_type": "customer_satisfaction",
                "year": 2024,
                "target_demographic": "mixed",
            },
        },
        {
            "path": downloaded_csv_paths[3],
            "metadata": {
                "data_type": "financial",
                "sector": "technology",
                "year": 2024,
                "report_type": "quarterly_earnings",
            },
        },
    ],
)

---

## Engineering standards

**URL:** llms-txt#engineering-standards

code_review = CulturalKnowledge(
    name="Code Review Standards",
    summary="Focus on maintainability, security, and performance",
    categories=["engineering", "code-review"],
    content=(
        "- Check for security vulnerabilities first\n"
        "- Verify error handling is comprehensive\n"
        "- Ensure code is self-documenting\n"
        "- Suggest performance optimizations where relevant"
    ),
)
```

---

## Image to Text Agent

**URL:** llms-txt#image-to-text-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/images/usage/image-to-text

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## DeepSeek

**URL:** llms-txt#deepseek

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/deepseek

The DeepSeek model provides access to DeepSeek's language models.

| Parameter               | Type            | Default                      | Description                                                       |
| ----------------------- | --------------- | ---------------------------- | ----------------------------------------------------------------- |
| `id`                    | `str`           | `"deepseek-chat"`            | The id of the DeepSeek model to use                               |
| `name`                  | `str`           | `"DeepSeek"`                 | The name of the model                                             |
| `provider`              | `str`           | `"DeepSeek"`                 | The provider of the model                                         |
| `api_key`               | `Optional[str]` | `None`                       | The API key for DeepSeek (defaults to DEEPSEEK\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.deepseek.com"` | The base URL for the DeepSeek API                                 |
| `retries`               | `int`           | `0`                          | Number of retries to attempt before raising a ModelProviderError  |
| `delay_between_retries` | `int`           | `1`                          | Delay between retries, in seconds                                 |
| `exponential_backoff`   | `bool`          | `False`                      | If True, the delay between retries is doubled each time           |

DeepSeek extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Create team with knowledge base integration

**URL:** llms-txt#create-team-with-knowledge-base-integration

**Contents:**
- Usage

team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[web_agent],
    model=OpenAIChat(id="gpt-5-mini"),
    knowledge=agno_docs_knowledge,
    show_members_responses=True,
    markdown=True,
)

if __name__ == "__main__":
    team_with_knowledge.print_response("Tell me about the Agno framework", stream=True)
bash  theme={null}
    pip install agno ddgs lancedb
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/knowledge/01_team_with_knowledge.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## SiliconFlow

**URL:** llms-txt#siliconflow

**Contents:**
- Authentication
- Example
- Params

Source: https://docs.agno.com/integrations/models/gateways/siliconflow/overview

Learn how to use Siliconflow models in Agno.

Siliconflow is a platform for providing endpoints for Large Language models.

Explore Siliconflowâ€™s models [here](https://siliconflow.ai/models).

Set your `SILICONFLOW_API_KEY` environment variable. Get your key [from Siliconflow here](https://siliconflow.ai).

Use `Siliconflow` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/gateways/siliconflow/usage/basic-stream). </Note>

| Parameter  | Type            | Default                                   | Description                                                             |
| ---------- | --------------- | ----------------------------------------- | ----------------------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Meta-Llama-3.1-8B-Instruct"` | The id of the SiliconFlow model to use                                  |
| `name`     | `str`           | `"SiliconFlow"`                           | The name of the model                                                   |
| `provider` | `str`           | `"SiliconFlow"`                           | The provider of the model                                               |
| `api_key`  | `Optional[str]` | `None`                                    | The API key for SiliconFlow (defaults to SILICONFLOW\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.siliconflow.cn/v1"`         | The base URL for the SiliconFlow API                                    |

`Siliconflow` also supports the params of [OpenAI](/reference/models/openai).

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Siliconflow` with your `Agent`:

<CodeGroup>
```

---

## Agent that uses a structured output

**URL:** llms-txt#agent-that-uses-a-structured-output

**Contents:**
- Usage

structured_output_agent = Agent(
    model=Nebius(id="Qwen/Qwen3-30B-A3B"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    output_schema=MovieScript,
)

structured_output_agent.print_response("New York")
bash  theme={null}
    export NEBIUS_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/nebius/structured_output.py
      bash Windows theme={null}
      python cookbook/models/nebius/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Introduction to Knowledge

**URL:** llms-txt#introduction-to-knowledge

**Contents:**
- The Problem with Knowledge-Free Agents
- Real-World Impact
  - Intelligent Text-to-SQL Agents
  - Customer Support Excellence
  - Internal Knowledge Assistant
- Ready to Get Started?

Source: https://docs.agno.com/basics/knowledge/overview

Understand why Knowledge is essential for building intelligent, context-aware AI agents that provide accurate, relevant responses.

Imagine an AI agent being asked about a company's HR policies and, instead of generic advice, returning precise answers based on the actual employee handbook. Or consider a customer support agent that knows specific product details, pricing, and troubleshooting guides. This is the power of Knowledge in Agno.

## The Problem with Knowledge-Free Agents

Without access to specific information, AI agents can only rely on their general training data. This leads to:

* **Generic responses** that don't match your specific context
* **Outdated information** from training data that's months or years old
* **Hallucinations** when the agent guesses at facts it doesn't actually know
* **Limited usefulness** for domain-specific tasks or company-specific workflows

### Intelligent Text-to-SQL Agents

Build agents that know your exact database schema, column names, and common query patterns. Instead of guessing at table structures, they retrieve the specific schema information needed for each query, ensuring accurate SQL generation.

### Customer Support Excellence

Create a support agent with access to your complete product documentation, FAQ database, and troubleshooting guides. Customers get accurate answers instantly, without waiting for human agents to look up information.

### Internal Knowledge Assistant

Deploy an agent that knows your company's processes, policies, and institutional knowledge. New employees can get onboarding help, and existing team members can quickly find answers to complex procedural questions.

## Ready to Get Started?

Transform agents from generic assistants to domain experts:

<CardGroup cols={3}>
  <Card title="Learn How It Works" icon="book-open" href="/basics/knowledge/how-it-works">
    Understand the simple RAG pipeline behind intelligent knowledge retrieval
  </Card>

<Card title="Build Your First Agent" icon="rocket" href="/basics/knowledge/getting-started">
    Follow our quick tutorial to create a knowledge-powered agent in minutes
  </Card>

<Card title="Knowledge with Agents" icon="robot" href="/basics/knowledge/agents/overview">
    Learn how to use knowledge with agents.
  </Card>

<Card title="Knowledge with Teams" icon="users" href="/basics/knowledge/teams/overview">
    Learn how to use knowledge with teams.
  </Card>

<Card title="Further Knowledge Concepts" icon="book-open" href="/basics/knowledge/terminology">
    Learn how to use knowledge with teams.
  </Card>
</CardGroup>

---

## Agentic Session State

**URL:** llms-txt#agentic-session-state

Source: https://docs.agno.com/basics/state/team/usage/agentic-session-state

This example demonstrates how to enable agentic session state in teams and agents, allowing them to automatically manage and update their session state during interactions. The agents can modify the session state autonomously based on the conversation context.

<Steps>
  <Step title="Create a Python file">
    Create a file `agentic_session_state.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the team">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Create a custom reasoning agent with specific instructions

**URL:** llms-txt#create-a-custom-reasoning-agent-with-specific-instructions

**Contents:**
- Example Use Cases
- When to Use Reasoning Agents
- Developer Resources

custom_reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Focus heavily on mathematical rigor",
        "Always provide step-by-step proofs",
    ],
)

main_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    reasoning_agent=custom_reasoning_agent,  # Use your custom agent
)
python logical_puzzle.py theme={null}
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat

task = (
        "Three missionaries and three cannibals need to cross a river. "
        "They have a boat that can carry up to two people at a time. "
        "If, at any time, the cannibals outnumber the missionaries on either side of the river, the cannibals will eat the missionaries. "
        "How can all six people get across the river safely? Provide a step-by-step solution and show the solution as an ASCII diagram."
    )

reasoning_agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        reasoning=True,
        markdown=True,
    )

reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
    python mathematical_proof.py theme={null}
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat

task = "Prove that for any positive integer n, the sum of the first n odd numbers is equal to n squared. Provide a detailed proof."

reasoning_agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        reasoning=True,
        markdown=True,
    )

reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
    python scientific_research.py theme={null}
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat

task = (
        "Read the following abstract of a scientific paper and provide a critical evaluation of its methodology, "
        "results, conclusions, and any potential biases or flaws:\n\n"
        "Abstract: This study examines the effect of a new teaching method on student performance in mathematics. "
        "A sample of 30 students was selected from a single school and taught using the new method over one semester. "
        "The results showed a 15% increase in test scores compared to the previous semester. "
        "The study concludes that the new teaching method is effective in improving mathematical performance among high school students."
    )

reasoning_agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        reasoning=True,
        markdown=True,
    )

reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
    python planning_itinerary.py theme={null}
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat

task = "Plan a 3-day itinerary from Los Angeles to Las Vegas, including must-see attractions, dining recommendations, and optimal travel times."

reasoning_agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        reasoning=True,
        markdown=True,
    )

reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
    python creative_writing.py theme={null}
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat

task = "Write a short story about life in 500,000 years. Consider technological, biological, and societal evolution."

reasoning_agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        reasoning=True,
        markdown=True,
    )

reasoning_agent.print_response(task, stream=True, show_full_reasoning=True)
    ```
  </Tab>
</Tabs>

## When to Use Reasoning Agents

**Use reasoning agents when:**

* Your task requires multiple sequential steps
* You need the agent to call tools iteratively and build on results
* You want automated chain-of-thought without manually calling reasoning tools
* You need self-validation and error correction
* The problem benefits from exploring multiple approaches before settling on a solution

**Consider alternatives when:**

* You're using a native reasoning model (gpt-5-mini, DeepSeek-R1) for simple tasks: just use the model directly
* You want explicit control over when the agent thinks vs. acts: use [Reasoning Tools](/basics/reasoning/reasoning-tools) instead
* The task is straightforward and doesn't require multi-step thinking

<Tip>
  **Pro tip:** Start with `reasoning_max_steps=5` for simpler problems to avoid
  unnecessary overhead. Increase to 10-15 for complex multi-step tasks. Monitor
  with `show_full_reasoning=True` to see how many steps your agent actually
  needs.
</Tip>

## Developer Resources

* View [Reasoning Agent Examples](/basics/reasoning/usage/agents/basic-cot)
* View [Reasoning Agent Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/reasoning/agents)
* View [Reasoning Team Examples](/basics/reasoning/usage/agents/team-cot)
* View [Reasoning Team Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/reasoning/teams)

**Examples:**

Example 1 (unknown):
```unknown
## Example Use Cases

<Tabs>
  <Tab title="Logical Puzzles">
    **Breaking down complex logic problems:**
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Mathematical Proofs">
    **Problems requiring rigorous validation:**
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Scientific Research">
    **Critical evaluation and multi-faceted analysis:**
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Planning & Itineraries">
    **Sequential planning and optimization:**
```

---

## Setup basic agent

**URL:** llms-txt#setup-basic-agent

**Contents:**
- Usage

agno_support_agent = Agent(
    id="agno-support-agent",
    name="Agno Support Agent",
    model=Claude(id="claude-sonnet-4-0"),
    db=db,
    tools=[mcp_tools],
    add_history_to_context=True,
    num_history_runs=3,
    markdown=True,
)

agent_os = AgentOS(
    description="Example app with MCP Tools",
    agents=[agno_support_agent],
)

app = agent_os.get_app()

if __name__ == "__main__":
    """Run your AgentOS.

You can see test your AgentOS at:
    http://localhost:7777/docs

"""
    # Don't use reload=True here, this can cause issues with the lifespan
    agent_os.serve(app="mcp_tools_example:app")

bash  theme={null}
    export ANTHROPIC_API_KEY=your_anthropic_api_key
    bash  theme={null}
    pip install -U agno anthropic fastapi uvicorn sqlalchemy pgvector psycopg
    bash  theme={null}
    # Using Docker
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash Mac theme={null}
      python cookbook/agent_os/mcp/mcp_tools_example.py
      bash Windows theme={null}
      python cookbook/agent_os/mcp/mcp_tools_example.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Server">
    <CodeGroup>
```

---

## Configure Knowledge page

**URL:** llms-txt#configure-knowledge-page

knowledge:
  display_name: "Knowledge Base"
  dbs:
    - db_id: db-0001
      domain_config:
        display_name: Product documentation

---

## Initialize Anthropic client

**URL:** llms-txt#initialize-anthropic-client

---

## Use in agent - wrap in list

**URL:** llms-txt#use-in-agent---wrap-in-list

**Contents:**
  - Progressive Filtering
- Best Practices for Filter Expressions
  - Filter Design
- Troubleshooting
  - Filter Not Working
  - Complex Filters Failing
  - Vector Database Support
  - Agentic Filtering Compatibility

agent.print_response(
    "What's new?",
    knowledge_filters=[current_content]
)
python  theme={null}
from agno.filters import AND, EQ, GT

async def progressive_search(agent, query, base_filters=None):
    """Try broad search first, then narrow if too many results."""

# First attempt: broad search
    broad_results = await agent.aget_relevant_docs_from_knowledge(
        query=query,
        filters=base_filters,  # Already a list
        num_documents=10
    )

if len(broad_results) > 8:
        # Too many results, add more specific filters
        specific_filter = AND(
            base_filters[0] if base_filters else EQ("status", "active"),
            GT("relevance_score", 0.8)
        )

return await agent.aget_relevant_docs_from_knowledge(
            query=query,
            filters=[specific_filter],  # â† Wrapped in list
            num_documents=5
        )

return broad_results
python  theme={null}
    # Add content with explicit metadata
    knowledge.add_content(
        path="doc.pdf",
        metadata={"status": "published", "category": "tech"}
    )

# Now filter will work
    filter_expr = EQ("status", "published")
    python  theme={null}
    from agno.filters import EQ, GT, AND

filter_expr = AND(EQ("status", "published"), GT("views", 100))
    print(filter_expr.to_dict())
    python  theme={null}
    # Test each part separately
    filter1 = EQ("status", "published")  # Test
    filter2 = GT("date", "2024-01-01")   # Test
    filter3 = IN("region", ["US", "EU"]) # Test

# Then combine
    combined = AND(filter1, filter2, filter3)
    python  theme={null}
    import json

try:
        filter_dict = filter_expr.to_dict()
        json_str = json.dumps(filter_dict)
        json.loads(json_str)  # Verify it parses
        print("Valid filter structure")
    except (TypeError, ValueError) as e:
        print(f"Invalid filter: {e}")
    python  theme={null}
    # Clear nested structure
    filter_expr = OR(
        AND(EQ("a", 1), EQ("b", 2)),
        EQ("c", 3)
    )

# Break down complex filters for readability
    condition1 = AND(EQ("a", 1), EQ("b", 2))
    condition2 = EQ("c", 3)
    filter_expr = OR(condition1, condition2)
    python  theme={null}
  # Works with all vector databases
  knowledge_filters=[{"department": "hr", "year": 2024}]

# Only works with PgVector currently
  knowledge_filters=[AND(EQ("department", "hr"), EQ("year", 2024))]
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Progressive Filtering

Start broad, then narrow down based on results:
```

Example 2 (unknown):
```unknown
## Best Practices for Filter Expressions

### Filter Design

* **Start Simple**: Begin with basic filters and add complexity as needed
* **Test Combinations**: Verify that your logical combinations work as expected
* **Document Your Schema**: Keep track of available metadata fields and their possible values
* **Performance Considerations**: Some filter combinations may be slower than others

## Troubleshooting

### Filter Not Working

<AccordionGroup>
  <Accordion title="Verify metadata keys exist">
    Check that the keys you're filtering on actually exist in your knowledge base:
```

Example 3 (unknown):
```unknown
</Accordion>

  <Accordion title="Check filter structure">
    Print the filter to verify it's constructed correctly:
```

Example 4 (unknown):
```unknown
</Accordion>
</AccordionGroup>

### Complex Filters Failing

<AccordionGroup>
  <Accordion title="Break down into smaller filters">
    Test each condition individually:
```

---

## Create collaborative research team

**URL:** llms-txt#create-collaborative-research-team

team = Team(
    name="Hackernews Research Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[hackernews_agent],
    determine_input_for_members=False,  # The member gets the input directly, without the team leader synthesizing it
    instructions=[
        "Conduct thorough research based on the structured input",
        "Address all focus areas mentioned in the research topic",
        "Tailor the research to the specified target audience",
        "Provide the requested number of sources",
    ],
    show_members_responses=True,
)

---

## Pinecone Vector Database

**URL:** llms-txt#pinecone-vector-database

**Contents:**
- Setup
- Example
- PineconeDb Params
- Developer Resources

Source: https://docs.agno.com/integrations/vectordb/pinecone/overview

Learn how to use Pinecone as a vector database for your Knowledge Base

Follow the instructions in the [Pinecone Setup Guide](https://docs.pinecone.io/guides/get-started/quickstart) to get started quickly with Pinecone.

<Info>
  We do not yet support Pinecone v6.x.x. We are actively working to achieve
  compatibility. In the meantime, we recommend using **Pinecone v5.4.2** for the
  best experience.
</Info>

<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      Pinecone also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_pineconedb_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/pinecone_db/pinecone_db.py)

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  We do not yet support Pinecone v6.x.x. We are actively working to achieve
  compatibility. In the meantime, we recommend using **Pinecone v5.4.2** for the
  best experience.
</Info>

## Example
```

Example 2 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      Pinecone also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Exceptions & Retries

**URL:** llms-txt#exceptions-&-retries

**Contents:**
- Using RetryAgentRun
- Using StopAgentRun

Source: https://docs.agno.com/basics/tools/exceptions

Learn how to use exceptions and retries to control the behavior of your tools.

If after a tool call you need to provide feedback to the model to change its behavior or exit the tool call loop, you can raise one of the following exceptions:

* `RetryAgentRun`: Use this exception when you want to provide instructions to the model for how to change its behavior and have the model retry the tool call. The exception message will be passed to the model as a tool call error, allowing the model to retry or adjust its approach in the next iteration of the LLM loop.

<Note>This does not retry the full agent runâ€”it only provides feedback to the model within the current run.</Note>

* `StopAgentRun`: Use this exception when you want to exit the model execution loop and end the agent run. When raised from a tool function, the agent exits the tool call loop, and the run status is set to `COMPLETED`. All session state, messages, tool calls, and tool results up to that point are stored in the database.

<Note>This does not cancel the agent run. It completes the run after exiting the tool call loop.</Note>

## Using RetryAgentRun

This example shows how to use the `RetryAgentRun` exception to provide feedback to the model, allowing it to adjust its behavior:

In this example, when `add_item` is called with fewer than 3 items, it raises `RetryAgentRun` with instructions. The model receives this as a tool call error and can call `add_item` again with additional items to meet the requirement.

## Using StopAgentRun

This example shows how to use the `StopAgentRun` exception to exit the tool call loop:

```python stop_in_tool_call.py theme={null}
from agno.agent import Agent
from agno.exceptions import StopAgentRun
from agno.models.openai import OpenAIChat
from agno.run import RunContext

def check_condition(run_context: RunContext, value: int) -> str:
    """Check a condition and stop tool calls if met."""
    if value > 100:
        raise StopAgentRun(
            f"Value {value} exceeds threshold. Stopping tool call execution."
        )
    return f"Value {value} is acceptable."

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[check_condition],
    markdown=True,
)

**Examples:**

Example 1 (unknown):
```unknown
In this example, when `add_item` is called with fewer than 3 items, it raises `RetryAgentRun` with instructions. The model receives this as a tool call error and can call `add_item` again with additional items to meet the requirement.

## Using StopAgentRun

This example shows how to use the `StopAgentRun` exception to exit the tool call loop:
```

---

## Reasoning Tools

**URL:** llms-txt#reasoning-tools

**Contents:**
- Example

Source: https://docs.agno.com/basics/tools/reasoning_tools/reasoning-tools

The `ReasoningTools` toolkit allows an Agent to use reasoning like any other tool, at any point during execution. Unlike traditional approaches that reason once at the start to create a fixed plan, this enables the Agent to reflect after each step, adjust its thinking, and update its actions on the fly.

We've found that this approach significantly improves an Agent's ability to solve complex problems it would otherwise fail to handle. By giving the Agent space to "think" about its actions, it can examine its own responses more deeply, question its assumptions, and approach the problem from different angles.

The toolkit includes the following tools:

* `think`: This tool is used as a scratchpad by the Agent to reason about the question and work through it step by step. It helps break down complex problems into smaller, manageable chunks and track the reasoning process.
* `analyze`: This tool is used to analyze the results from a reasoning step and determine the next actions.

Here's an example of how to use the `ReasoningTools` toolkit:

The toolkit comes with default instructions and few-shot examples to help the Agent use the tool effectively. Here is how you can enable them:

`ReasoningTools` can be used with any model provider that supports function calling. Here is an example with of a reasoning Agent using `OpenAIChat`:

This Agent can be used to ask questions that elicit thoughtful analysis, such as:

**Examples:**

Example 1 (unknown):
```unknown
The toolkit comes with default instructions and few-shot examples to help the Agent use the tool effectively. Here is how you can enable them:
```

Example 2 (unknown):
```unknown
`ReasoningTools` can be used with any model provider that supports function calling. Here is an example with of a reasoning Agent using `OpenAIChat`:
```

Example 3 (unknown):
```unknown
This Agent can be used to ask questions that elicit thoughtful analysis, such as:
```

Example 4 (unknown):
```unknown
or,
```

---

## Create a Shopping List Manager Agent that maintains state

**URL:** llms-txt#create-a-shopping-list-manager-agent-that-maintains-state

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    # Initialize the session state with an empty shopping list (default session state for all sessions)
    session_state={"shopping_list": []},
    db=SqliteDb(db_file="tmp/example.db"),
    tools=[add_item, remove_item, list_items],
    # You can use variables from the session state in the instructions
    instructions=dedent("""\
        Your job is to manage a shopping list.

The shopping list starts empty. You can add items, remove items by name, and list all items.

Current shopping list: {shopping_list}
    """),
    markdown=True,
)

---

## Get Status

**URL:** llms-txt#get-status

Source: https://docs.agno.com/reference-api/schema/agui/get-status

---

## enable_user_memories=True,

**URL:** llms-txt#enable_user_memories=true,

---

## Now switch to Google Gemini using same session

**URL:** llms-txt#now-switch-to-google-gemini-using-same-session

gemini_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions="You are a helpful assistant.",
    db=db,
    add_history_to_context=True,
)

---

## LanceDB Vector Database

**URL:** llms-txt#lancedb-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/lancedb/overview

Learn how to use LanceDB as a vector database for your Knowledge Base

```python agent_with_knowledge.py theme={null}
import typer
from typing import Optional
from rich.prompt import Prompt

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.lancedb import LanceDb
from agno.vectordb.search import SearchType

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Sentence Transformer Embedder

**URL:** llms-txt#sentence-transformer-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/sentencetransformers/usage/sentence-transformer-embedder

```python  theme={null}
from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = SentenceTransformerEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Agent Intermediate Steps Streaming

**URL:** llms-txt#agent-intermediate-steps-streaming

Source: https://docs.agno.com/basics/agents/usage/intermediate-steps

This example demonstrates how to stream intermediate steps during agent execution, providing visibility into tool calls and execution events.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MongoDB for Agent

**URL:** llms-txt#mongodb-for-agent

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/integrations/database/mongo/usage/mongodb-for-agent

Agno supports using MongoDB as a storage backend for Agents using the `MongoDb` class.

You need to provide either `db_url` or `client`. The following example uses `db_url`.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python mongodb_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.mongo import MongoDb
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Team

**URL:** llms-txt#team

**Contents:**
- Parameters
- Functions
  - `run`
  - `arun`
  - `print_response`
  - `aprint_response`
  - `cli_app`
  - `acli_app`
  - `get_session_summary`
  - `get_user_memories`

Source: https://docs.agno.com/reference/teams/team

| Parameter                          | Type                                                             | Default    | Description                                                                                                                                                                                                          |                               |        |         |                                                                                  |
| ---------------------------------- | ---------------------------------------------------------------- | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | ------ | ------- | -------------------------------------------------------------------------------- |
| `members`                          | `List[Union[Agent, Team]]`                                       | -          | List of agents or teams that make up this team                                                                                                                                                                       |                               |        |         |                                                                                  |
| `id`                               | `Optional[str]`                                                  | `None`     | Team UUID (autogenerated if not set)                                                                                                                                                                                 |                               |        |         |                                                                                  |
| `model`                            | `Optional[Union[Model, str]]`                                    | `None`     | Model to use for the team. Can be a Model object or a model string (`provider:model_id`)                                                                                                                             |                               |        |         |                                                                                  |
| `name`                             | `Optional[str]`                                                  | `None`     | Name of the team                                                                                                                                                                                                     |                               |        |         |                                                                                  |
| `role`                             | `Optional[str]`                                                  | `None`     | Role of the team within its parent team                                                                                                                                                                              |                               |        |         |                                                                                  |
| `respond_directly`                 | `bool`                                                           | `False`    | If True, the team leader won't process responses from the members and instead will return them directly                                                                                                              |                               |        |         |                                                                                  |
| `determine_input_for_members`      | `bool`                                                           | `True`     | Set to False if you want to send the run input directly to the member agents                                                                                                                                         |                               |        |         |                                                                                  |
| `delegate_to_all_members`          | `bool`                                                           | `False`    | If True, the team leader will delegate tasks to all members automatically, without any decision from the team leader                                                                                                 |                               |        |         |                                                                                  |
| `user_id`                          | `Optional[str]`                                                  | `None`     | Default user ID for this team                                                                                                                                                                                        |                               |        |         |                                                                                  |
| `session_id`                       | `Optional[str]`                                                  | `None`     | Default session ID for this team (autogenerated if not set)                                                                                                                                                          |                               |        |         |                                                                                  |
| `session_state`                    | `Optional[Dict[str, Any]]`                                       | `None`     | Session state (stored in the database to persist across runs)                                                                                                                                                        |                               |        |         |                                                                                  |
| `add_session_state_to_context`     | `bool`                                                           | `False`    | Set to True to add the session\_state to the context                                                                                                                                                                 |                               |        |         |                                                                                  |
| `enable_agentic_state`             | `bool`                                                           | `False`    | Set to True to give the team tools to update the session\_state dynamically                                                                                                                                          |                               |        |         |                                                                                  |
| `overwrite_db_session_state`       | `bool`                                                           | `False`    | Set to True to overwrite the session state in the database with the session state provided in the run                                                                                                                |                               |        |         |                                                                                  |
| `cache_session`                    | `bool`                                                           | `False`    | If True, cache the current Team session in memory for faster access                                                                                                                                                  |                               |        |         |                                                                                  |
| `resolve_in_context`               | `bool`                                                           | `True`     | If True, resolve the session\_state, dependencies, and metadata in the user and system messages                                                                                                                      |                               |        |         |                                                                                  |
| `description`                      | `Optional[str]`                                                  | `None`     | A description of the Team that is added to the start of the system message                                                                                                                                           |                               |        |         |                                                                                  |
| `instructions`                     | `Optional[Union[str, List[str], Callable]]`                      | `None`     | List of instructions for the team                                                                                                                                                                                    |                               |        |         |                                                                                  |
| `expected_output`                  | `Optional[str]`                                                  | `None`     | Provide the expected output from the Team                                                                                                                                                                            |                               |        |         |                                                                                  |
| `additional_context`               | `Optional[str]`                                                  | `None`     | Additional context added to the end of the system message                                                                                                                                                            |                               |        |         |                                                                                  |
| `markdown`                         | `bool`                                                           | `False`    | If markdown=true, add instructions to format the output using markdown                                                                                                                                               |                               |        |         |                                                                                  |
| `add_datetime_to_context`          | `bool`                                                           | `False`    | If True, add the current datetime to the instructions to give the team a sense of time                                                                                                                               |                               |        |         |                                                                                  |
| `add_location_to_context`          | `bool`                                                           | `False`    | If True, add the current location to the instructions to give the team a sense of location                                                                                                                           |                               |        |         |                                                                                  |
| `timezone_identifier`              | `Optional[str]`                                                  | `None`     | Allows for custom timezone for datetime instructions following the TZ Database format                                                                                                                                |                               |        |         |                                                                                  |
| `add_name_to_context`              | `bool`                                                           | `False`    | If True, add the team name to the instructions                                                                                                                                                                       |                               |        |         |                                                                                  |
| `add_member_tools_to_context`      | `bool`                                                           | `False`    | If True, add the tools available to team members to the context                                                                                                                                                      |                               |        |         |                                                                                  |
| `system_message`                   | `Optional[Union[str, Callable, Message]]`                        | `None`     | Provide the system message as a string or function                                                                                                                                                                   |                               |        |         |                                                                                  |
| `system_message_role`              | `str`                                                            | `"system"` | Role for the system message                                                                                                                                                                                          |                               |        |         |                                                                                  |
| `additional_input`                 | `Optional[List[Union[str, Dict, BaseModel, Message]]]`           | `None`     | A list of extra messages added after the system message and before the user message                                                                                                                                  |                               |        |         |                                                                                  |
| `db`                               | `Optional[BaseDb]`                                               | `None`     | Database to use for this team                                                                                                                                                                                        |                               |        |         |                                                                                  |
| `memory_manager`                   | `Optional[MemoryManager]`                                        | `None`     | Memory manager to use for this team                                                                                                                                                                                  |                               |        |         |                                                                                  |
| `dependencies`                     | `Optional[Dict[str, Any]]`                                       | `None`     | User provided dependencies                                                                                                                                                                                           |                               |        |         |                                                                                  |
| `add_dependencies_to_context`      | `bool`                                                           | `False`    | If True, add the dependencies to the user prompt                                                                                                                                                                     |                               |        |         |                                                                                  |
| `knowledge`                        | `Optional[Knowledge]`                                            | `None`     | Add a knowledge base to the team                                                                                                                                                                                     |                               |        |         |                                                                                  |
| `knowledge_filters`                | `Optional[Dict[str, Any]]`                                       | `None`     | Filters to apply to knowledge base searches                                                                                                                                                                          |                               |        |         |                                                                                  |
| `enable_agentic_knowledge_filters` | `Optional[bool]`                                                 | `False`    | Let the team choose the knowledge filters                                                                                                                                                                            |                               |        |         |                                                                                  |
| `update_knowledge`                 | `bool`                                                           | `False`    | Add a tool that allows the Team to update Knowledge                                                                                                                                                                  |                               |        |         |                                                                                  |
| `add_knowledge_to_context`         | `bool`                                                           | `False`    | If True, add references to the user prompt                                                                                                                                                                           |                               |        |         |                                                                                  |
| `knowledge_retriever`              | `Optional[Callable[..., Optional[List[Union[Dict, str]]]]]`      | `None`     | Retrieval function to get references                                                                                                                                                                                 |                               |        |         |                                                                                  |
| `references_format`                | `Literal["json", "yaml"]`                                        | `"json"`   | Format of the references                                                                                                                                                                                             |                               |        |         |                                                                                  |
| `share_member_interactions`        | `bool`                                                           | `False`    | If True, send all member interactions (request/response) during the current run to members that have been delegated a task to                                                                                        |                               |        |         |                                                                                  |
| `get_member_information_tool`      | `bool`                                                           | `False`    | If True, add a tool to get information about the team members                                                                                                                                                        |                               |        |         |                                                                                  |
| `search_knowledge`                 | `bool`                                                           | `True`     | Add a tool to search the knowledge base (aka Agentic RAG)                                                                                                                                                            |                               |        |         |                                                                                  |
| `send_media_to_model`              | `bool`                                                           | `True`     | If False, media (images, videos, audio, files) is only available to tools and not sent to the LLM                                                                                                                    |                               |        |         |                                                                                  |
| `store_media`                      | `bool`                                                           | `True`     | If True, store media in the database                                                                                                                                                                                 |                               |        |         |                                                                                  |
| `store_tool_messages`              | `bool`                                                           | `True`     | If True, store tool results in the database                                                                                                                                                                          |                               |        |         |                                                                                  |
| `store_history_messages`           | `bool`                                                           | `True`     | If True, store history messages in the database                                                                                                                                                                      |                               |        |         |                                                                                  |
| `tools`                            | `Optional[List[Union[Toolkit, Callable, Function, Dict]]]`       | `None`     | A list of tools provided to the Model                                                                                                                                                                                |                               |        |         |                                                                                  |
| `tool_choice`                      | `Optional[Union[str, Dict[str, Any]]]`                           | `None`     | Controls which (if any) tool is called by the team model                                                                                                                                                             |                               |        |         |                                                                                  |
| `tool_call_limit`                  | `Optional[int]`                                                  | `None`     | Maximum number of tool calls allowed                                                                                                                                                                                 |                               |        |         |                                                                                  |
| `max_tool_calls_from_history`      | `Optional[int]`                                                  | `None`     | Maximum number of tool calls from history to keep in context. If None, all tool calls from history are included. If set to N, only the last N tool calls from history are added to the context for memory management |                               |        |         |                                                                                  |
| `tool_hooks`                       | `Optional[List[Callable]]`                                       | `None`     | A list of hooks to be called before and after the tool call                                                                                                                                                          |                               |        |         |                                                                                  |
| `pre_hooks`                        | `Optional[Union[List[Callable[..., Any]], List[BaseGuardrail]]]` | `None`     | Functions called right after team session is loaded, before processing starts                                                                                                                                        |                               |        |         |                                                                                  |
| `post_hooks`                       | `Optional[Union[List[Callable[..., Any]], List[BaseGuardrail]]]` | `None`     | Functions called after output is generated but before the response is returned                                                                                                                                       |                               |        |         |                                                                                  |
| `input_schema`                     | `Optional[Type[BaseModel]]`                                      | `None`     | Input schema for validating input                                                                                                                                                                                    |                               |        |         |                                                                                  |
| `output_schema`                    | `Optional[Type[BaseModel]]`                                      | `None`     | Output schema for the team response                                                                                                                                                                                  |                               |        |         |                                                                                  |
| `parser_model`                     | `Optional[Union[Model, str]]`                                    | `None`     | Provide a secondary model to parse the response from the primary model. Can be a Model object or a model string (`provider:model_id`)                                                                                |                               |        |         |                                                                                  |
| `parser_model_prompt`              | `Optional[str]`                                                  | `None`     | Provide a prompt for the parser model                                                                                                                                                                                |                               |        |         |                                                                                  |
| `output_model`                     | `Optional[Union[Model, str]]`                                    | `None`     | Provide an output model to parse the response from the team. Can be a Model object or a model string (`provider:model_id`)                                                                                           |                               |        |         |                                                                                  |
| `output_model_prompt`              | `Optional[str]`                                                  | `None`     | Provide a prompt for the output model                                                                                                                                                                                |                               |        |         |                                                                                  |
| `use_json_mode`                    | `bool`                                                           | `False`    | If `output_schema` is set, sets the response mode of the model                                                                                                                                                       |                               |        |         |                                                                                  |
| `parse_response`                   | `bool`                                                           | `True`     | If True, parse the response                                                                                                                                                                                          |                               |        |         |                                                                                  |
| `enable_agentic_memory`            | `bool`                                                           | `False`    | Enable the team to manage memories of the user                                                                                                                                                                       |                               |        |         |                                                                                  |
| `enable_user_memories`             | `bool`                                                           | `False`    | If True, the team creates/updates user memories at the end of runs                                                                                                                                                   |                               |        |         |                                                                                  |
| `add_memories_to_context`          | `Optional[bool]`                                                 | `None`     | If True, the team adds a reference to the user memories in the response                                                                                                                                              |                               |        |         |                                                                                  |
| `enable_session_summaries`         | `bool`                                                           | `False`    | If True, the team creates/updates session summaries at the end of runs                                                                                                                                               |                               |        |         |                                                                                  |
| `session_summary_manager`          | `Optional[SessionSummaryManager]`                                | `None`     | Session summary manager                                                                                                                                                                                              |                               |        |         |                                                                                  |
| `add_session_summary_to_context`   | `Optional[bool]`                                                 | `None`     | If True, the team adds session summaries to the context                                                                                                                                                              |                               |        |         |                                                                                  |
| `compress_tool_results`            | `bool`                                                           | `False`    | If True, compress tool call results to save context space                                                                                                                                                            |                               |        |         |                                                                                  |
| `compression_manager`              | `Optional[CompressionManager]`                                   | `None`     | Custom compression manager for compressing tool call results                                                                                                                                                         |                               |        |         |                                                                                  |
| `add_history_to_context`           | `bool`                                                           | `False`    | Add messages from the chat history to the messages list sent to the Model. This only applies to the team leader, not the members.                                                                                    |                               |        |         |                                                                                  |
| `num_history_runs`                 | `Optional[int]`                                                  | `None`     | Number of historical runs to include in the messages.                                                                                                                                                                |                               |        |         |                                                                                  |
| `num_history_messages`             | `Optional[int]`                                                  | `None`     | Number of historical messages to include messages list sent to the Model.                                                                                                                                            | `add_team_history_to_members` | `bool` | `False` | If True, send the team-level history to the members, not the agent-level history |
| `num_team_history_runs`            | `int`                                                            | `3`        | Number of historical runs to include in the messages sent to the members                                                                                                                                             |                               |        |         |                                                                                  |
| `search_session_history`           | `Optional[bool]`                                                 | `False`    | If True, adds a tool to allow searching through previous sessions                                                                                                                                                    |                               |        |         |                                                                                  |
| `num_history_sessions`             | `Optional[int]`                                                  | `None`     | Number of past sessions to include in the search                                                                                                                                                                     |                               |        |         |                                                                                  |
| `read_team_history`                | `bool`                                                           | `False`    | If True, adds a tool to allow the team to read the team history (deprecated and will be removed in a future version)                                                                                                 |                               |        |         |                                                                                  |
| `read_chat_history`                | `bool`                                                           | `False`    | If True, adds a tool to allow the team to read the chat history                                                                                                                                                      |                               |        |         |                                                                                  |
| `metadata`                         | `Optional[Dict[str, Any]]`                                       | `None`     | Metadata stored with this team                                                                                                                                                                                       |                               |        |         |                                                                                  |
| `reasoning`                        | `bool`                                                           | `False`    | Enable reasoning for the team                                                                                                                                                                                        |                               |        |         |                                                                                  |
| `reasoning_model`                  | `Optional[Union[Model, str]]`                                    | `None`     | Model to use for reasoning. Can be a Model object or a model string (`provider:model_id`)                                                                                                                            |                               |        |         |                                                                                  |
| `reasoning_agent`                  | `Optional[Agent]`                                                | `None`     | Agent to use for reasoning                                                                                                                                                                                           |                               |        |         |                                                                                  |
| `reasoning_min_steps`              | `int`                                                            | `1`        | Minimum number of reasoning steps                                                                                                                                                                                    |                               |        |         |                                                                                  |
| `reasoning_max_steps`              | `int`                                                            | `10`       | Maximum number of reasoning steps                                                                                                                                                                                    |                               |        |         |                                                                                  |
| `stream`                           | `Optional[bool]`                                                 | `None`     | Stream the response from the Team                                                                                                                                                                                    |                               |        |         |                                                                                  |
| `stream_events`                    | `bool`                                                           | `False`    | Stream the intermediate steps from the Team                                                                                                                                                                          |                               |        |         |                                                                                  |
| `stream_member_events`             | `bool`                                                           | `True`     | Stream the member events from the Team members                                                                                                                                                                       |                               |        |         |                                                                                  |
| `store_events`                     | `bool`                                                           | `False`    | Store the events from the Team                                                                                                                                                                                       |                               |        |         |                                                                                  |
| `events_to_skip`                   | `Optional[List[Union[RunEvent, TeamRunEvent]]]`                  | `None`     | List of events to skip from the Team                                                                                                                                                                                 |                               |        |         |                                                                                  |
| `store_member_responses`           | `bool`                                                           | `False`    | Store member agent runs inside the team's RunOutput                                                                                                                                                                  |                               |        |         |                                                                                  |
| `debug_mode`                       | `bool`                                                           | `False`    | Enable debug logs                                                                                                                                                                                                    |                               |        |         |                                                                                  |
| `debug_level`                      | `Literal[1, 2]`                                                  | `1`        | Debug level: 1 = basic, 2 = detailed                                                                                                                                                                                 |                               |        |         |                                                                                  |
| `show_members_responses`           | `bool`                                                           | `False`    | Enable member logs - Sets the debug\_mode for team and members                                                                                                                                                       |                               |        |         |                                                                                  |
| `retries`                          | `int`                                                            | `0`        | Number of retries to attempt when running the Team                                                                                                                                                                   |                               |        |         |                                                                                  |
| `delay_between_retries`            | `int`                                                            | `1`        | Delay between retries (in seconds)                                                                                                                                                                                   |                               |        |         |                                                                                  |
| `exponential_backoff`              | `bool`                                                           | `False`    | Exponential backoff: if True, the delay between retries is doubled each time                                                                                                                                         |                               |        |         |                                                                                  |
| `telemetry`                        | `bool`                                                           | `True`     | Log minimal telemetry for analytics                                                                                                                                                                                  |                               |        |         |                                                                                  |

* `input` (Union\[str, List, Dict, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `retries` (Optional\[int]): Number of retries to attempt
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `output_schema` (Optional\[Type\[BaseModel]]): Output schema to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode
* `yield_run_response` (bool): Whether to yield the run response (only for streaming)

* `Union[TeamRunOutput, Iterator[Union[RunOutputEvent, TeamRunOutputEvent]]]`: Either a TeamRunOutput or an iterator of events, depending on the `stream` parameter

Run the team asynchronously.

* `input` (Union\[str, List, Dict, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `retries` (Optional\[int]): Number of retries to attempt
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `output_schema` (Optional\[Type\[BaseModel]]): Output schema to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode
* `yield_run_response` (bool): Whether to yield the run response (only for streaming)

* `Union[TeamRunOutput, AsyncIterator[Union[RunOutputEvent, TeamRunOutputEvent]]]`: Either a TeamRunOutput or an async iterator of events, depending on the `stream` parameter

Run the team and print the response.

* `input` (Union\[List, Dict, str, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `show_message` (bool): Whether to show the message (default: True)
* `show_reasoning` (bool): Whether to show reasoning (default: True)
* `show_full_reasoning` (bool): Whether to show full reasoning (default: False)
* `console` (Optional\[Any]): Console to use for output
* `tags_to_include_in_markdown` (Optional\[Set\[str]]): Tags to include in markdown content
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `markdown` (Optional\[bool]): Whether to format output as markdown
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode
* `show_member_responses` (Optional\[bool]): Whether to show member responses

### `aprint_response`

Run the team and print the response asynchronously.

* `input` (Union\[List, Dict, str, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `show_message` (bool): Whether to show the message (default: True)
* `show_reasoning` (bool): Whether to show reasoning (default: True)
* `show_full_reasoning` (bool): Whether to show full reasoning (default: False)
* `console` (Optional\[Any]): Console to use for output
* `tags_to_include_in_markdown` (Optional\[Set\[str]]): Tags to include in markdown content
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `markdown` (Optional\[bool]): Whether to format output as markdown
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode
* `show_member_responses` (Optional\[bool]): Whether to show member responses

Run an interactive command-line interface to interact with the team.

* `input` (Optional\[str]): The input to send to the team
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":sunglasses:")
* `stream` (bool): Whether to stream the response (default: False)
* `markdown` (bool): Whether to format output as markdown (default: False)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

Run an interactive command-line interface to interact with the team asynchronously.

* `input` (Optional\[str]): The input to send to the team
* `session_id` (Optional\[str]): Session ID to use
* `user_id` (Optional\[str]): User ID to use
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":sunglasses:")
* `stream` (bool): Whether to stream the response (default: False)
* `markdown` (bool): Whether to format output as markdown (default: False)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

### `get_session_summary`

Get the session summary for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* Session summary for the given session

### `get_user_memories`

Get the user memories for the given user ID.

* `user_id` (Optional\[str]): User ID to use (if not provided, the current user is used)

* `Optional[List[UserMemory]]`: The user memories

Add a tool to the team.

* `tool` (Union\[Toolkit, Callable, Function, Dict]): The tool to add

Replace the tools of the team.

* `tools` (List\[Union\[Toolkit, Callable, Function, Dict]]): The tools to set

Cancel a run by run ID.

* `run_id` (str): The run ID to cancel

* `bool`: True if the run was successfully cancelled

Get the run output for the given run ID.

* `run_id` (str): The run ID
* `session_id` (Optional\[str]): Session ID to use

* `Optional[Union[TeamRunOutput, RunOutput]]`: The run output

### `get_last_run_output`

Get the last run output for the session.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Optional[TeamRunOutput]`: The last run output

Get the session for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Optional[TeamSession]`: The team session

Save a session to the database.

* `session` (TeamSession): The session to save

Save a session to the database asynchronously.

* `session` (TeamSession): The session to save

* `session_id` (str): Session ID to delete

### `get_session_name`

Get the session name for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `str`: The session name

### `set_session_name`

Set the session name.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)
* `autogenerate` (bool): Whether to autogenerate the name
* `session_name` (Optional\[str]): The name to set

* `TeamSession`: The updated session

### `get_session_state`

Get the session state for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Dict[str, Any]`: The session state

### `update_session_state`

Update the session state for the given session ID.

* `session_id` (str): Session ID to use
* `session_state_updates` (Dict\[str, Any]): The session state keys and values to update. Overwrites the existing session state.

* `Dict[str, Any]`: The updated session state

### `get_session_metrics`

Get the session metrics for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Optional[Metrics]`: The session metrics

### `get_chat_history`

Get the chat history for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs

* `List[Message]`: The chat history

### `get_session_messages`

Get the messages for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)
* `member_ids` (Optional\[List\[str]]): The ids of the members to get the messages from
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs
* `limit` (Optional\[int]): The number of messages to return, counting from the latest. Defaults to all messages
* `skip_roles` (Optional\[List\[str]]): Skip messages with these roles
* `skip_statuses` (Optional\[List\[RunStatus]]): Skip messages with these statuses
* `skip_history_messages` (bool): Skip messages that were tagged as history in previous runs. Defaults to True
* `skip_member_messages` (bool): Skip messages created by members of the team. Defaults to True

* `List[Message]`: The messages for the session

---

## print(run.content)

**URL:** llms-txt#print(run.content)

---

## Qdrant Hybrid Search

**URL:** llms-txt#qdrant-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/qdrant/usage/qdrant-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Qdrant">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Qdrant">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## SQLite

**URL:** llms-txt#sqlite

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/sqlite/overview

Learn to use Sqlite as a database for your Agents

Agno supports using [Sqlite](https://www.sqlite.org) as a database with the `SqliteDb` class.

```python sqlite_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## Install required packages

**URL:** llms-txt#install-required-packages

**Contents:**
- SDK Integration
  - Basic Usage

pip install agno litellm
shell  theme={null}
export LITELLM_API_KEY=your_api_key_here
python  theme={null}
from agno.agent import Agent
from agno.models.litellm import LiteLLM

**Examples:**

Example 1 (unknown):
```unknown
Set up your API key:
Regardless of the model used(OpenAI, Hugging Face, or XAI) the API key is referenced as `LITELLM_API_KEY`.
```

Example 2 (unknown):
```unknown
## SDK Integration

The `LiteLLM` class provides direct integration with the LiteLLM Python SDK.

### Basic Usage
```

---

## Airflow

**URL:** llms-txt#airflow

**Contents:**
- Example

Source: https://docs.agno.com/integrations/toolkits/others/airflow

The following agent will use Airflow to save and read a DAG file.

```python cookbook/tools/airflow_tools.py theme={null}
from agno.agent import Agent
from agno.tools.airflow import AirflowTools

agent = Agent(
    tools=[AirflowTools(dags_dir="dags", save_dag=True, read_dag=True)], markdown=True
)

dag_content = """
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

---

## Agent with Knowledge

**URL:** llms-txt#agent-with-knowledge

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/vercel/usage/knowledge

```python cookbook/models/vercel/knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.vercel import V0
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)

---

## Streaming Agent with Parser Model

**URL:** llms-txt#streaming-agent-with-parser-model

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/parser-model-stream

This example demonstrates how to use a parser model with streaming output, combining Claude for parsing and OpenAI for generation.

```python parser_model_stream.py theme={null}
import random
from typing import Iterator, List

from agno.agent import Agent, RunOutputEvent
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa

class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )

agent = Agent(
    parser_model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    output_schema=NationalParkAdventure,
    model=OpenAIChat(id="gpt-5-mini"),
)

---

## Search & Retrieval

**URL:** llms-txt#search-&-retrieval

**Contents:**
- How Agents Search Knowledge
- Agentic Search: The Smart Difference
  - Traditional RAG vs. Agentic RAG
- Configuring Search in Agno
  - Types of Search Strategies
- What Affects Search Quality
  - Content Chunking Strategy
  - Embedding Model Quality
  - Practical Configuration

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/overview

Understand how agents intelligently search and retrieve information from knowledge bases to provide accurate, contextual responses.

When an agent needs information to answer a question, it doesn't dump everything into the prompt. Instead, it searches for just the most relevant pieces. This focused approach is what makes knowledge-powered agents both effective and efficientâ€”they get exactly what they need, when they need it.

## How Agents Search Knowledge

Think of an agent's search process like a skilled researcher who knows what to look for and where to find it:

<Steps>
  <Step title="Query Analysis">
    The agent analyzes the user's question to understand what type of
    information would be helpful.
  </Step>

<Step title="Search Strategy">
    Based on the analysis, the system formulates one or more searches (vector,
    keyword, or hybrid).
  </Step>

<Step title="Information Retrieval">
    The knowledge base returns the most relevant content chunks.
  </Step>

<Step title="Context Integration">
    The retrieved information is combined with the original question to generate
    a comprehensive response.
  </Step>
</Steps>

## Agentic Search: The Smart Difference

What makes Agno's approach special? Agents can programmatically decide when to search and how to use results. Think of it as giving your agent the keys to the library instead of handing it a fixed stack of books. You can even plug in custom retrieval logic to match your specific needs.

**Key capabilities:**

* **Automatic Decision Making** - The agent can choose to search when it needs additional informationâ€”or skip it when not necessary.

* **Smart Query Generation** - Implement logic to reformulate queries for better recallâ€”like expanding "vacation" to include "PTO" and "time off."

* **Multi-Step Search** - If the first search isn't enough, run follow-up searches with refined queries.

* **Context Synthesis** - Combine information from multiple results to produce a thorough, grounded answer.

### Traditional RAG vs. Agentic RAG

Here's how they compare in practice:

<Tabs>
  <Tab title="Traditional RAG">
    
  </Tab>

<Tab title="Agentic RAG">
    
  </Tab>
</Tabs>

## Configuring Search in Agno

You configure search behavior on your vector database, and Knowledge uses those settings when retrieving documents. It's a simple setup:

### Types of Search Strategies

Agno gives you three main approaches. Pick the one that fits your content and how users ask questions:

#### Vector Similarity Search

Finds content by meaning, not just matching words. When you ask "How do I reset my password?", it finds documents about "changing credentials" even though the exact words don't match.

* Your query becomes a vector (list of numbers capturing meaning)
* The system finds content with similar vectors
* Results are ranked by how close the meanings are

**Best for:** Conceptual questions where users might phrase things differently than your docs.

Classic text searchâ€”looks for exact words and phrases in your content. When using PgVector, this leverages Postgres's full-text search under the hood.

* Matches specific words and phrases
* Supports search operators (where your backend allows)
* Works great when users know the exact terminology

**Best for:** Finding specific terms, product names, error codes, or technical identifiers.

The best of both worldsâ€”combines semantic understanding with exact-match precision. This is usually your best bet for production.

* Runs both vector similarity and keyword matching
* Merges results intelligently
* Can add a reranker on top for even better ordering

**Best for:** Most real-world applications where you want both accuracy and flexibility.

<Tip>
  We recommend starting with <b>hybrid search with reranking</b> for strong
  recall and precision.
</Tip>

## What Affects Search Quality

### Content Chunking Strategy

How you split your content matters a lot:

* **Smaller chunks** (200-500 chars): Super precise, but might miss the big picture
* **Larger chunks** (1000-2000 chars): Better context, but less targeted
* **Semantic chunking**: Splits at natural topic boundariesâ€”usually the sweet spot

### Embedding Model Quality

Your embedder is what turns text into vectors that capture meaning:

* **General-purpose** (like OpenAI's text-embedding-3-small): Works well for most content
* **Domain-specific**: Better for specialized fields like medical or legal docs
* **Multilingual**: Essential if you're working in multiple languages

### Practical Configuration

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Agentic RAG">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

## Configuring Search in Agno

You configure search behavior on your vector database, and Knowledge uses those settings when retrieving documents. It's a simple setup:
```

Example 3 (unknown):
```unknown
### Types of Search Strategies

Agno gives you three main approaches. Pick the one that fits your content and how users ask questions:

#### Vector Similarity Search

Finds content by meaning, not just matching words. When you ask "How do I reset my password?", it finds documents about "changing credentials" even though the exact words don't match.

**How it works:**

* Your query becomes a vector (list of numbers capturing meaning)
* The system finds content with similar vectors
* Results are ranked by how close the meanings are

**Best for:** Conceptual questions where users might phrase things differently than your docs.

#### Keyword Search

Classic text searchâ€”looks for exact words and phrases in your content. When using PgVector, this leverages Postgres's full-text search under the hood.

**How it works:**

* Matches specific words and phrases
* Supports search operators (where your backend allows)
* Works great when users know the exact terminology

**Best for:** Finding specific terms, product names, error codes, or technical identifiers.

#### Hybrid Search

The best of both worldsâ€”combines semantic understanding with exact-match precision. This is usually your best bet for production.

**How it works:**

* Runs both vector similarity and keyword matching
* Merges results intelligently
* Can add a reranker on top for even better ordering

**Best for:** Most real-world applications where you want both accuracy and flexibility.
```

Example 4 (unknown):
```unknown
<Tip>
  We recommend starting with <b>hybrid search with reranking</b> for strong
  recall and precision.
</Tip>

## What Affects Search Quality

### Content Chunking Strategy

How you split your content matters a lot:

* **Smaller chunks** (200-500 chars): Super precise, but might miss the big picture
* **Larger chunks** (1000-2000 chars): Better context, but less targeted
* **Semantic chunking**: Splits at natural topic boundariesâ€”usually the sweet spot

### Embedding Model Quality

Your embedder is what turns text into vectors that capture meaning:

* **General-purpose** (like OpenAI's text-embedding-3-small): Works well for most content
* **Domain-specific**: Better for specialized fields like medical or legal docs
* **Multilingual**: Essential if you're working in multiple languages

### Practical Configuration
```

---

## Weave

**URL:** llms-txt#weave

**Contents:**
- Integrating Agno with Weave by WandB
- Prerequisites
- Logging Model Calls with Weave

Source: https://docs.agno.com/integrations/observability/weave

Integrate Agno with Weave by WandB to send traces and gain insights into your agent's performance.

## Integrating Agno with Weave by WandB

[Weave by Weights & Biases (WandB)](https://weave-docs.wandb.ai/) provides a powerful platform for logging and visualizing model calls. By integrating Agno with Weave, you can track and analyze your agent's performance and behavior.

Ensure you have the Weave package installed:

2. **Authentication**
   Go to [WandB](https://wandb.ai) and copy your API key

## Logging Model Calls with Weave

This example demonstrates how to use Weave to log model calls.

```python  theme={null}
import weave
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
2. **Authentication**
   Go to [WandB](https://wandb.ai) and copy your API key
```

Example 2 (unknown):
```unknown
## Logging Model Calls with Weave

This example demonstrates how to use Weave to log model calls.
```

---

## Create agent with PowerPoint skills

**URL:** llms-txt#create-agent-with-powerpoint-skills

powerpoint_agent = Agent(
    name="PowerPoint Creator",
    model=Claude(
        id="claude-sonnet-4-5-20250929",
        skills=[
            {"type": "anthropic", "skill_id": "pptx", "version": "latest"}
        ],
    ),
    instructions=[
        "You are a professional presentation creator with access to PowerPoint skills.",
        "Create well-structured presentations with clear slides and professional design.",
        "Keep text concise - no more than 6 bullet points per slide.",
    ],
    markdown=True,
)

---

## Iterative Workflow

**URL:** llms-txt#iterative-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/basics/workflows/workflow-patterns/iterative-workflow

Quality-driven processes requiring repetition until specific conditions are met

**Example Use-Cases**: Quality improvement loops, retry mechanisms, iterative refinement

Iterative workflows provide controlled repetition with deterministic exit conditions, ensuring consistent quality standards.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=edba198de555846a2ea8b2e5b65c6d8e" alt="Workflows loop steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-loop-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=96d954f1d665b4b01e0fb030c0544504 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f4ce273ba17af6b0b5b417ec2e73385c 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=15eb9ff0487ff79dccaa1a046ad7c32d 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=ac3e8fb6db4326f2e78948c467924f7c 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=45c09671d2bb3190bb14f23ecab28f85 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=05a4004d7c8228caefbacbc21a2efd2f 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=30027b401899598a38a73c6038d1d988" alt="Workflows loop steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-loop-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=aa15df4e2e353012150474b5fa26d5c5 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f411273ea9e60981989e16324940fbc0 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=a71354c338a1520f27797ca6e53dc378 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=cf1495378fc757cd5995b96c734cda71 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=3d32977904938d0cc22407d61c6efd8e 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=e3e1c672c2953455defceb971b803bce 2500w" />

## Developer Resources

* [Loop Steps Workflow](/basics/workflows/usage/loop-steps-workflow)

For complete API documentation, see [Loop Steps Reference](/reference/workflows/loop-steps).

---

## Distributed RAG with Advanced Reranking

**URL:** llms-txt#distributed-rag-with-advanced-reranking

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/distributed-rag-with-reranking

This example demonstrates how multiple specialized agents coordinate to provide comprehensive RAG responses using advanced reranking strategies for optimal information retrieval and synthesis. The team includes initial retrieval, reranking optimization, context analysis, and final synthesis.

```python cookbook/examples/teams/distributed_rag/03_distributed_rag_with_reranking.py theme={null}
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses using advanced reranking strategies for optimal
information retrieval and synthesis.

Team Composition:
- Initial Retriever: Performs broad initial retrieval from knowledge base
- Reranking Specialist: Applies advanced reranking for result optimization
- Context Analyzer: Analyzes context and relevance of reranked results
- Final Synthesizer: Synthesizes reranked results into optimal responses

Setup:
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno`
2. Run this script to see advanced reranking RAG in action
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker import CohereReranker
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.print_response.team import aprint_response, print_response
from agno.vectordb.lancedb import LanceDb, SearchType

---

## Get all content with pagination

**URL:** llms-txt#get-all-content-with-pagination

contents, total_count = knowledge.get_content(
    limit=20,
    page=1,
    sort_by="created_at",
    sort_order="desc"
)

---

## Async Agent with Streaming

**URL:** llms-txt#async-agent-with-streaming

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/vllm/usage/async-basic-stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Libraries">
    
  </Step>

<Step title="Start vLLM server">
    
  </Step>

<Step title="Run Agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start vLLM server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Get the stored Agent session, to check the response citations

**URL:** llms-txt#get-the-stored-agent-session,-to-check-the-response-citations

**Contents:**
- Usage

session = agent.get_session()
if session and session.runs and session.runs[-1].citations:
    print("Citations:")
    print(session.runs[-1].citations)

bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/openai/responses/pdf_input_url.py
      bash Windows theme={null}
      python cookbook/models/openai/responses/pdf_input_url.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Basic Agent Instructions

**URL:** llms-txt#basic-agent-instructions

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/context/agent/usage/instructions

This example demonstrates how to provide basic instructions to an agent to guide its response behavior and storytelling style.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Introduction

**URL:** llms-txt#introduction

**Contents:**
- Setup
- Examples

Source: https://docs.agno.com/examples/getting-started/introduction

This guide walks through the basics of building Agents with Agno.

The examples build on each other, introducing new concepts and capabilities progressively. Each example contains detailed comments, example prompts, and required dependencies.

Create a virtual environment:

Install the required dependencies:

Export your OpenAI API key:

<CardGroup cols={3}>
  <Card title="Basic Agent" icon="robot" iconType="duotone" href="./01-basic-agent">
    Build a news reporter with a vibrant personality. This Agent only shows basic LLM inference.
  </Card>

<Card title="Agent with Tools" icon="toolbox" iconType="duotone" href="./02-agent-with-tools">
    Add web search capabilities using DuckDuckGo for real-time information gathering.
  </Card>

<Card title="Agent with Knowledge" icon="brain" iconType="duotone" href="./03-agent-with-knowledge">
    Add a vector database to your agent to store and search knowledge.
  </Card>

<Card title="Agent with Storage" icon="database" iconType="duotone" href="./06-agent-with-storage">
    Add persistence to your agents with session management and history capabilities.
  </Card>

<Card title="Agent Team" icon="users" iconType="duotone" href="./17-agent-team">
    Create an agent team specializing in market research and financial analysis.
  </Card>

<Card title="Structured Output" icon="code" iconType="duotone" href="./05-structured-output">
    Generate a structured output using a Pydantic model.
  </Card>

<Card title="Custom Tools" icon="wrench" iconType="duotone" href="./04-write-your-own-tool">
    Create and integrate custom tools with your agent.
  </Card>

<Card title="Research Agent" icon="magnifying-glass" iconType="duotone" href="./18-research-agent-exa">
    Build an AI research agent using Exa with controlled output steering.
  </Card>

<Card title="Research Workflow" icon="diagram-project" iconType="duotone" href="./19-blog-generator-workflow">
    Create a research workflow combining web searches and content scraping.
  </Card>

<Card title="Image Agent" icon="image" iconType="duotone" href="./13-image-agent">
    Create an agent that can understand images.
  </Card>

<Card title="Image Generation" icon="paintbrush" iconType="duotone" href="./14-image-generation">
    Create an Agent that can generate images using DALL-E.
  </Card>

<Card title="Video Generation" icon="video" iconType="duotone" href="./15-video-generation">
    Create an Agent that can generate videos using ModelsLabs.
  </Card>

<Card title="Audio Agent" icon="microphone" iconType="duotone" href="./16-audio-agent">
    Create an Agent that can process audio input and generate responses.
  </Card>

<Card title="Agent with State" icon="database" iconType="duotone" href="./07-agent-state">
    Create an Agent with session state management.
  </Card>

<Card title="Agent Context" icon="sitemap" iconType="duotone" href="./08-agent-context">
    Evaluate dependencies at agent.run and inject them into the instructions.
  </Card>

<Card title="Agent Session" icon="clock-rotate-left" iconType="duotone" href="./09-agent-session">
    Create an Agent with persistent session memory across conversations.
  </Card>

<Card title="User Memories" icon="memory" iconType="duotone" href="./10-user-memories-and-summaries">
    Create an Agent that stores user memories and summaries.
  </Card>

<Card title="Function Retries" icon="rotate" iconType="duotone" href="./11-retry-function-call">
    Handle function retries for failed or unsatisfactory outputs.
  </Card>

<Card title="Human in the Loop" icon="user-check" iconType="duotone" href="./12-human-in-the-loop">
    Add user confirmation and safety checks for interactive agent control.
  </Card>
</CardGroup>

Each example includes runnable code and detailed explanations. We recommend following them in order, as concepts build upon previous examples.

**Examples:**

Example 1 (unknown):
```unknown
Install the required dependencies:
```

Example 2 (unknown):
```unknown
Export your OpenAI API key:
```

---

## Model Inheritance

**URL:** llms-txt#model-inheritance

Source: https://docs.agno.com/basics/teams/usage/other/model-inheritance

This example demonstrates how agents automatically inherit the model from their parent team.

**When the Team has a model:**

* Agents without a model use the Team's `model`
* Agents with their own model keep their own model
* In nested teams, agents use the `model` from their direct parent team
* The `reasoning_model`, `parser_model`, and `output_model` must be set explicitly on each team member or team

**When the Team has no model:**

* The Team and all agents default to OpenAI `gpt-4o`

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Use the function to log a model call

**URL:** llms-txt#use-the-function-to-log-a-model-call

**Contents:**
- Notes

run("Share a 2 sentence horror story")
```

* **Environment Variables**: Ensure your environment variable is correctly set for the WandB API key.
* **Initialization**: Call `weave.init("project-name")` to initialize Weave with your project name.
* **Decorators**: Use `@weave.op()` to decorate functions you want to log with Weave.

By following these steps, you can effectively integrate Agno with Weave, enabling comprehensive logging and visualization of your AI agents' model calls.

---

## Set environment variables for Arize Phoenix

**URL:** llms-txt#set-environment-variables-for-arize-phoenix

os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('ARIZE_PHOENIX_API_KEY')}"
os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "https://app.phoenix.arize.com"

---

## Share Memory and History between Agents

**URL:** llms-txt#share-memory-and-history-between-agents

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/memory/agent/usage/share-memory-and-history-between-agents

This example shows how to share memory and history between agents.

You can set `add_history_to_context=True` to add the history to the context of the agent.

You can set `enable_user_memories=True` to enable user memory generation at the end of each run.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## HackerNews Team

**URL:** llms-txt#hackernews-team

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code
- What to Expect
- Usage
- Next Steps

Source: https://docs.agno.com/examples/use-cases/teams/hackernews_team

Build a team that aggregates, curates, and analyzes trending HackerNews stories. This example combines multiple specialized agents to research topics, read articles, and generate comprehensive summaries with structured output.

By building this team, you'll understand:

* How to integrate multiple data sources (HackerNews API, web search, article readers)
* How to define structured output schemas using Pydantic models
* How to coordinate agents with specific instructions for complex workflows
* How to combine real-time data with web research for comprehensive analysis

Build news aggregation platforms, trend analysis systems, content curation tools, or automated newsletter generators.

The team coordinates three specialized agents to create detailed articles:

1. **Discover**: HackerNews researcher finds trending stories from HackerNews
2. **Read**: Article reader extracts full content from story URLs
3. **Research**: Web searcher finds additional context and related information
4. **Synthesize**: Team combines all findings into structured articles with summaries and references

The team outputs structured data following a defined schema with title, summary, and reference links.

The team will research HackerNews stories, read the full articles, and search for additional context. Each agent contributes their specialized capability: finding stories, extracting article content, and web research.

The output is a structured Article object with a title, comprehensive summary, and reference links. You'll see responses from all team members showing how they collaborate to gather and synthesize information.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* Modify the query to track specific topics or keywords on HackerNews
* Adjust the `Article` schema to include additional fields like categories or sentiment
* Change the number of stories analyzed in the prompt
* Explore [Input & Output](/basics/input-output/overview) for custom data schemas

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The team will research HackerNews stories, read the full articles, and search for additional context. Each agent contributes their specialized capability: finding stories, extracting article content, and web research.

The output is a structured Article object with a title, comprehensive summary, and reference links. You'll see responses from all team members showing how they collaborate to gather and synthesize information.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Team">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Workflow using Steps with Nested Pattern

**URL:** llms-txt#workflow-using-steps-with-nested-pattern

Source: https://docs.agno.com/basics/workflows/usage/workflow-using-steps-nested

This example demonstrates **Workflows 2.0** nested patterns using `Steps` to encapsulate a complex workflow with conditional parallel execution.

This example demonstrates **Workflows** nested patterns using `Steps` to encapsulate
a complex workflow with conditional parallel execution. It combines `Condition`, `Parallel`,
and `Steps` for modular and adaptive content creation.

```python workflow_using_steps_nested.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

---

## Flash Thinking Agent

**URL:** llms-txt#flash-thinking-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/google/usage/flash-thinking

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setup an Agent focused on coding tasks, with access to the Daytona tools

**URL:** llms-txt#setup-an-agent-focused-on-coding-tasks,-with-access-to-the-daytona-tools

agent = Agent(
    name="Coding Agent with Daytona tools",
    id="coding-agent",
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[daytona_tools],
    markdown=True,
        instructions=[
        "You are an expert at writing and validating Python code. You have access to a remote, secure Daytona sandbox.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the Daytona sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "You can use the run_python_code tool to run Python code in the Daytona sandbox.",
        "Guidelines:",
        "- ALWAYS share the complete code with the user, properly formatted in code blocks",
        "- Verify code functionality by executing it in the sandbox before sharing",
        "- Iterate and debug code as needed to ensure it works correctly",
        "- Use pandas, matplotlib, and other Python libraries for data analysis when appropriate",
        "- Create proper visualizations when requested and add them as image artifacts to show inline",
        "- Handle file uploads and downloads properly",
        "- Explain your approach and the code's functionality in detail",
        "- Format responses with both code and explanations for maximum clarity",
        "- Handle errors gracefully and explain any issues encountered",
    ],
)

---

## json_mode_agent.print_response("New York")

**URL:** llms-txt#json_mode_agent.print_response("new-york")

**Contents:**
- Usage

bash  theme={null}
    export DEEPSEEK_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/deepseek/structured_output.py
      bash Windows theme={null}
      python cookbook/models/deepseek/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add topics from Wikipedia synchronously

**URL:** llms-txt#add-topics-from-wikipedia-synchronously

knowledge.add_content(
    metadata={"source": "wikipedia", "type": "encyclopedia"},
    topics=["Manchester United", "Artificial Intelligence"],
    reader=WikipediaReader(),
)

---

## Spider

**URL:** llms-txt#spider

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/spider

**SpiderTools** is an open source web Scraper & Crawler that returns LLM-ready data. To start using Spider, you need an API key from the [Spider dashboard](https://spider.cloud).

The following example requires the `spider-client` library.

The following agent will run a search query to get the latest news in USA and scrape the first search result. The agent will return the scraped data in markdown format.

| Parameter         | Type             | Default | Description                                             |
| ----------------- | ---------------- | ------- | ------------------------------------------------------- |
| `max_results`     | `Optional[int]`  | `None`  | Default maximum number of results.                      |
| `url`             | `Optional[str]`  | `None`  | Default URL for operations.                             |
| `optional_params` | `Optional[dict]` | `None`  | Additional parameters for operations.                   |
| `enable_search`   | `bool`           | `True`  | Enable web search functionality.                        |
| `enable_scrape`   | `bool`           | `True`  | Enable web scraping functionality.                      |
| `enable_crawl`    | `bool`           | `True`  | Enable web crawling functionality.                      |
| `all`             | `bool`           | `False` | Enable all tools. Overrides individual flags when True. |

| Function | Description                                                                                                                                                                                        |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search` | Searches the web for the given query. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum results. Returns search results in JSON format.         |
| `scrape` | Scrapes the content of a webpage. Parameters include `url` (str) for the URL of the webpage to scrape. Returns markdown of the webpage.                                                            |
| `crawl`  | Crawls the web starting from a URL. Parameters include `url` (str) for the URL to crawl and `limit` (Optional\[int], default=10) for maximum pages to crawl. Returns crawl results in JSON format. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/spider.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/spider_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will run a search query to get the latest news in USA and scrape the first search result. The agent will return the scraped data in markdown format.
```

---

## Control Plane

**URL:** llms-txt#control-plane

**Contents:**
- OS Management
- User Management
  - Inviting Members
  - Member Roles
- General Settings
- Feature Access
- Next Steps

Source: https://docs.agno.com/agent-os/control-plane

The main web interface for interacting with and managing your AgentOS instances

The AgentOS Control Plane is your primary web interface for accessing and managing all AgentOS features. This intuitive dashboard serves as the central hub where you interact with your agents, manage knowledge bases, track sessions, monitor performance, and control user access.

<Frame>
  <img src="https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=47cda181dd5fe3e35b00952cc2fc1e85" alt="AgentOS Control Plane Dashboard" style={{ borderRadius: "0.5rem" }} data-og-width="3477" width="3477" data-og-height="2005" height="2005" data-path="images/agentos-full-screenshot.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?w=280&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=64b3f57b511b5082368b71ab2461fc91 280w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?w=560&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=611e80fc18ec8b4d10f2c01711242bea 560w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?w=840&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=d3d30a7be52757d5d2a444385094d2c0 840w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?w=1100&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=e94a89850b6d5f063a1f5b13c9503002 1100w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?w=1650&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=8a17eadbd7c02cdc2fb6f9e234f4ac61 1650w, https://mintcdn.com/agno-v2/Is_2Bv3MNVYdZh1v/images/agentos-full-screenshot.png?w=2500&fit=max&auto=format&n=Is_2Bv3MNVYdZh1v&q=85&s=2450b803257dcc93c2621c855964fb2b 2500w" />
</Frame>

Connect and inspect your OS runtimes from a single interface. Switch between local development and live production instances, monitor connection health, and configure endpoints for your different environments.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/CnjZpOWVs1q9bnAO/videos/agentos-select-os.mp4?fit=max&auto=format&n=CnjZpOWVs1q9bnAO&q=85&s=c6514be6950c2e9c7b103f071ac85b11" type="video/mp4" data-path="videos/agentos-select-os.mp4" />
  </video>
</Frame>

Manage your organization members and their access to AgentOS features. Configure your organization name, invite team members, and control permissions from a centralized interface.

Add new team members to your organization by entering their email addresses. You can invite multiple users at once by separating emails with commas or pressing Enter/Tab between addresses.

Control what each member can access:

* **Owner**: Full administrative access including billing and member management
* **Member**: Access to AgentOS features and collaboration capabilities

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/CnjZpOWVs1q9bnAO/videos/agentos-invite-member.mp4?fit=max&auto=format&n=CnjZpOWVs1q9bnAO&q=85&s=f54f71b63fd4b2110e211e0d1f0602c6" type="video/mp4" data-path="videos/agentos-invite-member.mp4" />
  </video>
</Frame>

Configure your account preferences and organization settings. Access your profile information, manage billing and subscription details, and adjust organization-wide preferences from a centralized settings interface.

The control plane provides direct access to all main AgentOS capabilities through an intuitive interface:

<Note>
  **Getting Started Tip**: The control plane is your gateway to all AgentOS
  features. Start by connecting your OS instance, then explore each feature
  section to familiarize yourself with the interface.
</Note>

<CardGroup cols={3}>
  <Card title="Chat Interface" icon="comment" href="/agent-os/features/chat-interface">
    Start conversations with your agents and access multi-agent interactions
  </Card>

<Card title="Knowledge Management" icon="book" href="/basics/knowledge/overview">
    Upload and organize documents with search and browsing capabilities
  </Card>

<Card title="Memory System" icon="brain" href="/basics/memory/overview">
    Browse stored memories and search through conversation history
  </Card>

<Card title="Session Tracking" icon="clock" href="/basics/sessions/overview">
    Track and analyze agent interactions and performance
  </Card>

<Card title="Evaluation & Testing" icon="chart-bar" href="/basics/evals/overview">
    Test and evaluate agent performance with comprehensive metrics
  </Card>

<Card title="Metrics & Monitoring" icon="chart-line" href="/basics/sessions/metrics/overview">
    Monitor system performance and usage analytics
  </Card>
</CardGroup>

Ready to get started with the AgentOS control plane? Here's what you need to do:

<CardGroup cols={3}>
  <Card title="Create Your First OS" icon="plus" href="/agent-os/creating-your-first-os">
    Set up a new AgentOS instance from scratch using our templates
  </Card>

<Card title="Connect Your AgentOS" icon="link" href="/agent-os/connecting-your-os">
    Learn how to connect your local development environment to the platform
  </Card>
</CardGroup>

---

## JSON Reader

**URL:** llms-txt#json-reader

Source: https://docs.agno.com/reference/knowledge/reader/json

JSONReader is a reader class that allows you to read data from JSON files.

<Snippet file="json-reader-reference.mdx" />

---

## Example 1: Basic research with simple string

**URL:** llms-txt#example-1:-basic-research-with-simple-string

**Contents:**
- Usage

agent.print_response(
    "Perform a comprehensive research on the current flagship GPUs from NVIDIA, AMD and Intel. Return a table of model name, MSRP USD, TDP watts, and launch date. Include citations for each cell."
)

bash  theme={null}
    export OPENAI_API_KEY=xxx
    export EXA_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai exa_py
    bash Mac theme={null}
      python cookbook/examples/agents/deep_research_agent_exa.py
      bash Windows theme={null}
      python cookbook/examples/agents/deep_research_agent_exa.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create workflow with direct execution

**URL:** llms-txt#create-workflow-with-direct-execution

workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

workflow.print_response("Write about the latest AI developments")
```

This was a synchronous non-streaming example of this pattern. To checkout async and streaming versions, see the cookbooks-

* [Parallel Steps Workflow (sync streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow_stream.py)
* [Parallel Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow.py)
* [Parallel Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_04_workflows_parallel_execution/async/parallel_steps_workflow_stream.py)

---

## Get sample videos from https://www.pexels.com/search/videos/sample/

**URL:** llms-txt#get-sample-videos-from-https://www.pexels.com/search/videos/sample/

**Contents:**
- Usage

video_path = Path(__file__).parent.joinpath("sample_video.mp4")

agent.print_response("Tell me about this video?", videos=[Video(filepath=video_path)])
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/video_input_local_file_upload.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/video_input_local_file_upload.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Aggregate both agents into a multi-agent system

**URL:** llms-txt#aggregate-both-agents-into-a-multi-agent-system

**Contents:**
- Features
  - Observability & Tracing
  - Evaluation & Analytics
  - Alerting
- Notes

multi_ai_team = Team(
    members=[web_search_agent, finance_agent],
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a helpful financial assistant. Answer user questions about stocks, companies, and financial data.",
    markdown=True,
)

if __name__ == "__main__":
    print("Welcome to the Financial Conversational Agent! Type 'exit' to quit.")
    messages = []
    while True:
        print("********************************")
        user_input = input("You: ")
        if user_input.strip().lower() in ["exit", "quit"]:
            print("Goodbye!")
            break
        messages.append({"role": "user", "content": user_input})
        conversation = "\n".join(
            [
                ("User: " + m["content"])
                if m["role"] == "user"
                else ("Agent: " + m["content"])
                for m in messages
            ]
        )
        response = multi_ai_team.run(
            f"Conversation so far:\n{conversation}\n\nRespond to the latest user message."
        )
        agent_reply = getattr(response, "content", response)
        print("---------------------------------")
        print("Agent:", agent_reply)
        messages.append({"role": "agent", "content": str(agent_reply)})
python  theme={null}
  instrument_agno(Maxim().logger(), {"debug" : True})
  ```
* **Maxim Docs**: For more information on Maxim's features and capabilities, refer to the [Maxim documentation](https://getmaxim.ai/docs).

By following these steps, you can effectively integrate Agno with Maxim, enabling comprehensive observability, evaluation, and monitoring of your AI agents.

**Examples:**

Example 1 (unknown):
```unknown
<img src="https://mintcdn.com/agno-v2/7E-fsqZkCqV5M6b3/images/maxim.gif?s=2269ee92857eb7024d3a8fe6f836fa54" alt="agno.gif" data-og-width="1280" width="1280" data-og-height="720" height="720" data-path="images/maxim.gif" data-optimize="true" data-opv="3" />

## Features

### Observability & Tracing

Maxim provides comprehensive observability for your Agno agents:

* **Agent Tracing**: Track your agent's complete lifecycle, including tool calls, agent trajectories, and decision flows
* **Token Usage**: Monitor prompt and completion token consumption
* **Model Information**: Track which models are being used and their performance
* **Tool Calls**: Detailed logging of all tool executions and their results
* **Performance Metrics**: Latency, cost, and error rate tracking

### Evaluation & Analytics

* **Auto Evaluations**: Automatically evaluate captured logs based on filters and sampling
* **Human Evaluations**: Use human evaluation or rating to assess log quality
* **Node Level Evaluations**: Evaluate any component of your trace for detailed insights
* **Dashboards**: Visualize traces over time, usage metrics, latency & error rates

### Alerting

Set thresholds on error rates, cost, token usage, user feedback, and latency to get real-time alerts via Slack or PagerDuty.

## Notes

* **Environment Variables**: Ensure your environment variables are correctly set for the API key and repository ID.
* **Instrumentation Order**: Call `instrument_agno()` **before** creating or executing any agents to ensure proper tracing.
* **Debug Mode**: Enable debug mode to see detailed logging information:
```

---

## MongoDB for Team

**URL:** llms-txt#mongodb-for-team

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/integrations/database/mongo/usage/mongodb-for-team

Agno supports using MongoDB as a storage backend for Teams using the `MongoDb` class.

You need to provide either `db_url` or `client`. The following example uses `db_url`.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python mongodb_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""
from typing import List

from agno.agent import Agent
from agno.db.mongo import MongoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

---

## WhatsApp Agent with Media Support

**URL:** llms-txt#whatsapp-agent-with-media-support

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/whatsapp/agent-with-media

WhatsApp agent that analyzes images, videos, and audio using multimodal AI

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Multimodal AI**: Gemini 2.0 Flash for image, video, and audio processing
* **Image Analysis**: Object recognition, scene understanding, text extraction
* **Video Processing**: Content analysis and summarization
* **Audio Support**: Voice message transcription and response
* **Context Integration**: Combines media analysis with conversation history

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Agent with Ollama Parser Model

**URL:** llms-txt#agent-with-ollama-parser-model

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/parser-model-ollama

This example demonstrates how to use an Ollama model as a parser for structured output, combining different models for generation and parsing.

```python parser_model_ollama.py theme={null}
import random
from typing import List

from agno.agent import Agent, RunOutput
from agno.models.ollama import Ollama
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint

class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )

agent = Agent(
    model=OpenAIChat(id="o3"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    output_schema=NationalParkAdventure,
    parser_model=Ollama(id="Osmosis/Osmosis-Structure-0.6B"),
)

national_parks = [
    "Yellowstone National Park",
    "Yosemite National Park",
    "Grand Canyon National Park",
    "Zion National Park",
    "Grand Teton National Park",
    "Rocky Mountain National Park",
    "Acadia National Park",
    "Mount Rainier National Park",
    "Great Smoky Mountains National Park",
    "Rocky National Park",
]

---

## Thinking Agent

**URL:** llms-txt#thinking-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/dashscope/usage/thinking-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Send a direct message

**URL:** llms-txt#send-a-direct-message

agent.print_response(
    "Send direct message to the user @AgnoAgi telling them I want to learn more about them and a link to their community.",
    markdown=True,
)

---

## Notion Tools

**URL:** llms-txt#notion-tools

**Contents:**
- Prerequisites
  - Step 1: Create a Notion Integration
  - Step 2: Create a Notion Database
  - Step 3: Share Database with Your Integration
  - Step 4: Get Your Database ID
- Example

Source: https://docs.agno.com/integrations/toolkits/others/notion

The NotionTools toolkit enables Agents to interact with your Notion pages.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.6" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.6">v2.2.6</Tooltip>
</Badge>

Notion is a powerful workspace management tool that allows you to create, organize, and collaborate on your work.

Agno provides tools for interacting with Notion databases to help you automate your workflows and streamline your work.

To use `NotionTools`, you need to install `notion-client`:

### Step 1: Create a Notion Integration

1. Go to [https://www.notion.so/my-integrations](https://www.notion.so/my-integrations)
2. Click on **"+ New integration"**
3. Fill in the details:
   * **Name**: Give it a name like "Agno Query Classifier"
   * **Associated workspace**: Select your workspace
   * **Type**: Internal integration
4. Click **"Submit"**
5. Copy the **"Internal Integration Token"** (starts with `secret_`)
   * âš ï¸ Keep this secret! This is your `NOTION_API_KEY`

### Step 2: Create a Notion Database

1. Open Notion and create a new page
2. Add a **Database** (you can use "/database" command)
3. Set up the database with these properties:
   * **Name** (Title) - Already exists by default
   * **Tag** (Select) - Click "+" to add a new property
     * Property type: **Select**
     * Property name: **Tag**
     * Add these options:
       * travel
       * tech
       * general-blogs
       * fashion
       * documents

### Step 3: Share Database with Your Integration

1. Open your database page in Notion
2. Click the **"..."** (three dots) menu in the top right
3. Scroll down and click **"Add connections"**
4. Search for your integration name (e.g., "Agno Query Classifier")
5. Click on it to grant access

### Step 4: Get Your Database ID

Your database ID is in the URL of your database page:

The `database_id` is the 32-character string (with hyphens) between the workspace name and the `?v=`.

Copy this database ID.

The following example demonstrates how to use `NotionTools` to create, update and search for Notion pages with specific tags.

```python  theme={null}
from agno.agent import Agent
from agno.tools.notion import NotionTools

**Examples:**

Example 1 (unknown):
```unknown
### Step 1: Create a Notion Integration

1. Go to [https://www.notion.so/my-integrations](https://www.notion.so/my-integrations)
2. Click on **"+ New integration"**
3. Fill in the details:
   * **Name**: Give it a name like "Agno Query Classifier"
   * **Associated workspace**: Select your workspace
   * **Type**: Internal integration
4. Click **"Submit"**
5. Copy the **"Internal Integration Token"** (starts with `secret_`)
   * âš ï¸ Keep this secret! This is your `NOTION_API_KEY`

### Step 2: Create a Notion Database

1. Open Notion and create a new page
2. Add a **Database** (you can use "/database" command)
3. Set up the database with these properties:
   * **Name** (Title) - Already exists by default
   * **Tag** (Select) - Click "+" to add a new property
     * Property type: **Select**
     * Property name: **Tag**
     * Add these options:
       * travel
       * tech
       * general-blogs
       * fashion
       * documents

### Step 3: Share Database with Your Integration

1. Open your database page in Notion
2. Click the **"..."** (three dots) menu in the top right
3. Scroll down and click **"Add connections"**
4. Search for your integration name (e.g., "Agno Query Classifier")
5. Click on it to grant access

### Step 4: Get Your Database ID

Your database ID is in the URL of your database page:
```

Example 2 (unknown):
```unknown
The `database_id` is the 32-character string (with hyphens) between the workspace name and the `?v=`.

Example:
```

Example 3 (unknown):
```unknown
Copy this database ID.
```

Example 4 (unknown):
```unknown
## Example

The following example demonstrates how to use `NotionTools` to create, update and search for Notion pages with specific tags.
```

---

## Generate Video using Replicate

**URL:** llms-txt#generate-video-using-replicate

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/video/usage/generate-video-replicate

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom Toolkits

**URL:** llms-txt#custom-toolkits

Source: https://docs.agno.com/basics/tools/creating-tools/toolkits

Learn how to write your own toolkits.

Many advanced use-cases will require writing custom Toolkits. A Toolkit is a collection of functions that can be added to an Agent. The functions in a Toolkit are designed to work together, share internal state and provide a better development experience.

Here's the general flow:

1. Create a class inheriting the `agno.tools.Toolkit` class.
2. Add your functions to the class.
3. Include all the functions in the `tools` argument to the `Toolkit` constructor.

<Tip>
  Important Tips:

* Fill in the docstrings for each function with detailed descriptions of the function and its arguments.
  * Remember that this function is provided to the LLM and is not used elsewhere in code, so the docstring should make sense to an LLM and the name of the functions need to be descriptive.
</Tip>

See the [Toolkit Reference](/reference/tools/toolkit) for more details.

---

## Create agents

**URL:** llms-txt#create-agents

researcher = Agent(name="Researcher", tools=[HackerNewsTools(), DuckDuckGoTools()])
writer = Agent(name="Writer")
reviewer = Agent(name="Reviewer")

---

## Run periodically or before high-cost operations

**URL:** llms-txt#run-periodically-or-before-high-cost-operations

**Contents:**
- Mitigation Strategy #5: Set Tool Call Limits
- Common Pitfalls
  - The user\_id Pitfall

prune_old_memories(db, user_id="john_doe@example.com")
python  theme={null}
agent = Agent(
    db=db,
    enable_agentic_memory=True,
    tool_call_limit=5  # Prevents excessive memory operations
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Mitigation Strategy #5: Set Tool Call Limits

Prevent runaway memory operations by limiting tool calls per conversation:
```

Example 2 (unknown):
```unknown
## Common Pitfalls

### The user\_id Pitfall

**The Problem:** Forgetting to set `user_id` causes all memories to default to `user_id="default"`, mixing different users' memories together.
```

---

## Context Expander Agent - Specialized in expanding context

**URL:** llms-txt#context-expander-agent---specialized-in-expanding-context

context_expander = Agent(
    name="Context Expander",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Expand context by finding related and supplementary information",
    knowledge=context_knowledge,
    search_knowledge=True,
    instructions=[
        "Find related information that complements the primary retrieval.",
        "Look for background context, related topics, and supplementary details.",
        "Search for information that helps understand the broader context.",
        "Identify connections between different pieces of information.",
    ],
    markdown=True,
)

---

## Will load the session state from the session with the id "user_1_session_1"

**URL:** llms-txt#will-load-the-session-state-from-the-session-with-the-id-"user_1_session_1"

team.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

---

## After resolving all requirements, you can continue the run:

**URL:** llms-txt#after-resolving-all-requirements,-you-can-continue-the-run:

**Contents:**
- Streaming Human-in-the-Loop Flows
- Learn More
- Developer Resources

response = agent.continue_run(run_id=run_response.run_id, requirements=run_response.requirements)
python  theme={null}
response = agent.continue_run(run_response=run_response)
python  theme={null}
for run_event in agent.run("Perform sensitive operation", stream=True):
    if run_event.is_paused:
        for requirement in run_event.active_requirements:
            # You handle any active requirements here

response = agent.continue_run(run_id=run_event.run_id, requirements=run_event.requirements, stream=True)
```

<Note>
  You can also stream the events resulting from calling the `continue_run` or `acontinue_run` methods.
</Note>

<CardGroup cols={3}>
  <Card title="User Confirmation" icon="circle-check" href="/basics/hitl/user-confirmation">
    Require explicit user approval before executing tool calls
  </Card>

<Card title="User Input" icon="keyboard" href="/basics/hitl/user-input">
    Gather specific information from users during execution
  </Card>

<Card title="Dynamic User Input" icon="comments" href="/basics/hitl/dynamic-user-input">
    Let agents request user input dynamically when needed
  </Card>

<Card title="External Tool Execution" icon="plug" href="/basics/hitl/external-execution">
    Execute tools outside of the agent's control
  </Card>
</CardGroup>

## Developer Resources

* View more [Examples](/basics/hitl/usage/)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop)

**Examples:**

Example 1 (unknown):
```unknown
The `continue_run` method continues with the state of the agent at the time of the pause.

You can also call the `continue_run` method passing the `RunOutput` of the specific run to continue:
```

Example 2 (unknown):
```unknown
## Streaming Human-in-the-Loop Flows

You can also stream the responses you get during a Human-in-the-Loop flow. This is useful when you want to process or show the response in real-time.

It works similarly to non-streaming. You will just need to handle the events you get from streaming the Agent run.

If any event is paused, then you will need to handle the active requirements:
```

---

## Single shared database for everything

**URL:** llms-txt#single-shared-database-for-everything

db = SqliteDb(db_file="tmp/agno.db")

agent = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,  # Uses shared db
)

---

## Video Caption Generator Agent

**URL:** llms-txt#video-caption-generator-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/video-caption-agent

This example demonstrates how to create an agent that can process videos to generate and embed captions using MoviePy and OpenAI tools.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Create Ollama embedder

**URL:** llms-txt#create-ollama-embedder

embedder = OllamaEmbedder(id="nomic-embed-text", dimensions=768)

---

## Normal conversation: 10 Ã— 500 tokens = 5,000 tokens

**URL:** llms-txt#normal-conversation:-10-Ã—-500-tokens-=-5,000-tokens

---

## Use the cached content - no need to resend the file

**URL:** llms-txt#use-the-cached-content---no-need-to-resend-the-file

**Contents:**
- Thinking Models
- Structured Outputs
- Tool Use
- Params

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001", cached_content=cache.name),
)
run_output = agent.run("Find a lighthearted moment from this transcript")
python  theme={null}
from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.5-pro", thinking_budget=1280, include_thoughts=True),
    markdown=True,
)

agent.print_response("Solve this logic puzzle...")
python  theme={null}
agent = Agent(
    model=Gemini(id="gemini-3-pro-preview", thinking_level="low"),  # "low" or "high"
    markdown=True,
)
python  theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from pydantic import BaseModel

class MovieScript(BaseModel):
    name: str
    genre: str
    storyline: str

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    output_schema=MovieScript,
)
python  theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response("Whats happening in France?")
```

Read more about tool use [here](/integrations/models/native/google/usage/tool-use).

| Parameter                     | Type                       | Default                  | Description                                                          |
| ----------------------------- | -------------------------- | ------------------------ | -------------------------------------------------------------------- |
| `id`                          | `str`                      | `"gemini-2.0-flash-001"` | The id of the Gemini model to use                                    |
| `name`                        | `str`                      | `"Gemini"`               | The name of the model                                                |
| `provider`                    | `str`                      | `"Google"`               | The provider of the model                                            |
| `api_key`                     | `Optional[str]`            | `None`                   | Google API key (defaults to `GOOGLE_API_KEY` env var)                |
| `vertexai`                    | `bool`                     | `False`                  | Use Vertex AI instead of AI Studio                                   |
| `project_id`                  | `Optional[str]`            | `None`                   | Google Cloud project ID for Vertex AI                                |
| `location`                    | `Optional[str]`            | `None`                   | Google Cloud region for Vertex AI                                    |
| `temperature`                 | `Optional[float]`          | `None`                   | Controls randomness in the model's output                            |
| `top_p`                       | `Optional[float]`          | `None`                   | Controls diversity via nucleus sampling                              |
| `top_k`                       | `Optional[int]`            | `None`                   | Controls diversity via top-k sampling                                |
| `max_output_tokens`           | `Optional[int]`            | `None`                   | Maximum number of tokens to generate                                 |
| `stop_sequences`              | `Optional[list[str]]`      | `None`                   | Sequences where the model should stop generating                     |
| `seed`                        | `Optional[int]`            | `None`                   | Random seed for reproducibility                                      |
| `logprobs`                    | `Optional[bool]`           | `None`                   | Whether to return log probabilities of output tokens                 |
| `presence_penalty`            | `Optional[float]`          | `None`                   | Penalizes new tokens based on whether they appear in the text so far |
| `frequency_penalty`           | `Optional[float]`          | `None`                   | Penalizes new tokens based on their frequency in the text so far     |
| `search`                      | `bool`                     | `False`                  | Enable Google Search grounding                                       |
| `grounding`                   | `bool`                     | `False`                  | Enable legacy grounding (use `search` for 2.0+)                      |
| `grounding_dynamic_threshold` | `Optional[float]`          | `None`                   | Dynamic threshold for grounding                                      |
| `url_context`                 | `bool`                     | `False`                  | Enable URL context extraction                                        |
| `vertexai_search`             | `bool`                     | `False`                  | Enable Vertex AI Search                                              |
| `vertexai_search_datastore`   | `Optional[str]`            | `None`                   | Vertex AI Search datastore path                                      |
| `file_search_store_names`     | `Optional[list[str]]`      | `None`                   | File Search store names for RAG                                      |
| `file_search_metadata_filter` | `Optional[str]`            | `None`                   | Metadata filter for File Search                                      |
| `response_modalities`         | `Optional[list[str]]`      | `None`                   | Output types: `"TEXT"`, `"IMAGE"`, `"AUDIO"`                         |
| `speech_config`               | `Optional[dict]`           | `None`                   | TTS voice configuration                                              |
| `thinking_budget`             | `Optional[int]`            | `None`                   | Token budget for reasoning (Gemini 2.5+)                             |
| `include_thoughts`            | `Optional[bool]`           | `None`                   | Include thought summaries in response                                |
| `thinking_level`              | `Optional[str]`            | `None`                   | Thinking intensity: `"low"` or `"high"`                              |
| `cached_content`              | `Optional[Any]`            | `None`                   | Reference to cached context                                          |
| `safety_settings`             | `Optional[list]`           | `None`                   | Content safety configuration                                         |
| `function_declarations`       | `Optional[List[Any]]`      | `None`                   | List of function declarations for the model                          |
| `generation_config`           | `Optional[Any]`            | `None`                   | Custom generation configuration                                      |
| `generative_model_kwargs`     | `Optional[Dict[str, Any]]` | `None`                   | Additional keyword arguments for the generative model                |
| `request_params`              | `Optional[Dict[str, Any]]` | `None`                   | Additional parameters for the request                                |
| `client_params`               | `Optional[Dict[str, Any]]` | `None`                   | Additional parameters for client configuration                       |

`Gemini` is a subclass of the [Model](/reference/models/model) class and has access to the same params.

**Examples:**

Example 1 (unknown):
```unknown
## Thinking Models

Gemini 2.5+ models support extended thinking for complex reasoning tasks. See [Google's thinking documentation](https://ai.google.dev/gemini-api/docs/thinking) for more details.
```

Example 2 (unknown):
```unknown
You can also use `thinking_level` for simpler control:
```

Example 3 (unknown):
```unknown
Read more about thinking models [here](/integrations/models/native/google/usage/flash-thinking).

## Structured Outputs

Gemini supports native structured outputs using Pydantic models:
```

Example 4 (unknown):
```unknown
Read more about structured outputs [here](/integrations/models/native/google/usage/structured-output).

## Tool Use

Gemini supports function calling to interact with external tools and APIs:
```

---

## app.router.routes.append(route)

**URL:** llms-txt#app.router.routes.append(route)

**Contents:**
- Middleware and Dependencies

app = agent_os.get_app()

if __name__ == "__main__":
    """Run the AgentOS application.

You can see the docs at:
    http://localhost:7777/docs

"""
    agent_os.serve(app="custom_fastapi_app:app", reload=True)
python  theme={null}
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer

**Examples:**

Example 1 (unknown):
```unknown
## Middleware and Dependencies

You can add middleware and dependencies to your custom FastAPI app:
```

---

## Mistral Embedder

**URL:** llms-txt#mistral-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/mistral/usage/mistral-embedder

```python  theme={null}
import asyncio

from agno.knowledge.embedder.mistral import MistralEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = MistralEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## "I don't pain anymore, i draw instead.", stream=True, user_id=john_doe_id

**URL:** llms-txt#"i-don't-pain-anymore,-i-draw-instead.",-stream=true,-user_id=john_doe_id

---

## Agent with Metrics

**URL:** llms-txt#agent-with-metrics

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/gateways/groq/usage/metrics

```python cookbook/models/groq/metrics.py theme={null}
from agno.agent import Agent, RunOutput
from agno.models.groq import Groq
from agno.tools.yfinance import YFinanceTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
)

run_output: RunOutput = agent.run("What is the stock price of NVDA")
pprint_run_response(run_output)

---

## 1. Send a request that requires confirmation

**URL:** llms-txt#1.-send-a-request-that-requires-confirmation

curl -X POST http://localhost:7777/agents/data_manager/runs \
  -F "message=Delete 50 old records from the users table" \
  -F "user_id=test_user" \
  -F "session_id=test_session"

---

## Initialize Weave with your project name

**URL:** llms-txt#initialize-weave-with-your-project-name

---

## Get Evaluation Run

**URL:** llms-txt#get-evaluation-run

Source: https://docs.agno.com/reference-api/schema/evals/get-evaluation-run

get /eval-runs/{eval_run_id}
Retrieve detailed results and metrics for a specific evaluation run.

---

## Note: Replace '''hello_world''' with your actual template name

**URL:** llms-txt#note:-replace-'''hello_world'''-with-your-actual-template-name

---

## The agent will first search for relevant URLs, then analyze their content in detail

**URL:** llms-txt#the-agent-will-first-search-for-relevant-urls,-then-analyze-their-content-in-detail

**Contents:**
- Usage

agent.print_response(
    "Analyze the content of the following URL: https://docs.agno.com/introduction and also give me latest updates on AI agents"
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/url_context_with_search.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/url_context_with_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Standalone Memory

**URL:** llms-txt#standalone-memory

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/standalone-memory

```python standalone_memory.py theme={null}
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager, UserMemory
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

memory = MemoryManager(db=PostgresDb(db_url=db_url))

---

## Format & Validate

**URL:** llms-txt#format-&-validate

**Contents:**
- Format
- Validate

Source: https://docs.agno.com/templates/infra-management/format-and-validate

Formatting the codebase using a set standard saves us time and mental energy. Agno templates are pre-configured with [ruff](https://docs.astral.sh/ruff/) that you can run using a helper script or directly.

Linting and Type Checking add an extra layer of protection to the codebase. We highly recommending running the validate script before pushing any changes.

Agno templates are pre-configured with [ruff](https://docs.astral.sh/ruff/) and [mypy](https://mypy.readthedocs.io/en/stable/) that you can run using a helper script or directly. Checkout the `pyproject.toml` file for the configuration.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Validate

Linting and Type Checking add an extra layer of protection to the codebase. We highly recommending running the validate script before pushing any changes.

Agno templates are pre-configured with [ruff](https://docs.astral.sh/ruff/) and [mypy](https://mypy.readthedocs.io/en/stable/) that you can run using a helper script or directly. Checkout the `pyproject.toml` file for the configuration.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Primary Retriever Agent - Specialized in main document retrieval

**URL:** llms-txt#primary-retriever-agent---specialized-in-main-document-retrieval

primary_retriever = Agent(
    name="Primary Retriever",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Retrieve primary documents and core information from knowledge base",
    knowledge=primary_knowledge,
    search_knowledge=True,
    instructions=[
        "Search the knowledge base for directly relevant information to the user's query.",
        "Focus on retrieving the most relevant and specific documents first.",
        "Provide detailed information with proper context.",
        "Ensure accuracy and completeness of retrieved information.",
    ],
    markdown=True,
)

---

## Retrieve the name

**URL:** llms-txt#retrieve-the-name

**Contents:**
  - Auto-Generation
- Next Steps
- Developer Resources

name = workflow.get_session_name(session_id="session_123")
print(name)  # "AI Trends Analysis Q4 2024"
python  theme={null}
workflow = Workflow(
    name="Research Pipeline",
    description="Automated research and analysis pipeline",
    db=SqliteDb(db_file="workflows.db"),
    steps=[...],
)

workflow.run(input="Research topic", session_id="session_123")
workflow.set_session_name(session_id="session_123", autogenerate=True)

name = workflow.get_session_name(session_id="session_123")
print(name)  # "Automated research and analysis pipel - 2024-11-19 14:30"
```

Now that you understand workflow sessions, explore these features:

<CardGroup cols={2}>
  <Card title="Workflow History" icon="clock-rotate-left" href="/basics/chat-history/workflow/overview">
    Enable workflows to learn from previous runs
  </Card>

<Card title="Session State" icon="database" href="/basics/state/workflows/overview">
    Share data between workflow steps across runs
  </Card>

<Card title="Session Management" icon="tag" href="/basics/sessions/session-management">
    Manage session names, IDs, and organization
  </Card>

<Card title="Conversational Workflows" icon="comments" href="/basics/workflows/conversational-workflows">
    Make workflows interactive with WorkflowAgent
  </Card>

<Card title="Metrics" icon="chart-line" href="/basics/sessions/metrics/workflow">
    Track workflow performance and usage
  </Card>

<Card title="Database Setup" icon="database" href="/basics/database/overview">
    Configure databases for session storage
  </Card>
</CardGroup>

## Developer Resources

* View the [Workflow schema](/reference/workflows/workflow)
* View the [WorkflowSession schema](/reference/workflows/session)
* Browse [workflow session examples](https://github.com/agno-agi/agno/tree/main/cookbook/workflows)

**Examples:**

Example 1 (unknown):
```unknown
### Auto-Generation

Workflow sessions can auto-generate timestamp-based names:
```

---

## Agent Metrics

**URL:** llms-txt#agent-metrics

**Contents:**
- Example Usage

Source: https://docs.agno.com/basics/sessions/metrics/agent

Learn about agent run and session metrics.

When you run an agent in Agno, the response you get (**RunOutput**) includes detailed metrics about the run. These metrics help you understand resource usage (like **token usage** and **time**), performance, and other aspects of the model and tool calls.

Metrics are available at multiple levels:

* **Per message**: Each message (assistant, tool, etc.) has its own metrics.
* **Per run**: Each `RunOutput` has its own metrics.
* **Per session**: The `AgentSession` contains aggregated `session_metrics` that are the sum of all `RunOutput.metrics` for the session.

Suppose you have an agent that performs some tasks and you want to analyze the metrics after running it. Here's how you can access and print the metrics:

You run the following code to create an agent and run it with the following configuration:

```python  theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.db.sqlite import SqliteDb
from rich.pretty import pprint

agent = Agent(
    model=Gemini(id="gemini-2.5-flash"),
    tools=[DuckDuckGoTools()],
    db=SqliteDb(db_file="tmp/agents.db"),
    markdown=True,
)

run_response = agent.run(
    "What is current news in the world?"
)

---

## In-Memory Storage for Agents

**URL:** llms-txt#in-memory-storage-for-agents

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/in-memory/usage/in-memory-for-agent

Example using `InMemoryDb` with agent.

```python  theme={null}
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb

---

## Get your Supabase project and password

**URL:** llms-txt#get-your-supabase-project-and-password

SUPABASE_PROJECT = getenv("SUPABASE_PROJECT")
SUPABASE_PASSWORD = getenv("SUPABASE_PASSWORD")

SUPABASE_DB_URL = (
    f"postgresql://postgres:{SUPABASE_PASSWORD}@db.{SUPABASE_PROJECT}:5432/postgres"
)

---

## Company information agent with structured output

**URL:** llms-txt#company-information-agent-with-structured-output

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a stock.",
    output_schema=CompanyAnalysis,
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com", "wsj.com"],
            text=False,
            show_results=True,
            highlights=False,
        )
    ],
)

---

## Initialize vector database

**URL:** llms-txt#initialize-vector-database

vector_db = PgVector(
    table_name="recipes",
    db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
)

---

## Load knowledge from web search

**URL:** llms-txt#load-knowledge-from-web-search

knowledge.add_content(
    topics=["agno"],
    reader=WebSearchReader(
        max_results=3,
        search_engine="duckduckgo",
        chunk=True,
    ),
)

---

## Conditional Steps

**URL:** llms-txt#conditional-steps

Source: https://docs.agno.com/reference/workflows/conditional-steps

| Parameter     | Type                                                                               | Default  | Description                                   |
| ------------- | ---------------------------------------------------------------------------------- | -------- | --------------------------------------------- |
| `evaluator`   | `Union[Callable[[StepInput], bool], Callable[[StepInput], Awaitable[bool]], bool]` | Required | Function or boolean to evaluate the condition |
| `steps`       | `WorkflowSteps`                                                                    | Required | Steps to execute if the condition is met      |
| `name`        | `Optional[str]`                                                                    | `None`   | Name of the condition step                    |
| `description` | `Optional[str]`                                                                    | `None`   | Description of the condition step             |

---

## Web search agent with tool hooks

**URL:** llms-txt#web-search-agent-with-tool-hooks

website_agent = Agent(
    name="Website Agent",
    id="website-agent",
    role="Search the website for information",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools(cache_results=True)],
    instructions=[
        "Search the website for information",
    ],
    tool_hooks=[logger_hook],
)

---

## AgentOS as MCP Server

**URL:** llms-txt#agentos-as-mcp-server

**Contents:**
- Why use MCP?
- Example

Source: https://docs.agno.com/agent-os/mcp/mcp

Learn how and why to expose your AgentOS as an MCP server

Your AgentOS will by default be exposed as an API. But you can also expose it as an **MCP server**.

This is done by setting `enable_mcp_server=True` when creating your AgentOS instance:

The [MCP protocol](https://modelcontextprotocol.io) has become the industry standard to handle connecting AI applications with external tools and data sources.

By exposing your AgentOS as an MCP server, external clients that can handle MCP-compatible applications will be able to connect to your AgentOS and interact with it.

```python enable_mcp_example.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown
## Why use MCP?

The [MCP protocol](https://modelcontextprotocol.io) has become the industry standard to handle connecting AI applications with external tools and data sources.

By exposing your AgentOS as an MCP server, external clients that can handle MCP-compatible applications will be able to connect to your AgentOS and interact with it.

## Example
```

---

## Branching Workflow

**URL:** llms-txt#branching-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/basics/workflows/workflow-patterns/branching-workflow

Complex decision trees requiring dynamic path selection based on content analysis

**Example Use-Cases**: Expert routing, content type detection, multi-path processing

Dynamic routing workflows provide intelligent path selection while maintaining predictable execution within each chosen branch.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=11256e6d5ebe78ee137ba56647bb732c" alt="Workflows router steps diagram" data-og-width="2493" width="2493" data-og-height="921" height="921" data-path="images/workflows-router-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=280&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=bbbf338fb349e3d6e9e66f92873ca74b 280w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=560&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=3c341c1b87faa0eca3092ea8f93c5d0b 560w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=840&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=75dabdfd37b0806915bd56520c176d0a 840w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=1100&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=8e91a1cb88327098c3420a0bd4994e69 1100w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=1650&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=e948b1e5b7f48fde2637265f2daef7f5 1650w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=2500&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=74139a11491c79a755974923831ad406 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=593bc69b1647af6571151c051145e7c6" alt="Workflows router steps diagram" data-og-width="2493" width="2493" data-og-height="921" height="921" data-path="images/workflows-router-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=280&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=575bc73e719bb2ccf703278e5aaaa4b3 280w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=560&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=6886d723d73a9fc1ffec318b2fe33d3c 560w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=840&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=0c364bb81456fc509a64fcac0cb8373a 840w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=1100&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=d9ede7db094389fc173c590ea28aa21c 1100w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=1650&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=117488ab5bcecbf9e982474dd91580e8 1650w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=2500&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=e0f5a19e133076f0ba74ced4f21ba411 2500w" />

## Developer Resources

* [Router Steps Workflow](/basics/workflows/usage/router-steps-workflow)

For complete API documentation, see [Router Steps Reference](/reference/workflows/router-steps).

---

## Get the vector database

**URL:** llms-txt#get-the-vector-database

db = Chroma(embedding_function=OpenAIEmbeddings(), persist_directory=str(chroma_db_dir))

---

## Clean workflow with clear branching

**URL:** llms-txt#clean-workflow-with-clear-branching

media_workflow = Workflow(
    name="AI Media Generation Workflow",
    description="Generate and analyze images or videos using AI agents",
    steps=[
        Router(
            name="Media Type Router",
            description="Routes to appropriate media generation pipeline",
            selector=media_sequence_selector,
            choices=[image_sequence, video_sequence],  # Clear choices
        )
    ],
)

---

## Initialize vector database connection

**URL:** llms-txt#initialize-vector-database-connection

vector_db = Qdrant(
    collection="thai-recipes", url="http://localhost:6333", embedder=embedder
)

---

## LangGraph

**URL:** llms-txt#langgraph

python cookbook/evals/performance/comparison/langgraph_instantiation.py

---

## AgentOS API Overview

**URL:** llms-txt#agentos-api-overview

**Contents:**
- Authentication
- Core Resources

Source: https://docs.agno.com/reference-api/overview

Complete API reference for interacting with AgentOS programmatically

Welcome to the comprehensive API reference for the Agno AgentOS API. This RESTful API enables you to programmatically interact with your AgentOS instance, manage agents, teams, and workflows, and integrate AgentOS capabilities into your applications.

AgentOS supports bearer-token authentication via a single Security Key.

* When `OS_SECURITY_KEY` environment variable is set on the server, all routes require:

* When `OS_SECURITY_KEY` is not set, authentication is disabled for that instance.

See the dedicated guide: [Secure your AgentOS with a Security Key](/agent-os/security).

The AgentOS API is organized around several key resources:

<CardGroup cols={3}>
  <Card title="Agents" icon="robot" href="/reference-api/schema/agents/list-all-agents">
    Create, manage, and execute individual agent runs with tools and knowledge
  </Card>

<Card title="Teams" icon="users" href="/reference-api/schema/teams/list-all-teams">
    Orchestrate multiple agents working together on complex tasks
  </Card>

<Card title="Workflows" icon="diagram-project" href="/reference-api/schema/workflows/list-all-workflows">
    Define and execute multi-step automated processes
  </Card>

<Card title="Sessions" icon="clock" href="/reference-api/schema/sessions/list-sessions">
    Track conversation history and maintain context across interactions
  </Card>

<Card title="Memory" icon="brain" href="/reference-api/schema/memory/list-memories">
    Store and retrieve persistent memories for personalized interactions
  </Card>

<Card title="Knowledge" icon="book" href="/reference-api/schema/knowledge/list-content">
    Upload, manage, and query knowledge bases for your agents
  </Card>

<Card title="Evals" icon="chart-bar" href="/reference-api/schema/evals/list-evaluation-runs">
    Run evaluations and track performance metrics for your agents
  </Card>
</CardGroup>

---

## Desi Vocal

**URL:** llms-txt#desi-vocal

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/desi-vocal

DesiVocalTools provides text-to-speech capabilities using Indian voices through the Desi Vocal API.

The following agent can convert text to speech using Indian voices:

| Parameter               | Type            | Default                                  | Description                                                |
| ----------------------- | --------------- | ---------------------------------------- | ---------------------------------------------------------- |
| `api_key`               | `Optional[str]` | `None`                                   | Desi Vocal API key. Uses DESI\_VOCAL\_API\_KEY if not set. |
| `voice_id`              | `Optional[str]` | `"f27d74e5-ea71-4697-be3e-f04bbd80c1a8"` | Default voice ID to use for text-to-speech.                |
| `enable_get_voices`     | `bool`          | `True`                                   | Enable voice listing functionality.                        |
| `enable_text_to_speech` | `bool`          | `True`                                   | Enable text-to-speech conversion functionality.            |

| Function         | Description                                                   |
| ---------------- | ------------------------------------------------------------- |
| `get_voices`     | Retrieve list of available voices with their IDs and details. |
| `text_to_speech` | Convert text to speech using specified or default voice.      |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/desi_vocal.py)
* [Desi Vocal API Documentation](https://desivocal.com/docs)
* [Indian TTS Best Practices](https://desivocal.com/best-practices)

---

## Setup the agent

**URL:** llms-txt#setup-the-agent

basic_agent = Agent(
    id="basic-agent",
    name="Calculator Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    markdown=True,
    instructions="You are an assistant that can answer arithmetic questions. Always use the Calculator tools you have.",
    tools=[CalculatorTools()],
)

---

## Analyze team leader message metrics

**URL:** llms-txt#analyze-team-leader-message-metrics

print("=" * 50)
print("TEAM LEADER MESSAGE METRICS")
print("=" * 50)

if run_output.messages:
    for message in run_output.messages:
        if message.role == "assistant":
            if message.content:
                print(f"ðŸ“ Message: {message.content[:100]}...")
            elif message.tool_calls:
                print(f"ðŸ”§ Tool calls: {message.tool_calls}")

print("-" * 30, "Metrics", "-" * 30)
            pprint(message.metrics)
            print("-" * 70)

---

## Load the knowledge base

**URL:** llms-txt#load-the-knowledge-base

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

---

## memory_manager=memory_manager,

**URL:** llms-txt#memory_manager=memory_manager,

---

## for response in run_response_strem:

**URL:** llms-txt#for-response-in-run_response_strem:

---

## Building Teams

**URL:** llms-txt#building-teams

Source: https://docs.agno.com/basics/teams/building-teams

Learn how to build Teams with Agno.

To build effective teams, start simple -- just a model, team members, and instructions. Once that works, add more functionality as needed.

Here's a minimal example of a team with specialized agents:

```python news_weather_team.py theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

---

## LightRAG Async

**URL:** llms-txt#lightrag-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/lightrag/usage/async-lightrag-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## This metadata can include user IDs, document types, dates, or any other attributes

**URL:** llms-txt#this-metadata-can-include-user-ids,-document-types,-dates,-or-any-other-attributes

vector_db = Weaviate(
    collection="recipes",
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to False if using Weaviate Cloud and True if using local instance
)

knowledge = Knowledge(
    name="Weaviate Knowledge Base",
    description="A knowledge base for Weaviate",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

---

## Add JWT middleware configured for cookie-based authentication

**URL:** llms-txt#add-jwt-middleware-configured-for-cookie-based-authentication

app.add_middleware(
    JWTMiddleware,
    secret_key=JWT_SECRET, # or use JWT_SECRET_KEY environment variable
    algorithm="HS256",
    excluded_route_paths=[
        "/set-auth-cookie",
        "/clear-auth-cookie",
    ],
    token_source=TokenSource.COOKIE,  # Extract JWT from cookies
    cookie_name="auth_token",  # Name of the cookie containing the JWT
    user_id_claim="sub",  # Extract user_id from 'sub' claim
    session_id_claim="session_id",  # Extract session_id from 'session_id' claim
    dependencies_claims=[
        "name",
        "email",
        "roles",
        "org",
    ],  # Additional claims to extract
    validate=True,  # We want to ensure the token is valid
)

agent_os = AgentOS(
    description="JWT Cookie-Based AgentOS",
    agents=[profile_agent],
    base_app=app,
)

---

## Firecrawl

**URL:** llms-txt#firecrawl

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/firecrawl

Use Firecrawl with Agno to scrape and crawl the web.

**FirecrawlTools** enable an Agent to perform web crawling and scraping tasks.

The following example requires the `firecrawl-py` library and an API key which can be obtained from [Firecrawl](https://firecrawl.dev).

The following agent will scrape the content from [https://finance.yahoo.com/](https://finance.yahoo.com/) and return a summary of the content:

| Parameter        | Type             | Default                     | Description                                                                  |
| ---------------- | ---------------- | --------------------------- | ---------------------------------------------------------------------------- |
| `api_key`        | `str`            | `None`                      | API key for authentication. Uses FIRECRAWL\_API\_KEY env var if not provided |
| `enable_scrape`  | `bool`           | `True`                      | Enables website scraping functionality                                       |
| `enable_crawl`   | `bool`           | `False`                     | Enables website crawling functionality                                       |
| `enable_mapping` | `bool`           | `False`                     | Enables website mapping functionality                                        |
| `enable_search`  | `bool`           | `False`                     | Enables web search functionality                                             |
| `all`            | `bool`           | `False`                     | Enables all functionality when set to True                                   |
| `formats`        | `List[str]`      | `None`                      | List of formats to be used for operations                                    |
| `limit`          | `int`            | `10`                        | Maximum number of items to retrieve                                          |
| `poll_interval`  | `int`            | `30`                        | Interval in seconds between polling for results                              |
| `search_params`  | `Dict[str, Any]` | `None`                      | Parameters for search operations                                             |
| `api_url`        | `str`            | `https://api.firecrawl.dev` | API URL to use for the Firecrawl app                                         |

| Function         | Description                                                                                                                                                                                                                                             |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scrape_website` | Scrapes a website using Firecrawl. Parameters include `url` to specify the URL to scrape. The function supports optional formats if specified. Returns the results of the scraping in JSON format.                                                      |
| `crawl_website`  | Crawls a website using Firecrawl. Parameters include `url` to specify the URL to crawl, and an optional `limit` to define the maximum number of pages to crawl. The function supports optional formats and returns the crawling results in JSON format. |
| `map_website`    | Maps a website structure using Firecrawl. Parameters include `url` to specify the URL to map. Returns the mapping results in JSON format.                                                                                                               |
| `search`         | Performs a web search using Firecrawl. Parameters include `query` for the search term and optional `limit` for maximum results. Returns search results in JSON format.                                                                                  |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/firecrawl.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/firecrawl_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will scrape the content from [https://finance.yahoo.com/](https://finance.yahoo.com/) and return a summary of the content:
```

---

## MongoDB Hybrid Search

**URL:** llms-txt#mongodb-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/mongodb/usage/mongo-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run MongoDB">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run MongoDB">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Find content from a specific year

**URL:** llms-txt#find-content-from-a-specific-year

EQ("year", 2024)
python  theme={null}
from agno.filters import IN

**Examples:**

Example 1 (unknown):
```unknown
#### IN (Contains Any)

Match content where a metadata field contains any of the specified values.
```

---

## How to Switch Between Different Models

**URL:** llms-txt#how-to-switch-between-different-models

**Contents:**
- Recommended Approach
- Safe Model Switching

Source: https://docs.agno.com/faq/switching-models

When working with Agno, you may need to switch between different models. While Agno supports 20+ model providers, switching between different providers can cause compatibility issues. Switching models within the same provider is generally safer and more reliable.

## Recommended Approach

**Safe:** Switch models within the same provider (OpenAI â†’ OpenAI, Google â†’ Google)\
**Risky:** Switch between different providers (OpenAI â†” Google â†” Anthropic)

Cross-provider switching is risky because message history between model providers are often not interchangeable, as some require messages that others don't. However, we are actively working to improve interoperability.

## Safe Model Switching

The safest way to switch models is to change the model ID while keeping the same provider:

```python  theme={null}
from uuid import uuid4
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat

db = SqliteDb(db_file="tmp/agent_sessions.db")

session_id = str(uuid4())
user_id = "user@example.com"

---

## Create coordinated reasoning RAG team

**URL:** llms-txt#create-coordinated-reasoning-rag-team

**Contents:**
- Usage

coordinated_reasoning_team = Team(
    name="Coordinated Reasoning RAG Team",
    model=Claude(id="claude-sonnet-4-20250514"),
    members=[
        information_gatherer,
        reasoning_analyst,
        evidence_evaluator,
        response_coordinator,
    ],
    instructions=[
        "Work together to provide comprehensive, well-reasoned responses.",
        "Information Gatherer: First search and gather all relevant information.",
        "Reasoning Analyst: Then apply structured reasoning to analyze the information.",
        "Evidence Evaluator: Evaluate the evidence quality and identify any gaps.",
        "Response Coordinator: Finally synthesize everything into a clear, reasoned response.",
        "All agents should use reasoning tools to structure their contributions.",
        "Show your reasoning process transparently in responses.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_reasoning_demo():
    """Demonstrate async coordinated reasoning RAG with streaming."""
    print("ðŸ§  Async Coordinated Reasoning RAG Team Demo")
    print("=" * 60)

query = "What are Agents and how do they work with tools? Explain the reasoning behind their design."

# Add documentation content
    await knowledge.add_contents_async(urls=["https://docs.agno.com/introduction/agents.md"])

# Run async with streaming and reasoning
    await coordinated_reasoning_team.aprint_response(
        query, stream=True, show_full_reasoning=True
    )

def sync_reasoning_demo():
    """Demonstrate sync coordinated reasoning RAG."""
    print("ðŸ§  Coordinated Reasoning RAG Team Demo")
    print("=" * 50)

query = "What are Agents and how do they work with tools? Explain the reasoning behind their design."

# Add documentation content
    knowledge.add_contents(urls=["https://docs.agno.com/introduction/agents.md"])

# Run with detailed reasoning output
    coordinated_reasoning_team.print_response(
        query, stream=True, show_full_reasoning=True
    )

if __name__ == "__main__":
    # Choose which demo to run
    # asyncio.run(async_reasoning_demo())

sync_reasoning_demo()
bash  theme={null}
    pip install agno cohere lancedb tantivy sqlalchemy
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export CO_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/search_coordination/02_coordinated_reasoning_rag.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Simple function we will use as a post-hook

**URL:** llms-txt#simple-function-we-will-use-as-a-post-hook

**Contents:**
  - Post-hook Parameters
- Guardrails
- The `@hook` Decorator
  - Background Execution
  - When to Use Background Hooks
- Developer Resources

def validate_output_length(
    run_output: RunOutput,
) -> None:
    """Post-hook to validate output length."""
    max_length = 1000
    if len(run_output.content) > max_length:
        raise OutputCheckError(
            f"Output too long. Max {max_length} characters allowed",
            check_trigger=CheckTrigger.OUTPUT_NOT_ALLOWED,
        )

agent = Agent(
    name="My Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    # Provide the post-hook to the Agent using the post_hooks parameter
    post_hooks=[validate_output_length],
)
python  theme={null}
from agno.hooks import hook

@hook(run_in_background=True)
async def send_notification(run_output, agent):
    """This hook will run in the background without blocking the response."""
    await send_email_notification(run_output.content)
```

<Warning>
  Background execution requires [AgentOS](/agent-os/overview). When running agents directly (not through AgentOS), hooks marked with `run_in_background=True` will still execute synchronously.
</Warning>

### When to Use Background Hooks

Background hooks are ideal for:

* **Logging and analytics**: Record metrics without affecting response time
* **Notifications**: Send emails, Slack messages, or webhooks
* **Async data storage**: Write to external databases or APIs
* **Non-critical post-processing**: Tasks that don't affect the response

<Note>
  Hooks running in background mode (both pre-hooks and post-hooks) cannot modify any of the hook parameters (like `run_input`, `run_output`, `run_context`, etc.) since the agent may process the request before the hook completes.
  Use background mode primarily for post-hooks or for pre-hooks that only perform logging/monitoring. This means background mode is not suitable for Guardrails.
</Note>

For complete documentation on background task execution, see the [Background Tasks](/agent-os/background-tasks/overview) guide.

For the full decorator API, see the [@hook Decorator Reference](/reference/hooks/hook-decorator).

## Developer Resources

* View [Agent Examples](/basics/hooks/usage/agent/input-transformation-pre-hook)
* View [Team Examples](/basics/hooks/usage/team/input-transformation-pre-hook)
* View [Agent Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/hooks)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/hooks)

**Examples:**

Example 1 (unknown):
```unknown
You can see complete examples of post-hooks in the [Examples](/basics/hooks/usage/agent/input-transformation-pre-hook) section.

### Post-hook Parameters

Post-hooks run automatically during the Agent run and receive the following parameters:

* `run_output`: The output from the Agent run that can be validated or modified
* `agent`: Reference to the Agent instance
* `session`: The current agent session
* `run_context`: The current run context. See the [Run Context](/reference/run/run-context) reference.
* `user_id`: The user ID for the run (optional)
* `debug_mode`: Whether debug mode is enabled (optional)

The framework automatically injects only the parameters your hook function accepts, so you can define hooks with just the parameters you need.

You can learn more about the parameters in the [Post-hooks](/reference/hooks/post-hooks) reference.

## Guardrails

A popular use case for hooks are Guardrails: built-in safeguards for your Agents.

You can learn more about them in the [Guardrails](/basics/guardrails/overview) section.

## The `@hook` Decorator

The `@hook` decorator allows you to configure individual hook behavior. Currently, it supports marking hooks to run in the background when used with [AgentOS](/agent-os/overview).

### Background Execution

By default, hooks are either executed synchronously or asynchronously, in an API context they still block the response until they complete. For hooks that perform non-critical tasks (logging, analytics, notifications), you can mark them to run in the background:
```

---

## Team Reliability with Stock Tools

**URL:** llms-txt#team-reliability-with-stock-tools

Source: https://docs.agno.com/basics/evals/reliability/usage/reliability-team-advanced

Example showing how to evaluate team reliability with real-world tools like stock price lookup.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Run by ID

**URL:** llms-txt#get-run-by-id

Source: https://docs.agno.com/reference-api/schema/sessions/get-run-by-id

get /sessions/{session_id}/runs/{run_id}
Retrieve a specific run by its ID from a session. Response schema varies based on the run type (agent run, team run, or workflow run).

---

## Generate Image with Intermediate Steps

**URL:** llms-txt#generate-image-with-intermediate-steps

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/generate-image-with-intermediate-steps

This example demonstrates how to create an agent that generates images using DALL-E while streaming during the image creation process.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Neon

**URL:** llms-txt#neon

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/neon/overview

Learn to use Neon as a database provider for your Agents

Agno supports using [Neon](https://neon.com/) with the `PostgresDb` class.

You can get started with Neon following their [Get Started guide](https://neon.com/docs/get-started/signing-up).

You can also read more about the [`PostgresDb` class](/integrations/database/postgres/overview) in its section.

```python neon_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from os import getenv

---

## Team with Agentic Memory

**URL:** llms-txt#team-with-agentic-memory

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/team/usage/team-with-agentic-memory

This example demonstrates how to use agentic memory with a team. Unlike simple memory storage, agentic memory allows the AI to actively create, update, and delete user memories during each run based on the conversation context, providing intelligent memory management.

```python cookbook/examples/teams/memory/02_team_with_agentic_memory.py theme={null}
"""
This example shows you how to use persistent memory with an Agent.

During each run the Agent can create/update/delete user memories.

To enable this, set `enable_agentic_memory=True` in the Agent config.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager  # noqa: F401
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
)

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    members=[agent],
    db=db,
    enable_agentic_memory=True,
)

team.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
    stream=True,
    user_id=john_doe_id,
)

team.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)

---

## Running Workflows

**URL:** llms-txt#running-workflows

**Contents:**
- Running your Workflow

Source: https://docs.agno.com/basics/workflows/running-workflows

Learn how to run a workflow and get the response.

The `Workflow.run()` function runs the agent and generates a response, either as a `WorkflowRunOutput` object or a stream of `WorkflowRunOutput` objects.

Many of our examples use `workflow.print_response()` which is a helper utility to print the response in the terminal. This uses `workflow.run()` under the hood.

## Running your Workflow

Here's how to run your workflow. The response is captured in the `response`.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.db.sqlite import SqliteDb
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.run.workflow import WorkflowRunOutput
from agno.utils.pprint import pprint_run_response

---

## Custom Memory Manager

**URL:** llms-txt#custom-memory-manager

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/agent/usage/custom-memory-manager

This example shows how you can configure the Memory Manager.

We also set custom system prompts for the memory manager. You can either override the entire system prompt or add additional instructions which is added to the end of the system prompt.

```python custom_memory_manager.py theme={null}
from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager
from agno.models.openai import OpenAIChat
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url)

---

## Claude Agent Skills

**URL:** llms-txt#claude-agent-skills

**Contents:**
- Available Skills
- Prerequisites
- File Download Helper Setup
  - How File Download Works
  - Create `file_download_helper.py`
- Basic Usage
- PowerPoint Skills
  - Example: Q4 Business Review Presentation

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/skills

Create PowerPoint presentations, Excel spreadsheets, Word documents, and analyze PDFs with Claude Agent Skills

[Claude Agent Skills](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview) provide capabilities beyond what can get done with prompts alone.

With **Skills**, Claude will gain access to filesystem-based resources. These will be loaded on demand, removing the need to provide the same guidance multiple times as it happens with prompts.

You can read more about how **Skills** work on [Anthropic docs](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview#how-skills-work).

* **PowerPoint (pptx)**: Create professional presentations with slides, layouts, and formatting
* **Excel (xlsx)**: Generate spreadsheets with formulas, charts, and data analysis
* **Word (docx)**: Create and edit documents with rich formatting
* **PDF (pdf)**: Analyze and extract information from PDF documents

You can also create custom skills for Claude to use. You can read more about that on [Anthropic docs](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview#custom-skills-examples).

Before using Claude Agent Skills, ensure you have:

1. **Python 3.8 or higher**
2. **Anthropic API key** with access to Claude models
3. **Beta access** to Claude Agent Skills

## File Download Helper Setup

<Warning>
  **Important:** Files created by Agent Skills are NOT automatically saved to your local filesystem. They are created in a sandboxed execution environment and must be downloaded using the Anthropic Files API.

Before running any of the examples below, you must create the `file_download_helper.py` file in the same directory as your script.
</Warning>

### How File Download Works

1. Claude creates the file in the sandbox
2. Returns a **file ID** in the tool result
3. You download the file using the helper function below

### Create `file_download_helper.py`

Save the following code as `file_download_helper.py` in your project directory:

Once you've created this file, you can import it in your scripts:

Enable skills by passing them to the Claude model configuration:

The framework automatically:

* Configures the required betas (`code-execution-2025-08-25`, `skills-2025-10-02`)
* Adds the code execution tool
* Uses the beta API client
* Sets up the container with skill configurations

You can enable multiple skills at once:

Create professional presentations with slides, layouts, and formatting.

### Example: Q4 Business Review Presentation

```python  theme={null}
import os
from agno.agent import Agent
from agno.models.anthropic import Claude
from anthropic import Anthropic
from file_download_helper import download_skill_files

**Examples:**

Example 1 (unknown):
```unknown
Once you've created this file, you can import it in your scripts:
```

Example 2 (unknown):
```unknown
## Basic Usage

Enable skills by passing them to the Claude model configuration:
```

Example 3 (unknown):
```unknown
The framework automatically:

* Configures the required betas (`code-execution-2025-08-25`, `skills-2025-10-02`)
* Adds the code execution tool
* Uses the beta API client
* Sets up the container with skill configurations

You can enable multiple skills at once:
```

Example 4 (unknown):
```unknown
## PowerPoint Skills

Create professional presentations with slides, layouts, and formatting.

### Example: Q4 Business Review Presentation
```

---

## Session Management

**URL:** llms-txt#session-management

**Contents:**
- Session IDs
- Access to Messages & Chat History
- Session Naming
  - Manual Naming
  - Auto-generated Names
- Session Caching
  - When It Helps

Source: https://docs.agno.com/basics/sessions/session-management

Manage session identifiers, names, and performance optimization

Session management in Agno gives you control over how sessions are identified, named, and cached for optimal performance.

Every session has a unique identifier (`session_id`) that tracks conversations across multiple runs:

* **Auto-generated**: If not provided, Agno generates a UUID automatically
* **Manual**: You can provide your own session IDs for custom tracking
* **Per-user**: Combine with `user_id` to track multiple users' sessions

## Access to Messages & Chat History

You can access the messages in a session using the `get_messages` method:

This gives you all messages for all runs in the session, including tool calls and system messages.

For a simpler list of only user and assistant messages, you can use the `get_chat_history` method:

See the detailed [AgentSession reference](/reference/agents/session) and [TeamSession reference](/reference/teams/session) for more information.

Session names are human-readable labels that make it easier to identify and manage conversationsâ€”perfect for inbox-style UIs, support queues, or linking a conversation back to an external ticket.

Set custom names using `set_session_name()`:

* Treat the session ID as the source of truth; names are just metadata for humans
* Rename conversations whenever the topic shiftsâ€”there's no limit on how often you call the method
* Need guardrails or naming policies? Wrap `set_session_name` in your own helper before exposing it to end-users

### Auto-generated Names

Let the AI generate meaningful names from conversation content:

Calling `set_session_name(autogenerate=True)` asks the model to read the first few messages in the session and generate a short (â‰¤5 words) label. The method returns the updated session object. Use `get_session_name()` to retrieve the generated name.

* **Delay generation** until the conversation has meaningful context (e.g., after 2â€“3 messages)
* **Provide a fallback**: wrap the call in your own helper that falls back to a human-entered name or a ticket ID if the generation fails
* **Batch jobs**: loop over session IDs from your database and call `set_session_name(..., autogenerate=True)` once for each. The API is synchronous, so plan around model latency
* **Costs**: Each generation is an extra model call. Use cheaper models via `session_summary_manager` or run it out-of-band if you're cost sensitive

Session caching stores the session object in memory to improve performance. `cache_session=True` keeps the hydrated session object in memory after the first database read, avoiding extra queries for subsequent runs.

* **Many sequential turns** in the same session (support chats, copilots, etc.)
* **Latency-sensitive** deployments where every DB round trip matters
* **Resource-heavy databases** (remote Postgres, serverless drivers) where connection setup dominates

<Warning>
  This is only for development and testing purposes. It is not recommended for production use.
</Warning>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Access to Messages & Chat History

You can access the messages in a session using the `get_messages` method:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

This gives you all messages for all runs in the session, including tool calls and system messages.

For a simpler list of only user and assistant messages, you can use the `get_chat_history` method:

<CodeGroup>
```

---

## Workflow using steps

**URL:** llms-txt#workflow-using-steps

Source: https://docs.agno.com/basics/workflows/usage/workflow-using-steps

This example demonstrates how to use the Steps object to organize multiple individual steps into logical sequences.

This example demonstrates **Workflows** using the Steps object to organize multiple
individual steps into logical sequences. This pattern allows you to define reusable step
sequences and choose which sequences to execute in your workflow.

**When to use**: When you have logical groupings of steps that you want to organize, reuse,
or selectively execute. Ideal for creating modular workflow components that can be mixed
and matched based on different scenarios.

```python workflow_using_steps.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.step import Step
from agno.workflow.steps import Steps
from agno.workflow.workflow import Workflow

---

## Example 2: List all channels in the Slack workspace

**URL:** llms-txt#example-2:-list-all-channels-in-the-slack-workspace

agent.print_response("List all channels in our Slack workspace", markdown=True)

---

## Router with Loop Steps Workflow

**URL:** llms-txt#router-with-loop-steps-workflow

Source: https://docs.agno.com/basics/workflows/usage/router-with-loop-steps

This example demonstrates **Workflows 2.0** advanced pattern combining Router-based intelligent path selection with Loop execution for iterative quality improvement.

This example shows how to create adaptive workflows that select optimal research strategies and execution patterns based on topic complexity.

**When to use**: When different topic types require fundamentally different research
methodologies - some needing simple single-pass research, others requiring iterative
deep-dive analysis. Ideal for content-adaptive workflows where processing complexity
should match content complexity.

```python router_with_loop_steps.py theme={null}
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.loop import Loop
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## The workflow decorator creates a parent span

**URL:** llms-txt#the-workflow-decorator-creates-a-parent-span

**Contents:**
- Notes

result = analyze_data("Analyze the benefits of observability in AI systems")
print(result)
python  theme={null}
import asyncio
from traceloop.sdk import Traceloop
from agno.agent import Agent
from agno.models.openai import OpenAIChat

Traceloop.init(app_name="agno_async")

def get_weather(city: str) -> str:
    """Get the weather for a city."""
    return f"The weather in {city} is sunny, 72Â°F"

agent = Agent(
    name="WeatherAgent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[get_weather],
    debug_mode=True,
)

async def main():
    # Async execution is automatically traced
    response = await agent.arun("What's the weather in San Francisco?")
    print(response.content)

asyncio.run(main())
```

* **Initialization**: Call `Traceloop.init()` before creating any agents to ensure proper instrumentation.
* **Development Mode**: Use `disable_batch=True` during development for immediate trace visibility.
* **Async Support**: Both sync (`run()`) and async (`arun()`) methods are fully instrumented.
* **Privacy Control**: Set `TRACELOOP_TRACE_CONTENT=false` to disable logging of prompts and completions.

**Examples:**

Example 1 (unknown):
```unknown
* ### Example: Async Agent with Tools

Async agent execution is fully supported with automatic tool call tracing:
```

---

## Add documents with metadata

**URL:** llms-txt#add-documents-with-metadata

knowledge_base.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2020,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2020,
            },
        },
    ],
    reader=PDFReader(chunk=True),
)

---

## Team with Knowledge Tools

**URL:** llms-txt#team-with-knowledge-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/knowledge-tool-team

This is a team reasoning example with knowledge tools.

<Tip>
  Enabling the knowledge option on the team leader helps optimize delegation and enhances multi-agent collaboration by selectively invoking deeper knowledge when required.
</Tip>

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Dalle

**URL:** llms-txt#dalle

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/dalle

You need to install the `openai` library.

Set the `OPENAI_API_KEY` environment variable.

The following agent will use DALL-E to generate an image based on a text prompt.

```python cookbook/tools/dalle_tools.py theme={null}
from agno.agent import Agent
from agno.tools.dalle import DalleTools

**Examples:**

Example 1 (unknown):
```unknown
Set the `OPENAI_API_KEY` environment variable.
```

Example 2 (unknown):
```unknown
## Example

The following agent will use DALL-E to generate an image based on a text prompt.
```

---

## Stream all events

**URL:** llms-txt#stream-all-events

**Contents:**
  - Streaming member events
  - Handling Events
  - Storing Events
  - Event Types
  - Custom Events
- Specify Run User and Session
- Passing Images / Audio / Video / Files
- Passing Output Schema
- Cancelling a Run
- Developer Resources

response_stream = team.run(
    "What is the weather in Tokyo?",
    stream=True,
    stream_events=True
)
python  theme={null}
...

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o"),
    stream_member_events=False
)

response_stream = team.run(
    "What is the weather in Tokyo?",
    stream=True,
    stream_events=True)
python  theme={null}
from agno.team import Team
from agno.run.team import TeamRunEvent
from agno.run.agent import RunEvent
from agno.models.openai import OpenAIChat
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools

weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days", tools=[DuckDuckGoTools()])
team = Team(
    name="Test Team",
    members=[weather_agent],
    model=OpenAIChat(id="gpt-4o-mini")
)

response_stream = team.run("What is the weather in Tokyo?", stream=True, stream_events=True)

for event in response_stream:
    if event.event == TeamRunEvent.run_content:
        print(event.content, end="", flush=True)
    elif event.event == TeamRunEvent.tool_call_started:
        print(f"Team tool call started")
    elif event.event == TeamRunEvent.tool_call_completed:
        print(f"Team tool call completed")
    elif event.event == RunEvent.tool_call_started:
        print(f"Member tool call started")
    elif event.event == RunEvent.tool_call_completed:
        print(f"Member tool call completed")
    elif event.event == TeamRunEvent.run_started:
        print(f"Run started")
    elif event.event == TeamRunEvent.run_completed:
        print(f"Run completed")
python  theme={null}
...

team = Team(
    name="Story Team",
    members=[],
    model=OpenAIChat(id="gpt-4o"),
    store_events=True
)
python  theme={null}
team = Team(
    name="Story Team",
    members=[],
    model=OpenAIChat(id="gpt-4o"),
    store_events=True,
    events_to_skip=[]  # Include all events
)
python  theme={null}
from dataclasses import dataclass
from agno.run.team import CustomEvent

@dataclass
class CustomerProfileEvent(CustomEvent):
    """CustomEvent for customer profile."""

customer_name: Optional[str] = None
    customer_email: Optional[str] = None
    customer_phone: Optional[str] = None
python  theme={null}
from agno.tools import tool

@tool()
async def get_customer_profile():
    """Example custom tool that simply yields a custom event."""

yield CustomerProfileEvent(
        customer_name="John Doe",
        customer_email="john.doe@example.com",
        customer_phone="1234567890",
    )
python  theme={null}
team.run("Get me my monthly report", user_id="john@example.com", session_id="session_123")
python  theme={null}
team.run("Tell me a 5 second short story about this image", images=[Image(url="https://example.com/image.jpg")])
python  theme={null}
from pydantic import BaseModel
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat

class DetailedReport(BaseModel):
    overview: str
    findings: list[str]

agent = Agent(name="Analyst", model=OpenAIChat(id="gpt-4o-mini"))
team = Team(members=[agent], model=OpenAIChat(id="gpt-4o-mini"))
team.run("Analyze the market", output_schema=DetailedReport)
```

For more information see the [Input & Output](/basics/input-output/overview) documentation.

A run can be cancelled by calling the `Team.cancel_run()` method.

See more details in the [Cancelling a Run](/execution-control/run-cancellation/overview) documentation.

## Developer Resources

* View the [Team reference](/reference/teams/team)
* View the [TeamRunOutput schema](/reference/teams/team-response)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/README.md)

**Examples:**

Example 1 (unknown):
```unknown
### Streaming member events

When streaming events with `stream_events=True`, the team leader also streams the events from team members to the caller.

When your team is running asynchronously (using `arun`), the members will run concurrently if the team leader delegates to multiple members in one request.

This means you will receive member events concurrently and the order of the events is not guaranteed.

You can disable this by setting `stream_member_events=False`.
```

Example 2 (unknown):
```unknown
### Handling Events

You can process events as they arrive by iterating over the response stream:
```

Example 3 (unknown):
```unknown
### Storing Events

You can store all the events that happened during a run on the `TeamRunOutput` object.
```

Example 4 (unknown):
```unknown
By default the `TeamRunContentEvent` and `RunContentEvent` events are not stored. You can modify which events are skipped by setting the `events_to_skip` parameter.

For example:
```

---

## - External IDs for cloud integrations

**URL:** llms-txt#--external-ids-for-cloud-integrations

**Contents:**
  - Content Retrieval and Management

**Examples:**

Example 1 (unknown):
```unknown
### Content Retrieval and Management
```

---

## No system message should be provided

**URL:** llms-txt#no-system-message-should-be-provided

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

---

## Distributed RAG with PgVector

**URL:** llms-txt#distributed-rag-with-pgvector

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/distributed-rag-pgvector

This example demonstrates how multiple specialized agents coordinate to provide comprehensive RAG responses using distributed PostgreSQL vector databases with pgvector for scalable, production-ready retrieval. The team includes vector retrieval, hybrid search, data validation, and response composition specialists.

```python cookbook/examples/teams/distributed_rag/01_distributed_rag_pgvector.py theme={null}
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses using distributed PostgreSQL vector databases with
pgvector for scalable, production-ready retrieval.

Team Composition:
- Vector Retriever: Specialized in vector similarity search using pgvector
- Hybrid Searcher: Combines vector and text search for comprehensive results
- Data Validator: Validates retrieved data quality and relevance
- Response Composer: Composes final responses with proper source attribution

Setup:
1. Run: `./cookbook/run_pgvector.sh` to start a postgres container with pgvector
2. Run: `pip install openai sqlalchemy 'psycopg[binary]' pgvector agno`
3. Run this script to see distributed PgVector RAG in action
"""

import asyncio  # noqa: F401

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.vectordb.pgvector import PgVector, SearchType

---

## Create an evaluation

**URL:** llms-txt#create-an-evaluation

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[CalculatorTools()]),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
)

---

## Teams Use Cases

**URL:** llms-txt#teams-use-cases

**Contents:**
- Getting Started
- Featured Examples

Source: https://docs.agno.com/examples/use-cases/teams/overview

Explore Agno's Team use cases showcasing what multi-agent Teams have to offer.

Welcome to Agno's Team use cases! Here you'll discover practical examples of multi-agent teams that collaborate to solve complex tasks. You can either:

* Run the examples individually
* Clone the entire [Agno cookbook](https://github.com/agno-agi/agno/tree/main/cookbook)

Have an interesting example to share? Please consider [contributing](https://github.com/agno-agi/agno-docs) to our growing collection.

If you're just getting started, follow the [Getting Started](/examples/getting-started) guide for a step-by-step tutorial. The examples build on each other, introducing new concepts and capabilities progressively.

Explore these popular team examples to see what's possible with Agno:

<CardGroup cols={3}>
  <Card title="Content Team" icon="pen-nib" iconType="duotone" href="/examples/use-cases/teams/content_team">
    Build a content creation team with specialized researchers and writers.
  </Card>

<Card title="HackerNews Team" icon="newspaper" iconType="duotone" href="/examples/use-cases/teams/hackernews_team">
    Aggregate, curate, and analyze trending HackerNews stories with multiple agents.
  </Card>

<Card title="Multi Language Team" icon="language" iconType="duotone" href="/examples/use-cases/teams/multi_language_team">
    Automatically detect user language and route to native-speaking agents.
  </Card>

<Card title="Reasoning Team" icon="brain" iconType="duotone" href="/examples/use-cases/teams/reasoning_team">
    Combine web search, financial data, and transparent analytical thinking.
  </Card>

<Card title="AI Support Team" icon="headset" iconType="duotone" href="/examples/use-cases/teams/ai_support_team">
    Route customer inquiries to specialized agents for documentation and support.
  </Card>
</CardGroup>

---

## Create Your First AgentOS

**URL:** llms-txt#create-your-first-agentos

**Contents:**
- Prerequisites
- Installation
- Minimal Setup
- Running Your OS
- Connecting to the Control Plane
- Next Steps

Source: https://docs.agno.com/agent-os/creating-your-first-os

Quick setup guide to get your first AgentOS instance running locally

Get started with AgentOS by setting up a minimal local instance.
This guide will have you running your first agent in minutes, with optional paths to add advanced features through our examples.

<Check>
  AgentOS is a FastAPI app that you can run locally or in your cloud. If you want to build AgentOS using an existing FastAPI app, check out the [Custom FastAPI App](/agent-os/custom-fastapi/overview) guide.
</Check>

* Python 3.9+
* An LLM provider API key (e.g., `OPENAI_API_KEY`)

Create and activate a virtual environment:

Install dependencies:

<Tip>
  It is recommended to use all of the `async` building blocks of Agno when building your AgentOS. For example, use async tool definitions, async database connections, async pre/post hooks, etc.

This helps unlock maximum concurrency and performance when running your AgentOS.
</Tip>

Access your running instance:

* **App Interface**: `http://localhost:7777` - Use this URL when connecting to the AgentOS control plane
* **API Documentation**: `http://localhost:7777/docs` - Interactive API documentation and testing
* **Configuration**: `http://localhost:7777/config` - View AgentOS configuration
* **API Reference**: View the [AgentOS API documentation](/reference-api/overview) for programmatic access

## Connecting to the Control Plane

With your AgentOS now running locally (`http://localhost:7777`), you can connect it to the AgentOS control plane for a enhanced management experience. The control plane provides a centralized interface to interact with your agents, manage knowledge bases, track sessions, and monitor performance.

<CardGroup cols={3}>
  <Card title="Connect to Control Plane" icon="link" href="/agent-os/connecting-your-os">
    Connect your running OS to the AgentOS control plane interface
  </Card>

<Card title="Browse Examples" icon="code" href="/agent-os/usage/demo">
    Explore comprehensive examples for advanced AgentOS configurations
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Install dependencies:
```

Example 3 (unknown):
```unknown
## Minimal Setup

Create `my_os.py`:
```

Example 4 (unknown):
```unknown
<Tip>
  It is recommended to use all of the `async` building blocks of Agno when building your AgentOS. For example, use async tool definitions, async database connections, async pre/post hooks, etc.

  This helps unlock maximum concurrency and performance when running your AgentOS.
</Tip>

## Running Your OS

Start your AgentOS:
```

---

## This will raise an error - unexpected field

**URL:** llms-txt#this-will-raise-an-error---unexpected-field

**Contents:**
- Usage

try:
    hackernews_agent.print_response(
        input={
            "topic": "AI",
            "focus_areas": ["Machine Learning"],
            "target_audience": "Developers",
            "sources_required": 5,
            "unexpected_field": "value",  # This field is not in the TypedDict
        }
    )
except ValueError as e:
    print("\n=== Expected Error for Unexpected Field ===")
    print(f"Error: {e}")
bash  theme={null}
    pip install -U agno openai
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch input_schema_on_agent_as_typed_dict.py
    bash Mac theme={null}
      python input_schema_on_agent_as_typed_dict.py
      bash Windows   theme={null}
      python input_schema_on_agent_as_typed_dict.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Download all sample CVs and get their paths

**URL:** llms-txt#download-all-sample-cvs-and-get-their-paths

downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

---

## db=db,

**URL:** llms-txt#db=db,

---

## Download files

**URL:** llms-txt#download-files

**Contents:**
- Usage
- Configuration
  - Model Requirements
  - Beta Version
  - Skill Configuration
- Additional Resources

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
if response.messages:
    for msg in response.messages:
        if hasattr(msg, "provider_data") and msg.provider_data:
            files = download_skill_files(msg.provider_data, client)
            if files:
                print(f"Downloaded {len(files)} file(s):")
                for file in files:
                    print(f"  - {file}")
                break
bash  theme={null}
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U anthropic agno
    bash Mac theme={null}
      python your_script.py
      bash Windows theme={null}
      python your_script.py
      python  theme={null}
skills=[
    {"type": "anthropic", "skill_id": "pptx", "version": "latest"},
    {"type": "anthropic", "skill_id": "xlsx", "version": "latest"},
    {"type": "anthropic", "skill_id": "docx", "version": "latest"},
    {"type": "anthropic", "skill_id": "pdf", "version": "latest"},
]
```

## Additional Resources

* [Claude Agent Skills Documentation](https://docs.anthropic.com/en/docs/agents-and-tools/agent-skills/quickstart)
* [Anthropic API Reference](https://docs.anthropic.com/en/api)
* [Anthropic Files API](https://docs.anthropic.com/en/docs/build-with-claude/files)

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Create file download helper">
    Create `file_download_helper.py` using the code provided in the [File Download Helper Setup](#file-download-helper-setup) section above.
  </Step>

  <Step title="Run Example">
    Create a Python file with any of the examples above and run:

    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Ensure tmp directory exists

**URL:** llms-txt#ensure-tmp-directory-exists

log_file_path = Path("tmp/log.txt")
log_file_path.parent.mkdir(parents=True, exist_ok=True)

---

## Tavily

**URL:** llms-txt#tavily

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/tavily

**TavilyTools** enable an Agent to search the web using the Tavily API.

The following examples requires the `tavily-python` library and an API key from [Tavily](https://tavily.com/).

The following agent will run a search on Tavily for "language models" and print the response.

| Parameter               | Type                           | Default      | Description                                                                               |
| ----------------------- | ------------------------------ | ------------ | ----------------------------------------------------------------------------------------- |
| `api_key`               | `Optional[str]`                | `None`       | Tavily API key. If not provided, will use TAVILY\_API\_KEY environment variable.          |
| `enable_search`         | `bool`                         | `True`       | Enable search functionality.                                                              |
| `enable_search_context` | `bool`                         | `False`      | Enable search context functionality using Tavily's context API.                           |
| `all`                   | `bool`                         | `False`      | Enable all available functions in the toolkit.                                            |
| `max_tokens`            | `int`                          | `6000`       | Maximum number of tokens to use in search results.                                        |
| `include_answer`        | `bool`                         | `True`       | Whether to include an AI-generated answer summary in the response.                        |
| `search_depth`          | `Literal['basic', 'advanced']` | `'advanced'` | Depth of search - 'basic' for faster results or 'advanced' for more comprehensive search. |
| `format`                | `Literal['json', 'markdown']`  | `'markdown'` | Output format - 'json' for raw data or 'markdown' for formatted text.                     |

| Function                  | Description                                                                                                                                                                                                                                                                    |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `web_search_using_tavily` | Search the web for a given query using Tavily API. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum number of results. Returns JSON string of results with titles, URLs, content and relevance scores in specified format. |
| `web_search_with_tavily`  | Alternative search function that uses Tavily's search context API. Parameters include `query` (str) for the search query. Returns contextualized search results. Only available when `enable_search_context` is True.                                                          |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/tavily.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/tavily_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will run a search on Tavily for "language models" and print the response.
```

---

## Define a TypedDict schema

**URL:** llms-txt#define-a-typeddict-schema

class ResearchTopicDict(TypedDict):
    topic: str
    focus_areas: List[str]
    target_audience: str
    sources_required: int

---

## print(run_response.metrics)

**URL:** llms-txt#print(run_response.metrics)

**Contents:**
- Usage

bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
        python cookbook/models/openai/chat/basic.py
      bash Windows theme={null}
        python cookbook/models/openai/chat/basic.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The agent will learn from this interaction

**URL:** llms-txt#the-agent-will-learn-from-this-interaction

**Contents:**
  - 2. Agentic Culture (`enable_agentic_culture=True`)
  - 3. Manual Culture Management

agent.print_response(
    "How do I set up a FastAPI service using Docker?",
    stream=True,
)
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="agno.db")

agent = Agent(
    db=db,
    add_culture_to_context=True,  # Read culture
    enable_agentic_culture=True,  # Agent-controlled culture tools
)
python  theme={null}
from agno.culture.manager import CultureManager
from agno.db.schemas.culture import CulturalKnowledge
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude

db = SqliteDb(db_file="agno.db")

**Examples:**

Example 1 (unknown):
```unknown
**Best for:** Production systems where you want agents to continuously improve their approach based on what works.

### 2. Agentic Culture (`enable_agentic_culture=True`)

The agent gets full control over culture management through built-in tools. It decides when and what to add to the cultural knowledge base.
```

Example 2 (unknown):
```unknown
With agentic culture, the agent is equipped with tools to manage cultural knowledge when it deems relevant during the conversation itself, not just after completion.

**Best for:** Complex workflows where the agent should actively decide what principles to establish or update during the task.

### 3. Manual Culture Management

Create cultural knowledge explicitly using the `CultureManager` or by directly instantiating `CulturalKnowledge` objects. Perfect for seeding organizational standards.
```

---

## Trace

**URL:** llms-txt#trace

**Contents:**
- Trace Attributes
- Methods
  - `to_dict()`
  - `from_dict()`
- Usage

Source: https://docs.agno.com/reference/tracing/trace

A `Trace` represents one complete agent execution from start to finish. Each trace has a unique `trace_id` that groups all related spans together.

| Attribute     | Type            | Default  | Description                                                  |
| ------------- | --------------- | -------- | ------------------------------------------------------------ |
| `trace_id`    | `str`           | Required | Unique trace identifier                                      |
| `name`        | `str`           | Required | Trace name (typically the root span name, e.g., `Agent.run`) |
| `status`      | `str`           | Required | Overall status: `OK`, `ERROR`, or `UNSET`                    |
| `duration_ms` | `int`           | Required | Total execution time in milliseconds                         |
| `start_time`  | `datetime`      | Required | When the trace started                                       |
| `end_time`    | `datetime`      | Required | When the trace completed                                     |
| `total_spans` | `int`           | `0`      | Total number of spans in this trace                          |
| `error_count` | `int`           | `0`      | Number of spans that errored                                 |
| `run_id`      | `Optional[str]` | `None`   | Associated agent/team/workflow run ID                        |
| `session_id`  | `Optional[str]` | `None`   | Associated session ID                                        |
| `user_id`     | `Optional[str]` | `None`   | Associated user ID                                           |
| `agent_id`    | `Optional[str]` | `None`   | Associated agent ID                                          |
| `team_id`     | `Optional[str]` | `None`   | Associated team ID                                           |
| `workflow_id` | `Optional[str]` | `None`   | Associated workflow ID                                       |
| `created_at`  | `datetime`      | Required | When the trace record was created                            |

Convert the trace to a dictionary.

Create a trace from a dictionary.

* `data` (`dict`): Dictionary containing trace data

```python  theme={null}
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="tmp/traces.db")

**Examples:**

Example 1 (unknown):
```unknown
**Returns:** `dict`

### `from_dict()`

Create a trace from a dictionary.
```

Example 2 (unknown):
```unknown
**Parameters:**

* `data` (`dict`): Dictionary containing trace data

**Returns:** `Trace`

## Usage
```

---

## Add from local file to the knowledge base, but don't skip if it already exists

**URL:** llms-txt#add-from-local-file-to-the-knowledge-base,-but-don't-skip-if-it-already-exists

**Contents:**
- Usage

asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
        skip_if_exists=False,
    )
)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/basic_operations/11_skip_if_exists.py
      bash Windows theme={null}
      python cookbook/knowledge/basic_operations/11_skip_if_exists.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Optimize memories for a user

**URL:** llms-txt#optimize-memories-for-a-user

**Contents:**
- Using Memory Tools

optimized = agent.memory_manager.optimize_memories(
    user_id="user_123",
    strategy=MemoryOptimizationStrategyType.SUMMARIZE,
    apply=True,  # Set to False to preview without saving
)
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.memory import MemoryTools

**Examples:**

Example 1 (unknown):
```unknown
See the [Memory Optimization](/basics/memory/working-with-memories/memory-optimization) guide for detailed usage and best practices.

## Using Memory Tools

Instead of automatic memory management, you can give your agent explicit tools to create, retrieve, update, and delete memories. This approach gives the agent more control and reasoning ability, so it can decide when to store something versus when to search for existing memories.

**When to use Memory Tools:**

* You want the agent to reason about whether something is worth remembering
* You need fine-grained control over memory operations (create, update, delete separately)
* You're building a system where the agent should explicitly search memories rather than having them auto-loaded
```

---

## - "Get the top 5 stories (you can try accepting and declining the confirmation)"

**URL:** llms-txt#--"get-the-top-5-stories-(you-can-try-accepting-and-declining-the-confirmation)"

**Contents:**
- Usage

response = agent.run("What are the top 2 hackernews stories?")

for requirement in run_response.active_requirements:
    if requirement.needs_confirmation:
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{requirement.tool.tool_name}({requirement.tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

# Confirm or reject the requirement
        if message == "n":
            requirement.reject()
        else:
            requirement.confirm()

run_response = agent.continue_run(
    run_id=run_response.run_id,
    requirements=run_response.requirements,
)
pprint.pprint_run_response(run_response)
bash  theme={null}
    pip install openai agno
    bash  theme={null}
    python human_in_the_loop.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## LangChain

**URL:** llms-txt#langchain

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/langchain/usage/langchain-db

```python cookbook/knowledge/vector_db/langchain/langchain_db.py theme={null}
import pathlib
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.langchaindb import LangChainVectorDb
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings

---

## Define structured models for each step

**URL:** llms-txt#define-structured-models-for-each-step

class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )

class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )

class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )

---

## Coordinated Reasoning RAG Team

**URL:** llms-txt#coordinated-reasoning-rag-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/coordinated-reasoning-rag

This example demonstrates how multiple specialized agents coordinate to provide comprehensive RAG responses with distributed reasoning capabilities. Each agent has specific reasoning responsibilities to ensure thorough analysis.

```python cookbook/examples/teams/search_coordination/02_coordinated_reasoning_rag.py theme={null}
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses with distributed reasoning capabilities. Each agent
has specific reasoning responsibilities to ensure thorough analysis.

Team Composition:
- Information Gatherer: Searches knowledge base and gathers raw information
- Reasoning Analyst: Applies logical reasoning to analyze gathered information
- Evidence Evaluator: Evaluates evidence quality and identifies gaps
- Response Coordinator: Synthesizes everything into a final reasoned response

Setup:
1. Run: `pip install agno anthropic cohere lancedb tantivy sqlalchemy`
2. Export your ANTHROPIC_API_KEY and CO_API_KEY
3. Run this script to see coordinated reasoning RAG in action
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.cohere import CohereReranker
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.tools.reasoning import ReasoningTools
from agno.vectordb.lancedb import LanceDb, SearchType

---

## Configure email settings

**URL:** llms-txt#configure-email-settings

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

sender_email = "verified-sender@example.com"  # Your verified SES email
sender_name = "Sender Name"
region_name = "us-east-1"

agent = Agent(
    name="Research Newsletter Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        AWSSESTool(
            sender_email=sender_email,
            sender_name=sender_name,
            region_name=region_name
        ),
        DuckDuckGoTools(),
    ],
    markdown=True,
        instructions=[
        "When given a prompt:",
        "1. Extract the recipient's complete email address (e.g. user@domain.com)",
        "2. Research the latest AI developments using DuckDuckGo",
        "3. Compose a concise, engaging email summarising 3 â€“ 4 key developments",
        "4. Send the email using AWS SES via the send_email tool",
    ],
)

agent.print_response(
    "Research recent AI developments in healthcare and email the summary to johndoe@example.com"
)
```

| Parameter           | Type   | Default       | Description                              |
| ------------------- | ------ | ------------- | ---------------------------------------- |
| `sender_email`      | `str`  | `None`        | Verified SES sender address.             |
| `sender_name`       | `str`  | `None`        | Display name that appears to recipients. |
| `region_name`       | `str`  | `"us-east-1"` | AWS region where SES is provisioned.     |
| `enable_send_email` | `bool` | `True`        | Enable the send\_email functionality.    |
| `all`               | `bool` | `False`       | Enable all functionality.                |

| Function     | Description                                                                          |
| ------------ | ------------------------------------------------------------------------------------ |
| `send_email` | Send a plain-text email. Accepts the arguments: `subject`, `body`, `receiver_email`. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/aws_ses.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/aws_ses_tools.py)
* [Amazon SES Documentation](https://docs.aws.amazon.com/ses/latest/dg/)

---

## Initialize ChromaDB

**URL:** llms-txt#initialize-chromadb

vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

---

## User Input Required for Tool Execution

**URL:** llms-txt#user-input-required-for-tool-execution

Source: https://docs.agno.com/basics/hitl/usage/user-input-required

This example demonstrates how to create tools that require user input before execution, allowing for dynamic data collection during agent runs.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Knowledge Management

**URL:** llms-txt#knowledge-management

**Contents:**
- What You Can Add
- Adding Content
- Useful Links

* **View Content**: Browse your Knowledge base contents
* **Add Content**: Upload new documents, add URLs, or input text directly
* **Edit Content**: Modify metadata on existing Knowledge entries
* **Delete Content**: Remove outdated content

* Files: `.pdf`, `.csv`, `.json`, `.txt`, `.doc`, `.docx` , `.md`, `.xlsx`, `.xls`, `.pptx`.

* Web: Website URLs (pages) or direct file links

* Text: Type or paste content directly

<Tip>
  Available processing options (Readers and Chunkers) are provided by your OS
  and may vary by file/URL type.
</Tip>

* Click `ADD NEW CONTENT`, then choose `FILE`, `WEB`, or `TEXT`.

* Drag & drop or select files. You can also add a file URL.
* Add details per item: Name, Description, Metadata, Reader, and optional Chunker.
* Names must be unique across items.
* Save to upload one or many at once.

* Enter one or more URLs and add them to the list.
* Add details per item as above (Name, Description, Metadata, Reader/Chunker).
* Save to upload all listed URLs.

* Paste or type content.
* Set Name, optional Description/Metadata, and Reader/Chunker.
* Add Content to upload.

<CardGroup cols={3}>
  <Card title="Agent Knowledge" icon="user" href="/basics/knowledge/agents/overview">
    Learn how to add knowledge to agents and use RAG
  </Card>

<Card title="Team Knowledge" icon="users" href="/basics/knowledge/teams/overview">
    Understand knowledge sharing in team environments
  </Card>
</CardGroup>

---

## Agent with Input Schema as TypedDict

**URL:** llms-txt#agent-with-input-schema-as-typeddict

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/input-schema-on-agent-as-typed-dict

This example demonstrates how to define an input schema for an agent using `TypedDict`, ensuring structured input validation.

```python input_schema_on_agent_as_typed_dict.py theme={null}
from typing import List, Optional, TypedDict

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools

---

## SurrealDB for Workflow

**URL:** llms-txt#surrealdb-for-workflow

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/surrealdb/usage/surrealdb-for-workflow

Agno supports using SurrealDB as a storage backend for Workflows using the `SurrealDb` class.

Run SurreabDB locally with the following command:

```python surrealdb_for_workflow.py theme={null}

from agno.agent import Agent
from agno.db.surrealdb import SurrealDb
from agno.models.anthropic import Claude
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

**Examples:**

Example 1 (unknown):
```unknown

```

---

## AI Support Team

**URL:** llms-txt#ai-support-team

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/teams/ai_support_team

Build an intelligent customer support team that routes inquiries to specialized agents for documentation search, issue escalation, and feedback collection. This example demonstrates advanced routing with knowledge bases and external integrations.

By building this team, you'll understand:

* How to create a routing system that classifies and directs customer inquiries
* How to integrate knowledge bases with vector search for documentation assistance
* How to connect external tools like Slack for escalation and feedback workflows
* How to combine multiple specialized agents with distinct responsibilities

Build customer support platforms, help desk systems, technical documentation assistants, or automated ticketing systems.

The team uses intelligent routing to direct inquiries to specialized agents:

1. **Classify**: Team leader analyzes the inquiry type (question, bug, feedback)
2. **Route**: Directs to appropriate agent based on classification
3. **Process**: Specialized agent handles the inquiry with their tools
4. **Integrate**: Connects with external systems (Slack, knowledge base)
5. **Respond**: Provides professional response back to the user

Each agent has specific tools and instructions for their domain of responsibility.

```python ai_support_team.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.website_reader import WebsiteReader
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.slack import SlackTools
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="website_documents",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
    ),
)

knowledge.add_content(
    url="https://docs.agno.com/introduction",
    reader=WebsiteReader(
        # Number of links to follow from the seed URLs
        max_links=10,
    ),
)
support_channel = "testing"
feedback_channel = "testing"

doc_researcher_agent = Agent(
    name="Doc researcher Agent",
    role="Search the knowledge base for information",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools(), ExaTools()],
    knowledge=knowledge,
    search_knowledge=True,
    instructions=[
        "You are a documentation expert for given product. Search the knowledge base thoroughly to answer user questions.",
        "Always provide accurate information based on the documentation.",
        "If the question matches an FAQ, provide the specific FAQ answer from the documentation.",
        "When relevant, include direct links to specific documentation pages that address the user's question.",
        "If you're unsure about an answer, acknowledge it and suggest where the user might find more information.",
        "Format your responses clearly with headings, bullet points, and code examples when appropriate.",
        "Always verify that your answer directly addresses the user's specific question.",
        "If you cannot find the answer in the documentation knowledge base, use the DuckDuckGoTools or ExaTools to search the web for relevant information to answer the user's question.",
    ],
)

escalation_manager_agent = Agent(
    name="Escalation Manager Agent",
    role="Escalate the issue to the slack channel",
    model=OpenAIChat(id="gpt-4o"),
    tools=[SlackTools()],
    instructions=[
        "You are an escalation manager responsible for routing critical issues to the support team.",
        f"When a user reports an issue, always send it to the #{support_channel} Slack channel with all relevant details using the send_message toolkit function.",
        "Include the user's name, contact information (if available), and a clear description of the issue.",
        "After escalating the issue, respond to the user confirming that their issue has been escalated.",
        "Your response should be professional and reassuring, letting them know the support team will address it soon.",
        "Always include a ticket or reference number if available to help the user track their issue.",
        "Never attempt to solve technical problems yourself - your role is strictly to escalate and communicate.",
    ],
)

feedback_collector_agent = Agent(
    name="Feedback Collector Agent",
    role="Collect feedback from the user",
    model=OpenAIChat(id="gpt-4o"),
    tools=[SlackTools()],
    description="You are an AI agent that can collect feedback from the user.",
    instructions=[
        "You are responsible for collecting user feedback about the product or feature requests.",
        f"When a user provides feedback or suggests a feature, use the Slack tool to send it to the #{feedback_channel} channel using the send_message toolkit function.",
        "Include all relevant details from the user's feedback in your Slack message.",
        "After sending the feedback to Slack, respond to the user professionally, thanking them for their input.",
        "Your response should acknowledge their feedback and assure them that it will be taken into consideration.",
        "Be warm and appreciative in your tone, as user feedback is valuable for improving our product.",
        "Do not promise specific timelines or guarantee that their suggestions will be implemented.",
    ],
)

customer_support_team = Team(
    name="Customer Support Team",
    model=OpenAIChat(id="gpt-4o"),
    members=[doc_researcher_agent, escalation_manager_agent, feedback_collector_agent],
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
    determine_input_for_members=False,
    respond_directly=True,
    instructions=[
        "You are the lead customer support agent responsible for classifying and routing customer inquiries.",
        "Carefully analyze each user message and determine if it is: a question that needs documentation research, a bug report that requires escalation, or product feedback.",
        "For general questions about the product, route to the doc_researcher_agent who will search documentation for answers.",
        "If the doc_researcher_agent cannot find an answer to a question, escalate it to the escalation_manager_agent.",
        "For bug reports or technical issues, immediately route to the escalation_manager_agent.",
        "For feature requests or product feedback, route to the feedback_collector_agent.",
        "Always provide a clear explanation of why you're routing the inquiry to a specific agent.",
        "After receiving a response from the appropriate agent, relay that information back to the user in a professional and helpful manner.",
        "Ensure a seamless experience for the user by maintaining context throughout the conversation.",
    ],
)

---

## Performance with Database Logging

**URL:** llms-txt#performance-with-database-logging

Source: https://docs.agno.com/basics/evals/performance/usage/performance-db-logging

Example showing how to store performance evaluation results in the database.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Audio to text Agent

**URL:** llms-txt#audio-to-text-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/audio/usage/audio-to-text

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## - `deepseek-r1-distill-llama-70b` as the reasoning model

**URL:** llms-txt#--`deepseek-r1-distill-llama-70b`-as-the-reasoning-model

---

## Web Tools

**URL:** llms-txt#web-tools

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/webtools

WebTools provides utilities for working with web URLs including URL expansion and web-related operations.

The following agent can work with web URLs and expand shortened links:

| Parameter           | Type   | Default | Description                                  |
| ------------------- | ------ | ------- | -------------------------------------------- |
| `retries`           | `int`  | `3`     | Number of retry attempts for URL operations. |
| `enable_expand_url` | `bool` | `True`  | Enable URL expansion functionality.          |

| Function     | Description                                       |
| ------------ | ------------------------------------------------- |
| `expand_url` | Expand shortened URLs to their final destination. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/webtools.py)
* [HTTPX Documentation](https://www.python-httpx.org/)
* [URL Standards](https://tools.ietf.org/html/rfc3986)

---

## Integrate with AgentOS

**URL:** llms-txt#integrate-with-agentos

**Contents:**
- Access AgentOS Routes

agent_os = AgentOS(
    agents=[Agent(id="basic-agent", model=OpenAIChat(id="gpt-5-mini"))],
    base_app=app
)

app = agent_os.get_app()
python  theme={null}
agent_os = AgentOS(agents=[agent])
app = agent_os.get_app()

**Examples:**

Example 1 (unknown):
```unknown
## Access AgentOS Routes

You can programmatically access and inspect the routes added by AgentOS:
```

---

## Delegate to All Members (Cooperation)

**URL:** llms-txt#delegate-to-all-members-(cooperation)

Source: https://docs.agno.com/basics/teams/usage/basic-flows/delegate-to-all-members

This example demonstrates a collaborative team of AI agents working together to research topics across different platforms.

The team consists of two specialized agents:

1. **Reddit Researcher** - Uses DuckDuckGo to find and analyze relevant Reddit posts
2. **HackerNews Researcher** - Uses HackerNews API to find and analyze relevant HackerNews posts

The agents work in a collaborative mode by using `delegate_to_all_members=True`, meaning they:

* Both are given the same task at the same time
* Work towards reaching consensus through discussion
* Are coordinated by a team leader that guides the discussion

The team leader moderates the discussion and determines when consensus is reached.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Memories are automatically created from this conversation

**URL:** llms-txt#memories-are-automatically-created-from-this-conversation

agent.print_response("My name is Sarah and I prefer email over phone calls.")

---

## Define steps for image pipeline

**URL:** llms-txt#define-steps-for-image-pipeline

generate_image_step = Step(
    name="generate_image",
    agent=image_generator,
    description="Generate a detailed image creation prompt based on the user's request",
)

describe_image_step = Step(
    name="describe_image",
    agent=image_describer,
    description="Analyze and describe the generated image concept in vivid detail",
)

---

## Set up the database

**URL:** llms-txt#set-up-the-database

db = SqliteDb(db_file="tmp/agentos.db")

---

## Blog to Podcast Agent

**URL:** llms-txt#blog-to-podcast-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/audio/usage/blog-to-podcast

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Simple function we will use as a pre-hook

**URL:** llms-txt#simple-function-we-will-use-as-a-pre-hook

**Contents:**
  - Pre-hook Parameters
- Post-hooks
  - Common Use Cases
  - Basic Example

def validate_input_length(
    run_input: RunInput,
) -> None:
    """Pre-hook to validate input length."""
    max_length = 1000
    if len(run_input.input_content) > max_length:
        raise InputCheckError(
            f"Input too long. Max {max_length} characters allowed",
            check_trigger=CheckTrigger.INPUT_NOT_ALLOWED,
        )

agent = Agent(
    name="My Agent",
    model=OpenAIChat(id="gpt-4o"),
    # Provide the pre-hook to the Agent using the pre_hooks parameter
    pre_hooks=[validate_input_length],
)
python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.exceptions import CheckTrigger, OutputCheckError
from agno.run.agent import RunOutput

**Examples:**

Example 1 (unknown):
```unknown
You can see complete examples of pre-hooks in the [Examples](/basics/hooks/usage/agent/input-transformation-pre-hook) section.

### Pre-hook Parameters

Pre-hooks run automatically during the Agent run and receive the following parameters:

* `run_input`: The input to the Agent run that can be validated or modified
* `agent`: Reference to the Agent instance
* `session`: The current agent session
* `run_context`: The current run context. See the [Run Context](/reference/run/run-context) reference.
* `debug_mode`: Whether debug mode is enabled (optional)

The framework automatically injects only the parameters your hook function accepts, so you can define hooks with just the parameters you need.

You can learn more about the parameters in the [Pre-hooks](/reference/hooks/pre-hooks) reference.

## Post-hooks

Post-hooks execute **after** your Agent generates a response, allowing you to validate, transform, or enrich the output before it reaches the user.

They're perfect for output filtering, compliance checks, response enrichment, or any other output transformation you need.

### Common Use Cases

**Output Validation**

* Validate response format, length, and content quality.
* Remove sensitive or inappropriate information from responses.
* Ensure compliance with business rules and regulations.

**Output Transformation**

* Add metadata or additional context to responses.
* Transform output format for different clients or use cases.
* Enrich responses with additional data or formatting.

### Basic Example

Let's create a simple post-hook that validates the output length and raises an error if it's too long:
```

---

## Download all sample sales files and get their paths

**URL:** llms-txt#download-all-sample-sales-files-and-get-their-paths

downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

---

## Last run output with metrics and tool calls

**URL:** llms-txt#last-run-output-with-metrics-and-tool-calls

**Contents:**
- Choosing a Pattern
- Learn More

last_run = agent.get_last_run_output()
```

**When to use:** Building your own UI, analytics, debugging, or when you need raw transcripts.

## Choosing a Pattern

* **Short chats:** Leave defaults (history off) or enable `add_history_to_context` with `num_history_runs=3`
* **Long-lived threads:** Combine limited history (`num_history_runs=2`) with [session summaries](/basics/sessions/session-summaries) to keep tokens manageable
* **Tool-heavy agents:** Use `max_tool_calls_from_history` to limit tool call noise in context
* **Audit/debug flows:** Enable `read_chat_history=True` so the model looks things up only when needed
* **Cross-session recall:** Use `search_session_history=True` with `num_history_sessions=2` (keep low to avoid context limits)
* **Programmatic workflows:** Call `get_session_messages()` / `get_chat_history()` directly in your code

For comprehensive guides, detailed examples, and advanced patterns:

<CardGroup cols={3}>
  <Card title="Chat History in Agents" icon="robot" iconType="duotone" href="/basics/chat-history/agent/overview">
    Complete guide to agent history management with detailed examples and advanced patterns.
  </Card>

<Card title="Chat History in Teams" icon="users" iconType="duotone" href="/basics/chat-history/team/overview">
    Team-specific history features, member coordination, and shared context patterns.
  </Card>

<Card title="Chat History Overview" icon="book" iconType="duotone" href="/basics/chat-history/overview">
    Overview of all history capabilities across agents, teams, and workflows.
  </Card>
</CardGroup>

---

## Agent with Reasoning Effort

**URL:** llms-txt#agent-with-reasoning-effort

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/reasoning-effort

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get child spans of a specific parent

**URL:** llms-txt#get-child-spans-of-a-specific-parent

**Contents:**
- Example: Analyzing a Run

children = db.get_spans(parent_span_id=root_span.span_id)
python  theme={null}
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="tmp/traces.db")

**Examples:**

Example 1 (unknown):
```unknown
**Parameters:**

| Parameter        | Type            | Description              |
| ---------------- | --------------- | ------------------------ |
| `trace_id`       | `Optional[str]` | Filter by trace ID       |
| `parent_span_id` | `Optional[str]` | Filter by parent span ID |

**Returns:** `List[Span]`

<Note>
  Unlike `get_traces()`, `get_spans()` returns a list without a count.
</Note>

## Example: Analyzing a Run
```

---

## Span

**URL:** llms-txt#span

**Contents:**
- Span Attributes
- Common Span Names
- Attributes by Operation Type
  - LLM Spans
  - Tool Spans
- Methods
  - `to_dict()`
  - `from_dict()`
- Usage

Source: https://docs.agno.com/reference/tracing/span

A `Span` represents a single operation within an agent execution. Spans form a parent-child hierarchy within a trace, allowing you to understand the execution flow.

| Attribute        | Type             | Default  | Description                                               |
| ---------------- | ---------------- | -------- | --------------------------------------------------------- |
| `span_id`        | `str`            | Required | Unique span identifier                                    |
| `trace_id`       | `str`            | Required | Parent trace ID (groups related spans)                    |
| `parent_span_id` | `Optional[str]`  | `None`   | Parent span ID (`None` for root spans)                    |
| `name`           | `str`            | Required | Operation name (e.g., `OpenAIChat.invoke`, `get_weather`) |
| `status_code`    | `str`            | Required | Status: `OK`, `ERROR`, or `UNSET`                         |
| `status_message` | `Optional[str]`  | `None`   | Status message (typically error details)                  |
| `duration_ms`    | `int`            | Required | Execution time in milliseconds                            |
| `start_time`     | `datetime`       | Required | When the span started                                     |
| `end_time`       | `datetime`       | Required | When the span completed                                   |
| `attributes`     | `Optional[dict]` | `None`   | OpenTelemetry attributes (tokens, params, etc.)           |
| `events`         | `Optional[list]` | `None`   | Span events                                               |
| `kind`           | `Optional[str]`  | `None`   | Span kind (e.g., `INTERNAL`, `CLIENT`)                    |

Spans are automatically created for various operations:

| Span Name Pattern    | Description     |
| -------------------- | --------------- |
| `{AgentName}.run`    | Agent execution |
| `{TeamName}.run`     | Team execution  |
| `{ModelName}.invoke` | LLM model call  |
| `{tool_name}`        | Tool execution  |

## Attributes by Operation Type

The `attributes` field contains OpenTelemetry semantic attributes that vary by operation:

| Attribute                    | Description         |
| ---------------------------- | ------------------- |
| `llm.token_count.prompt`     | Input token count   |
| `llm.token_count.completion` | Output token count  |
| `llm.model_name`             | Model identifier    |
| `llm.provider`               | Model provider name |

| Attribute         | Description                  |
| ----------------- | ---------------------------- |
| `tool.name`       | Tool function name           |
| `tool.parameters` | Tool input parameters (JSON) |

Convert the span to a dictionary.

Create a span from a dictionary.

* `data` (`dict`): Dictionary containing span data

```python  theme={null}
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="tmp/traces.db")

**Examples:**

Example 1 (unknown):
```unknown
**Returns:** `dict`

### `from_dict()`

Create a span from a dictionary.
```

Example 2 (unknown):
```unknown
**Parameters:**

* `data` (`dict`): Dictionary containing span data

**Returns:** `Span`

## Usage
```

---

## Agent Context

**URL:** llms-txt#agent-context

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/08-agent-context

This example shows how to inject external dependencies into an agent. The context is evaluated when the agent is run, acting like dependency injection for Agents.

Example prompts to try:

* "Summarize the top stories on HackerNews"
* "What are the trending tech discussions right now?"
* "Analyze the current top stories and identify trends"
* "What's the most upvoted story today?"

```python agent_context.py theme={null}
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat

def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)

---

## Setup our AgentOS app

**URL:** llms-txt#setup-our-agentos-app

**Contents:**
- Usage
- Key Features
- Team Members

agent_os = AgentOS(
    teams=[research_team],
    interfaces=[AGUI(team=research_team)],
)
app = agent_os.get_app()

if __name__ == "__main__":
    """Run our AgentOS.

You can see the configuration and available apps at:
    http://localhost:7777/config

"""
    agent_os.serve(app="research_team:app", reload=True)
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python cookbook/os/interfaces/agui/research_team.py
      bash Windows theme={null}
      python cookbook/os/interfaces/agui/research_team.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* **Multi-Agent Collaboration**: Researcher and writer working together
* **Specialized Roles**: Distinct expertise and responsibilities
* **Transparent Process**: See individual agent contributions
* **Coordinated Workflow**: Structured research-to-content pipeline
* **Web Interface**: Professional team interaction through AG-UI

* **Researcher**: Information gathering and analysis specialist
* **Writer**: Content creation and structuring expert
* **Workflow**: Sequential collaboration from research to final content

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create distributed reranking RAG team

**URL:** llms-txt#create-distributed-reranking-rag-team

**Contents:**
- Usage

distributed_reranking_team = Team(
    name="Distributed Reranking RAG Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[
        initial_retriever,
        reranking_specialist,
        context_analyzer,
        final_synthesizer,
    ],
    instructions=[
        "Work together to provide optimal RAG responses using advanced reranking.",
        "Initial Retriever: First perform broad comprehensive retrieval.",
        "Reranking Specialist: Apply advanced reranking for result optimization.",
        "Context Analyzer: Analyze and validate the reranked results.",
        "Final Synthesizer: Create optimal responses from reranked information.",
        "Leverage advanced reranking for superior result quality.",
        "Demonstrate the benefits of specialized reranking in team coordination.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_reranking_rag_demo():
    """Demonstrate async distributed reranking RAG processing."""
    print("ðŸŽ¯ Async Distributed Reranking RAG Demo")
    print("=" * 45)

query = "What's the best way to prepare authentic Tom Kha Gai? I want traditional methods and modern variations."

# Add content to knowledge bases
    await reranked_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    await validation_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# Run async distributed reranking RAG
    await aprint_response(input=query, team=distributed_reranking_team)

def sync_reranking_rag_demo():
    """Demonstrate sync distributed reranking RAG processing."""
    print("ðŸŽ¯ Distributed Reranking RAG Demo")
    print("=" * 35)

query = "What's the best way to prepare authentic Tom Kha Gai? I want traditional methods and modern variations."

# Add content to knowledge bases
    reranked_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    validation_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# Run distributed reranking RAG
    print_response(distributed_reranking_team, query)

def advanced_culinary_demo():
    """Demonstrate advanced reranking for complex culinary queries."""
    print("ðŸ‘¨â€ðŸ³ Advanced Culinary Analysis with Reranking RAG")
    print("=" * 55)

query = """I want to understand the science behind Thai curry pastes. Can you explain:
    - Traditional preparation methods vs modern techniques
    - How different ingredients affect flavor profiles
    - Regional variations and their historical origins
    - Best practices for storage and usage
    - How to adapt recipes for different dietary needs"""

# Add content to knowledge bases
    reranked_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    validation_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

print_response(distributed_reranking_team, query)

if __name__ == "__main__":
    # Choose which demo to run
    asyncio.run(async_reranking_rag_demo())

# advanced_culinary_demo()

# sync_reranking_rag_demo()
bash  theme={null}
    pip install agno openai lancedb tantivy pypdf sqlalchemy cohere
    bash  theme={null}
    export OPENAI_API_KEY=****
    export COHERE_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/distributed_rag/03_distributed_rag_with_reranking.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Define steps

**URL:** llms-txt#define-steps

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

---

## OpenRouter

**URL:** llms-txt#openrouter

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/openrouter

The OpenRouter model provides unified access to various language models through OpenRouter.

| Parameter               | Type            | Default                          | Description                                                           |
| ----------------------- | --------------- | -------------------------------- | --------------------------------------------------------------------- |
| `id`                    | `str`           | `"openai/gpt-4o-mini"`           | The id of the OpenRouter model to use                                 |
| `name`                  | `str`           | `"OpenRouter"`                   | The name of the model                                                 |
| `provider`              | `str`           | `"OpenRouter"`                   | The provider of the model                                             |
| `api_key`               | `Optional[str]` | `None`                           | The API key for OpenRouter (defaults to OPENROUTER\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://openrouter.ai/api/v1"` | The base URL for the OpenRouter API                                   |
| `app_name`              | `Optional[str]` | `"agno"`                         | Application name for OpenRouter request headers                       |
| `retries`               | `int`           | `0`                              | Number of retries to attempt before raising a ModelProviderError      |
| `delay_between_retries` | `int`           | `1`                              | Delay between retries, in seconds                                     |
| `exponential_backoff`   | `bool`          | `False`                          | If True, the delay between retries is doubled each time               |

OpenRouter extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Agent execution is automatically traced

**URL:** llms-txt#agent-execution-is-automatically-traced

response = agent.run("What is the capital of France?")
print(response.content)
python  theme={null}
from traceloop.sdk import Traceloop
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
* ### Example: Development Mode (Disable Batching)

For local development, disable batching to see traces immediately:
```

---

## Async Basic Stream.Py

**URL:** llms-txt#async-basic-stream.py

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/huggingface/usage/async-basic-stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Last N Session Messages

**URL:** llms-txt#last-n-session-messages

Source: https://docs.agno.com/basics/state/agent/usage/last-n-session-messages

This example demonstrates how to configure agents to search through previous sessions and limit the number of historical sessions included in context. This helps manage context length while maintaining relevant conversation history.

<Steps>
  <Step title="Create a Python file">
    Create a file called `last_n_session_messages.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Please download a sample video file to test this Agent

**URL:** llms-txt#please-download-a-sample-video-file-to-test-this-agent

---

## DashScope

**URL:** llms-txt#dashscope

**Contents:**
- Authentication
- Example
- Parameters
- Thinking Models

Source: https://docs.agno.com/integrations/models/native/dashscope/overview

Learn how to use DashScope models in Agno.

Leverage DashScope's powerful command models and more.

[DashScope](https://dashscope.aliyun.com/) supports a wide range of models

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `qwen-plus` model is good for most use-cases.

Set your `DASHSCOPE_API_KEY` environment variable. Get your key from [here](https://dashscope.aliyun.com/api-keys).

Use `DashScope` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/native/dashscope/usage/basic). </Note>

| Parameter          | Type             | Default                                                    | Description                                                                                                                                                                                                                                |
| ------------------ | ---------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `id`               | `str`            | `"qwen-plus"`                                              | The id of the Qwen model to use                                                                                                                                                                                                            |
| `name`             | `str`            | `"Qwen"`                                                   | The name of the model                                                                                                                                                                                                                      |
| `provider`         | `str`            | `"Dashscope"`                                              | The provider of the model                                                                                                                                                                                                                  |
| `api_key`          | `Optional[str]`  | `None`                                                     | The API key for DashScope (defaults to DASHSCOPE\_API\_KEY env var)                                                                                                                                                                        |
| `base_url`         | `str`            | `"https://dashscope-intl.aliyuncs.com/compatible-mode/v1"` | The base URL for the DashScope API. For users in Mainland China, you must set the base URL to [https://dashscope.aliyuncs.com/compatible-mode/v1](https://dashscope.aliyuncs.com/compatible-mode/v1) to avoid the â€œinvalid API keyâ€ error. |
| `enable_thinking`  | `bool`           | `False`                                                    | Enable thinking process for reasoning models                                                                                                                                                                                               |
| `include_thoughts` | `Optional[bool]` | `None`                                                     | Include thinking process in response (alternative parameter)                                                                                                                                                                               |
| `thinking_budget`  | `Optional[int]`  | `None`                                                     | Budget for thinking tokens in reasoning models                                                                                                                                                                                             |

`DashScope` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/integrations/models/native/openai/completion/overview).

DashScope supports reasoning models with thinking capabilities:

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `DashScope` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note> View more examples [here](/integrations/models/native/dashscope/usage/basic). </Note>

## Parameters

| Parameter          | Type             | Default                                                    | Description                                                                                                                                                                                                                                |
| ------------------ | ---------------- | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `id`               | `str`            | `"qwen-plus"`                                              | The id of the Qwen model to use                                                                                                                                                                                                            |
| `name`             | `str`            | `"Qwen"`                                                   | The name of the model                                                                                                                                                                                                                      |
| `provider`         | `str`            | `"Dashscope"`                                              | The provider of the model                                                                                                                                                                                                                  |
| `api_key`          | `Optional[str]`  | `None`                                                     | The API key for DashScope (defaults to DASHSCOPE\_API\_KEY env var)                                                                                                                                                                        |
| `base_url`         | `str`            | `"https://dashscope-intl.aliyuncs.com/compatible-mode/v1"` | The base URL for the DashScope API. For users in Mainland China, you must set the base URL to [https://dashscope.aliyuncs.com/compatible-mode/v1](https://dashscope.aliyuncs.com/compatible-mode/v1) to avoid the â€œinvalid API keyâ€ error. |
| `enable_thinking`  | `bool`           | `False`                                                    | Enable thinking process for reasoning models                                                                                                                                                                                               |
| `include_thoughts` | `Optional[bool]` | `None`                                                     | Include thinking process in response (alternative parameter)                                                                                                                                                                               |
| `thinking_budget`  | `Optional[int]`  | `None`                                                     | Budget for thinking tokens in reasoning models                                                                                                                                                                                             |

`DashScope` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/integrations/models/native/openai/completion/overview).

## Thinking Models

DashScope supports reasoning models with thinking capabilities:
```

---

## Multi-User, Multi-Session Chat Concurrently

**URL:** llms-txt#multi-user,-multi-session-chat-concurrently

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/memory/agent/usage/multi-user-multi-session-chat-concurrent

This example shows how to run a multi-user, multi-session chat concurrently.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Performance on Agent Instantiation

**URL:** llms-txt#performance-on-agent-instantiation

Source: https://docs.agno.com/basics/evals/performance/usage/performance-agent-instantiation

Example showing how to analyze the runtime and memory usage of an Agent.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Streamable HTTP Transport

**URL:** llms-txt#streamable-http-transport

Source: https://docs.agno.com/basics/tools/mcp/transports/streamable_http

The new [Streamable HTTP transport](https://modelcontextprotocol.io/specification/draft/basic/transports#streamable-http) replaces the HTTP+SSE transport from protocol version `2024-11-05`.

This transport enables the MCP server to handle multiple client connections, and can also use SSE for server-to-client streaming.

To use it, initialize the `MCPTools` passing the URL of the MCP server and setting the transport to `streamable-http`:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

---

## Pdf Input Local

**URL:** llms-txt#pdf-input-local

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/pdf-input-local

```python cookbook/models/openai/responses/pdf_input_local.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai.responses import OpenAIResponses
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

---

## Team with Reasoning Tools

**URL:** llms-txt#team-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/reasoning-tool-team

This is a multi-agent team reasoning example with reasoning tools.

<Tip>
  Enabling the reasoning option on the team leader helps optimize delegation and enhances multi-agent collaboration by selectively invoking deeper reasoning when required.
</Tip>

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## You can also get the response in a variable:

**URL:** llms-txt#you-can-also-get-the-response-in-a-variable:

---

## Nexus

**URL:** llms-txt#nexus

**Contents:**
- Authentication
- Example
- Params

Source: https://docs.agno.com/integrations/models/gateways/nexus/overview

Learn how to use Nexus models in Agno.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.0.6" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.0.6">v2.0.6</Tooltip>
</Badge>

Nexus is a routing platform that provides endpoints for various Large Language Models through a unified API interface.

Explore Nexus's capabilities and documentation [here](https://nexusrouter.com/).

Nexus requires API keys for the underlying model providers. Set the appropriate environment variables for the models you plan to use:

Use `Nexus` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/gateways/nexus/usage/basic-stream). </Note>

| Parameter  | Type            | Default                         | Description                                                 |
| ---------- | --------------- | ------------------------------- | ----------------------------------------------------------- |
| `id`       | `str`           | `"gpt-4o-mini"`                 | The id of the model to use through Nexus                    |
| `name`     | `str`           | `"Nexus"`                       | The name of the model                                       |
| `provider` | `str`           | `"Nexus"`                       | The provider of the model                                   |
| `api_key`  | `Optional[str]` | `None`                          | The API key for Nexus (defaults to NEXUS\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.nexusflow.ai/v1"` | The base URL for the Nexus API                              |

`Nexus` also supports the params of [OpenAI](/reference/models/openai).

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Nexus` with your `Agent`:

<CodeGroup>
```

---

## Clean AgentOS setup with tuple middleware pattern! âœ¨

**URL:** llms-txt#clean-agentos-setup-with-tuple-middleware-pattern!-âœ¨

agent_os = AgentOS(
    description="JWT Protected AgentOS",
    agents=[research_agent],
    base_app=app,
)

---

## With Docker and Docker Compose

**URL:** llms-txt#with-docker-and-docker-compose

---

## Step 3: Define instructions

**URL:** llms-txt#step-3:-define-instructions

complete_instructions = dedent("""
    You are a Senior Social Media Intelligence Analyst specializing in cross-platform
    brand monitoring and strategic analysis.

DATA COLLECTION STRATEGY:
    - Use X Tools to gather direct social media mentions with full engagement metrics
    - Use Exa Tools to find broader web discussions, articles, and forum conversations
    - Cross-reference findings between social and web sources for comprehensive coverage

ANALYSIS FRAMEWORK:
    - Classify sentiment as Positive/Negative/Neutral/Mixed with detailed reasoning
    - Weight analysis by engagement volume and author influence (verified accounts = 1.5x)
    - Identify engagement patterns: viral advocacy, controversy, influence concentration
    - Extract cross-platform themes and recurring discussion points

INTELLIGENCE SYNTHESIS:
    - Detect crisis indicators through sentiment velocity and coordination patterns
    - Identify competitive positioning and feature gap discussions
    - Surface growth opportunities and advocacy moments
    - Generate strategic recommendations with clear priority levels

### Executive Dashboard
    - **Brand Health Score**: [1-10] with supporting evidence
    - **Net Sentiment**: [%positive - %negative] with trend analysis
    - **Key Drivers**: Top 3 positive and negative factors
    - **Alert Level**: Normal/Monitor/Crisis with threshold reasoning

### Quantitative Metrics
    | Sentiment | Posts | % | Avg Engagement | Influence Score |
    |-----------|-------|---|----------------|-----------------|
    [Detailed breakdown with engagement weighting]

### Strategic Recommendations
    **IMMEDIATE (â‰¤48h)**: Crisis response, high-impact replies
    **SHORT-TERM (1-2 weeks)**: Content strategy, community engagement
    **LONG-TERM (1-3 months)**: Product positioning, market strategy

ANALYSIS PRINCIPLES:
    - Evidence-based conclusions with supporting metrics
    - Actionable insights that drive business decisions
    - Cross-platform correlation analysis
    - Influence-weighted sentiment scoring
    - Proactive risk and opportunity identification
""")

---

## Audio input requires specific audio-enabled models like gpt-5-mini-audio-preview

**URL:** llms-txt#audio-input-requires-specific-audio-enabled-models-like-gpt-5-mini-audio-preview

**Contents:**
- Usage

agent = Agent(
    model=LiteLLM(id="gpt-5-mini-audio-preview"),
    markdown=True,
)
agent.print_response(
    "What's the audio about?",
    audio=[Audio(content=mp3_data, format="mp3")],
    stream=True,
)

bash  theme={null}
    export LITELLM_API_KEY=xxx
    bash  theme={null}
    pip install -U litellm agno
    bash Mac theme={null}
      python cookbook/models/litellm/audio_input_agent.py
      bash Windows theme={null}
      python cookbook/models/litellm/audio_input_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## GCS Content

**URL:** llms-txt#gcs-content

Source: https://docs.agno.com/reference/knowledge/remote-content/gcs-content

GCSContent is a class that allows you to add content from a GCS bucket to the knowledge base.

<Snippet file="gcs-remote-content-params.mdx" />

---

## Refresh Metrics

**URL:** llms-txt#refresh-metrics

Source: https://docs.agno.com/reference-api/schema/metrics/refresh-metrics

post /metrics/refresh
Manually trigger recalculation of system metrics from raw data. This operation analyzes system activity logs and regenerates aggregated metrics. Useful for ensuring metrics are up-to-date or after system maintenance.

---

## RAG with Sentence Transformer

**URL:** llms-txt#rag-with-sentence-transformer

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/rag-sentence-transformer

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set environment variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>
      
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set environment variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Enable agentic filtering

**URL:** llms-txt#enable-agentic-filtering

**Contents:**
- Usage

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    enable_agentic_knowledge_filters=True,
)

agent.print_response(
    "Tell me about revenue performance and top selling products in the region north_america and data_type sales",
    markdown=True,
)

bash  theme={null}
    pip install -U agno lancedb openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/knowledge/filters/agentic_filtering.py
      bash Windows theme={null}
      python cookbook/knowledge/filters/agentic_filtering.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Searches for: return policies, time limits, return procedures

**URL:** llms-txt#searches-for:-return-policies,-time-limits,-return-procedures

---

## Loop Steps

**URL:** llms-txt#loop-steps

Source: https://docs.agno.com/reference/workflows/loop-steps

| Parameter        | Type                                                                                                 | Default  | Description                                 |
| ---------------- | ---------------------------------------------------------------------------------------------------- | -------- | ------------------------------------------- |
| `steps`          | `WorkflowSteps`                                                                                      | Required | Steps to execute in each loop iteration     |
| `name`           | `Optional[str]`                                                                                      | `None`   | Name of the loop step                       |
| `description`    | `Optional[str]`                                                                                      | `None`   | Description of the loop step                |
| `max_iterations` | `int`                                                                                                | `3`      | Maximum number of iterations for the loop   |
| `end_condition`  | `Optional[Union[Callable[[List[StepOutput]], bool], Callable[[List[StepOutput]], Awaitable[bool]]]]` | `None`   | Function to evaluate if the loop should end |

---

## Initialize Pinecone

**URL:** llms-txt#initialize-pinecone

api_key = getenv("PINECONE_API_KEY")
index_name = "filtering-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)

---

## Discord

**URL:** llms-txt#discord

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/social/discord

**DiscordTools** enable an agent to send messages, read message history, manage channels, and delete messages in Discord.

The following example requires a Discord bot token which can be obtained from [here](https://discord.com/developers/applications).

| Parameter                   | Type   | Default | Description                                                   |
| --------------------------- | ------ | ------- | ------------------------------------------------------------- |
| `bot_token`                 | `str`  | -       | Discord bot token for authentication.                         |
| `enable_messaging`          | `bool` | `True`  | Whether to enable sending messages to channels.               |
| `enable_history`            | `bool` | `True`  | Whether to enable retrieving message history from channels.   |
| `enable_channel_management` | `bool` | `True`  | Whether to enable fetching channel info and listing channels. |
| `enable_message_management` | `bool` | `True`  | Whether to enable deleting messages from channels.            |

| Function               | Description                                                                                   |
| ---------------------- | --------------------------------------------------------------------------------------------- |
| `send_message`         | Send a message to a specified channel. Returns a success or error message.                    |
| `get_channel_info`     | Retrieve information about a specified channel. Returns the channel info as a JSON string.    |
| `list_channels`        | List all channels in a specified server (guild). Returns the list of channels as JSON.        |
| `get_channel_messages` | Retrieve message history from a specified channel. Returns messages as a JSON string.         |
| `delete_message`       | Delete a specific message by ID from a specified channel. Returns a success or error message. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/discord.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/discord.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Define the custom retriever

**URL:** llms-txt#define-the-custom-retriever

---

## Delete Memory

**URL:** llms-txt#delete-memory

Source: https://docs.agno.com/reference-api/schema/memory/delete-memory

delete /memories/{memory_id}
Permanently delete a specific user memory. This action cannot be undone.

---

## Few-Shot Learning with Customer Support Team

**URL:** llms-txt#few-shot-learning-with-customer-support-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/few-shot-learning

This example demonstrates few-shot learning by providing example conversations to teach a customer support team proper response patterns. The team learns from provided examples to handle different types of customer issues with appropriate escalation and communication patterns.

```python cookbook/examples/teams/basic/few_shot_learning.py theme={null}
"""
This example shows a straightforward use case of additional_input
to teach a customer support team proper response patterns.
"""

from agno.agent import Agent
from agno.models.message import Message
from agno.models.openai import OpenAIChat
from agno.team import Team

---

## Initialize PostgresTools with connection details

**URL:** llms-txt#initialize-postgrestools-with-connection-details

postgres_tools = PostgresTools(
    host="localhost",
    port=5532,
    db_name="ai",
    user="ai",
    password="ai"
)

---

## Database Support

**URL:** llms-txt#database-support

**Contents:**
- Adding a database to your Agent

Source: https://docs.agno.com/basics/database/overview

Enable your Agents to store session history, memories, and more.

Adding a database to your Agents adds persistence, allowing them to remember previous messages, user memories, and more.

It works by equipping your Agents and Teams with a database that is used to store and retrieve:

* [Sessions](/basics/sessions/overview)
* [User Memories](/basics/memory/overview)
* [Knowledge](/basics/knowledge/content-db)
* [Evals](/basics/evals/overview)
* [Culture](/basics/culture/overview)

<Note>
  Culture is an experimental feature.
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=aa1aa1bf3b18e6d008bb7f1e17b3f539" alt="Team structure" style={{ maxWidth: '100%', transform: 'scale(1.1)' }} data-og-width="3741" width="3741" data-og-height="906" height="906" data-path="images/storage-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=0023c1e7e592ae9b779bbf99c23d5289 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=0737ecbdf0c0114eb46283bac2da9452 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=19c47c18c749b0725fd22d07296ef4f6 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=04ce3da485e8c4bd0750f7146296ad0e 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=7f65a9a54f14f0368da8cbe008dc3981 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=97fcf30feacd1e264ea90d71efa06ed6 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=18c17269a4ced29b5a160768cc86e2d6" alt="Team structure" style={{ maxWidth: '100%', transform: 'scale(1.1)' }} data-og-width="3741" width="3741" data-og-height="906" height="906" data-path="images/storage-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=f0d7c601809b3f6874f90b2b45b4be96 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=814c6ba4c626e21a645fa54c26dd5c25 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=715ac7ca26004ca669051e5797be9f13 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=7045e13a52b0ffbd6fcf45f48bffa7c8 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=3bebd2a3ef5ea85c99cb85a0c490a53c 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/storage-overview-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c083d35f8fe5acfb87feac38e21eaae4 2500w" />

## Adding a database to your Agent

To enable session persistence on your Agent, create a database and provide it to the Agent:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SQLiteDb

---

## Example 3: Widescreen generator for panoramic images

**URL:** llms-txt#example-3:-widescreen-generator-for-panoramic-images

widescreen_agent = Agent(
    tools=[
        NanoBananaTools(
            aspect_ratio="16:9"  # Widescreen format
        )
    ],
    name="Widescreen NanoBanana Generator",
)

---

## Count tokens before

**URL:** llms-txt#count-tokens-before

strategy = SummarizeStrategy()
tokens_before = strategy.count_tokens(original_memories)

---

## Get recent traces

**URL:** llms-txt#get-recent-traces

traces, total_count = db.get_traces(limit=20)

---

## Team Session

**URL:** llms-txt#team-session

**Contents:**
- TeamSession Attributes
- TeamSession Methods
  - `upsert_run(run: TeamRunOutput)`
  - `get_run(run_id: str) -> Optional[RunOutput]`
  - `get_messages(...) -> List[Message]`
  - `get_session_summary() -> Optional[SessionSummary]`
  - `get_chat_history(last_n_runs: Optional[int] = None) -> List[Message]`

Source: https://docs.agno.com/reference/teams/session

## TeamSession Attributes

| Parameter      | Type                                              | Default  | Description                                             |
| -------------- | ------------------------------------------------- | -------- | ------------------------------------------------------- |
| `session_id`   | `str`                                             | Required | Session UUID                                            |
| `team_id`      | `Optional[str]`                                   | `None`   | ID of the team that this session is associated with     |
| `user_id`      | `Optional[str]`                                   | `None`   | ID of the user interacting with this team               |
| `workflow_id`  | `Optional[str]`                                   | `None`   | ID of the workflow that this session is associated with |
| `team_data`    | `Optional[Dict[str, Any]]`                        | `None`   | Team Data: name, team\_id, model, and mode              |
| `session_data` | `Optional[Dict[str, Any]]`                        | `None`   | Session Data: session\_state, images, videos, audio     |
| `metadata`     | `Optional[Dict[str, Any]]`                        | `None`   | Metadata stored with this team                          |
| `runs`         | `Optional[List[Union[TeamRunOutput, RunOutput]]]` | `None`   | List of all runs in the session                         |
| `summary`      | `Optional[SessionSummary]`                        | `None`   | Summary of the session                                  |
| `created_at`   | `Optional[int]`                                   | `None`   | The unix timestamp when this session was created        |
| `updated_at`   | `Optional[int]`                                   | `None`   | The unix timestamp when this session was last updated   |

## TeamSession Methods

### `upsert_run(run: TeamRunOutput)`

Adds a TeamRunOutput to the runs list. If a run with the same `run_id` already exists, it updates the existing run.

### `get_run(run_id: str) -> Optional[RunOutput]`

Retrieves a specific run by its `run_id`.

### `get_messages(...) -> List[Message]`

Returns the messages belonging to the session that fit the given criteria.

* `team_id` (Optional\[str]): The id of the team to get the messages from
* `member_ids` (Optional\[List\[str]]): The ids of the members to get the messages from
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs
* `limit` (Optional\[int]): The number of messages to return, counting from the latest. Defaults to all messages
* `skip_roles` (Optional\[List\[str]]): Skip messages with these roles
* `skip_statuses` (Optional\[List\[RunStatus]]): Skip messages with these statuses
* `skip_history_messages` (bool): Skip messages that were tagged as history in previous runs. Defaults to True
* `skip_member_messages` (bool): Skip messages created by members of the team. Defaults to True

* `List[Message]`: The messages for the session

### `get_session_summary() -> Optional[SessionSummary]`

Get the session summary for the session

### `get_chat_history(last_n_runs: Optional[int] = None) -> List[Message]`

Get the chat history (user and assistant messages) for the session. Use `get_messages()` for more filtering options.

* `last_n_runs` (Optional\[int]): Number of recent runs to include. If None, all runs will be considered

* `List[Message]`: The chat history for the session

---

## for route in agent_os.get_routes():

**URL:** llms-txt#for-route-in-agent_os.get_routes():

---

## Custom reader configuration

**URL:** llms-txt#custom-reader-configuration

reader = PDFReader(
    chunk_size=1000,
    chunking_strategy=SemanticChunking(),
)

knowledge_base = Knowledge(
    vector_db=vector_db,
)

---

## Reddit search agent with tool hooks

**URL:** llms-txt#reddit-search-agent-with-tool-hooks

reddit_agent = Agent(
    name="Reddit Agent",
    id="reddit-agent",
    role="Search reddit for information",
    tools=[RedditTools(cache_results=True)],
    instructions=[
        "Find information about the company on Reddit",
    ],
    tool_hooks=[logger_hook],
)

---

## Example 1: Create a task

**URL:** llms-txt#example-1:-create-a-task

print("\n=== Create a task ===")
todoist_agent.print_response("Create a todoist task to buy groceries tomorrow at 10am")

---

## Multi-user, Multi-session Chat

**URL:** llms-txt#multi-user,-multi-session-chat

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/memory/agent/usage/multi-user-multi-session-chat

This example demonstrates how to run a multi-user, multi-session chat.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## -*- Only include tables that are in the target_metadata

**URL:** llms-txt#-*--only-include-tables-that-are-in-the-target_metadata

def include_name(name, type_, parent_names):
    if type_ == "table":
        return name in target_metadata.tables
    else:
        return True
...
```

---

## Create an agent with the knowledge base

**URL:** llms-txt#create-an-agent-with-the-knowledge-base

**Contents:**
- Usage
- Params

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

if __name__ == "__main__":
    asyncio.run(
        knowledge.add_content_async(
            path="cookbook/knowledge/testing_resources/cv_1.pdf",
        )
    )
    # Create and use the agent
    asyncio.run(
        agent.aprint_response(
            "What skills does an applicant require to apply for the Software Engineer position?",
            markdown=True,
        )
    )
bash  theme={null}
    pip install -U pypdf sqlalchemy psycopg pgvector agno openai  
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python examples/basics/knowledge/readers/pdf_reader_async.py
      bash Windows theme={null}
      python examples/basics/knowledge/readers/pdf_reader_async.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="pdf-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 3: Get all tasks

**URL:** llms-txt#example-3:-get-all-tasks

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

print("\n=== Get all tasks ===")
todoist_agent.print_response("Get all the todoist tasks")
```

| Parameter   | Type  | Default | Description                                             |
| ----------- | ----- | ------- | ------------------------------------------------------- |
| `api_token` | `str` | `None`  | If you want to manually supply the TODOIST\_API\_TOKEN. |

| Function           | Description                                                                                     |
| ------------------ | ----------------------------------------------------------------------------------------------- |
| `create_task`      | Creates a new task in Todoist with optional project assignment, due date, priority, and labels. |
| `get_task`         | Fetches a specific task.                                                                        |
| `update_task`      | Updates an existing task with new properties such as content, due date, priority, etc.          |
| `close_task`       | Marks a task as completed.                                                                      |
| `delete_task`      | Deletes a specific task from Todoist.                                                           |
| `get_active_tasks` | Retrieves all active (non-completed) tasks.                                                     |
| `get_projects`     | Retrieves all projects in Todoist.                                                              |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/todoist.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/todoist_tool.py)

---

## RedisDb

**URL:** llms-txt#redisdb

Source: https://docs.agno.com/reference/storage/redis

`RedisDb` is a class that implements the Db interface using Redis as the backend storage system. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-redis-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## JsonDb

**URL:** llms-txt#jsondb

Source: https://docs.agno.com/reference/storage/json

`JsonDb` is a class that implements the Db interface using JSON files as the backend storage system. It provides a simple, file-based storage solution for agent sessions with each session stored in a separate JSON file.

<Snippet file="db-json-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Preview cultural knowledge (truncated for readability)

**URL:** llms-txt#preview-cultural-knowledge-(truncated-for-readability)

**Contents:**
- Cultural Knowledge Data Model
- Best Practices
- Common Use Cases
  - Technical Documentation Standards

for knowledge in all_knowledge:
    print(knowledge.preview())
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Cultural Knowledge Data Model

Each cultural knowledge entry in your database contains the following fields:

| Field        | Type   | Description                                       |
| ------------ | ------ | ------------------------------------------------- |
| `id`         | `str`  | Unique identifier (auto-generated)                |
| `name`       | `str`  | Name/title of the cultural knowledge              |
| `content`    | `str`  | The main content of the principle/knowledge       |
| `summary`    | `str`  | Brief summary of the knowledge                    |
| `categories` | `list` | Categories (e.g., "communication", "engineering") |
| `notes`      | `list` | Additional notes or context                       |
| `metadata`   | `dict` | Arbitrary metadata (source, version, etc.)        |
| `input`      | `str`  | Original input that generated this knowledge      |
| `created_at` | `int`  | Timestamp when created (epoch seconds)            |
| `updated_at` | `int`  | Timestamp when last updated (epoch seconds)       |
| `agent_id`   | `str`  | ID of the agent that created it                   |
| `team_id`    | `str`  | ID of the team associated with it                 |

## Best Practices

1. **Start with Manual Seeding:** Define core organizational principles, communication standards, and best practices upfront
2. **Use Automatic Updates in Production:** Let `update_cultural_knowledge=True` handle the evolution naturally
3. **Review Periodically:** Check what cultural knowledge has accumulated and refine as needed
4. **Keep Culture Focused:** Culture should contain universal principles, not task-specific details
5. **Combine with Memory:** Use Culture for "how we do things" and Memory for "what I know about you"

## Common Use Cases

### Technical Documentation Standards
```

---

## Check if team has session state and display information

**URL:** llms-txt#check-if-team-has-session-state-and-display-information

print("\nðŸ“Š Team Session Info:")
session = team.get_session()
print(f"   Session ID: {session.session_id}")
print(f"   Session State: {session.session_data['session_state']}")

---

## Session Summaries

**URL:** llms-txt#session-summaries

**Contents:**
- The Problem: Growing Token Costs
- The Solution: Automatic Summaries
- How It Works
- Enable Session Summaries
  - Customizing Generation
- Use Summary in Context
- When to Use Session Summaries

Source: https://docs.agno.com/basics/sessions/session-summaries

Automatically condense long conversations into concise summaries

As conversations grow longer, passing the entire chat history to your LLM becomes expensive and slow. Session summaries solve this by automatically condensing conversations into concise summaries that capture the key points.

Think of it like taking notes during a long meeting - you don't need a transcript of everything said, just the important bits.

## The Problem: Growing Token Costs

Without summaries, every message adds to your context window:

This quickly becomes expensive and hits context limits.

## The Solution: Automatic Summaries

Session summaries condense your history:

* âœ… Dramatically reduced token costs
* âœ… Avoid context window limits
* âœ… Maintain conversation continuity
* âœ… Automatic creation and updates

Session summaries follow a simple three-step pattern:

<Steps>
  <Step title="Enable Summary Generation">
    Set `enable_session_summaries=True` on your agent or team. Summaries are automatically created and updated after runs when there are meaningful messages to summarize, then stored in your database.
  </Step>

<Step title="Use Summaries in Context">
    Set `add_session_summary_to_context=True` to include the summary in your messages (this is enabled by default if you enable session summary generation). Instead of sending dozens of historical messages, only the condensed summary is sent, dramatically reducing tokens while maintaining context.
  </Step>

<Step title="Customize (Optional)">
    Use [`SessionSummaryManager`](/reference/session/summary_manager) to control summary generation - use a cheaper model, customize prompts, or change the summary format. This lets you optimize costs by using a lightweight model for summaries while keeping your main agent powerful.
  </Step>
</Steps>

## Enable Session Summaries

Turn on `enable_session_summaries=True` to have Agno maintain a rolling summary for each session. Summaries sit alongside the stored history and can be reused later to save tokens.

### Customizing Generation

* Provide a [`SessionSummaryManager`](/reference/session/summary_manager) to specify a cheaper model or custom prompt
* Run summary generation out-of-band by instantiating a lightweight Agent that just calls `get_session_summary` across all sessions

## Use Summary in Context

`add_session_summary_to_context=True` is enabled by default if you enable session summary generation. If you don't want summaries to be generated, but still want to use them in context, you can set `add_session_summary_to_context=True`. Alternatively, if you don't want to use summaries in context, you can set `add_session_summary_to_context=False`.

Agno automatically loads the latest summary from storage before each run. You can still mix in recent history:

## When to Use Session Summaries

* Long-running customer support conversations
* Multi-day or multi-week interactions
* Conversations with 10+ turns
* Production systems where cost matters

**âš ï¸ Consider alternatives for:**

* Short conversations (fewer than 5 turns)
* When full detail is critical
* Real-time chat with recent context only

**Examples:**

Example 1 (unknown):
```unknown
Run 1: 100 tokens
Run 2: 250 tokens (100 history + 150 new)
Run 3: 450 tokens (250 history + 200 new)
Run 4: 750 tokens (450 history + 300 new)
...exponential growth
```

Example 2 (unknown):
```unknown
Run 1: 100 tokens
Run 2: 250 tokens
[Summary created: 50 tokens]
Run 3: 250 tokens (50 summary + 200 new)
Run 4: 350 tokens (50 summary + 300 new)
...linear growth
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Customizing Generation

* Provide a [`SessionSummaryManager`](/reference/session/summary_manager) to specify a cheaper model or custom prompt
* Run summary generation out-of-band by instantiating a lightweight Agent that just calls `get_session_summary` across all sessions

## Use Summary in Context

`add_session_summary_to_context=True` is enabled by default if you enable session summary generation. If you don't want summaries to be generated, but still want to use them in context, you can set `add_session_summary_to_context=True`. Alternatively, if you don't want to use summaries in context, you can set `add_session_summary_to_context=False`.

<CodeGroup>
```

---

## When initializing the knowledge, we can attach metadata that will be used for filtering

**URL:** llms-txt#when-initializing-the-knowledge,-we-can-attach-metadata-that-will-be-used-for-filtering

---

## Response Synthesizer Agent - Specialized in creating comprehensive responses

**URL:** llms-txt#response-synthesizer-agent---specialized-in-creating-comprehensive-responses

response_synthesizer = Agent(
    name="Response Synthesizer",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Create comprehensive final response with proper citations",
    instructions=[
        "Synthesize information from team members into a comprehensive response.",
        "Include proper source citations and references.",
        "Ensure accuracy and completeness of the final answer.",
        "Structure the response clearly with appropriate formatting.",
    ],
    markdown=True,
)

---

## Make sure you define the `__call__` method that receives

**URL:** llms-txt#make-sure-you-define-the-`__call__`-method-that-receives

---

## memory_manager.clear()

**URL:** llms-txt#memory_manager.clear()

---

## Basic Setup

**URL:** llms-txt#basic-setup

**Contents:**
- Installation
- Two Ways to Enable Tracing
  - Option 1: Using `setup_tracing()`

Source: https://docs.agno.com/basics/tracing/basic-setup

Configure and enable tracing for your Agno agents

This guide walks you through setting up tracing for your Agno agents. Tracing is designed to be simple: install dependencies, enable tracing, and all your agents are automatically instrumented.

Install the required OpenTelemetry packages:

<Note>
  These packages provide the OpenTelemetry instrumentation infrastructure and the Agno-specific instrumentation logic.
</Note>

## Two Ways to Enable Tracing

There are two ways to enable tracing in Agno:

1. **`setup_tracing()`** - Use this function for standalone scripts, notebooks, and custom applications. Provides full control over configuration options like batch processing, queue sizes, and export delays.

2. **AgentOS `tracing=True`** - Use this parameter when deploying agents through AgentOS. Simpler setup for production deployments with sensible defaults.

### Option 1: Using `setup_tracing()`

For standalone scripts, notebooks, or custom applications:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tracing import setup_tracing

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  These packages provide the OpenTelemetry instrumentation infrastructure and the Agno-specific instrumentation logic.
</Note>

## Two Ways to Enable Tracing

There are two ways to enable tracing in Agno:

1. **`setup_tracing()`** - Use this function for standalone scripts, notebooks, and custom applications. Provides full control over configuration options like batch processing, queue sizes, and export delays.

2. **AgentOS `tracing=True`** - Use this parameter when deploying agents through AgentOS. Simpler setup for production deployments with sensible defaults.

### Option 1: Using `setup_tracing()`

For standalone scripts, notebooks, or custom applications:
```

---

## Search for either customer feedback or survey data from the last two years

**URL:** llms-txt#search-for-either-customer-feedback-or-survey-data-from-the-last-two-years

**Contents:**
- Using Filters with Teams

feedback_filter = AND(
    OR(
        EQ("data_type", "feedback"),
        EQ("data_type", "survey")
    ),
    GT("year", 2022)
)

agent.print_response(
    "What do our customers think about our new features?",
    knowledge_filters=[feedback_filter]  # â† List wrapper required
)
python  theme={null}
from agno.team.team import Team
from agno.agent import Agent
from agno.filters import IN, AND, NOT, EQ

**Examples:**

Example 1 (unknown):
```unknown
## Using Filters with Teams

Teams can also leverage filtered knowledge searches:
```

---

## Create a Steps sequence that chains these above steps together

**URL:** llms-txt#create-a-steps-sequence-that-chains-these-above-steps-together

article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

---

## Only search recent content

**URL:** llms-txt#only-search-recent-content

recent_filter = GT("year", current_year - 2)

---

## the tool call loop will exit and the run will complete

**URL:** llms-txt#the-tool-call-loop-will-exit-and-the-run-will-complete

**Contents:**
- Developer Resources

agent.print_response("Use the check_condition tool to check if 150 is acceptable", stream=True)
```

In this example, when `check_condition` is called with a value greater than 100, it raises `StopAgentRun`. The tool call loop exits immediately, and the run completes with status `COMPLETED`. All session state, messages, and tool calls up to that point are stored in the database.

<Tip>
  Make sure to set `AGNO_DEBUG=True` if you want to see the debug logs.
</Tip>

## Developer Resources

* View the [RetryAgentRun schema](/reference/tools/retry-agent-run)
* View the [StopAgentRun schema](/reference/tools/stop-agent-run)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/exceptions)

---

## and +91 1234567890 with the recipient's WhatsApp ID

**URL:** llms-txt#and-+91-1234567890-with-the-recipient's-whatsapp-id

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "Send a template message using the '''hello_world''' template in English to +91 1234567890"
)
```

| Parameter         | Type            | Default   | Description                                                                                                               |
| ----------------- | --------------- | --------- | ------------------------------------------------------------------------------------------------------------------------- |
| `access_token`    | `Optional[str]` | `None`    | WhatsApp Business API access token. If not provided, uses `WHATSAPP_ACCESS_TOKEN` environment variable.                   |
| `phone_number_id` | `Optional[str]` | `None`    | WhatsApp Business Account phone number ID. If not provided, uses `WHATSAPP_PHONE_NUMBER_ID` environment variable.         |
| `version`         | `str`           | `"v22.0"` | API version to use. If not provided, uses `WHATSAPP_VERSION` environment variable or defaults to "v22.0".                 |
| `recipient_waid`  | `Optional[str]` | `None`    | Default recipient WhatsApp ID (e.g., "1234567890"). If not provided, uses `WHATSAPP_RECIPIENT_WAID` environment variable. |
| `async_mode`      | `bool`          | `False`   | Enable asynchronous methods for sending messages.                                                                         |

| Function                      | Description                                                                                                                                                                                           |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `send_text_message_sync`      | Sends a text message to a WhatsApp user (synchronous). Parameters: `text` (str), `recipient` (Optional\[str]), `preview_url` (bool), `recipient_type` (str).                                          |
| `send_template_message_sync`  | Sends a template message to a WhatsApp user (synchronous). Parameters: `recipient` (Optional\[str]), `template_name` (str), `language_code` (str), `components` (Optional\[List\[Dict\[str, Any]]]).  |
| `send_text_message_async`     | Sends a text message to a WhatsApp user (asynchronous). Parameters: `text` (str), `recipient` (Optional\[str]), `preview_url` (bool), `recipient_type` (str).                                         |
| `send_template_message_async` | Sends a template message to a WhatsApp user (asynchronous). Parameters: `recipient` (Optional\[str]), `template_name` (str), `language_code` (str), `components` (Optional\[List\[Dict\[str, Any]]]). |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/whatsapp.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/whatsapp_tools.py)

---

## Async Agent with Tools

**URL:** llms-txt#async-agent-with-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/async-tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Linear

**URL:** llms-txt#linear

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/linear

**LinearTool** enable an Agent to perform [Linear](https://linear.app/) tasks.

The following examples require Linear API key, which can be obtained from [here](https://linear.app/settings/account/security).

The following agent will use Linear API to search for issues in a project for a specific user.

| Parameter | Type  | Default | Description         |
| --------- | ----- | ------- | ------------------- |
| `api_key` | `str` | `None`  | Add Linear API key. |

| Function                   | Description                                                      |
| -------------------------- | ---------------------------------------------------------------- |
| `get_user_details`         | Fetch authenticated user details.                                |
| `get_issue_details`        | Retrieve details of a specific issue by issue ID.                |
| `create_issue`             | Create a new issue within a specific project and team.           |
| `update_issue`             | Update the title or state of a specific issue by issue ID.       |
| `get_user_assigned_issues` | Retrieve issues assigned to a specific user by user ID.          |
| `get_workflow_issues`      | Retrieve issues within a specific workflow state by workflow ID. |
| `get_high_priority_issues` | Retrieve issues with a high priority (priority `<=` 2).          |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/linear.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/linear_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Linear API to search for issues in a project for a specific user.
```

---

## Agent Infra Railway

**URL:** llms-txt#agent-infra-railway

**Contents:**
  - Local environment
  - Production environment
  - How to get started

Source: https://docs.agno.com/templates/agent-infra-railway/introduction

The Agent Infra Railway template provides a simple Railway project for running AgentOS.

### Local environment

Runs both AgentOS and PostgreSQL using Docker Desktop.

Ideal for development, testing, and verifying your setup before deploying to the cloud.

### Production environment

AgentOS is deployed on Railway using the Dockerfile and a PgVector template is used as the database

The template automates infrastructure creation and deployment steps using a single script.

### How to get started

* Clone the template repository and install the dependencies
* Start with the local setup to ensure everything works end-to-end.
* Move to Railway deployment using the provided guides once your local environment is ready.

<Snippet file="setup.mdx" />

<Snippet file="create-agent-infra-railway-codebase.mdx" />

You can also clone the template directory and follow the instructions in the template documentation.

<CodeGroup>
  
</CodeGroup>

After creating your codebase, the next step is to get it up and running locally using docker

---

## Autonomous Startup Team

**URL:** llms-txt#autonomous-startup-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/autonomous_startup_team

This example shows how to create an autonomous startup team that can self-organize and drive innovative projects.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Count tokens after optimization

**URL:** llms-txt#count-tokens-after-optimization

tokens_after = strategy.count_tokens(memories_after)
print(f"  Token count: {tokens_after} tokens")

---

## Run a vector-based query

**URL:** llms-txt#run-a-vector-based-query

**Contents:**
- Usage

results = vector_db.search("chicken coconut soup", limit=5)
print("Vector Search Results:", results)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/search_type/vector_search.py
      bash Windows theme={null}
      python cookbook/knowledge/search_type/vector_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Async MongoDB

**URL:** llms-txt#async-mongodb

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/async-mongo/overview

Learn to use MongoDB asynchronously as a database for your Agents

Agno supports using [MongoDB](https://www.mongodb.com/) asynchronously, with the `AsyncMongoDb` class.

<Tip>
  **v2 Migration Support**: If you're upgrading from Agno v1, MongoDB is fully supported in the v2 migration script. See the [migration guide](/how-to/v2-migration) for details.
</Tip>

```python async_mongodb_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.mongo import AsyncMongoDb

---

## File Generation

**URL:** llms-txt#file-generation

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/file-generation/file-generation

The `FileGenerationTools` toolkit enables Agents and Teams to generate files in multiple formats.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.0.6" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.0.6">v2.0.6</Tooltip>
</Badge>

<Tip>
  Supported file types:

* JSON
  * CSV
  * PDF
  * TXT
</Tip>

1. **Install the libraries:**

2. **Set your credentials:**
   For OpenAI API:

The following agent will generate files in different formats based on user requests.

<Note>
  You can use the `output_directory` parameter to specify a custom output directory for the generated files.
  If not specified, the files will be available in the `RunOutput` object.
</Note>

| Parameter                | Type   | Default | Description                                      |
| ------------------------ | ------ | ------- | ------------------------------------------------ |
| `enable_json_generation` | `bool` | `True`  | Enables JSON file generation                     |
| `enable_csv_generation`  | `bool` | `True`  | Enables CSV file generation                      |
| `enable_pdf_generation`  | `bool` | `True`  | Enables PDF file generation (requires reportlab) |
| `enable_txt_generation`  | `bool` | `True`  | Enables text file generation                     |
| `output_directory`       | `str`  | `None`  | Custom output directory path                     |
| `all`                    | `bool` | `False` | Enables all file generation types when True      |

| Name                 | Description                                                  |
| -------------------- | ------------------------------------------------------------ |
| `generate_json_file` | Generates a JSON file from data (dict, list, or JSON string) |
| `generate_csv_file`  | Generates a CSV file from tabular data                       |
| `generate_pdf_file`  | Generates a PDF document from text content                   |
| `generate_text_file` | Generates a plain text file from string content              |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/file_generation.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/file_generation_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
2. **Set your credentials:**
   For OpenAI API:
```

Example 2 (unknown):
```unknown
## Example

The following agent will generate files in different formats based on user requests.
```

---

## Website Reader Async

**URL:** llms-txt#website-reader-async

**Contents:**
- Code
- Usage
- Params

Source: https://docs.agno.com/basics/knowledge/readers/usage/website-reader-async

The **Website Reader** with asynchronous processing crawls and processes entire websites efficiently, following links to create comprehensive knowledge bases from web content.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Snippet file="run-pgvector-step.mdx" />

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

<Snippet file="website-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## After a few conversations, optimize memories for a user

**URL:** llms-txt#after-a-few-conversations,-optimize-memories-for-a-user

**Contents:**
  - Preview Optimization Results

optimized = memory_manager.optimize_memories(
    user_id="user_123",
    strategy=MemoryOptimizationStrategyType.SUMMARIZE,
    apply=True,  # Set to False to preview without saving
)

print(f"Optimized {len(optimized)} memories")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Preview Optimization Results

Before applying optimization, you can preview the results:
```

---

## Ask "How are you?" in all supported languages

**URL:** llms-txt#ask-"how-are-you?"-in-all-supported-languages

**Contents:**
- Send input directly to members

multi_language_team.print_response(
    "How are you?", stream=True  # English
)

multi_language_team.print_response(
    "ãŠå…ƒæ°—ã§ã™ã‹?", stream=True  # Japanese
)
python  theme={null}
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field

class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements."""

topic: str = Field(description="The main research topic")
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  `respond_directly` is not compatible with `delegate_to_all_members`.
</Note>

<Note>
  When using `respond_directly` and the team leader decides to delegate the task to multiple members at the same time, the final content will be the results of all member responses concatenated together.
</Note>

## Send input directly to members

By default, the team leader determines what "task" to give each member based on the user input.

Set `determine_input_for_members=False` to send the original user input **directly** to member agents. The team leader still selects which members to delegate to, but doesn't transform the input.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=50e30700e0ab7e2dee727e085960825a" alt="Send input directly to members flow" data-og-width="4896" width="4896" data-og-height="906" height="906" data-path="images/teams/team-raw-input-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=0915e2f2d4c1b30d9b7617f2225dd597 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c6f8a0bb2ea0872d494c591cd797c20f 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=ba0fb4acdd28b52c72b5e990e9290252 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=fbc8d51f02a5f844ddca981e45b37f8d 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=7b3f4e882827ed6dcf70345bc629d15e 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=93a164ca96e5c33a1c7b7d46961c69c5 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=e3d140d90d77da02c526c606c9446a52" alt="Send input directly to members flow" data-og-width="4896" width="4896" data-og-height="906" height="906" data-path="images/teams/team-raw-input-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=88866626fd38333a840b7737a79b31d9 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=090e93bb46d57b37465123d3e97242ff 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=dc7f8458ae6685266454c72485ccedf7 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=941c0045feaec734857677b822a17e2a 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=04eba2381d53da383c59061d6eaf67db 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-raw-input-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=afe6376289414c8e72dedf7f17c19483 2500w" />

<Tip>
  This is useful for structured inputs (like Pydantic models) that you want members to receive unchanged, or when you have specialized agents and want queries routed automatically without modification.
</Tip>

**Example:** Send structured Pydantic input directly to a specialized research agent:
```

---

## Create and use workflow

**URL:** llms-txt#create-and-use-workflow

**Contents:**
- Params
- Developer Resources

if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=db,
        steps=[research_step, content_planning_step],
    )
    content_creation_workflow.print_response(
        input="AI trends in 2024",
        markdown=True,
    )
```

<Snippet file="db-surrealdb-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/surrealdb/surrealdb_for_workflow.py)

---

## Setup the Supabase database

**URL:** llms-txt#setup-the-supabase-database

db = PostgresDb(db_url=SUPABASE_DB_URL)

---

## Daytona

**URL:** llms-txt#daytona

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/daytona

Enable your Agents to run code in a remote, secure sandbox.

**Daytona** offers secure and elastic infrastructure for runnning your AI-generated code. At Agno, we integrate with it to enable your Agents and Teams to run code in your Daytona sandboxes.

The Daytona tools require the `daytona_sdk` Python package:

You will also need a Daytona API key. You can get it from your [Daytona account](https://app.daytona.io/account):

The following example demonstrates how to create an agent that can run Python code in a Daytona sandbox:

```python cookbook/tools/daytona_tools.py theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.daytona import DaytonaTools

daytona_tools = DaytonaTools()

**Examples:**

Example 1 (unknown):
```unknown
You will also need a Daytona API key. You can get it from your [Daytona account](https://app.daytona.io/account):
```

Example 2 (unknown):
```unknown
## Example

The following example demonstrates how to create an agent that can run Python code in a Daytona sandbox:
```

---

## Async SQLite for Workflow

**URL:** llms-txt#async-sqlite-for-workflow

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/async-sqlite/usage/async-sqlite-for-workflow

Agno supports using SQLite asynchronously as a storage backend for Workflows, with the `AsyncSqliteDb` class.

```python async_sqlite_for_workflow.py theme={null}
"""
Run: `pip install openai ddgs sqlalchemy aiosqlite` to install dependencies
"""

from agno.agent import Agent
from agno.db.sqlite import AsyncSqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Composio

**URL:** llms-txt#composio

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions

Source: https://docs.agno.com/integrations/toolkits/others/composio

[**ComposioTools**](https://docs.composio.dev/framework/phidata) enable an Agent to work with tools like Gmail, Salesforce, Github, etc.

The following example requires the `composio-agno` library.

The following agent will use Github Tool from Composio Toolkit to star a repo.

The following parameters are used when calling the GitHub star repository action:

| Parameter | Type  | Default | Description                          |
| --------- | ----- | ------- | ------------------------------------ |
| `owner`   | `str` | -       | The owner of the repository to star. |
| `repo`    | `str` | -       | The name of the repository to star.  |

Composio Toolkit provides 1000+ functions to connect to different software tools.
Open this [link](https://composio.dev/tools) to view the complete list of functions.

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Github Tool from Composio Toolkit to star a repo.
```

---

## Redis for Team

**URL:** llms-txt#redis-for-team

**Contents:**
- Usage
  - Run Redis
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/redis/usage/redis-for-team

Agno supports using Redis as a storage backend for Teams using the `RedisDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Redis** on port **6379** using:

<Snippet file="db-redis-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/redis/redis_for_team.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Memory creation requires a db to be provided

**URL:** llms-txt#memory-creation-requires-a-db-to-be-provided

**Contents:**
- Agent Performance with Storage
- Agent Instantiation Performance
- Team Instantiation Performance
- Team Performance with Memory Updates

db = SqliteDb(db_file="tmp/memory.db")

def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        system_message="Be concise, reply with one sentence.",
        db=db,
        enable_user_memories=True,
    )

response = agent.run("My name is Tom! I'm 25 years old and I live in New York.")
    print(f"Agent response: {response.content}")

response_with_memory_updates_perf = PerformanceEval(
    name="Memory Updates Performance",
    func=run_agent,
    num_iterations=5,
    warmup_runs=0,
)

if __name__ == "__main__":
    response_with_memory_updates_perf.run(print_results=True, print_summary=True)

python storage_performance.py theme={null}
"""Run `pip install openai agno` to install dependencies."""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat

db = SqliteDb(db_file="tmp/storage.db")

def run_agent():
    agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        system_message="Be concise, reply with one sentence.",
        add_history_to_context=True,
        db=db,
    )
    response_1 = agent.run("What is the capital of France?")
    print(response_1.content)

response_2 = agent.run("How many people live there?")
    print(response_2.content)

return response_2.content

response_with_storage_perf = PerformanceEval(
    name="Storage Performance",
    func=run_agent,
    num_iterations=1,
    warmup_runs=0,
)

if __name__ == "__main__":
    response_with_storage_perf.run(print_results=True, print_summary=True)
python agent_instantiation.py theme={null}
"""Run `pip install agno openai` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval

def instantiate_agent():
    return Agent(system_message="Be concise, reply with one sentence.")

instantiation_perf = PerformanceEval(
    name="Instantiation Performance", func=instantiate_agent, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)
python team_instantiation.py theme={null}
"""Run `pip install agno openai` to install dependencies."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat
from agno.team import Team

team_member = Agent(model=OpenAIChat(id="gpt-5-mini"))

def instantiate_team():
    return Team(members=[team_member])

instantiation_perf = PerformanceEval(
    name="Instantiation Performance Team", func=instantiate_team, num_iterations=1000
)

if __name__ == "__main__":
    instantiation_perf.run(print_results=True, print_summary=True)
python team_performance_with_memory_updates.py theme={null}

"""Run `pip install agno openai` to install dependencies."""

import asyncio
import random

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat
from agno.team import Team

cities = [
    "New York",
    "Los Angeles",
    "Chicago",
    "Houston",
    "Miami",
    "San Francisco",
    "Seattle",
    "Boston",
    "Washington D.C.",
    "Atlanta",
    "Denver",
    "Las Vegas",
]

**Examples:**

Example 1 (unknown):
```unknown
## Agent Performance with Storage

Test agent performance with storage:
```

Example 2 (unknown):
```unknown
## Agent Instantiation Performance

Test agent instantiation performance:
```

Example 3 (unknown):
```unknown
## Team Instantiation Performance

Test team instantiation performance:
```

Example 4 (unknown):
```unknown
## Team Performance with Memory Updates

Test team performance with memory updates:
```

---

## Example usage with different types of videos

**URL:** llms-txt#example-usage-with-different-types-of-videos

youtube_agent.print_response(
    "Analyze this video: https://www.youtube.com/watch?v=zjkBMFhNj_g",
    stream=True,
)

---

## Enable scrape method for raw HTML content

**URL:** llms-txt#enable-scrape-method-for-raw-html-content

**Contents:**
  - All Functions with JavaScript Rendering

scrapegraph_scrape = ScrapeGraphTools(enable_scrape=True, enable_smartscraper=False)

scrape_agent = Agent(
    tools=[scrapegraph_scrape],
    model=agent_model,
    markdown=True,
    stream=True,
)

scrape_agent.print_response(
    "Use the scrape tool to get the complete raw HTML content from https://en.wikipedia.org/wiki/2025_FIFA_Club_World_Cup"
)
python cookbook/tools/scrapegraph_tools.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### All Functions with JavaScript Rendering

Enable all ScrapeGraph functions with heavy JavaScript support:
```

---

## Agentic RAG with PgVector

**URL:** llms-txt#agentic-rag-with-pgvector

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag-pgvector

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run PgVector">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run PgVector">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Templates

**URL:** llms-txt#templates

**Contents:**
- What are Templates?
- How to get started with a template:

Source: https://docs.agno.com/deploy/templates

## What are Templates?

**Templates are standardized codebases for AgentOS.** They contain:

* An **AgentOS instance** using FastAPI.
* A **PostgreSQL database** for storing sessions, memories, evals and knowledge.
* A set of **pre-built Agents, Teams and Workflows** to use as a starting point.

Instead of starting from scratch, you can use a template to get started and ship your AgentOS to production. They are setup to run locally using docker and on cloud providers. They're a fantastic starting point and are used by various companies using Agno. You'll definitely need to customize them to fit your specific needs, but they'll get you started much faster.

Agno currently supports the following templates:

Docker: [agent-infra-docker](https://github.com/agno-agi/agent-infra-docker)
AWS: [agent-infra-aws](https://github.com/agno-agi/agent-infra-aws)
Railway: [agent-infra-railway](https://github.com/agno-agi/agent-infra-railway)

* [Modal](https://modal.com/)
* [Render](https://render.com/)
* [Fly.io](https://fly.io/)
* [GCP](https://cloud.google.com/)

Additionally, all templates contain a dev and prd environment. The dev environment is used for development and testing. The prd environment is used for production. We recommend using docker for the dev environment for most use cases and selecting a cloud provider of your choice for the prd environment.

## How to get started with a template:

1. Make sure you have the agno and agno-infra installed.

2. Create your codebase using: `ag infra create` and choose a template.

3. `cd` into your codebase and run the dev environment.

4. Run the prd environment after you've tested the dev environment and are ready to deploy to production.

* The instructions for dev and prd environment are different for each template. Please refer to the template documentation for specific instructions.
* Instead of using `ag infra create`, you can clone the repository and follow the instructions in the template documentation.

<CardGroup cols={3}>
  <Card title="Agent Infra Docker " icon="server" href="/templates/agent-infra-docker/introduction">
    An AgentOS template with Docker compose file.
  </Card>

<Card title="Agent Infra AWS" icon="server" href="/templates/agent-infra-aws/introduction">
    An AgentOS template with AWS infrastructure.
  </Card>

<Card title="Agent Infra Railway" icon="server" href="/templates/agent-infra-railway/introduction">
    An AgentOS template with Railway infrastructure.
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

2. Create your codebase using: `ag infra create` and choose a template.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Audio Sentiment Analysis Team

**URL:** llms-txt#audio-sentiment-analysis-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/audio-sentiment-analysis

This example demonstrates how a team can collaborate to perform sentiment analysis on audio conversations using transcription and sentiment analysis agents working together.

```python cookbook/examples/teams/multimodal/audio_sentiment_analysis.py theme={null}
import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.media import Audio
from agno.models.google import Gemini
from agno.team import Team

transcription_agent = Agent(
    name="Audio Transcriber",
    role="Transcribe audio conversations accurately",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Transcribe audio with speaker identification",
        "Maintain conversation structure and flow",
    ],
)

sentiment_analyst = Agent(
    name="Sentiment Analyst",
    role="Analyze emotional tone and sentiment in conversations",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze sentiment for each speaker separately",
        "Identify emotional patterns and conversation dynamics",
        "Provide detailed sentiment insights",
    ],
)

---

## Add content - gets embedded automatically

**URL:** llms-txt#add-content---gets-embedded-automatically

knowledge.add_content(
    text_content="The sky is blue during the day and dark at night."
)

---

## Nebius

**URL:** llms-txt#nebius

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/nebius

The Nebius model provides access to Nebius's text and image models.

| Parameter               | Type            | Default                                    | Description                                                      |
| ----------------------- | --------------- | ------------------------------------------ | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"meta-llama/Meta-Llama-3.1-70B-Instruct"` | The id of the Nebius model to use                                |
| `name`                  | `str`           | `"Nebius"`                                 | The name of the model                                            |
| `provider`              | `str`           | `"Nebius"`                                 | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                                     | The API key for Nebius (defaults to NEBIUS\_API\_KEY env var)    |
| `base_url`              | `str`           | `"https://api.tokenfactory.nebius.com/v1"` | The base URL for the Nebius API                                  |
| `retries`               | `int`           | `0`                                        | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                                        | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                                    | If True, the delay between retries is doubled each time          |

Nebius extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Setting Environment Variables

**URL:** llms-txt#setting-environment-variables

**Contents:**
- macOS
  - Setting Environment Variables in Shell
- Windows
  - Setting Environment Variables in PowerShell
  - Setting Environment Variables in Windows Command Prompt

Source: https://docs.agno.com/faq/environment-variables

To configure your environment for applications, you may need to set environment variables. This guide provides instructions for setting environment variables in both macOS (Shell) and Windows (PowerShell and Windows Command Prompt).

### Setting Environment Variables in Shell

#### Temporary Environment Variables

These environment variables will only be available in the current shell session.

To display the environment variable:

#### Permanent Environment Variables

To make environment variables persist across sessions, add them to your shell configuration file (e.g., `.bashrc`, `.bash_profile`, `.zshrc`).

To display the environment variable:

### Setting Environment Variables in PowerShell

#### Temporary Environment Variables

These environment variables will only be available in the current PowerShell session.

To display the environment variable:

#### Permanent Environment Variables

To make environment variables persist across sessions, add them to your PowerShell profile script (e.g., `Microsoft.PowerShell_profile.ps1`).

Add the following line to the profile script:

Save and close the file, then reload the profile:

To display the environment variable:

### Setting Environment Variables in Windows Command Prompt

#### Temporary Environment Variables

These environment variables will only be available in the current Command Prompt session.

To display the environment variable:

#### Permanent Environment Variables

To make environment variables persist across sessions, you can use the `setx` command:

Note: After setting an environment variable using `setx`, you need to restart the Command Prompt or any applications that need to read the new environment variable.

To display the environment variable in a new Command Prompt session:

By following these steps, you can effectively set and display environment variables in macOS Shell, Windows Command Prompt, and PowerShell. This will ensure your environment is properly configured for your applications.

**Examples:**

Example 1 (unknown):
```unknown
To display the environment variable:
```

Example 2 (unknown):
```unknown
#### Permanent Environment Variables

To make environment variables persist across sessions, add them to your shell configuration file (e.g., `.bashrc`, `.bash_profile`, `.zshrc`).

For Zsh:
```

Example 3 (unknown):
```unknown
To display the environment variable:
```

Example 4 (unknown):
```unknown
## Windows

### Setting Environment Variables in PowerShell

#### Temporary Environment Variables

These environment variables will only be available in the current PowerShell session.
```

---

## Web Fetch

**URL:** llms-txt#web-fetch

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/web-fetch

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure chunking strategy with a reader

**URL:** llms-txt#configure-chunking-strategy-with-a-reader

reader = PDFReader(
    chunking_strategy=SemanticChunking(similarity_threshold=0.7)
)

---

## AgentOS with MCPTools

**URL:** llms-txt#agentos-with-mcptools

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/mcp/mcp-tools-example

Complete AgentOS setup with MCPTools enabled on agents

```python cookbook/agent_os/mcp/mcp_tools_example.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools

---

## Define research team for complex analysis

**URL:** llms-txt#define-research-team-for-complex-analysis

research_team = Team(
    name="Research Team",
    model=Claude(id="claude-sonnet-4-5-20250929"),
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=Claude(id="claude-sonnet-4-5-20250929"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

---

## Filtering on ChromaDB

**URL:** llms-txt#filtering-on-chromadb

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-chroma-db

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in ChromaDB.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.chroma import ChromaDb

---

## 3. Create knowledge base

**URL:** llms-txt#3.-create-knowledge-base

knowledge = Knowledge(
    name="Company Documentation",
    vector_db=vector_db,
    max_results=10,
    contents_db=contents_db,
)

---

## MLX Transcribe

**URL:** llms-txt#mlx-transcribe

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/mlx-transcribe

**MLX Transcribe** is a tool for transcribing audio files using MLX Whisper.

1. **Install ffmpeg**

* macOS: `brew install ffmpeg`
   * Ubuntu: `sudo apt-get install ffmpeg`
   * Windows: Download from [https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)

2. **Install mlx-whisper library**

3. **Prepare audio files**

* Create a 'storage/audio' directory
   * Place your audio files in this directory
   * Supported formats: mp3, mp4, wav, etc.

4. **Download sample audio** (optional)
   * Visit the [audio-samples](https://audio-samples.github.io/) (as an example) and save the audio file to the `storage/audio` directory.

The following agent will use MLX Transcribe to transcribe audio files.

```python cookbook/tools/mlx_transcribe_tools.py theme={null}

from pathlib import Path
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mlx_transcribe import MLXTranscribeTools

**Examples:**

Example 1 (unknown):
```unknown
3. **Prepare audio files**

   * Create a 'storage/audio' directory
   * Place your audio files in this directory
   * Supported formats: mp3, mp4, wav, etc.

4. **Download sample audio** (optional)
   * Visit the [audio-samples](https://audio-samples.github.io/) (as an example) and save the audio file to the `storage/audio` directory.

## Example

The following agent will use MLX Transcribe to transcribe audio files.
```

---

## Share Memory between Agents

**URL:** llms-txt#share-memory-between-agents

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/memory/agent/usage/agents-share-memory

This example demonstrates how to share memory between Agents.

This means that memories created by one Agent, will be available to the other Agents.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Create a knowledge base containing information from a URL

**URL:** llms-txt#create-a-knowledge-base-containing-information-from-a-url

agno_docs = Knowledge(
    # Use LanceDB as the vector database and store embeddings in the `agno_docs` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)
agno_docs.add_content(
    url="https://docs.agno.com/llms-full.txt"
)

knowledge_tools = KnowledgeTools(
    knowledge=agno_docs,
    think=True,
    search=True,
    analyze=True,
    add_few_shot=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[knowledge_tools],
    markdown=True,
)

if __name__ == "__main__":
    agent.print_response("How do I build multi-agent teams with Agno?", stream=True)
python  theme={null}
from agno.tools.knowledge import KnowledgeTools

knowledge_tools = KnowledgeTools(
    knowledge=my_knowledge_base,
    think=True,                # Enable the think tool
    search=True,               # Enable the search tool
    analyze=True,              # Enable the analyze tool
    add_instructions=True,     # Add default instructions
    add_few_shot=True,         # Add few-shot examples
    few_shot_examples=None,    # Optional custom few-shot examples
)
```

**Examples:**

Example 1 (unknown):
```unknown
The toolkit comes with default instructions and few-shot examples to help the Agent use the tools effectively. Here is how you can configure them:
```

---

## Get all spans for a trace

**URL:** llms-txt#get-all-spans-for-a-trace

**Contents:**
- Building a Span Tree

spans = db.get_spans(trace_id=trace.trace_id)

for span in spans:
    print(f"Span: {span.name}")
    print(f"  Duration: {span.duration_ms}ms")
    print(f"  Status: {span.status_code}")
    
    # Check for token usage (LLM spans)
    if span.attributes:
        tokens = span.attributes.get("llm.token_count.completion")
        if tokens:
            print(f"  Tokens: {tokens}")
python  theme={null}
def print_span_tree(spans, parent_id=None, indent=0):
    """Recursively print spans as a tree."""
    for span in spans:
        if span.parent_span_id == parent_id:
            prefix = "  " * indent + ("â””â”€ " if indent > 0 else "")
            print(f"{prefix}{span.name} ({span.duration_ms}ms)")
            print_span_tree(spans, span.span_id, indent + 1)

**Examples:**

Example 1 (unknown):
```unknown
## Building a Span Tree
```

---

## Workflow logs at DEBUG level when debug_mode is enabled

**URL:** llms-txt#workflow-logs-at-debug-level-when-debug_mode-is-enabled

---

## Create a team for collaborative structured output generation

**URL:** llms-txt#create-a-team-for-collaborative-structured-output-generation

**Contents:**
- Usage

movie_team = Team(
    name="Movie Script Team",
    members=[image_analyst, script_writer],
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Create structured movie scripts from visual content.",
        "Image Analyst: First analyze the image for visual elements and context.",
        "Script Writer: Transform analysis into structured movie concepts.",
        "Ensure all output follows the MovieScript schema precisely.",
    ],
    output_schema=MovieScript,
)

response = movie_team.run(
    "Write a movie about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

for event in response:
    pprint(event.content)
bash  theme={null}
    pip install agno pydantic rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/image_to_structured_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Get all routes

**URL:** llms-txt#get-all-routes

**Contents:**
- Developer Resources

routes = agent_os.get_routes()

for route in routes:
    print(f"Route: {route.path}")
    if hasattr(route, 'methods'):
        print(f"Methods: {route.methods}")
```

## Developer Resources

* [AgentOS Reference](/reference/agent-os/agent-os)
* [Full Example](/agent-os/usage/custom-fastapi)
* [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

## Video As Input

**URL:** llms-txt#video-as-input

Source: https://docs.agno.com/basics/multimodal/video/video_input

Learn how to use video as input with Agno agents.

Agno supports videos as input to agents and teams.  Take a look at the [compatibility matrix](/basics/models/compatibility#multimodal-support) to see which models support videos as input.

Let's create an agent that can understand video input.

```python video_agent.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    markdown=True,
)

---

## Singlestore for Agent

**URL:** llms-txt#singlestore-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/singlestore/usage/singlestore-for-agent

Agno supports using Singlestore as a storage backend for Agents using the `SingleStoreDb` class.

Obtain the credentials for Singlestore from [here](https://portal.singlestore.com/)

```python singlestore_for_agent.py theme={null}
from os import getenv

from agno.agent import Agent
from agno.db.singlestore.singlestore import SingleStoreDb
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Delete a memory

**URL:** llms-txt#delete-a-memory

print("\nDeleting memory")
assert memory_id_2 is not None
memory.delete_user_memory(user_id=jane_doe_id, memory_id=memory_id_2)
print("Memory deleted\n")
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

---

## Create an agent with the PostgresTools

**URL:** llms-txt#create-an-agent-with-the-postgrestools

agent = Agent(tools=[postgres_tools])

---

## Combine model, tools, and instructions into a complete agent

**URL:** llms-txt#combine-model,-tools,-and-instructions-into-a-complete-agent

**Contents:**
  - 4b. Create the Analysis Function
  - 4c. Create a Test Function
  - 4d. Complete Working Example

social_media_agent = Agent(
    name="Social Media Intelligence Analyst",
    model=model,                 # The GPT-5 mini model we chose
    tools=tools,                 # The X and Exa tools we configured
    instructions=complete_instructions,  # The strategy we defined
    markdown=True,               # Enable rich formatting for reports
    show_tool_calls=True,        # Show transparency in data collection
)

print(f"Agent created: {social_media_agent.name}")
python  theme={null}
def analyze_brand_sentiment(query: str, tweet_count: int = 20):
    """
    Execute comprehensive social media intelligence analysis.

Args:
        query: Brand or topic search query (e.g., "Tesla OR @elonmusk")
        tweet_count: Number of recent tweets to analyze
    """

# Create a detailed prompt for the agent
    analysis_prompt = f"""
    Conduct comprehensive social media intelligence analysis for: "{query}"

ANALYSIS PARAMETERS:
    - Twitter Analysis: {tweet_count} most recent tweets with engagement metrics
    - Web Intelligence: Related articles, discussions, and broader context via Exa
    - Cross-Platform Synthesis: Correlate social sentiment with web discussions
    - Strategic Focus: Brand positioning, competitive analysis, risk assessment

METHODOLOGY:
    1. Gather direct social media mentions and engagement data
    2. Search for related web discussions and broader context
    3. Analyze sentiment patterns and engagement indicators
    4. Identify cross-platform themes and influence networks
    5. Generate strategic recommendations with evidence backing

Provide comprehensive intelligence report following the structured format.
    """

# Execute the analysis
    return social_media_agent.print_response(analysis_prompt, stream=True)

print("Analysis function created")
python  theme={null}
def test_agent():
    """Test the complete agent with a sample query."""
    print("Testing social media intelligence agent...")
    analyze_brand_sentiment("Agno OR AgnoAGI", tweet_count=10)

print("Test function ready")
python  theme={null}
"""
Complete Social Media Intelligence Agent
Built with Agno framework
"""
from pathlib import Path
from textwrap import dedent
from dotenv import load_dotenv

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.x import XTools
from agno.tools.exa import ExaTools

**Examples:**

Example 1 (unknown):
```unknown
For detailed information about each Agent parameter, see the [Agent Reference Documentation](/reference/agents/agent).

### 4b. Create the Analysis Function
```

Example 2 (unknown):
```unknown
### 4c. Create a Test Function
```

Example 3 (unknown):
```unknown
### 4d. Complete Working Example

Here's your complete `app/social_media_agent.py` file:
```

---

## AgentQL

**URL:** llms-txt#agentql

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/agentql

**AgentQLTools** enable an Agent to browse and scrape websites using the AgentQL API.

The following example requires the `agentql` library and an API token which can be obtained from [AgentQL](https://agentql.com/).

The following agent will open a web browser and scrape all the text from the page.

<Note>
  AgentQL will open up a browser instance (don't close it) and do scraping on
  the site.
</Note>

| Parameter                      | Type   | Default | Description                                       |
| ------------------------------ | ------ | ------- | ------------------------------------------------- |
| `api_key`                      | `str`  | `None`  | API key for AgentQL                               |
| `scrape`                       | `bool` | `True`  | Whether to use the scrape text tool               |
| `agentql_query`                | `str`  | `None`  | Custom AgentQL query                              |
| `enable_scrape_website`        | `bool` | `True`  | Enable the scrape\_website functionality.         |
| `enable_custom_scrape_website` | `bool` | `True`  | Enable the custom\_scrape\_website functionality. |
| `all`                          | `bool` | `False` | Enable all functionality.                         |

| Function                | Description                                          |
| ----------------------- | ---------------------------------------------------- |
| `scrape_website`        | Used to scrape all text from a web page              |
| `custom_scrape_website` | Uses the custom `agentql_query` to scrape a web page |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/agentql.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/agentql_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will open a web browser and scrape all the text from the page.
```

---

## Filtering on MongoDB

**URL:** llms-txt#filtering-on-mongodb

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-mongo-db

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in MongoDB.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.mongodb import MongoVectorDb

---

## MoviePy Video Tools

**URL:** llms-txt#moviepy-video-tools

**Contents:**
- Prerequisites
- Example
- Toolkit Functions
- Toolkit Params
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/moviepy

Agno MoviePyVideoTools enable an Agent to process videos, extract audio, generate SRT caption files, and embed rich, word-highlighted captions.

To use `MoviePyVideoTools`, you need to install `moviepy` and its dependency `ffmpeg`:

**Important for Captioning Workflow:**
The `create_srt` and `embed_captions` tools require a transcription of the video's audio. `MoviePyVideoTools` itself does not perform speech-to-text. You'll typically use another tool, such as `OpenAITools` with its `transcribe_audio` function, to generate the transcription (often in SRT format) which is then used by these tools.

The following example demonstrates a complete workflow where an agent uses `MoviePyVideoTools` in conjunction with `OpenAITools` to:

1. Extract audio from a video file
2. Transcribe the audio using OpenAI's speech-to-text
3. Generate an SRT caption file from the transcription
4. Embed the captions into the video with word-level highlighting

These are the functions exposed by `MoviePyVideoTools`:

| Function                | Description                                                                                            |
| ----------------------- | ------------------------------------------------------------------------------------------------------ |
| `enable_extract_audio`  | Extracts the audio track from a video file and saves it to a specified output path.                    |
| `enable_create_srt`     | Saves a given transcription (expected in SRT format) to a `.srt` file at the specified output path.    |
| `enable_embed_captions` | Embeds captions from an SRT file into a video, creating a new video file with word-level highlighting. |

These parameters are passed to the `MoviePyVideoTools` constructor:

| Parameter           | Type   | Default | Description                        |
| ------------------- | ------ | ------- | ---------------------------------- |
| `process_video`     | `bool` | `True`  | Enables the `extract_audio` tool.  |
| `generate_captions` | `bool` | `True`  | Enables the `create_srt` tool.     |
| `embed_captions`    | `bool` | `True`  | Enables the `embed_captions` tool. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/moviepy_video.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/moviepy_video_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
**Important for Captioning Workflow:**
The `create_srt` and `embed_captions` tools require a transcription of the video's audio. `MoviePyVideoTools` itself does not perform speech-to-text. You'll typically use another tool, such as `OpenAITools` with its `transcribe_audio` function, to generate the transcription (often in SRT format) which is then used by these tools.

## Example

The following example demonstrates a complete workflow where an agent uses `MoviePyVideoTools` in conjunction with `OpenAITools` to:

1. Extract audio from a video file
2. Transcribe the audio using OpenAI's speech-to-text
3. Generate an SRT caption file from the transcription
4. Embed the captions into the video with word-level highlighting
```

---

## No system message should be provided (Gemini requires only the image)

**URL:** llms-txt#no-system-message-should-be-provided-(gemini-requires-only-the-image)

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

---

## Qdrant FastEmbed Embedder

**URL:** llms-txt#qdrant-fastembed-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/qdrant-fastembed/usage/qdrant-fastembed

```python  theme={null}
from agno.knowledge.embedder.fastembed import FastEmbedEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = FastEmbedEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Remove all content

**URL:** llms-txt#remove-all-content

**Contents:**
- Usage

knowledge.remove_all_content()
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/basic_operations/09_remove_content.py
      bash Windows theme={null}
      python cookbook/knowledge/basic_operations/09_remove_content.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## OpenAI Responses

**URL:** llms-txt#openai-responses

**Contents:**
- Authentication
- Example
- Parameters

Source: https://docs.agno.com/integrations/models/native/openai/responses/overview

Learn how to use OpenAI Responses with Agno.

`OpenAIResponses` is a class for interacting with OpenAI models using the Responses API. This class provides a streamlined interface for working with OpenAI's newer Responses API, which is distinct from the traditional Chat API. It supports advanced features like tool use, file processing, and knowledge retrieval.

Set your `OPENAI_API_KEY` environment variable. You can get one [from OpenAI here](https://platform.openai.com/account/api-keys).

Use `OpenAIResponses` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/native/openai/responses/usage/basic-stream). </Note>

For more information, please refer to the [OpenAI Responses docs](https://platform.openai.com/docs/api-reference/responses) as well.

| Parameter               | Type                                               | Default             | Description                                                                       |
| ----------------------- | -------------------------------------------------- | ------------------- | --------------------------------------------------------------------------------- |
| `id`                    | `str`                                              | `"gpt-5-mini"`      | The id of the OpenAI model to use with Responses API                              |
| `name`                  | `str`                                              | `"OpenAIResponses"` | The name of the model                                                             |
| `provider`              | `str`                                              | `"OpenAI"`          | The provider of the model                                                         |
| `instructions`          | `Optional[str]`                                    | `None`              | System-level instructions for the assistant                                       |
| `response_format`       | `Optional[Union[str, Dict]]`                       | `None`              | Response format specification for structured outputs                              |
| `temperature`           | `Optional[float]`                                  | `None`              | Controls randomness in the model's output (0.0 to 2.0)                            |
| `top_p`                 | `Optional[float]`                                  | `None`              | Controls diversity via nucleus sampling (0.0 to 1.0)                              |
| `max_completion_tokens` | `Optional[int]`                                    | `None`              | Maximum number of completion tokens to generate                                   |
| `truncation_strategy`   | `Optional[Dict[str, Any]]`                         | `None`              | Strategy for truncating messages when they exceed context limits                  |
| `tool_choice`           | `Optional[Union[str, Dict]]`                       | `None`              | Controls which function is called by the model                                    |
| `parallel_tool_calls`   | `Optional[bool]`                                   | `None`              | Whether to enable parallel function calling                                       |
| `metadata`              | `Optional[Dict[str, str]]`                         | `None`              | Developer-defined metadata to associate with the response                         |
| `strict_output`         | `bool`                                             | `True`              | Controls schema adherence for structured outputs                                  |
| `api_key`               | `Optional[str]`                                    | `None`              | The API key for authenticating with OpenAI (defaults to OPENAI\_API\_KEY env var) |
| `organization`          | `Optional[str]`                                    | `None`              | The organization ID to use for requests                                           |
| `base_url`              | `Optional[Union[str, httpx.URL]]`                  | `None`              | The base URL for the OpenAI API                                                   |
| `timeout`               | `Optional[float]`                                  | `None`              | Request timeout in seconds                                                        |
| `max_retries`           | `Optional[int]`                                    | `None`              | Maximum number of retries for failed requests                                     |
| `default_headers`       | `Optional[Any]`                                    | `None`              | Default headers to include in all requests                                        |
| `http_client`           | `Optional[Union[httpx.Client, httpx.AsyncClient]]` | `None`              | HTTP client instance for making requests                                          |

`OpenAIResponses` is a subclass of the [Model](/reference/models/model) class and has access to the same params.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `OpenAIResponses` with your `Agent`:

<CodeGroup>
```

---

## Download the audio file from the URL as bytes

**URL:** llms-txt#download-the-audio-file-from-the-url-as-bytes

**Contents:**
- Usage

response = requests.get(url)
audio_content = response.content

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(content=audio_content)],
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai requests agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/audio_input_bytes_content.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/audio_input_bytes_content.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Audio Input (Local file)

**URL:** llms-txt#audio-input-(local-file)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/audio-input-local-file-upload

```python cookbook/models/google/gemini/audio_input_local_file_upload.py theme={null}
from pathlib import Path
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

---

## Reader

**URL:** llms-txt#reader

Source: https://docs.agno.com/reference/knowledge/reader/base

Reader is the base class for all reader classes in Agno.

<Snippet file="base-reader-reference.mdx" />

---

## Groq DeepSeek R1

**URL:** llms-txt#groq-deepseek-r1

Source: https://docs.agno.com/basics/reasoning/usage/models/groq/groq

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Groq API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Groq API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Set workflow logger to DEBUG to see these logs

**URL:** llms-txt#set-workflow-logger-to-debug-to-see-these-logs

custom_workflow_logger.setLevel(logging.DEBUG)

---

## Batch processing for large datasets

**URL:** llms-txt#batch-processing-for-large-datasets

**Contents:**
  - Issue: Running Out of Memory

def load_content_in_batches(knowledge, content_dir, batch_size=10):
    files = [f for f in os.listdir(content_dir) if f.endswith('.pdf')]
    
    for i in range(0, len(files), batch_size):
        batch_files = files[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}")
        
        for file in batch_files:
            knowledge.add_content(
                path=os.path.join(content_dir, file),
                skip_if_exists=True
            )
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Issue: Running Out of Memory

**What's happening:** Loading too many large files at once, or chunk sizes are too large.

**Quick fixes:**

1. Process content in smaller batches (see code above)
2. Reduce chunk size in your chunking strategy
3. Use `include` and `exclude` patterns to limit what gets processed
4. Clear old/outdated content regularly with `knowledge.remove_content_by_id()`
```

---

## Performance

**URL:** llms-txt#performance

**Contents:**
- Agent Performance
  - Instantiation Time

Source: https://docs.agno.com/get-started/performance

Get extreme performance out of the box with Agno.

If you're building with Agno, you're guaranteed best-in-class performance by default. Our obsession with performance is necessary because even simple AI workflows can spawn hundreds of Agents and because many tasks are long-running -- stateless, horizontal scalability is key for success.

At Agno, we optimize performance across 3 dimensions:

1. **Agent performance:** We optimize static operations (instantiation, memory footprint) and runtime operations (tool calls, memory updates, history management).
2. **System performance:** The AgentOS API is async by default and has a minimal memory footprint. The system is stateless and horizontally scalable, with a focus on preventing memory leaks. It handles parallel and batch embedding generation during knowledge ingestion, metrics collection in background tasks, and other system-level optimizations.
3. **Agent reliability and accuracy:** Monitored through evals, which weâ€™ll explore later.

Let's measure the time it takes to instantiate an Agent and the memory footprint of an Agent. Here are the numbers (last measured in Oct 2025, on an Apple M4 MacBook Pro):

* **Agent instantiation:** \~3Î¼s on average
* **Memory footprint:** \~6.6Kib on average

We'll show below that Agno Agents instantiate **529Ã— faster than Langgraph**, **57Ã— faster than PydanticAI**, and **70Ã— faster than CrewAI**. Agno Agents also use **24Ã— lower memory than Langgraph**, **4Ã— lower than PydanticAI**, and **10Ã— lower than CrewAI**.

<Note>
  Run time performance is bottlenecked by inference and hard to benchmark accurately, so we focus on minimizing overhead, reducing memory usage, and parallelizing tool calls.
</Note>

### Instantiation Time

Let's measure instantiation time for an Agent with 1 tool. We'll run the evaluation 1000 times to get a baseline measurement. We'll compare Agno to LangGraph, CrewAI and Pydantic AI.

<Note>
  The code for this benchmark is available [here](https://github.com/agno-agi/agno/tree/main/cookbook/evals/performance). You should run the evaluation yourself on your own machine, please, do not take these results at face value.
</Note>

```shell  theme={null}

---

## Create workflow steps

**URL:** llms-txt#create-workflow-steps

research_step = Step(
    name="Research Step",
    agent=researcher_agent,
)

writing_step = Step(
    name="Writing Step",
    agent=writer_agent,
)

---

## Migrating to Agno v2.0

**URL:** llms-txt#migrating-to-agno-v2.0

**Contents:**
- Installing Agno v2
- Migrating your Agno DB
- Migrating your Agno code
  - 1. Agents and Teams
  - 2. Storage

Source: https://docs.agno.com/how-to/v2-migration

Guide to migrate your Agno applications from v1 to v2.

If you have questions during your migration, we can help! Find us on [Discord](https://discord.gg/4MtYHHrgA8) or [Discourse](https://community.agno.com/).

<Tip>
  Reference the [v2.0 Changelog](/how-to/v2-changelog) for the full list of
  changes.
</Tip>

## Installing Agno v2

If you are already using Agno, you can upgrade to v2 by running:

Otherwise, you can install the latest version of Agno v2 by running:

## Migrating your Agno DB

If you used our `Storage` or `Memory` functionalities to store Agent sessions and memories in your database, you can start by migrating your tables.

Use our migration script: [`libs/agno/migrations/v1_to_v2/migrate_to_v2.py`](https://github.com/agno-agi/agno/blob/main/libs/agno/migrations/v1_to_v2/migrate_to_v2.py)

The script supports PostgreSQL, MySQL, SQLite, and MongoDB. Update the database connection settings, the batch size (useful if you are migrating large tables) in the script and run it.

* The script won't cleanup the old tables, in case you still need them.
* The script is idempotent. If something goes wrong or if you stop it mid-run, you can run it again.
* Metrics are automatically converted from v1 to v2 format.

## Migrating your Agno code

Each section here covers a specific framework domain, with before and after examples and detailed explanations where needed.

### 1. Agents and Teams

[Agents](/basics/agents/overview) and [Teams](/basics/teams/overview) are the main building blocks in the Agno framework.

Below are some of the v2 updates we have made to the `Agent` and `Team` classes:

1.1. Streaming responses with `arun` now returns an `AsyncIterator`, not a coroutine. This is how you consume the resulting events now, when streaming a run:

1.2. The `RunResponse` class is now `RunOutput`. This is the type of the results you get when running an Agent:

1.3. The events you get when streaming an Agent result have been renamed:

* `RunOutputStartedEvent` â†’ `RunStartedEvent`
* `RunOutputCompletedEvent` â†’ `RunCompletedEvent`
* `RunOutputErrorEvent` â†’ `RunErrorEvent`
* `RunOutputCancelledEvent` â†’ `RunCancelledEvent`
* `RunOutputContinuedEvent` â†’ `RunContinuedEvent`
* `RunOutputPausedEvent` â†’ `RunPausedEvent`
* `RunOutputContentEvent` â†’ `RunContentEvent`

1.4. Similarly, for Team output events:

* `TeamRunOutputStartedEvent` â†’ `TeamRunStartedEvent`
* `TeamRunOutputCompletedEvent` â†’ `TeamRunCompletedEvent`
* `TeamRunOutputErrorEvent` â†’ `TeamRunErrorEvent`
* `TeamRunOutputCancelledEvent` â†’ `TeamRunCancelledEvent`
* `TeamRunOutputContentEvent` â†’ `TeamRunContentEvent`

1.5. The `add_state_in_messages` parameter has been deprecated. Variables in instructions are now resolved automatically by default.
1.6. The `context` parameter has been renamed to `dependencies`.

This is how it looked like on v1:

This is how it looks like now, on v2:

<Tip>
  See the full list of changes in the [Agent
  Updates](/how-to/v2-changelog#agent-updates) section of the changelog.
</Tip>

Storage is used to persist Agent sessions, state and memories in a database.

This is how Storage looks like on v1:

These are the changes we have made for v2:

2.1. The `Storage` classes have moved from `agno/storage` to `agno/db`. We will now refer to them as our `Db` classes.
2.2. The `mode` parameter has been deprecated. The same instance can now be used by Agents, Teams and Workflows.

2.3. The `table_name` parameter has been deprecated. One instance now handles multiple tables, you can define their names individually.

These are all the supported tables, each used to persist data related to a specific domain:

2.4. Previously running a `Team` would create a team session and sessions for every team member participating in the run. Now, only the `Team` session is created. The runs for the team leader and all members can be found in the `Team` session.

```python v2_storage_team_sessions.py theme={null}
team.run(...)

team_session = team.get_latest_session()

**Examples:**

Example 1 (unknown):
```unknown
Otherwise, you can install the latest version of Agno v2 by running:
```

Example 2 (unknown):
```unknown
## Migrating your Agno DB

If you used our `Storage` or `Memory` functionalities to store Agent sessions and memories in your database, you can start by migrating your tables.

Use our migration script: [`libs/agno/migrations/v1_to_v2/migrate_to_v2.py`](https://github.com/agno-agi/agno/blob/main/libs/agno/migrations/v1_to_v2/migrate_to_v2.py)

The script supports PostgreSQL, MySQL, SQLite, and MongoDB. Update the database connection settings, the batch size (useful if you are migrating large tables) in the script and run it.

Notice:

* The script won't cleanup the old tables, in case you still need them.
* The script is idempotent. If something goes wrong or if you stop it mid-run, you can run it again.
* Metrics are automatically converted from v1 to v2 format.

## Migrating your Agno code

Each section here covers a specific framework domain, with before and after examples and detailed explanations where needed.

### 1. Agents and Teams

[Agents](/basics/agents/overview) and [Teams](/basics/teams/overview) are the main building blocks in the Agno framework.

Below are some of the v2 updates we have made to the `Agent` and `Team` classes:

1.1. Streaming responses with `arun` now returns an `AsyncIterator`, not a coroutine. This is how you consume the resulting events now, when streaming a run:
```

Example 3 (unknown):
```unknown
1.2. The `RunResponse` class is now `RunOutput`. This is the type of the results you get when running an Agent:
```

Example 4 (unknown):
```unknown
1.3. The events you get when streaming an Agent result have been renamed:

* `RunOutputStartedEvent` â†’ `RunStartedEvent`
* `RunOutputCompletedEvent` â†’ `RunCompletedEvent`
* `RunOutputErrorEvent` â†’ `RunErrorEvent`
* `RunOutputCancelledEvent` â†’ `RunCancelledEvent`
* `RunOutputContinuedEvent` â†’ `RunContinuedEvent`
* `RunOutputPausedEvent` â†’ `RunPausedEvent`
* `RunOutputContentEvent` â†’ `RunContentEvent`

1.4. Similarly, for Team output events:

* `TeamRunOutputStartedEvent` â†’ `TeamRunStartedEvent`
* `TeamRunOutputCompletedEvent` â†’ `TeamRunCompletedEvent`
* `TeamRunOutputErrorEvent` â†’ `TeamRunErrorEvent`
* `TeamRunOutputCancelledEvent` â†’ `TeamRunCancelledEvent`
* `TeamRunOutputContentEvent` â†’ `TeamRunContentEvent`

1.5. The `add_state_in_messages` parameter has been deprecated. Variables in instructions are now resolved automatically by default.
1.6. The `context` parameter has been renamed to `dependencies`.

This is how it looked like on v1:
```

---

## Text Reader

**URL:** llms-txt#text-reader

Source: https://docs.agno.com/reference/knowledge/reader/text

TextReader is a reader class that allows you to read data from text files.

<Snippet file="text-reader-reference.mdx" />

---

## Firestore for Team

**URL:** llms-txt#firestore-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/firestore/usage/firestore-for-team

Agno supports using Firestore as a storage backend for Teams using the `FirestoreDb` class.

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Async Tool Use

**URL:** llms-txt#async-tool-use

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/async-tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ag infra patch

**URL:** llms-txt#ag-infra-patch

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/patch

Update resources for active infra

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

<ResponseField name="pull" type="bool">
  Pull `--pull` `-p`
</ResponseField>

---

## Enable AgentOS MCP

**URL:** llms-txt#enable-agentos-mcp

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/mcp/enable-mcp-example

Complete AgentOS setup with MCP support enabled

```python cookbook/agent_os/mcp/enable_mcp_example.py theme={null}

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Keyword Search

**URL:** llms-txt#keyword-search

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/keyword-search

```python keyword_search.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## topic="AI",

**URL:** llms-txt#topic="ai",

---

## Create the vector database

**URL:** llms-txt#create-the-vector-database

**Contents:**
- Usage

vector_db = LanceDb(
    table_name="recipes",  # Table name in the vector database
    uri=db_url,  # Location to initiate/create the vector database
    embedder=embedder,  # Without using this, it will use OpenAIChat embeddings by default
)

knowledge = Knowledge(
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes", url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

db = SqliteDb(db_file="data.db")

agent = Agent(
    session_id="session_id", 
    user_id="user",  
    model=model,
    knowledge=knowledge,
    db=db,
)

agent.print_response(
    "What is the first step of making Gluai Buat Chi from the knowledge base?",
    markdown=True,
)
bash  theme={null}
    pip install -U lancedb sqlalchemy agno
    bash Mac theme={null}
      python cookbook/agents/rag/rag_with_lance_db_and_sqlite.py
      bash Windows theme={null}
      python cookbook/agents/rag/rag_with_lance_db_and_sqlite.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the installation instructions at [Ollama's website](https://ollama.ai)
  </Step>

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Image to Text Analysis

**URL:** llms-txt#image-to-text-analysis

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/image-to-text

This example demonstrates how to create an agent that can analyze images and generate creative text content based on the visual content.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Define the custom async knowledge retriever

**URL:** llms-txt#define-the-custom-async-knowledge-retriever

---

## Advanced Session State Management

**URL:** llms-txt#advanced-session-state-management

Source: https://docs.agno.com/basics/state/agent/usage/session-state-advanced

This example demonstrates advanced session state management with multiple tools for managing a shopping list, including add, remove, and list operations.

<Steps>
  <Step title="Create a Python file">
    Create a file called `session_state_advanced.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Create a Loop step for deep tech research

**URL:** llms-txt#create-a-loop-step-for-deep-tech-research

deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

---

## Skip If Exists

**URL:** llms-txt#skip-if-exists

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/getting-started/usage/skip-if-exists

```python 11_skip_if_exists.py theme={null}
import asyncio
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

---

## Agent that uses structured outputs

**URL:** llms-txt#agent-that-uses-structured-outputs

**Contents:**
- Usage

structured_output_agent = Agent(
    model=Claude(id="claude-sonnet-4-5-20250929"),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

structured_output_agent.print_response("New York", stream=True)
bash  theme={null}
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U anthropic agno
    bash  theme={null}
    python structured_output_stream.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Agent automatically searches when needed

**URL:** llms-txt#agent-automatically-searches-when-needed

user: "What's our current return policy?"

---

## Multi-DB Tracing with tracing=True

**URL:** llms-txt#multi-db-tracing-with-tracing=true

Source: https://docs.agno.com/agent-os/tracing/usage/tracing-with-multi-db-scenario-and-tracing-flag

Learn how to trace agents with multiple databases using tracing=True in AgentOS

This example shows how to configure tracing when agents have separate databases using the `tracing=True` flag. A dedicated `tracing_db` ensures all traces are stored in one central location.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run AgentOS">
    <CodeGroup>

Your AgentOS will be available at `http://localhost:7777`. View traces in the AgentOS dashboard.
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Website Reader

**URL:** llms-txt#website-reader

Source: https://docs.agno.com/reference/knowledge/reader/website

WebsiteReader is a reader class that allows you to read data from websites.

<Snippet file="website-reader-reference.mdx" />

---

## Step 2: Create your Agno agent

**URL:** llms-txt#step-2:-create-your-agno-agent

agent = Agent(
    name="Market Analysis Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Provide professional market analysis with data-driven insights.",
    debug_mode=True,
)

---

## Storage Control

**URL:** llms-txt#storage-control

**Contents:**
- How Storage Control Works
- Storage Flags Reference
- Disable Media Storage
  - Recommended Workflow
- Disable Tool Storage
  - Considerations
- Disable History Storage
  - When to Toggle It
- Combining Storage Flags
- Learn More

Source: https://docs.agno.com/basics/sessions/persisting-sessions/storage-control

Control what session data gets persisted to your database

As sessions accumulate data, your database can grow quickly. Agno gives you fine-grained control over what gets persisted with three storage flags:

* **`store_media`** - Images, videos, audio, and file uploads
* **`store_tool_messages`** - Tool calls and their results
* **`store_history_messages`** - Historical messages from previous runs

<Note>
  Storage control works identically for Agents and Teams. This page shows examples for both.
</Note>

## How Storage Control Works

Think of these flags as filters for your database, not your agent. During a run, your agent or team sees everything: media, tool results, history. Everything works normally. The filtering only happens when saving to the database after the run finishes.

So you can turn off storing media or tool messages without breaking anything. Your agent still processes images, tools still run, history still flows to the model. You're just choosing what gets written to disk. Token metrics still reflect the real usage, even if you didn't persist all the data.

<Warning>
  When you optimise what messages you store in the database, it might cause hallucinations on the LLM, if any of the messages are essential to the conversation history. If you don't rely too heavily on history, you can safely optimise storage.

For example, if you remove tool call messages, it won't be available in subsequent runs where history is enabled, which could cause the LLM to think that tool was never used.
</Warning>

<Note>
  **Important: `store_tool_messages=False` removes tool-call and result pairs**

When you disable tool message storage, Agno removes both the tool result *and* the strips the tool call from the corresponding assistant message (message from the LLM). This is required to maintain valid message sequences that model providers expect.

Your metrics will still show the actual tokens used, including the removed tool messages.
</Note>

## Storage Flags Reference

| Flag                     | Default | What It Controls                                                             | Impact When Disabled                                       |
| ------------------------ | ------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------- |
| `store_media`            | `True`  | Images, videos, audio, files uploaded by users                               | Media not persisted to database                            |
| `store_tool_messages`    | `True`  | Tool calls and their results (also removes corresponding assistant messages) | Tool execution details not stored, saves significant space |
| `store_history_messages` | `True`  | Historical messages from previous runs                                       | Old history not stored, only current run persisted         |

## Disable Media Storage

Large media uploads (images, PDFs, audio) can dominate your session tables. Turn them off with `store_media=False` and store the original files elsewhere (S3, GCS, etc.).

During a run the model still receives the media (and tools can still process it); the flag only affects what is written to the database afterward.

### Recommended Workflow

1. Upload the file to your preferred storage service and keep the URL/ID
2. Pass that URL to the agent/team so it can fetch/process the file
3. Skip persisting the raw media via `store_media=False`

## Disable Tool Storage

Tool calls can easily bloat storage (think web-scraped pages or large API payloads). Toggle `store_tool_messages=False` to remove both the tool result and the tool-call from the corresponding assistant message that triggered it from the persisted run. Metrics still show the real token usage.

* Removing tool messages keeps the provider-friendly message ordering intact (no stray tool roles)
* When auditing tool behavior later, re-run the tool or log its output somewhere else before the run completes
* Pair this with `store_media=False` when tools return binary payloads you don't need in the session record

## Disable History Storage

`store_history_messages=False` keeps the runtime behavior (history still reaches the LLM) but scrubs the historic messages before persistence. Only the non-history messages are written to the database.

### When to Toggle It

* **Cost / storage**: Reduce table size for high-volume deployments
* **Privacy**: Avoid storing long-lived transcripts that contain sensitive data
* **Debugging current runs only**: Keep just the latest interaction record

## Combining Storage Flags

You can use multiple flags together to optimize your storage strategy:

Turn a flag **off** when you want to reduce storage footprint (e.g., large tool payloads you can always re-run, media living in S3, old history you don't need). Leave it **on** when you need full transcripts for audit/compliance, analytics, or agent coaching.

* [Sessions Overview](/basics/sessions/overview)
* [Session Storage](/basics/database/overview)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

During a run the model still receives the media (and tools can still process it); the flag only affects what is written to the database afterward.

### Recommended Workflow

1. Upload the file to your preferred storage service and keep the URL/ID
2. Pass that URL to the agent/team so it can fetch/process the file
3. Skip persisting the raw media via `store_media=False`

## Disable Tool Storage

Tool calls can easily bloat storage (think web-scraped pages or large API payloads). Toggle `store_tool_messages=False` to remove both the tool result and the tool-call from the corresponding assistant message that triggered it from the persisted run. Metrics still show the real token usage.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Considerations

* Removing tool messages keeps the provider-friendly message ordering intact (no stray tool roles)
* When auditing tool behavior later, re-run the tool or log its output somewhere else before the run completes
* Pair this with `store_media=False` when tools return binary payloads you don't need in the session record

## Disable History Storage

`store_history_messages=False` keeps the runtime behavior (history still reaches the LLM) but scrubs the historic messages before persistence. Only the non-history messages are written to the database.

<CodeGroup>
```

---

## Image Agent Bytes

**URL:** llms-txt#image-agent-bytes

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/xai/usage/image-agent-bytes

```python cookbook/models/xai/image_agent_bytes.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.xai import xAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=xAI(id="grok-2-vision-latest"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

---

## Step 1: Choose the AI model

**URL:** llms-txt#step-1:-choose-the-ai-model

model = OpenAIChat(id="gpt-5-mini")

---

## Azure AI Foundry

**URL:** llms-txt#azure-ai-foundry

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/azure

The Azure AI Foundry model provides access to Azure-hosted AI Foundry models.

| Parameter               | Type                              | Default            | Description                                                                        |
| ----------------------- | --------------------------------- | ------------------ | ---------------------------------------------------------------------------------- |
| `id`                    | `str`                             | `"gpt-4o"`         | The id of the model to use                                                         |
| `name`                  | `str`                             | `"AzureAIFoundry"` | The name of the model                                                              |
| `provider`              | `str`                             | `"Azure"`          | The provider of the model                                                          |
| `temperature`           | `Optional[float]`                 | `None`             | Controls randomness in the model's output (0.0 to 2.0)                             |
| `max_tokens`            | `Optional[int]`                   | `None`             | Maximum number of tokens to generate in the response                               |
| `frequency_penalty`     | `Optional[float]`                 | `None`             | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |
| `presence_penalty`      | `Optional[float]`                 | `None`             | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |
| `top_p`                 | `Optional[float]`                 | `None`             | Controls diversity via nucleus sampling (0.0 to 1.0)                               |
| `stop`                  | `Optional[Union[str, List[str]]]` | `None`             | Up to 4 sequences where the API will stop generating further tokens                |
| `seed`                  | `Optional[int]`                   | `None`             | Random seed for deterministic sampling                                             |
| `model_extras`          | `Optional[Dict[str, Any]]`        | `None`             | Additional model-specific parameters                                               |
| `strict_output`         | `bool`                            | `True`             | Controls schema adherence for structured outputs                                   |
| `request_params`        | `Optional[Dict[str, Any]]`        | `None`             | Additional parameters to include in the request                                    |
| `api_key`               | `Optional[str]`                   | `None`             | The API key for Azure AI Foundry (defaults to AZURE\_API\_KEY env var)             |
| `api_version`           | `Optional[str]`                   | `None`             | The API version to use (defaults to AZURE\_API\_VERSION env var)                   |
| `azure_endpoint`        | `Optional[str]`                   | `None`             | The Azure endpoint URL (defaults to AZURE\_ENDPOINT env var)                       |
| `timeout`               | `Optional[float]`                 | `None`             | Request timeout in seconds                                                         |
| `max_retries`           | `Optional[int]`                   | `None`             | Maximum number of retries for failed requests                                      |
| `http_client`           | `Optional[httpx.Client]`          | `None`             | HTTP client instance for making requests                                           |
| `client_params`         | `Optional[Dict[str, Any]]`        | `None`             | Additional parameters for client configuration                                     |
| `retries`               | `int`                             | `0`                | Number of retries to attempt before raising a ModelProviderError                   |
| `delay_between_retries` | `int`                             | `1`                | Delay between retries, in seconds                                                  |
| `exponential_backoff`   | `bool`                            | `False`            | If True, the delay between retries is doubled each time                            |

---

## Initialize and connect using server parameters

**URL:** llms-txt#initialize-and-connect-using-server-parameters

**Contents:**
- Complete example

mcp_tools = MCPTools(server_params=server_params, transport="streamable-http")
await mcp_tools.connect()

try:
    # Use mcp_tools with your agent
    pass
finally:
    await mcp_tools.close()
python streamable_http_server.py theme={null}
    from mcp.server.fastmcp import FastMCP

mcp = FastMCP("calendar_assistant")

@mcp.tool()
    def get_events(day: str) -> str:
        return f"There are no events scheduled for {day}."

@mcp.tool()
    def get_birthdays_this_week() -> str:
        return "It is your mom's birthday tomorrow"

if __name__ == "__main__":
        mcp.run(transport="streamable-http")
    python streamable_http_client.py theme={null}
    import asyncio

from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    from agno.tools.mcp import MCPTools, MultiMCPTools

# This is the URL of the MCP server we want to use.
    server_url = "http://localhost:8000/mcp"

async def run_agent(message: str) -> None:
        # Initialize and connect to the Streamable HTTP MCP server
        mcp_tools = MCPTools(transport="streamable-http", url=server_url)
        await mcp_tools.connect()

try:
            agent = Agent(
                model=OpenAIChat(id="gpt-5-mini"),
                tools=[mcp_tools],
                markdown=True,
            )
            await agent.aprint_response(message=message, stream=True, markdown=True)
        finally:
            await mcp_tools.close()

# Using MultiMCPTools, we can connect to multiple MCP servers at once, even if they use different transports.
    # In this example we connect to both our example server (Streamable HTTP transport), and a different server (stdio transport).
    async def run_agent_with_multimcp(message: str) -> None:
        # Initialize and connect to multiple MCP servers with different transports
        mcp_tools = MultiMCPTools(
            commands=["npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt"],
            urls=[server_url],
            urls_transports=["streamable-http"],
        )
        await mcp_tools.connect()

try:
            agent = Agent(
                model=OpenAIChat(id="gpt-5-mini"),
                tools=[mcp_tools],
                markdown=True,
            )
            await agent.aprint_response(message=message, stream=True, markdown=True)
        finally:
            await mcp_tools.close()

if __name__ == "__main__":
        asyncio.run(run_agent("Do I have any birthdays this week?"))
        asyncio.run(
            run_agent_with_multimcp(
                "Can you check when is my mom's birthday, and if there are any AirBnb listings in SF for two people for that day?"
            )
        )
    bash  theme={null}
    python streamable_http_server.py
    bash  theme={null}
    python streamable_http_client.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Complete example

Let's set up a simple local server and connect to it using the Streamable HTTP transport:

<Steps>
  <Step title="Setup the server">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup the client">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the server">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the client">
```

---

## All logging will now use the custom logger

**URL:** llms-txt#all-logging-will-now-use-the-custom-logger

**Contents:**
- Logging to a File

log_info("This is using our custom logger!")

agent = Agent()
agent.print_response("What is 2+2?")
python  theme={null}
import logging
from pathlib import Path

from agno.agent import Agent
from agno.utils.log import configure_agno_logging, log_info

**Examples:**

Example 1 (unknown):
```unknown
## Logging to a File

You can configure Agno to log to a file instead of the console:
```

---

## Example usage with a knowledge base:

**URL:** llms-txt#example-usage-with-a-knowledge-base:

**Contents:**
- Params
- Environment Variables
- Developer Resources

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="langdb_embeddings",
        embedder=LangDBEmbedder(),
    ),
    max_results=2,
)
bash  theme={null}
export LANGDB_API_KEY=xxx
export LANGDB_PROJECT_ID=xxx
```

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/embedders/langdb_embedder.py)

**Examples:**

Example 1 (unknown):
```unknown
## Params

| Parameter        | Type                       | Default | Description                                                 |
| ---------------- | -------------------------- | ------- | ----------------------------------------------------------- |
| `api_key`        | `str`                      | -       | The API key used for authenticating requests to LangDB      |
| `project_id`     | `str`                      | -       | The project ID associated with your LangDB account          |
| `model`          | `str`                      | -       | The name of the model used for generating embeddings        |
| `dimensions`     | `int`                      | -       | The dimensionality of the embeddings generated by the model |
| `base_url`       | `str`                      | -       | The base URL for the LangDB API endpoint                    |
| `request_params` | `Optional[Dict[str, Any]]` | -       | Additional parameters to include in the API request         |
| `client_params`  | `Optional[Dict[str, Any]]` | -       | Additional parameters for configuring the API client        |

## Environment Variables
```

---

## Asynchronous Agent with Image Input

**URL:** llms-txt#asynchronous-agent-with-image-input

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/meta/usage/image-input-bytes

```python cookbook/models/meta/llama/image_input_bytes.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.meta import LlamaOpenAI
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=LlamaOpenAI(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

---

## Add middleware

**URL:** llms-txt#add-middleware

**Contents:**
- Common Use Cases
- Middleware Execution Order

app.add_middleware(
    JWTMiddleware,
    secret_key="your-secret-key",
    validate=True
)

if __name__ == "__main__":
    agent_os.serve(app="agent_os_with_jwt_middleware:app", reload=True)
python  theme={null}
    class RateLimitMiddleware(BaseHTTPMiddleware):
        def __init__(self, app, requests_per_minute: int = 60):
            super().__init__(app)
            self.requests_per_minute = requests_per_minute
            # ... implementation

app.add_middleware(RateLimitMiddleware, requests_per_minute=100)
    python  theme={null}

class LoggingMiddleware(BaseHTTPMiddleware):
        async def dispatch(self, request: Request, call_next):
            start_time = time.time()
            response = await call_next(request)
            process_time = time.time() - start_time
            # Log request details...
            return response
    python  theme={null}
app.add_middleware(MiddlewareA)  # Runs third (closest to route)
app.add_middleware(MiddlewareB)  # Runs second
app.add_middleware(MiddlewareC)  # Runs first (outermost)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Test middleware thoroughly in your own staging environment before production deployment.
</Note>

<Tip>
  **Performance Impact:** Each middleware adds latency to requests
</Tip>

## Common Use Cases

<Tabs>
  <Tab title="Authentication">
    **Secure your AgentOS with JWT authentication:**

    * Extract tokens from headers or cookies
    * Automatic parameter injection (user\_id, session\_id)
    * Custom claims extraction for `dependencies` and `session_state`
    * Route exclusion for public endpoints

    [Learn more about JWT Middleware](/agent-os/middleware/jwt)
  </Tab>

  <Tab title="Rate Limiting">
    **Prevent API abuse with rate limiting:**
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Logging">
    **Monitor requests and responses:**
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

## Middleware Execution Order

<Warning>
  Middleware is executed in reverse order of addition. The last middleware added runs first.
</Warning>
```

---

## pprint(json_mode_response.content)

**URL:** llms-txt#pprint(json_mode_response.content)

---

## xAI Grok 3 Mini

**URL:** llms-txt#xai-grok-3-mini

Source: https://docs.agno.com/basics/reasoning/usage/models/xai/reasoning-effort

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your xAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your xAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Use async methods for better performance

**URL:** llms-txt#use-async-methods-for-better-performance

async def embed_texts():
    embedder = JinaEmbedder()
    texts = ["First text", "Second text", "Third text"]
    
    # Get embeddings in batches
    embeddings, usage = await embedder.async_get_embeddings_batch_and_usage(texts)
    print(f"Generated {len(embeddings)} embeddings")
    print(f"Usage info: {usage[0]}")

---

## agent = Agent(

**URL:** llms-txt#agent-=-agent(

---

## Find sales documents from North America in 2024

**URL:** llms-txt#find-sales-documents-from-north-america-in-2024

AND(
    EQ("data_type", "sales"),
    EQ("region", "north_america"),
    EQ("year", 2024)
)
python  theme={null}
from agno.filters import OR, EQ

**Examples:**

Example 1 (unknown):
```unknown
#### OR

At least one condition must be true.
```

---

## Agent with Streaming

**URL:** llms-txt#agent-with-streaming

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/vllm/usage/basic-stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Libraries">
    
  </Step>

<Step title="Start vLLM server">
    
  </Step>

<Step title="Run Agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start vLLM server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Agent Debug Mode

**URL:** llms-txt#agent-debug-mode

Source: https://docs.agno.com/basics/agents/usage/debug

Learn how to enable the debug mode

This example demonstrates how to enable debug mode, to get more verbose output and detailed information about agent execution.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Conditional Branching Workflow

**URL:** llms-txt#conditional-branching-workflow

Source: https://docs.agno.com/basics/workflows/usage/router-steps-workflow

This example demonstrates **Workflows 2.0** router pattern for intelligent, content-based workflow routing.

This example demonstrates **Workflows 2.0** to dynamically select the best execution path based on input
analysis, enabling adaptive workflows that choose optimal strategies per topic.

**When to use**: When you need mutually exclusive execution paths based on business logic.
Ideal for topic-specific workflows, expertise routing, or when different subjects require
completely different processing strategies. Unlike Conditions which can trigger multiple
parallel paths, Router selects exactly one path.

```python router_steps_workflow.py theme={null}
from typing import List

from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.router import Router
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

---

## Basic usage - automatically loads from JINA_API_KEY environment variable

**URL:** llms-txt#basic-usage---automatically-loads-from-jina_api_key-environment-variable

embeddings = JinaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## GitHub MCP agent

**URL:** llms-txt#github-mcp-agent

Source: https://docs.agno.com/basics/tools/mcp/usage/github

Using the [GitHub MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/github) to create an Agent that can explore, analyze and provide insights about GitHub repositories:

```python  theme={null}
"""ðŸ™ MCP GitHub Agent - Your Personal GitHub Explorer!

This example shows how to create a GitHub agent that uses MCP to explore,
analyze, and provide insights about GitHub repositories. The agent leverages the Model
Context Protocol (MCP) to interact with GitHub, allowing it to answer questions
about issues, pull requests, repository details and more.

Example prompts to try:
- "List open issues in the repository"
- "Show me recent pull requests"
- "What are the repository statistics?"
- "Find issues labeled as bugs"
- "Show me contributor activity"

Run: `pip install agno mcp openai` to install the dependencies
Environment variables needed:
- Create a GitHub personal access token following these steps:
    - https://github.com/modelcontextprotocol/servers/tree/main/src/github#setup
- export GITHUB_TOKEN: Your GitHub personal access token
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters

async def run_agent(message: str) -> None:
    """Run the GitHub agent with the given message."""

# Initialize the MCP server
    server_params = StdioServerParameters(
        command="npx",
        args=["-y", "@modelcontextprotocol/server-github"],
    )

# Create a client session to connect to the MCP server
    async with MCPTools(server_params=server_params) as mcp_tools:
        agent = Agent(
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a GitHub assistant. Help users explore repositories and their activity.

- Use headings to organize your responses
                - Be concise and focus on relevant information\
            """),
            markdown=True,
                    )

# Run the agent
        await agent.aprint_response(message, stream=True)

---

## Use with agents

**URL:** llms-txt#use-with-agents

agent.print_response(
    "What's our deployment process?",
    knowledge_filters=[{"department": "engineering", "type": "documentation"}]
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
#### Working with Metadata

Good filtering starts with thoughtful metadata design:
```

---

## API Usage

**URL:** llms-txt#api-usage

**Contents:**
- Passing parameters to your Agent / Team / Workflow

Source: https://docs.agno.com/agent-os/api/usage

Running your Agent / Team / Workflow with the AgentOS API

The AgentOS API provides endpoints:

* **Run an Agent**: `POST /agents/{agent_id}/runs`  (See the [API reference](/reference-api/schema/agents/create-agent-run))
* **Run a Team**: `POST /teams/{team_id}/runs`  (See the [API reference](/reference-api/schema/teams/create-team-run))
* **Run a Workflow**: `POST /workflows/{workflow_id}/runs`  (See the [API reference](/reference-api/schema/workflows/execute-workflow))

These endpoints support form-based input. Below is an example of how to run an agent with the API:

## Passing parameters to your Agent / Team / Workflow

Agent, Team and Workflow `run()` and `arun()` endpoints support various runtime parameters. See the [Agent run schema](/reference/agents/agent#run), [Team run schema](/reference/teams/team#run), [Workflow run schema](/reference/workflows/workflow#run) for more details.

It is a common pattern to want to pass `session_state`, `dependencies`, `metadata`, etc. to your Agent, Team or Workflow via the API.

To pass these parameters via the AgentOS API, you can simply specify them as form-based parameters.

Below is an example where `dependencies` are passed to the agent:

```python dependencies_to_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.os import AgentOS

**Examples:**

Example 1 (unknown):
```unknown
## Passing parameters to your Agent / Team / Workflow

Agent, Team and Workflow `run()` and `arun()` endpoints support various runtime parameters. See the [Agent run schema](/reference/agents/agent#run), [Team run schema](/reference/teams/team#run), [Workflow run schema](/reference/workflows/workflow#run) for more details.

It is a common pattern to want to pass `session_state`, `dependencies`, `metadata`, etc. to your Agent, Team or Workflow via the API.

To pass these parameters via the AgentOS API, you can simply specify them as form-based parameters.

Below is an example where `dependencies` are passed to the agent:
```

---

## Create agent with knowledge and session persistence

**URL:** llms-txt#create-agent-with-knowledge-and-session-persistence

**Contents:**
- What to Expect
- Usage
- Next Steps

agno_assist = Agent(
    name="Agno Assist",
    model=OpenAIChat(id="gpt-4o"),
    description="You help answer questions about the Agno framework.",
    instructions="Search your knowledge before answering the question.",  # Forces knowledge search
    knowledge=knowledge,
    db=SqliteDb(  # Stores conversation history
        session_table="agno_assist_sessions", 
        db_file="tmp/agents.db"
    ),
    add_history_to_context=True,
    add_datetime_to_context=True,
    markdown=True,
)

if __name__ == "__main__":
    agno_assist.print_response("What is Agno?")
    agno_assist.print_response("How do I create an agent with tools?")
    agno_assist.print_response("What vector databases does Agno support?")

bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai lancedb tantivy sqlalchemy pandas
    bash Mac theme={null}
      python agno_assist.py
      bash Windows theme={null}
      python agno_assist.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Replace the URL in `add_content_async()` with your own documentation
* Delete the `tmp/` directory to reload with new content
* Modify `instructions` to customize the agent's behavior
* Explore [Knowledge Bases](/basics/knowledge/knowledge-bases) for advanced configuration

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The agent will answer questions by searching the Agno documentation. On the first run, it indexes the documentation into a local vector database (stored in `tmp/` directory). Subsequent runs reuse the existing database for faster responses.

Each answer is grounded in the actual documentation content, and the agent maintains conversation history so you can ask follow-up questions.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## RetryAgentRun

**URL:** llms-txt#retryagentrun

**Contents:**
- Constructor
  - Parameters
- When to Use
- Behavior
- See Also

Source: https://docs.agno.com/reference/tools/retry-agent-run

API reference for the RetryAgentRun exception used to provide feedback to the model within the tool call loop.

The `RetryAgentRun` exception allows you to provide instructions to the model for how to change its behavior and have the model retry within the current agent run. The exception message is passed to the model as a tool call error, allowing the model to adjust its approach in the next iteration of the LLM loop.

<Note>
  This does **not** retry the full agent runâ€”it only provides feedback to the model within the current run's tool call loop.
</Note>

<ResponseField name="exc" type="str" required>
  The error message to pass to the model. This message provides instructions or feedback to help the model adjust its behavior in the next iteration.
</ResponseField>

<ResponseField name="user_message" type="Union[str, Message]" optional>
  An optional message to display to the user about the retry.
</ResponseField>

<ResponseField name="agent_message" type="Union[str, Message]" optional>
  An optional message from the agent's perspective about the retry.
</ResponseField>

<ResponseField name="messages" type="List[Union[dict, Message]]" optional>
  An optional list of messages to add to the conversation history.
</ResponseField>

Use `RetryAgentRun` when:

* **Validation fails**: Input doesn't meet requirements, and you want the model to try again with corrected input
* **State requirements**: The current state doesn't meet prerequisites, and you want the model to perform additional actions
* **Business logic**: A condition isn't met, and you want to guide the model on how to proceed
* **Iterative refinement**: You want the model to improve its approach based on feedback

When `RetryAgentRun` is raised:

1. The exception message is added to the tool call result as an error
2. The model receives this error in the next LLM call
3. The model can adjust its approach and retry the tool call or try a different approach
4. The agent run continues (does not stop or exit)
5. All messages and tool calls are preserved in the session

* [StopAgentRun Exception](/reference/tools/stop-agent-run)
* [Exceptions & Retries Guide](/basics/tools/exceptions)

---

## Couchbase connection settings

**URL:** llms-txt#couchbase-connection-settings

username = os.getenv("COUCHBASE_USER")
password = os.getenv("COUCHBASE_PASSWORD")
connection_string = os.getenv("COUCHBASE_CONNECTION_STRING")

---

## Recipe Creator

**URL:** llms-txt#recipe-creator

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/recipe-creator

This example shows how to create an intelligent recipe recommendation system that provides
detailed, personalized recipes based on your ingredients, dietary preferences, and time constraints.
The agent combines culinary knowledge, nutritional data, and cooking techniques to deliver
comprehensive cooking instructions.

Example prompts to try:

* "I have chicken, rice, and vegetables. What can I make in 30 minutes?"
* "Create a vegetarian pasta recipe with mushrooms and spinach"
* "Suggest healthy breakfast options with oats and fruits"
* "What can I make with leftover turkey and potatoes?"
* "Need a quick dessert recipe using chocolate and bananas"

```python recipe_creator.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

recipe_agent = Agent(
    name="ChefGenius",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
        You are ChefGenius, a passionate and knowledgeable culinary expert with expertise in global cuisine! ðŸ³

Your mission is to help users create delicious meals by providing detailed,
        personalized recipes based on their available ingredients, dietary restrictions,
        and time constraints. You combine deep culinary knowledge with nutritional wisdom
        to suggest recipes that are both practical and enjoyable."""),
    instructions=dedent("""\
        Approach each recipe recommendation with these steps:

1. Analysis Phase ðŸ“‹
           - Understand available ingredients
           - Consider dietary restrictions
           - Note time constraints
           - Factor in cooking skill level
           - Check for kitchen equipment needs

2. Recipe Selection ðŸ”
           - Use Exa to search for relevant recipes
           - Ensure ingredients match availability
           - Verify cooking times are appropriate
           - Consider seasonal ingredients
           - Check recipe ratings and reviews

3. Detailed Information ðŸ“
           - Recipe title and cuisine type
           - Preparation time and cooking time
           - Complete ingredient list with measurements
           - Step-by-step cooking instructions
           - Nutritional information per serving
           - Difficulty level
           - Serving size
           - Storage instructions

4. Extra Features âœ¨
           - Ingredient substitution options
           - Common pitfalls to avoid
           - Plating suggestions
           - Wine pairing recommendations
           - Leftover usage tips
           - Meal prep possibilities

Presentation Style:
        - Use clear markdown formatting
        - Present ingredients in a structured list
        - Number cooking steps clearly
        - Add emoji indicators for:
          ðŸŒ± Vegetarian
          ðŸŒ¿ Vegan
          ðŸŒ¾ Gluten-free
          ðŸ¥œ Contains nuts
          â±ï¸ Quick recipes
        - Include tips for scaling portions
        - Note allergen warnings
        - Highlight make-ahead steps
        - Suggest side dish pairings"""),
    markdown=True,
    add_datetime_to_context=True,
    )

---

## === AGENTS ===

**URL:** llms-txt#===-agents-===

story_writer = Agent(
    name="Story Writer",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are tasked with writing a 100 word story based on a given topic",
)

story_editor = Agent(
    name="Story Editor",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Review and improve the story's grammar, flow, and clarity",
)

story_formatter = Agent(
    name="Story Formatter",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="Break down the story into prologue, body, and epilogue sections",
)

---

## Pass a pydantic model that matches the input schema

**URL:** llms-txt#pass-a-pydantic-model-that-matches-the-input-schema

---

## Image Input for Tools

**URL:** llms-txt#image-input-for-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/image-input-for-tool

This example demonstrates how tools can receive and process images automatically through Agno's joint media access functionality. It shows initial image upload and analysis, DALL-E image generation within the same run, and cross-run media persistence.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Set up PostgreSQL">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Set up PostgreSQL">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

---

## Workflow Session State

**URL:** llms-txt#workflow-session-state

**Contents:**
- How Workflow Session State Works
  - 1. State Initialization
  - 2. Access and Modify State Data

Source: https://docs.agno.com/basics/state/workflows/overview

Coordinate state across workflow steps, agents, teams, and custom functions

Workflow session state enables sharing and updating state data across all components within a workflow: agents, teams, and custom functions.

Session state data will be persisted if a database is available, and loaded from there in subsequent runs of the workflow.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=eb9f2f37de2699763ae1cab8b26e5792" alt="Workflows session state diagram" data-og-width="2199" width="2199" data-og-height="1203" height="1203" data-path="images/workflows-session-state-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=be8235601f44c3d5c899e769bd7e10a7 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c54d2a340511cf08b6f62ee2dc725ab6 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=a8704d97da7f5fd6490acd6e9e847f21 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=88b25555c12572fbb92d1b7724e77cc6 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=e0c24367b61326e2115cfa74059d1316 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c5d4f68f953670a714b4a5d334f9b5fb 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5e56bc8354bbb79f31d6c4140e5dea2e" alt="Workflows session state diagram" data-og-width="2199" width="2199" data-og-height="1203" height="1203" data-path="images/workflows-session-state.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=494e73f94a71a1611b068a2561ed6c7a 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=3c754db0830ab28e7640f553c9dd93a9 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=fa17e5cf5ebf2d45a85a328abbd52bf7 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=e82eae67add368163f04fcb09dab4271 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=d66bd915da648ef787dd86aa2a5eede6 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-session-state.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5bf1239ad621457d50d67b28153b9a1b 2500w" />

## How Workflow Session State Works

### 1. State Initialization

Initialize session state when creating a workflow. The session state can start empty or with predefined data that all workflow components can access and modify.

### 2. Access and Modify State Data

All workflow components - agents, teams, and functions - can read from and write to the shared session state. This enables persistent data flow and coordination across the entire workflow execution.

From tools, you can access the session state via `run_context.session_state`.

**Example: Shopping List Management**

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from agno.run import RunContext

db = SqliteDb(db_file="tmp/workflow.db")

**Examples:**

Example 1 (unknown):
```unknown
### 2. Access and Modify State Data

All workflow components - agents, teams, and functions - can read from and write to the shared session state. This enables persistent data flow and coordination across the entire workflow execution.

From tools, you can access the session state via `run_context.session_state`.

**Example: Shopping List Management**
```

---

## The knowledge base tracks valid filter keys

**URL:** llms-txt#the-knowledge-base-tracks-valid-filter-keys

valid_filters = knowledge.get_filters()

---

## Neo4j

**URL:** llms-txt#neo4j

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/database/neo4j

The Neo4jTools toolkit enables agents to interact with Neo4j graph databases for querying and managing graph data.

**Neo4jTools** enables agents to interact with Neo4j graph databases for querying and managing graph data.

The following example requires the `neo4j` library.

You will also need a Neo4j database. The following example uses a Neo4j database running in a Docker container.

Make sure to set the `NEO4J_URI` environment variable to the URI of the Neo4j database.

The following agent can interact with Neo4j graph databases:

| Parameter                   | Type            | Default | Description                                       |
| --------------------------- | --------------- | ------- | ------------------------------------------------- |
| `uri`                       | `Optional[str]` | `None`  | Neo4j connection URI. Uses NEO4J\_URI if not set. |
| `user`                      | `Optional[str]` | `None`  | Neo4j username. Uses NEO4J\_USERNAME if not set.  |
| `password`                  | `Optional[str]` | `None`  | Neo4j password. Uses NEO4J\_PASSWORD if not set.  |
| `database`                  | `Optional[str]` | `None`  | Specific database name to connect to.             |
| `enable_list_labels`        | `bool`          | `True`  | Enable listing node labels.                       |
| `enable_list_relationships` | `bool`          | `True`  | Enable listing relationship types.                |
| `enable_get_schema`         | `bool`          | `True`  | Enable schema information retrieval.              |
| `enable_run_cypher`         | `bool`          | `True`  | Enable Cypher query execution.                    |

| Function             | Description                                           |
| -------------------- | ----------------------------------------------------- |
| `list_labels`        | List all node labels in the graph database.           |
| `list_relationships` | List all relationship types in the graph database.    |
| `get_schema`         | Get comprehensive schema information about the graph. |
| `run_cypher`         | Execute Cypher queries on the graph database.         |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/neo4j.py)
* [Neo4j Documentation](https://neo4j.com/docs/)
* [Cypher Query Language](https://neo4j.com/docs/cypher-manual/current/)

**Examples:**

Example 1 (unknown):
```unknown
You will also need a Neo4j database. The following example uses a Neo4j database running in a Docker container.
```

Example 2 (unknown):
```unknown
Make sure to set the `NEO4J_URI` environment variable to the URI of the Neo4j database.
```

Example 3 (unknown):
```unknown
Install libraries
```

Example 4 (unknown):
```unknown
Run the agent
```

---

## Add content with chunking applied

**URL:** llms-txt#add-content-with-chunking-applied

**Contents:**
- Choosing a Strategy

knowledge.add_content(
    path="documents/cookbook.pdf",
    reader=reader,
)
```

## Choosing a Strategy

The choice of chunking strategy depends on your content type and use case:

* **Text documents**: Semantic chunking maintains context and meaning
* **Structured documents**: Document or Markdown chunking preserves hierarchy
* **Tabular data**: CSV Row chunking treats each row as a separate entity
* **Mixed content**: Recursive chunking provides flexibility with multiple separators
* **Uniform processing**: Fixed Size chunking ensures consistent chunk dimensions

Each reader has a default chunking strategy that works well for its content type, but you can override it by specifying a `chunking_strategy` parameter when configuring the reader.

<Note>
  Consider your specific use case and performance requirements when choosing a chunking strategy, since different strategies vary in processing time and memory usage.
</Note>

---

## Run a hybrid search query

**URL:** llms-txt#run-a-hybrid-search-query

**Contents:**
- Usage

results = hybrid_db.search("chicken coconut soup", limit=5)
print("Hybrid Search Results:", results)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/search_type/hybrid_search.py
      bash Windows theme={null}
      python cookbook/knowledge/search_type/hybrid_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Agentic RAG with LightRAG

**URL:** llms-txt#agentic-rag-with-lightrag

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag-with-lightrag

This example demonstrates how to implement Agentic RAG using LightRAG as the vector database, with support for PDF documents, Wikipedia content, and web URLs.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/agentic_search" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Define a function to run the agent, decorated with weave.op()

**URL:** llms-txt#define-a-function-to-run-the-agent,-decorated-with-weave.op()

@weave.op()
def run(content: str):
    return agent.run(content)

---

## ag infra delete

**URL:** llms-txt#ag-infra-delete

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/delete

<ResponseField name="infra_name" type="str">
  Name of the infra to delete `-infra`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

---

## Find only HR policy documents

**URL:** llms-txt#find-only-hr-policy-documents

EQ("department", "hr")

---

## Initialize the Agno agent with the new storage backend and a DuckDuckGo search tool.

**URL:** llms-txt#initialize-the-agno-agent-with-the-new-storage-backend-and-a-duckduckgo-search-tool.

agent1 = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    debug_mode=False,
)

---

## Upload the file to Anthropic

**URL:** llms-txt#upload-the-file-to-anthropic

uploaded_file = client.beta.files.upload(
    file=Path(pdf_path),
)

if uploaded_file is not None:
    agent = Agent(
        model=Claude(
            id="claude-opus-4-20250514",
            betas=["files-api-2025-04-14"],
        ),
        markdown=True,
    )

agent.print_response(
        "Summarize the contents of the attached file.",
        files=[File(external=uploaded_file)],
    )
```

---

## Conditional fact-checking step

**URL:** llms-txt#conditional-fact-checking-step

fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

---

## MCP Toolbox

**URL:** llms-txt#mcp-toolbox

**Contents:**
- Prerequisites
- Quick Start

Source: https://docs.agno.com/basics/tools/mcp/mcp-toolbox

Learn how to use MCPToolbox with Agno to connect to MCP Toolbox for Databases with tool filtering capabilities.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.0.9" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.0.9">v2.0.9</Tooltip>
</Badge>

**MCPToolbox** enables Agents to connect to Google's [MCP Toolbox for Databases](https://googleapis.github.io/genai-toolbox/getting-started/introduction/) with advanced filtering capabilities. It extends Agno's `MCPTools` functionality to filter tools by toolset or tool name, allowing agents to load only the specific database tools they need.

You'll need the following to use MCPToolbox:

Our default setup will also require you to have Docker or Podman installed, to run the MCP Toolbox server and database for the examples.

Get started with MCPToolbox instantly using our fully functional demo.

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Our default setup will also require you to have Docker or Podman installed, to run the MCP Toolbox server and database for the examples.

## Quick Start

Get started with MCPToolbox instantly using our fully functional demo.
```

---

## Transform a regular model into a reasoning system

**URL:** llms-txt#transform-a-regular-model-into-a-reasoning-system

**Contents:**
  - What You'll See
- Reasoning with Tools
- Configuration Options
  - Display Options
  - Capturing Reasoning Events
  - Iteration Control
  - Custom Reasoning Agent

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    markdown=True,
)

reasoning_agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. Include an ASCII diagram of your solution.",
    stream=True,
    show_full_reasoning=True,  # Shows the complete reasoning process
)
python finance_reasoning.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Use tables to display data"],
    reasoning=True,
    markdown=True,
)

reasoning_agent.print_response(
    "Compare the market performance of NVDA, AMD, and INTC over the past quarter. What are the key drivers?",
    stream=True,
    show_full_reasoning=True,
)
python  theme={null}
agent.print_response(
    "Your question",
    show_full_reasoning=True,  # Display complete reasoning process (default: False)
)
python  theme={null}
reasoning_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    reasoning=True,
    reasoning_min_steps=2,  # Minimum reasoning steps (default: 1)
    reasoning_max_steps=15,  # Maximum reasoning steps (default: 10)
)
python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
### What You'll See

With `show_full_reasoning=True`, you'll see:

* **Each reasoning step** with its title, action, and result
* **The agent's thought process** including why it chose each approach
* **Tool calls made** during reasoning (if tools are provided)
* **Validation checks** performed to verify the solution
* **Confidence scores** for each step (0.0â€“1.0)
* **Self-corrections** if the agent detects errors
* **The final polished response** from your main agent

## Reasoning with Tools

Here's where reasoning agents truly excel: combining multi-step reasoning with tool use. The reasoning agent can call tools iteratively, analyze results, and build toward a comprehensive solution.
```

Example 2 (unknown):
```unknown
The reasoning agent will:

1. Break down the task (need stock data for 3 companies)
2. Use DuckDuckGo to search for current market data
3. Analyze each company's performance
4. Search for news about key drivers
5. Validate findings across multiple sources
6. Create a comprehensive comparison with tables
7. Provide a final answer with clear insights

## Configuration Options

### Display Options

Want to peek under the hood? Control what you see during reasoning:
```

Example 3 (unknown):
```unknown
### Capturing Reasoning Events

For building custom UIs or programmatically tracking reasoning progress, you can capture reasoning events (`ReasoningStarted`, `ReasoningStep`, `ReasoningCompleted`) as they happen during streaming. See the [Reasoning Reference](/reference/reasoning/reasoning#reasoning-event-types) for event attributes and complete code examples.

### Iteration Control

Adjust how many reasoning steps the agent takes:
```

Example 4 (unknown):
```unknown
* **`reasoning_min_steps`**: Ensures the agent thinks through at least this many steps before answering
* **`reasoning_max_steps`**: Prevents infinite loops by capping the iteration count

### Custom Reasoning Agent

For advanced use cases, you can provide your own reasoning agent:
```

---

## Define steps for video pipeline

**URL:** llms-txt#define-steps-for-video-pipeline

generate_video_step = Step(
    name="generate_video",
    agent=video_generator,
    description="Create a comprehensive video production plan and storyboard",
)

describe_video_step = Step(
    name="describe_video",
    agent=video_describer,
    description="Analyze and critique the video production plan with professional insights",
)

---

## Create agent with cached responses

**URL:** llms-txt#create-agent-with-cached-responses

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        cache_response=True,
        cache_ttl=3600
    ),
    tools=[...],  # Your tools
    instructions="Your instructions here"
)

---

## Docker

**URL:** llms-txt#docker

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
  - Container Management
  - Image Management
  - Volume Management
  - Network Management
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/local/docker

**DockerTools** enable an Agent to interact with Docker containers, images, volumes, and networks.

The Docker tools require the `docker` Python package. You'll also need Docker installed and running on your system.

The following example creates an agent that can manage Docker resources:

| Parameter                     | Type   | Default | Description                                                      |
| ----------------------------- | ------ | ------- | ---------------------------------------------------------------- |
| `enable_container_management` | `bool` | `True`  | Enables container management functions (list, start, stop, etc.) |
| `enable_image_management`     | `bool` | `True`  | Enables image management functions (pull, build, etc.)           |
| `enable_volume_management`    | `bool` | `False` | Enables volume management functions                              |
| `enable_network_management`   | `bool` | `False` | Enables network management functions                             |

### Container Management

| Function             | Description                                     |
| -------------------- | ----------------------------------------------- |
| `list_containers`    | Lists all containers or only running containers |
| `start_container`    | Starts a stopped container                      |
| `stop_container`     | Stops a running container                       |
| `remove_container`   | Removes a container                             |
| `get_container_logs` | Retrieves logs from a container                 |
| `inspect_container`  | Gets detailed information about a container     |
| `run_container`      | Creates and starts a new container              |
| `exec_in_container`  | Executes a command inside a running container   |

| Function        | Description                              |
| --------------- | ---------------------------------------- |
| `list_images`   | Lists all images on the system           |
| `pull_image`    | Pulls an image from a registry           |
| `remove_image`  | Removes an image                         |
| `build_image`   | Builds an image from a Dockerfile        |
| `tag_image`     | Tags an image                            |
| `inspect_image` | Gets detailed information about an image |

### Volume Management

| Function         | Description                              |
| ---------------- | ---------------------------------------- |
| `list_volumes`   | Lists all volumes                        |
| `create_volume`  | Creates a new volume                     |
| `remove_volume`  | Removes a volume                         |
| `inspect_volume` | Gets detailed information about a volume |

### Network Management

| Function                            | Description                               |
| ----------------------------------- | ----------------------------------------- |
| `list_networks`                     | Lists all networks                        |
| `create_network`                    | Creates a new network                     |
| `remove_network`                    | Removes a network                         |
| `inspect_network`                   | Gets detailed information about a network |
| `connect_container_to_network`      | Connects a container to a network         |
| `disconnect_container_from_network` | Disconnects a container from a network    |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/docker.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/docker_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following example creates an agent that can manage Docker resources:
```

---

## Get User Memory Statistics

**URL:** llms-txt#get-user-memory-statistics

Source: https://docs.agno.com/reference-api/schema/memory/get-user-memory-statistics

get /user_memory_stats
Retrieve paginated statistics about memory usage by user. Provides insights into user engagement and memory distribution across users.

---

## Pass a Pydantic model directly - no additional validation needed

**URL:** llms-txt#pass-a-pydantic-model-directly---no-additional-validation-needed

**Contents:**
- Usage

research_request = ResearchProject(
    project_name="Blockchain Development Tools",
    research_topics=["Ethereum", "Solana", "Web3 Libraries"],
    target_audience="Blockchain Developers",
    depth_level="advanced",
    max_sources=12,
    include_recent_only=False,
)

research_team.print_response(input=research_request)
bash  theme={null}
    pip install agno pydantic ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/06_input_schema_on_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## AgentOps

**URL:** llms-txt#agentops

**Contents:**
- Integrating Agno with AgentOps
- Prerequisites
- Logging Model Calls with AgentOps

Source: https://docs.agno.com/integrations/observability/agentops

Integrate Agno with AgentOps to send traces and logs to a centralized observability platform.

## Integrating Agno with AgentOps

[AgentOps](https://app.agentops.ai/) provides automatic instrumentation for your Agno agents to track all operations including agent interactions, team coordination, tool usage, and workflow execution.

1. **Install AgentOps**

Ensure you have the AgentOps package installed:

2. **Authentication**
   Go to [AgentOps](https://app.agentops.ai/) and copy your API key

## Logging Model Calls with AgentOps

This example demonstrates how to use AgentOps to log model calls.

```python  theme={null}
import agentops
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
2. **Authentication**
   Go to [AgentOps](https://app.agentops.ai/) and copy your API key
```

Example 2 (unknown):
```unknown
## Logging Model Calls with AgentOps

This example demonstrates how to use AgentOps to log model calls.
```

---

## Create and run an agent

**URL:** llms-txt#create-and-run-an-agent

agent = Agent(model=OpenAIChat(id="gpt-5-mini"))
response = agent.run("Share a 2 sentence horror story")

---

## Telegram

**URL:** llms-txt#telegram

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/social/telegram

**TelegramTools** enable an Agent to send messages to a Telegram chat using the Telegram Bot API.

The following agent will send a message to a Telegram chat.

```python cookbook/tools/telegram_tools.py theme={null}
from agno.agent import Agent
from agno.tools.telegram import TelegramTools

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will send a message to a Telegram chat.
```

---

## Tool Use Stream

**URL:** llms-txt#tool-use-stream

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/tool-use-stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Optional: Define a TypedDict with optional fields

**URL:** llms-txt#optional:-define-a-typeddict-with-optional-fields

class ResearchTopicWithOptionals(TypedDict, total=False):
    topic: str
    focus_areas: List[str]
    target_audience: str
    sources_required: int
    priority: Optional[str]

---

## ag infra restart

**URL:** llms-txt#ag-infra-restart

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/restart

Restart resources for active infra

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

<ResponseField name="pull" type="bool">
  Pull `--pull` `-p`
</ResponseField>

---

## Define communication standards

**URL:** llms-txt#define-communication-standards

**Contents:**
  - Code Review Principles

comm_standard = CulturalKnowledge(
    name="Customer Communication Tone",
    summary="Professional, empathetic, solution-focused",
    categories=["communication", "support"],
    content=(
        "- Acknowledge the customer's situation first\n"
        "- Provide clear, actionable steps\n"
        "- Avoid jargon unless necessary\n"
        "- Always offer next steps or alternatives"
    ),
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Code Review Principles
```

---

## AWS Bedrock Embedder

**URL:** llms-txt#aws-bedrock-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/aws-bedrock/usage/aws-bedrock-embedder

```python cookbook/knowledge/embedders/aws_bedrock_embedder.py theme={null}
import asyncio
from agno.knowledge.embedder.aws_bedrock import AwsBedrockEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## run: RunOutput = asyncio.run(agent.arun("Share a 2 sentence horror story"))

**URL:** llms-txt#run:-runoutput-=-asyncio.run(agent.arun("share-a-2-sentence-horror-story"))

---

## Get the response in a variable

**URL:** llms-txt#get-the-response-in-a-variable

---

## Create AgentOS

**URL:** llms-txt#create-agentos

**Contents:**
- Testing the Example

agent_os = AgentOS(
    id="agentos-hitl",
    agents=[agent],
)

app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="hitl_confirmation:app", port=7777)
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Testing the Example

Once the server is running, test the HITL flow:
```

---

## Example usage with different types of travel queries

**URL:** llms-txt#example-usage-with-different-types-of-travel-queries

if __name__ == "__main__":
    travel_agent.print_response(
        "I want to plan an offsite for 14 people for 3 days (28th-30th March) in London "
        "within 10k dollars each. Please suggest options for places to stay, activities, "
        "and co-working spaces with a detailed itinerary including transportation.",
        stream=True,
    )

---

## Filtering

**URL:** llms-txt#filtering

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/filtering

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## Generate Images

**URL:** llms-txt#generate-images

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/generate-images

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Test with vulnerable code - workflow stops at security gate

**URL:** llms-txt#test-with-vulnerable-code---workflow-stops-at-security-gate

**Contents:**
- Developer Resources

workflow.print_response("Scan this code: exec(input('Enter command: '))")
```

## Developer Resources

* [Early Stop Workflow](/basics/workflows/usage/early-stop-workflow)

---

## Our Agent will now be able to use our tool, when it deems it relevant

**URL:** llms-txt#our-agent-will-now-be-able-to-use-our-tool,-when-it-deems-it-relevant

**Contents:**
- How do tools work?
  - Tool definitions
  - Tool Execution
- Using a Toolkit

agent.print_response("What is the weather in San Francisco?", stream=True)
python  theme={null}
def get_weather(city: str) -> str:
    """
    Get the weather for a given city.

Args:
        city (str): The city to get the weather for.
    """
    return f"The weather in {city} is sunny."
json  theme={null}
{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the weather for a given city.",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "The city to get the weather for."
                }
            },
            "required": ["city"]
        }
    }
}
python  theme={null}

from pydantic import BaseModel, Field

class GetWeatherRequest(BaseModel):
    city: str = Field(description="The city to get the weather for")

def get_weather(request: GetWeatherRequest) -> str:
    """
    Get the weather for a given city.

Args:
        request (GetWeatherRequest): The request object containing the city to get the weather for.

"""
    return f"The weather in {request.city} is sunny."
json  theme={null}
{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the weather for a given city.",
        "parameters": {
            "type": "object",
            "properties": {
              "request": {
                "type": "object",
                "properties": {
                  "city": {
                    "type": "string",
                    "description": "The city to get the weather for."
                  }
                },
                "required": ["city"]
              }
            },
            "required": ["request"]
        }
    }
}
python async_tools.py theme={null}
  import asyncio
  import time

from agno.agent import Agent
  from agno.models.openai import OpenAIChat
  from agno.utils.log import logger

async def atask1(delay: int):
      """Simulate a task that takes a random amount of time to complete
      Args:
          delay (int): The amount of time to delay the task
      """
      logger.info("Task 1 has started")
      for _ in range(delay):
          await asyncio.sleep(1)
          logger.info("Task 1 has slept for 1s")
      logger.info("Task 1 has completed")
      return f"Task 1 completed in {delay:.2f}s"

async def atask2(delay: int):
      """Simulate a task that takes a random amount of time to complete
      Args:
          delay (int): The amount of time to delay the task
      """
      logger.info("Task 2 has started")
      for _ in range(delay):
          await asyncio.sleep(1)
          logger.info("Task 2 has slept for 1s")
      logger.info("Task 2 has completed")
      return f"Task 2 completed in {delay:.2f}s"

async def atask3(delay: int):
      """Simulate a task that takes a random amount of time to complete
      Args:
          delay (int): The amount of time to delay the task
      """
      logger.info("Task 3 has started")
      for _ in range(delay):
          await asyncio.sleep(1)
          logger.info("Task 3 has slept for 1s")
      logger.info("Task 3 has completed")
      return f"Task 3 completed in {delay:.2f}s"

async_agent = Agent(
      model=OpenAIChat(id="gpt-5-mini"),
      tools=[atask2, atask1, atask3],
      markdown=True,
  )

asyncio.run(
      async_agent.aprint_response("Please run all tasks with a delay of 3s", stream=True)
  )
  python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  In the example above, the `get_weather` function is a tool. When called, the tool result is shown in the output.
</Tip>

## How do tools work?

The heart of Agent execution is the LLM loop. The typical execution flow of the LLM loop is:

1. The agent sends the run context (system message, user message, chat history, etc) and tool definitions to the model.
2. The model responds with a message or a tool call.
3. If the model makes a tool call, the tool is executed and the result is returned to the model.
4. The model processes the updated context, repeating this loop until it produces a final message without any tool calls.
5. The agent returns this final response to the caller.

### Tool definitions

Agno automatically converts your tool functions into the required tool definition format for the model. Typically this is a JSON schema that describes the parameters and return type of the tool.

For example:
```

Example 2 (unknown):
```unknown
This will be converted into the following tool definition:
```

Example 3 (unknown):
```unknown
This tool definition is then sent to the model so that it knows how to call the tool when it is requested.
You'll notice as well that the `Args` section is automatically stripped from the definition, parsed and used to populate the definitions of individualproperties.

When using a Pydantic model in an argument of a tool function, Agno will automatically convert the model into the required tool definition format.

For example:
```

Example 4 (unknown):
```unknown
This will be converted into the following tool definition:
```

---

## CSV Row Chunking

**URL:** llms-txt#csv-row-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/csv-row

CSV row chunking is a method of splitting CSV files into smaller chunks based on the number of rows, rather than character count. This approach is particularly useful for structured data where you want to process CSV files in manageable row-based chunks while preserving the integrity of individual records.

<Snippet file="chunking-csv-row.mdx" />

---

## Add from GCS

**URL:** llms-txt#add-from-gcs

**Contents:**
- Usage
- Params

asyncio.run(
    knowledge.add_content_async(
        name="GCS PDF",
        remote_content=GCSContent(
            bucket_name="thai-recepies", blob_name="ThaiRecipes.pdf"
        ),
        metadata={"remote_content": "GCS"},
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "What is the best way to make a Thai curry?",
    markdown=True,
)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector google-cloud-storage
    bash Mac theme={null}
      python cookbook/knowledge/basic_operations/07_from_gcs.py
      bash Windows theme={null}
      python cookbook/knowledge/basic_operations/07_from_gcs.py
      ```
    </CodeGroup>
  </Step>
</Steps>

<Snippet file="gcs-remote-content-params.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Configure Google Cloud credentials">
    Set up your GCS credentials using one of these methods:

    * Service Account Key: Set `GOOGLE_APPLICATION_CREDENTIALS` environment variable
    * gcloud CLI: `gcloud auth application-default login`
    * Workload Identity (if running on Google Cloud)
  </Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Cal.com

**URL:** llms-txt#cal.com

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/calcom

The following example requires the `pytz` and `requests` libraries.

The following agent will use Cal.com to list all events in your Cal.com account for tomorrow.

| Parameter                      | Type   | Default | Description                                |
| ------------------------------ | ------ | ------- | ------------------------------------------ |
| `api_key`                      | `str`  | `None`  | Cal.com API key                            |
| `event_type_id`                | `int`  | `None`  | Event type ID for scheduling               |
| `user_timezone`                | `str`  | `None`  | User's timezone (e.g. "America/New\_York") |
| `enable_get_available_slots`   | `bool` | `True`  | Enable getting available time slots        |
| `enable_create_booking`        | `bool` | `True`  | Enable creating new bookings               |
| `enable_get_upcoming_bookings` | `bool` | `True`  | Enable getting upcoming bookings           |
| `enable_reschedule_booking`    | `bool` | `True`  | Enable rescheduling bookings               |
| `enable_cancel_booking`        | `bool` | `True`  | Enable canceling bookings                  |

| Function                | Description                                      |
| ----------------------- | ------------------------------------------------ |
| `get_available_slots`   | Gets available time slots for a given date range |
| `create_booking`        | Creates a new booking with provided details      |
| `get_upcoming_bookings` | Gets list of upcoming bookings                   |
| `get_booking_details`   | Gets details for a specific booking              |
| `reschedule_booking`    | Reschedules an existing booking                  |
| `cancel_booking`        | Cancels an existing booking                      |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/calcom.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/calcom_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will use Cal.com to list all events in your Cal.com account for tomorrow.
```

---

## Workflow requires debug mode to use the workflow logger

**URL:** llms-txt#workflow-requires-debug-mode-to-use-the-workflow-logger

**Contents:**
- Learn more

workflow = Workflow(
    debug_mode=True,
    steps=[Step(name="step1", agent=agent)]
)
workflow.run("Hello from workflow!")  # Workflow logs will go to workflow.log
```

<CardGroup cols={2}>
  <Card title="Telemetry" icon="chart-line" href="/basics/telemetry">
    Learn about Agno telemetry
  </Card>

<Card title="Debugging Agents" icon="bug" href="/basics/agents/debugging-agents">
    Debug your agents effectively
  </Card>
</CardGroup>

---

## sources_required=5,

**URL:** llms-txt#sources_required=5,

---

## Add custom middleware

**URL:** llms-txt#add-custom-middleware

**Contents:**
- Usage
- Middleware Features
- Developer Resources

app.add_middleware(
    RateLimitMiddleware,
    requests_per_minute=10,
    window_size=60,
)

app.add_middleware(
    RequestLoggingMiddleware,
    log_body=False,
    log_headers=False,
)

if __name__ == "__main__":
    """
    Run the essential middleware demo using AgentOS serve method.
    
    Features:
    1. Rate Limiting (10 requests/minute)
    2. Request/Response Logging
    """

agent_os.serve(
        app="custom_middleware:app",
        reload=True,
    )
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno openai ddgs "fastapi[standard]" uvicorn sqlalchemy pgvector psycopg
    bash  theme={null}
    # Using Docker
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash  theme={null}
    python custom_middleware.py
    bash  theme={null}
    curl http://localhost:7777/config
    bash  theme={null}
    for i in {1..15}; do curl http://localhost:7777/config; done
    bash  theme={null}
    curl -v http://localhost:7777/config
    python  theme={null}
    app.add_middleware(
        RateLimitMiddleware,
        requests_per_minute=100,  # Allow 100 requests per minute
        window_size=60,           # 60-second sliding window
    )
    
    ðŸ” Request #1: GET /config from 127.0.0.1
    âœ… Response: 200 in 45.2ms
    ðŸ” Request #2: POST /agents/demo-agent/runs from 127.0.0.1  
    âœ… Response: 200 in 1240.8ms
    python  theme={null}
    app.add_middleware(
        RequestLoggingMiddleware,
        log_body=True,     # Log request bodies
        log_headers=True,  # Log request headers
    )
    ```
  </Tab>
</Tabs>

## Developer Resources

* [Custom Middleware Documentation](/agent-os/middleware/custom)
* [FastAPI Middleware Documentation](https://fastapi.tiangolo.com/tutorial/middleware/)

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example">
```

---

## Create team with knowledge filters

**URL:** llms-txt#create-team-with-knowledge-filters

team_with_knowledge = Team(
    name="Team with Knowledge",
    members=[
        web_agent
    ],  # If you omit the member, the leader will search the knowledge base itself.
    model=OpenAIChat(id="gpt-5-mini"),
    knowledge=knowledge_base,
    show_members_responses=True,
    markdown=True,
    knowledge_filters={
        "user_id": "jordan_mitchell"
    },  # Filter to specific user's documents
)

---

## Video Input (Bytes Content)

**URL:** llms-txt#video-input-(bytes-content)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/video-input-bytes-content

```python cookbook/models/google/gemini/video_input_bytes_content.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://videos.pexels.com/video-files/5752729/5752729-uhd_2560_1440_30fps.mp4"

---

## History Management

**URL:** llms-txt#history-management

**Contents:**
- Common Patterns
  - Automatic History (Most Common)
  - On-Demand History Access
  - Programmatic Access

Source: https://docs.agno.com/basics/sessions/history-management

Control how conversation history is accessed and used

Agents and Teams with a database configured automatically track message and run history. You have multiple ways to access and use this history to give your agents and teams "memory" of past conversations.

### Automatic History (Most Common)

Enable `add_history_to_context=True` to automatically include recent messages in every run:

**When to use:** Chat-style products, quick prototypes, any scenario where responses need context from previous turns.

### On-Demand History Access

Enable `read_chat_history=True` to let the model decide when to look up history:

**When to use:** Analytics, auditing, or when you want the model to selectively access history rather than always including it.

### Programmatic Access

Retrieve history directly in your code:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

**When to use:** Chat-style products, quick prototypes, any scenario where responses need context from previous turns.

### On-Demand History Access

Enable `read_chat_history=True` to let the model decide when to look up history:
```

Example 3 (unknown):
```unknown
**When to use:** Analytics, auditing, or when you want the model to selectively access history rather than always including it.

### Programmatic Access

Retrieve history directly in your code:
```

---

## Gmail

**URL:** llms-txt#gmail

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/social/gmail

**Gmail** enables an Agent to interact with Gmail, allowing it to read, search, send, manage emails, and organize them with labels.

The Gmail toolkit requires Google API client libraries and proper authentication setup. Install the required dependencies:

You'll also need to set up Google Cloud credentials:

1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Create a project or select an existing one
3. Enable the Gmail API
4. Create OAuth 2.0 credentials
5. Set up environment variables:

| Parameter          | Type          | Default | Description                          |
| ------------------ | ------------- | ------- | ------------------------------------ |
| `creds`            | `Credentials` | `None`  | Pre-fetched OAuth credentials        |
| `credentials_path` | `str`         | `None`  | Path to credentials file             |
| `token_path`       | `str`         | `None`  | Path to token file                   |
| `scopes`           | `List[str]`   | `None`  | Custom OAuth scopes                  |
| `port`             | `int`         | `None`  | Port to use for OAuth authentication |

| Function                | Description                                                                                                                                                                                                            |
| ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `get_latest_emails`     | Get the latest X emails from the user's inbox. Parameters include `count` (int) for number of emails to retrieve.                                                                                                      |
| `get_emails_from_user`  | Get X number of emails from a specific sender. Parameters include `user` (str) for sender name or email, and `count` (int) for maximum number of emails.                                                               |
| `get_unread_emails`     | Get the latest X unread emails. Parameters include `count` (int) for maximum number of unread emails.                                                                                                                  |
| `get_starred_emails`    | Get X number of starred emails. Parameters include `count` (int) for maximum number of starred emails.                                                                                                                 |
| `get_emails_by_context` | Get X number of emails matching a specific context. Parameters include `context` (str) for search term, and `count` (int) for maximum number of emails.                                                                |
| `get_emails_by_date`    | Get emails within a specific date range. Parameters include `start_date` (int) for unix timestamp, `range_in_days` (Optional\[int]) for date range, and `num_emails` (Optional\[int], default=10).                     |
| `get_emails_by_thread`  | Retrieve all emails from a specific thread. Parameters include `thread_id` (str) for the thread ID.                                                                                                                    |
| `search_emails`         | Search emails using natural language queries. Parameters include `query` (str) for search query, and `count` (int) for number of emails to retrieve.                                                                   |
| `create_draft_email`    | Create and save an email draft with attachments. Parameters include `to` (str), `subject` (str), `body` (str), `cc` (Optional\[str]), and `attachments` (Optional\[Union\[str, List\[str]]]).                          |
| `send_email`            | Send an email with attachments. Parameters include `to` (str), `subject` (str), `body` (str), `cc` (Optional\[str]), and `attachments` (Optional\[Union\[str, List\[str]]]).                                           |
| `send_email_reply`      | Reply to an existing email thread. Parameters include `thread_id` (str), `message_id` (str), `to` (str), `subject` (str), `body` (str), `cc` (Optional\[str]), and `attachments` (Optional\[Union\[str, List\[str]]]). |
| `mark_email_as_read`    | Mark a specific email as read. Parameters include `message_id` (str) for the message ID.                                                                                                                               |
| `mark_email_as_unread`  | Mark a specific email as unread. Parameters include `message_id` (str) for the message ID.                                                                                                                             |
| `list_custom_labels`    | List all user-created custom labels. No parameters required.                                                                                                                                                           |
| `apply_label`           | Apply labels to emails (creates if needed). Parameters include `context` (str) for search query, `label_name` (str) for label name, and `count` (int, default=10).                                                     |
| `remove_label`          | Remove labels from emails. Parameters include `context` (str) for search query, `label_name` (str) for label name, and `count` (int, default=10).                                                                      |
| `delete_custom_label`   | Delete a custom label with confirmation. Parameters include `label_name` (str) and `confirm` (bool, default=False) for safety confirmation.                                                                            |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Example](/examples/basics/tools/others/gmail)

**Examples:**

Example 1 (unknown):
```unknown
You'll also need to set up Google Cloud credentials:

1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Create a project or select an existing one
3. Enable the Gmail API
4. Create OAuth 2.0 credentials
5. Set up environment variables:
```

Example 2 (unknown):
```unknown
## Example
```

---

## Not just: "send back", "item", "bought", "month"

**URL:** llms-txt#not-just:-"send-back",-"item",-"bought",-"month"

**Contents:**
  - Source Attribution

**Examples:**

Example 1 (unknown):
```unknown
### Source Attribution

Agents can provide references to where they found information, building trust and enabling verification.
```

---

## Should use parallel_extract

**URL:** llms-txt#should-use-parallel_extract

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "Extract information about the product features from https://parallel.ai and https://docs.parallel.ai"
)
```

| Parameter                | Type                  | Default                       | Description                                                                          |
| ------------------------ | --------------------- | ----------------------------- | ------------------------------------------------------------------------------------ |
| `api_key`                | `Optional[str]`       | `None`                        | Parallel API key. If not provided, will use PARALLEL\_API\_KEY environment variable. |
| `enable_search`          | `bool`                | `True`                        | Enable Search API functionality for AI-optimized web search.                         |
| `enable_extract`         | `bool`                | `True`                        | Enable Extract API functionality for content extraction from URLs.                   |
| `all`                    | `bool`                | `False`                       | Enable all tools. Overrides individual flags when True.                              |
| `max_results`            | `int`                 | `10`                          | Default maximum number of results for search operations.                             |
| `max_chars_per_result`   | `int`                 | `10000`                       | Default maximum characters per result for search operations.                         |
| `beta_version`           | `str`                 | `"search-extract-2025-10-10"` | Beta API version header.                                                             |
| `mode`                   | `Optional[str]`       | `None`                        | Default search mode. Options: "one-shot" or "agentic".                               |
| `include_domains`        | `Optional[List[str]]` | `None`                        | Default domains to restrict results to.                                              |
| `exclude_domains`        | `Optional[List[str]]` | `None`                        | Default domains to exclude from results.                                             |
| `max_age_seconds`        | `Optional[int]`       | `None`                        | Default cache age threshold. When set, minimum value is 600 seconds.                 |
| `timeout_seconds`        | `Optional[float]`     | `None`                        | Default timeout for content retrieval.                                               |
| `disable_cache_fallback` | `Optional[bool]`      | `None`                        | Default cache fallback behavior.                                                     |

| Function           | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `parallel_search`  | Search the web using Parallel's Search API with natural language objective. Parameters include `objective` (str) for natural-language description, `search_queries` (list) for traditional keyword queries, `max_results` (int) for maximum number of results, and `max_chars_per_result` (int) for maximum characters per result. Returns JSON formatted string with URLs, titles, publish dates, and relevant excerpts.                                                                                                                                                   |
| `parallel_extract` | Extract content from specific URLs using Parallel's Extract API. Parameters include `urls` (list) for URLs to extract from, `objective` (str) to guide extraction, `search_queries` (list) for targeting relevant content, `excerpts` (bool, default=True) to include text snippets, `max_chars_per_excerpt` (int) to limit excerpt characters, `full_content` (bool, default=False) to include complete page text, and `max_chars_for_full_content` (int) to limit full content characters. Returns JSON formatted string with extracted content in clean markdown format. |

## Developer Resources

* View [Example](/examples/basics/tools/search/parallel)
* View [Parallel SDK Documentation](https://docs.parallel.ai)
* View [Parallel API Reference](https://docs.parallel.ai/api-reference)

---

## Setup the Neon database

**URL:** llms-txt#setup-the-neon-database

db = PostgresDb(db_url=NEON_DB_URL)

---

## Example questions to try:

**URL:** llms-txt#example-questions-to-try:

---

## Cassandra Vector Database

**URL:** llms-txt#cassandra-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/cassandra/overview

Learn how to use Cassandra as a vector database for your Knowledge Base

Install cassandra packages

```python agent_with_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.cassandra import Cassandra
from agno.knowledge.embedder.mistral import MistralEmbedder
from agno.models.mistral import MistralChat
from cassandra.cluster import Cluster

**Examples:**

Example 1 (unknown):
```unknown
Run cassandra
```

Example 2 (unknown):
```unknown
## Example
```

---

## Create a knowledge base with the PDFs from the data/pdfs directory

**URL:** llms-txt#create-a-knowledge-base-with-the-pdfs-from-the-data/pdfs-directory

knowledge = Knowledge(
    vector_db=PgVector(
        table_name="pdf_documents",
        db_url=db_url,
    )
)

---

## store everything

**URL:** llms-txt#store-everything

debug_workflow = Workflow(
    name="Debug Workflow",
    store_events=True,
    steps=[...]
)

---

## Run agent with input="Trending startups and products."

**URL:** llms-txt#run-agent-with-input="trending-startups-and-products."

response: RunOutput = agent.run(input="Trending startups and products.")

---

## Multi Purpose CLI App with Workflow History

**URL:** llms-txt#multi-purpose-cli-app-with-workflow-history

Source: https://docs.agno.com/basics/chat-history/workflow/usage/multi-purpose-cli

This example demonstrates how to use workflow history in a multi purpose CLI.

This example shows how to use the `add_workflow_history_to_steps` flag to add workflow history to the steps.
In this case we have a multi-step workflow with a single agent.

We show different scenarios of a continuous execution of the workflow.
We have 5 different demos:

* Customer Support
* Medical Consultation
* Tutoring

```python 05_multi_purpose_cli.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## pprint(response)

**URL:** llms-txt#pprint(response)

**Contents:**
- Usage

bash  theme={null}
    pip install -U agno openai ddgs rich
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch response_as_variable.py
    bash Mac theme={null}
      python response_as_variable.py
      bash Windows theme={null}
      python response_as_variable.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Ask the agent to process web content

**URL:** llms-txt#ask-the-agent-to-process-web-content

**Contents:**
  - 3. Google Places Crawler

agent.print_response("Summarize the content from https://docs.agno.com/introduction", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

agent = Agent(
    tools=[
        ApifyTools(actors=["compass/crawler-google-places"])
    ]
)

**Examples:**

Example 1 (unknown):
```unknown
### 3. Google Places Crawler

The [Google Places Crawler](https://apify.com/compass/crawler-google-places) extracts data about businesses from Google Maps and Google Places.
```

---

## SQLite for Workflow

**URL:** llms-txt#sqlite-for-workflow

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/sqlite/usage/sqlite-for-workflow

Agno supports using SQLite as a storage backend for Workflows using the `SqliteDb` class.

```python sqlite_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db = SqliteDb(db_file="tmp/workflow.db")

---

## AWS Credentials

**URL:** llms-txt#aws-credentials

AWS_ACCESS_KEY_ID = getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = getenv("AWS_SECRET_ACCESS_KEY")

db = DynamoDb(
    region_name="us-east-1",
    # aws_access_key_id: AWS access key id
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    # aws_secret_access_key: AWS secret access key
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)

---

## Prompt Injection Guardrail

**URL:** llms-txt#prompt-injection-guardrail

Source: https://docs.agno.com/basics/guardrails/usage/agent/prompt-injection

This example demonstrates how to use Agno's built-in prompt injection guardrail to detect and stop prompt injection and jailbreak attempts.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Filter Expressions

**URL:** llms-txt#filter-expressions

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/filter-expressions

```python filter_expressions.py theme={null}
from agno.agent import Agent
from agno.filters import AND, EQ, IN, NOT
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.pgvector import PgVector

---

## Build a Knowledge base backed by Redis

**URL:** llms-txt#build-a-knowledge-base-backed-by-redis

knowledge = Knowledge(
    name="My Redis Vector Knowledge Base",
    description="This knowledge base uses Redis + RedisVL as the vector store",
    vector_db=vector_db,
)

---

## AgentOS Middleware

**URL:** llms-txt#agentos-middleware

**Contents:**
- Quick Start

Source: https://docs.agno.com/agent-os/middleware/overview

Add authentication, logging, monitoring, and security features to your AgentOS application using middleware

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.0" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.0">v2.1.0</Tooltip>
</Badge>

AgentOS is built on FastAPI, allowing you to add any [FastAPI/Starlette compatible middleware](https://fastapi.tiangolo.com/tutorial/middleware/) for authentication, logging, monitoring, and security. Agno provides built-in JWT middleware for authentication, and you can create custom middleware for rate limiting, request logging, and security headers.

Additionally, Agno provides some built-in middleware for common use cases, including authentication.

See the following guides:

<CardGroup cols={2}>
  <Card title="Custom Middleware" icon="code" href="/agent-os/middleware/custom">
    Create your own middleware for logging, rate limiting, monitoring, and security.
  </Card>

<Card title="JWT Middleware" icon="key" href="/agent-os/middleware/jwt">
    Built-in JWT authentication with automatic parameter injection and claims extraction.
  </Card>
</CardGroup>

Adding middleware to your AgentOS application is straightforward:

```python agent_os_with_jwt_middleware.py theme={null}
from agno.os import AgentOS
from agno.os.middleware import JWTMiddleware
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.agent import Agent

db = PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")

agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
)

---

## Email

**URL:** llms-txt#email

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/social/email

**EmailTools** enable an Agent to send an email to a user. The Agent can send an email to a user with a specific subject and body.

| Parameter           | Type            | Default | Description                         |
| ------------------- | --------------- | ------- | ----------------------------------- |
| `receiver_email`    | `Optional[str]` | `None`  | The email address of the receiver.  |
| `sender_name`       | `Optional[str]` | `None`  | The name of the sender.             |
| `sender_email`      | `Optional[str]` | `None`  | The email address of the sender.    |
| `sender_passkey`    | `Optional[str]` | `None`  | The passkey for the sender's email. |
| `enable_email_user` | `bool`          | `True`  | Enable the email\_user function.    |
| `all`               | `bool`          | `False` | Enable all available functions.     |

| Function     | Description                                                                                                                                                                                                                       |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `email_user` | Emails the user with the given subject and body. Parameters include `subject` (str) for the email subject and `body` (str) for the email content. Currently works with Gmail. Returns "email sent successfully" or error message. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/email.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/email_tools.py)

---

## Search with metadata filters for more precise results

**URL:** llms-txt#search-with-metadata-filters-for-more-precise-results

results = knowledge.search(
    query="vacation policy",
    max_results=5,
    filters={"department": "hr", "type": "policy"}
)

---

## Add Python Libraries

**URL:** llms-txt#add-python-libraries

**Contents:**
- Update pyproject.toml
- Generate requirements
- Rebuild Images
  - Rebuild dev images
  - Rebuild production images
- Recreate Resources
  - Recreate dev containers
  - Update ECS services

Source: https://docs.agno.com/templates/infra-management/python-packages

Agno templates are setup to manage dependencies using a [pyproject.toml](https://packaging.python.org/en/latest/specifications/declaring-project-metadata/#declaring-project-metadata) file, **which is used to generate the `requirements.txt` file using [uv](https://github.com/astral-sh/uv) or [pip-tools](https://pip-tools.readthedocs.io/en/latest/).**

Adding or Updating a python library is a 2 step process:

1. Add library to the `pyproject.toml` file
2. Auto-Generate the `requirements.txt` file

<Warning>
  We highly recommend auto-generating the `requirements.txt` file using this process.
</Warning>

## Update pyproject.toml

* Open the `pyproject.toml` file
* Add new libraries to the dependencies section.

## Generate requirements

After updating the `dependencies` in the `pyproject.toml` file, auto-generate the `requirements.txt` file using a helper script or running `pip-compile` directly.

If you'd like to upgrade all python libraries to their latest version, run:

After updating the `requirements.txt` file, rebuild your images.

### Rebuild dev images

### Rebuild production images

<Note>
  Remember to [authenticate with ECR](/templates/infra-management/production-app#ecr-images) if needed.
</Note>

## Recreate Resources

After rebuilding images, recreate the resources.

### Recreate dev containers

### Update ECS services

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

If you'd like to upgrade all python libraries to their latest version, run:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Rebuild Images

After updating the `requirements.txt` file, rebuild your images.

### Rebuild dev images

<CodeGroup>
```

---

## Pydantic AI

**URL:** llms-txt#pydantic-ai

**Contents:**
  - Memory Usage
  - Results

python cookbook/evals/performance/comparison/pydantic_ai_instantiation.py
```

LangGraph is on the right, **let's start it first and give it a head start**. Then CrewAI and Pydantic AI follow, and finally Agno. Agno obviously finishes first, but let's see by how much.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/Xc0-_OHxxYe_vtGw/videos/performance_benchmark.mp4?fit=max&auto=format&n=Xc0-_OHxxYe_vtGw&q=85&s=89288701c4cb61d9d2a551fd0b5630a6" type="video/mp4" data-path="videos/performance_benchmark.mp4" />
  </video>
</Frame>

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we've made a mistake, please let us know.

Taking Agno as the baseline, we can see that:

| Metric             | Agno | Langgraph   | PydanticAI | CrewAI     |
| ------------------ | ---- | ----------- | ---------- | ---------- |
| **Time (seconds)** | 1Ã—   | 529Ã— slower | 57Ã— slower | 70Ã— slower |
| **Memory (MiB)**   | 1Ã—   | 24Ã— higher  | 4Ã— higher  | 10Ã— higher |

Exact numbers from the benchmark:

| Metric             | Agno     | Langgraph | PydanticAI | CrewAI   |
| ------------------ | -------- | --------- | ---------- | -------- |
| **Time (seconds)** | 0.000003 | 0.001587  | 0.000170   | 0.000210 |
| **Memory (MiB)**   | 0.006642 | 0.161435  | 0.028712   | 0.065652 |

<Note>
  Agno agents are designed for performance and while we share benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.
</Note>

---

## Access Run Context and Session State in Custom Python Function Step

**URL:** llms-txt#access-run-context-and-session-state-in-custom-python-function-step

Source: https://docs.agno.com/basics/state/workflows/usage/access-session-state-in-custom-python-function-step

This example demonstrates how to access the run context in a custom python function step

<Steps>
  <Step title="Create a Python file">
    Create a file named `access_session_state_in_custom_python_function_step.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

<Step title="Run the workflow">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

  <Step title="Run the workflow">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## User Input Required Async

**URL:** llms-txt#user-input-required-async

Source: https://docs.agno.com/basics/hitl/usage/user-input-required-async

This example demonstrates how to use the requires_user_input parameter with asynchronous operations. It shows how to collect specific user input fields in an async environment.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Debugging Agents

**URL:** llms-txt#debugging-agents

**Contents:**
- Debug Mode

Source: https://docs.agno.com/basics/agents/debugging-agents

Learn how to debug Agno Agents.

Agno comes with a exceptionally well-built debug mode that helps you understand the flow of execution and the intermediate steps. For example:

* Inspect the messages sent to the model and the response it generates.
* Trace intermediate steps and monitor metrics like token usage, execution time, etc.
* Inspect tool calls, errors, and their results. This can help you identify issues with your tools.

To enable debug mode:

1. Set the `debug_mode` parameter on your agent, to enable it for all runs.
2. Set the `debug_mode` parameter on the `run` method, to enable it for the current run.
3. Set the `AGNO_DEBUG` environment variable to `True`, to enable debug mode for all agents.

```python  theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-5"),
    tools=[HackerNewsTools()],
    instructions="Write a report on the topic. Output only the report.",
    markdown=True,
    debug_mode=True,
    # debug_level=2, # Uncomment to get more detailed logs
)

---

## PDF URL Password Reader

**URL:** llms-txt#pdf-url-password-reader

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/pdf-url-password-reader

The **PDF URL Password Reader** processes password-protected PDF files directly from URLs, allowing you to handle secure remote documents.

```python examples/basics/knowledge/readers/pdf_reader_url_password.py theme={null}
from agno.agent import Agent
from agno.knowledge.content import ContentAuth
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## The beta features are now activated, the model will have access to use them.

**URL:** llms-txt#the-beta-features-are-now-activated,-the-model-will-have-access-to-use-them.

**Contents:**
- Natively Supported Beta Features

agent.print_response("What is the weather in Tokyo?")
```

## Natively Supported Beta Features

Some beta features will require additional configuration. For example, the context management beta requires you to configure the Agent Skills feature will require you to specify which document formats to work with.
In that case, you will need to use the `betas` parameter to activate the beta feature, and provide the rest of the configuration via the relevant parameter.

You can check how to use each one of these beta features in our docs.

Here are the ones supported natively with Agno:

* [Context Management](/integrations/models/native/anthropic/usage/context-management)
* [Code Execution](/integrations/models/native/anthropic/usage/code-execution)
* [File Upload](/integrations/models/native/anthropic/usage/pdf-input-url)
* [Prompt Caching](/integrations/models/native/anthropic/usage/prompt-caching)
* [Agent Skills](/integrations/models/native/anthropic/usage/skills)
* [Web Fetch](/integrations/models/native/anthropic/usage/web-fetch)

---

## Qdrant

**URL:** llms-txt#qdrant

Source: https://docs.agno.com/reference/vector-db/qdrant

<Snippet file="vector-db-qdrant-reference.mdx" />

---

## Verbosity Control

**URL:** llms-txt#verbosity-control

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/verbosity-control

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Agent with PDF Input (Local file)

**URL:** llms-txt#agent-with-pdf-input-(local-file)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/pdf-input-local

```python cookbook/models/google/gemini/pdf_input_local.py theme={null}
from pathlib import Path
from agno.agent import Agent
from agno.media import File
from agno.models.google import Gemini
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

---

## Upload the video file if it doesn't exist

**URL:** llms-txt#upload-the-video-file-if-it-doesn't-exist

**Contents:**
- Usage

if not video_file:
    try:
        logger.info(f"Uploading video: {video_path}")
        video_file = model.get_client().files.upload(
            file=video_path,
            config=dict(name=video_path.stem, display_name=video_path.stem),
        )

# Check whether the file is ready to be used.
        while video_file.state.name == "PROCESSING":
            time.sleep(2)
            video_file = model.get_client().files.get(name=video_file.name)

logger.info(f"Uploaded video: {video_file}")
    except Exception as e:
        logger.error(f"Error uploading video: {e}")

if __name__ == "__main__":
    agent.print_response(
        "Tell me about this video",
        videos=[Video(content=video_file)],
        stream=True,
    )
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/video_input_file_upload.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/video_input_file_upload.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Works with agentic filtering (agent decides filters dynamically)

**URL:** llms-txt#works-with-agentic-filtering-(agent-decides-filters-dynamically)

knowledge_filters = [{"department": "hr", "document_type": "policy"}]

---

## Each content object includes:

**URL:** llms-txt#each-content-object-includes:

**Contents:**
- Management Features
  - Content Deletion with Vector Cleanup

print(content.name)         # Content name
print(content.description)  # Description
print(content.metadata)     # Custom metadata
print(content.file_type)    # File type (.pdf, .txt, etc.)
print(content.size)         # File size in bytes
print(content.status)       # Processing status
print(content.created_at)   # When it was added
print(content.updated_at)   # Last modification
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Management Features

### Content Deletion with Vector Cleanup

Delete content and automatically clean up associated vectors:

This automatically:

1. Removes the content metadata from ContentsDB
2. Deletes associated vectors from the vector database
3. Maintains consistency between both databases
```

---

## Add Secrets

**URL:** llms-txt#add-secrets

**Contents:**
- Development Secrets
- Production Secrets

Source: https://docs.agno.com/templates/infra-management/secrets

Secret management is a critical part of your application security and should be taken seriously.

Local secrets are defined in the `infra/secrets` directory which is excluded from version control (see `.gitignore`). Its contents should be handled with the same security as passwords.

Production secrets are managed by [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html).

<Note>
  Incase you're missing the secrets dir, copy `infra/example_secrets`
</Note>

## Development Secrets

Apps running locally can read secrets using a `yaml` file, for example:

## Production Secrets

`AWS Secrets` are used to manage production secrets, which are read by the production apps.

```python prd_resources.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Production Secrets

`AWS Secrets` are used to manage production secrets, which are read by the production apps.
```

---

## CrewAI

**URL:** llms-txt#crewai

python cookbook/evals/performance/comparison/crewai_instantiation.py

---

## Agent Session State

**URL:** llms-txt#agent-session-state

**Contents:**
- State Management

Source: https://docs.agno.com/basics/state/agent/overview

Manage persistent state in agents across multiple runs within a session

Your Agents often need to access certain data during a run/session. It could be a todo list, the user's profile, or anything else.

When this data needs to remain accessible across runs, or needs to be updated during the session, you want to consider it **session state**.

Session state is accessible from tool calls, pre-hooks and post-hooks, and other functions that are part of the Agent run. You are also able to use it in the system message, to ultimately present it to the Model.

Session state is also persisted in the database, if one is available to the Agent, and is automatically loaded when the session is continued.

<Note>
  **Understanding Agent "Statelessness"**: Agents in Agno don't maintain working state directly on the `Agent` object in memory. Instead they provide state management capabilities:

* The `session.get_session_state(session_id=session_id)` method retrieves the session state of a particular session from the database
  * The `session_state` parameter on `Agent` provides the **default** state data for new sessions
  * Working state is managed per run and persisted to the database per session
  * The agent instance (or attributes thereof) itself is not modified during runs
</Note>

Now that we understand what session state is, let's see how it works:

* You can set the Agent's `session_state` parameter with a dictionary of default state variables. This will be the initial state.
* You can pass `session_state` to `agent.run()`. This will take precedence over the Agent's default state for that run.
* You can access the session state in tool calls and other functions, via `run_context.session_state`.
* The `session_state` will be stored in your database. Subsequent runs **within the same session** will load the state from the database. See the [guide](/basics/state/agent#maintaining-state-across-multiple-runs-within-a-session) for more information.
* You can use any data in your `session_state` in the system message, by referencing it in the `description` and `instructions` parameters. See the [guide](/basics/state/agent#using-state-in-instructions) for more information.
* You can have your Agent automatically update the session state by setting the `enable_agentic_state` parameter to `True`. See the [guide](/basics/state/agent#agentic-session-state) for more information.

Here's an example where an Agent is managing a shopping list:

```python session_state.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run import RunContext

---

## Newspaper

**URL:** llms-txt#newspaper

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/newspaper

**NewspaperTools** enable an Agent to read news articles using the Newspaper4k library.

The following example requires the `newspaper3k` library.

The following agent will summarize the wikipedia article on language models.

| Parameter                 | Type   | Default | Description                                                   |
| ------------------------- | ------ | ------- | ------------------------------------------------------------- |
| `enable_get_article_text` | `bool` | `True`  | Enables the functionality to retrieve the text of an article. |

| Function           | Description                                                                                                                                                                             |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `get_article_text` | Retrieves the text of an article from a specified URL. Parameters include `url` for the URL of the article. Returns the text of the article or an error message if the retrieval fails. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/newspaper.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/newspaper_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will summarize the wikipedia article on language models.
```

---

## Setup your Team

**URL:** llms-txt#setup-your-team

team = Team(db=db, ...)

---

## Implementing a Custom Retriever

**URL:** llms-txt#implementing-a-custom-retriever

**Contents:**
- Setup
  - Example: Custom Retriever for `Knowledge`

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/custom-retriever

Learn how to implement a custom retriever for precise control over document retrieval in your knowledge base.

In some cases, complete control over how an agent retrieves information from the knowledge base is required. This can be achieved by implementing a custom retriever function, which defines the logic for searching and retrieving documents from a vector database.

Follow the instructions in the [Qdrant Setup Guide](https://qdrant.tech/documentation/guides/installation/) to install Qdrant locally. Here is a guide to get API keys: [Qdrant API Keys](https://qdrant.tech/documentation/cloud/authentication/).

### Example: Custom Retriever for `Knowledge`

Below is a detailed example of how to implement a custom retriever function using the `agno` library. This example demonstrates how to set up a knowledge base with PDF documents, define a custom retriever, and use it with an agent.

```python  theme={null}
from typing import Optional
from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant
from qdrant_client import QdrantClient

---

## Content Team

**URL:** llms-txt#content-team

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/teams/content_team

Build a content creation team with specialized researchers and writers that collaborate to produce high-quality articles. This example demonstrates team coordination where agents work together on complex tasks.

By building this team, you'll understand:

* How to create specialized agents with distinct roles and capabilities
* How teams coordinate multiple agents using the coordinate mode
* How to combine web search tools with writing expertise for content creation
* How to structure team instructions for collaborative workflows

Build content creation platforms, automated journalism systems, research report generators, or marketing content pipelines.

The team uses coordinate mode to manage collaboration between specialized agents:

1. **Research**: Researcher agent searches the web using DuckDuckGo for relevant information
2. **Coordinate**: Team leader coordinates task distribution between members
3. **Write**: Writer agent crafts clear, engaging content based on research findings
4. **Integrate**: Team combines research and writing into cohesive articles

The team leader manages the workflow and ensures both agents contribute their expertise.

```python content_team.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Initialize PgVector

**URL:** llms-txt#initialize-pgvector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

---

## Chat History in Agents

**URL:** llms-txt#chat-history-in-agents

**Contents:**
- Agent-Level History
- History Reference
  - Add history to the agent context
- Read the chat history
- Search the session history
- Developer Resources

Source: https://docs.agno.com/basics/chat-history/agent/overview

Learn about managing chat history in agents.

Agents with storage enabled automatically have access to the run history of the session (also called the "conversation history" or "chat history").

<Note>
  For all forms of session history, you need to have a database assigned to the agent. See [Storage](/basics/database/overview) for more details.
</Note>

We can give the Agent access to the chat history in the following ways:

## Agent-Level History

* You can set `add_history_to_context=True` and `num_history_runs=5` to add the inputs and responses from the last 5 runs automatically to every request sent to the agent.
* You can be more granular about how many messages to add to include in the list sent to the model, by setting `num_history_messages`.
* You can set `read_chat_history=True` to provide a `get_chat_history()` tool to your agent allowing it to read any message in the entire chat history.
* You can set `read_tool_call_history=True` to provide a `get_tool_call_history()` tool to your agent allowing it to read tool calls in reverse chronological order.
* You can enable `search_session_history` to allow searching through previous sessions.

<Tip>
  Working with agent history can be tricky. Experiment with the above settings to find the best fit for your use case.
  See the [History Reference](#history-reference) for help on how to use the different history features.
</Tip>

<Tabs>
  <Tab title="Simple History">
    Start with **Agent History in Context** for basic conversation continuity:

<Tab title="Long Conversations">
    Add **Chat History Tool** when agents need to search history:

<Tab title="Multi-Session Memory">
    Enable **Multi-Session Search** for cross-session continuity:

<Note>
  **Database Requirement**: All history features require a database configured on the agent. See [Storage](/basics/database/overview) for setup.
</Note>

<Tip>
  **Performance Tip**: More history = larger context = slower and costlier requests. Start with `num_history_runs=3` and increase only if needed.
</Tip>

### Add history to the agent context

To add the history of the conversation to the context, you can set `add_history_to_context=True`.
This will add the inputs and responses from the last 3 runs (that is the default) to the context of the agent.
You can change the number of runs by setting `num_history_runs=n` where `n` is the number of runs to include.

You can either set `add_history_to_context=True` on the `Agent` or on the `run()` method directly.
See the [Persistent Session with History](/basics/chat-history/agent/usage/persistent-session-history) example for a complete implementation.

Learn more in the [Context Engineering](/basics/context/overview) documentation.

## Read the chat history

To read the chat history, you can set `read_chat_history=True`.
This will provide a `get_chat_history()` tool to your agent allowing it to read any message in the entire chat history.

See the [Chat History Management](/basics/chat-history/agent/usage/chat-history) page for a complete implementation.

## Search the session history

In some scenarios, you might want to fetch messages from across multiple sessions to provide context or continuity in conversations.

To enable fetching messages from the last N sessions, you need to use the following flags:

* `search_session_history`: Set this to `True` to allow searching through previous sessions.
* `num_history_sessions`: Specify the number of past sessions to include in the search. In the example below, it is set to `2` to include only the last 2 sessions.
* `num_history_sessions`: Specify the number of past sessions to include in the search.

<Tip>
  Keep `num_history_sessions` low (2 or 3) to avoid filling up the context length of the model, which can lead to performance issues.
</Tip>

## Developer Resources

<CardGroup cols={2}>
  <Card title="Chat History Example" icon="message" iconType="duotone" href="/basics/chat-history/agent/usage/chat-history">
    Learn how to manage and retrieve chat history in agent conversations.
  </Card>

<Card title="Persistent Session with History" icon="database" iconType="duotone" href="/basics/chat-history/agent/usage/persistent-session-history">
    Control how much conversation history is included using num\_history\_runs.
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Long Conversations">
    Add **Chat History Tool** when agents need to search history:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Multi-Session Memory">
    Enable **Multi-Session Search** for cross-session continuity:
```

---

## Input as Dictionary

**URL:** llms-txt#input-as-dictionary

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/input-as-dict

This example shows how to pass input to a team as a dictionary format, useful for multimodal inputs or structured data.

```python cookbook/examples/teams/basic/input_as_dict.py theme={null}
from agno.agent import Agent
from agno.team import Team

---

## Define the two distinct pipelines

**URL:** llms-txt#define-the-two-distinct-pipelines

image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[generate_image_step, describe_image_step],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[generate_video_step, describe_video_step],
)

def media_sequence_selector(step_input: StepInput) -> List[Step]:
    """
    Simple pipeline selector based on keywords in the message.

Args:
        step_input: StepInput containing message

Returns:
        List of Steps to execute
    """

# Check if message exists and is a string
    if not step_input.input or not isinstance(step_input.input, str):
        return [image_sequence]  # Default to image sequence

# Convert message to lowercase for case-insensitive matching
    message_lower = step_input.input.lower()

# Check for video keywords
    if "video" in message_lower:
        return [video_sequence]
    # Check for image keywords
    elif "image" in message_lower:
        return [image_sequence]
    else:
        # Default to image for any other case
        return [image_sequence]

---

## with optional fields

**URL:** llms-txt#with-optional-fields

optional_agent = Agent(
    name="Hackernews Agent with Optional Fields",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools()],
    input_schema=ResearchTopicWithOptionals,
)

print("\n=== Testing TypedDict with Optional Fields ===")
optional_agent.print_response(
    input={
        "topic": "Blockchain",
        "focus_areas": ["DeFi", "NFTs"],
        "target_audience": "Investors",
        # sources_required is optional, omitting it
        "priority": "high",
    }
)

---

## Download the .cursorrules file

**URL:** llms-txt#download-the-.cursorrules-file

curl -o .cursorrules https://raw.githubusercontent.com/agno-agi/agno/main/.cursorrules

---

## Async Filtering

**URL:** llms-txt#async-filtering

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/async-filtering

```python  theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## Add content to the knowledge

**URL:** llms-txt#add-content-to-the-knowledge

**Contents:**
- Usage

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(model=V0(id="v0-1.0-md"), knowledge=knowledge)
agent.print_response("How to make Thai curry?", markdown=True)
bash  theme={null}
    export V0_API_KEY=xxx
    bash  theme={null}
    pip install -U ddgs sqlalchemy pgvector pypdf openai agno
    bash  theme={null}
    docker run -d \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -e PGDATA=/var/lib/postgresql/data/pgdata \
      -v pgvolume:/var/lib/postgresql/data \
      -p 5532:5432 \
      --name pgvector \
      agnohq/pgvector:16
    bash Mac theme={null}
      python cookbook/models/vercel/knowledge.py
      bash Windows theme={null}
      python cookbook/models/vercel/knowledge.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run PgVector">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Parallel MCP agent

**URL:** llms-txt#parallel-mcp-agent

Source: https://docs.agno.com/basics/tools/mcp/usage/parallel

Using the [Parallel MCP server](https://docs.parallel.ai/integrations/mcp/search-mcp) to create an Agent that can search the web using Parallel's AI-optimized search capabilities:

---

## Advanced Imagen Tool with Vertex AI

**URL:** llms-txt#advanced-imagen-tool-with-vertex-ai

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/google/usage/imagen-tool-advanced

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set up Vertex AI authentication">
    Follow the [authentication guide](https://cloud.google.com/sdk/docs/initializing) to set up Vertex AI credentials.

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up Vertex AI authentication">
    Follow the [authentication guide](https://cloud.google.com/sdk/docs/initializing) to set up Vertex AI credentials.
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your API keys">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Video Generation Tools

**URL:** llms-txt#video-generation-tools

**Contents:**
- Prerequisites
- Developer Resources

Source: https://docs.agno.com/basics/multimodal/video/video_generation

Learn how to use video generation tools with Agno agents.

The following example demonstrates how to generate a video using `FalTools` with an agent. See [FAL](https://fal.ai/video) for more details.

The following example requires the `fal_client` library and an API key which can be obtained from [Fal](https://fal.ai/).

## Developer Resources

* View a [Replicate](/basics/multimodal/video/usage/generate-video-replicate) example.
* View a [Fal](/examples/basics/tools/others/fal) example.
* View a [ModelsLabs](/basics/multimodal/video/usage/generate-video-models-lab) example.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

---

## Dynamic Instructions via Function

**URL:** llms-txt#dynamic-instructions-via-function

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/context/agent/usage/instructions-via-function

This example demonstrates how to provide instructions to an agent via a function that can access the agent's properties, enabling dynamic and personalized instruction generation.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## This won't use the session history, but instead will use the memory tools to get the memories

**URL:** llms-txt#this-won't-use-the-session-history,-but-instead-will-use-the-memory-tools-to-get-the-memories

agent.print_response("What have you remembered about me?", stream=True, user_id="john_doe@example.com")
python  theme={null}
from agno.tools.memory import MemoryTools

memory_tools = MemoryTools(
    db=my_database,
    enable_think=True,            # Enable the think tool (true by default)
    enable_get_memories=True,     # Enable the get_memories tool (true by default)
    enable_add_memory=True,       # Enable the add_memory tool (true by default)
    enable_update_memory=True,    # Enable the update_memory tool (true by default)
    enable_delete_memory=True,    # Enable the delete_memory tool (true by default)
    enable_analyze=True,          # Enable the analyze tool (true by default)
    add_instructions=True,        # Add default instructions
    instructions=None,            # Optional custom instructions
    add_few_shot=True,           # Add few-shot examples
    few_shot_examples=None,      # Optional custom few-shot examples
)
```

**Examples:**

Example 1 (unknown):
```unknown
Here is how you can configure the toolkit:
```

---

## ************* Setup Knowledge Databases *************

**URL:** llms-txt#*************-setup-knowledge-databases-*************

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
documents_db = PostgresDb(
    db_url,
    id="agno_knowledge_db",
    knowledge_table="agno_knowledge_contents",
)
faq_db = PostgresDb(
    db_url,
    id="agno_faq_db",
    knowledge_table="agno_faq_contents",
)

---

## AG-UI

**URL:** llms-txt#ag-ui

**Contents:**
- Example usage
- Custom Events
- Core Components
- `AGUI` interface
  - Initialization Parameters
  - Key Method
- Endpoints
- Serving AgentOS
  - Parameters

Source: https://docs.agno.com/agent-os/interfaces/ag-ui/introduction

Expose Agno agents via the AG-UI protocol

AG-UI, the [Agent-User Interaction Protocol](https://github.com/ag-ui-protocol/ag-ui), standardizes how AI agents connect to frontend applications.

<Note>
  **Migration from Apps**: For migration from `AGUIApp`, see the [v2 migration guide](/how-to/v2-migration#7-apps-interfaces) for complete steps.
</Note>

<Steps>
  <Step title="Install backend dependencies">
    
  </Step>

<Step title="Run the backend">
    Expose an Agno agent through the AG-UI interface using `AgentOS` and `AGUI`.

<Step title="Run the frontend">
    Use Dojo (`ag-ui`'s frontend) as an advanced, customizable interface for AG-UI agents.

1. Clone: `git clone https://github.com/ag-ui-protocol/ag-ui.git`
    2. Install dependencies in `/ag-ui/typescript-sdk`: `pnpm install`
    3. Build the Agno package in `/ag-ui/integrations/agno`: `pnpm run build`
    4. Start Dojo following the instructions in the repository.
  </Step>

<Step title="Chat with the Agno Agent">
    With Dojo running, open `http://localhost:3000` and select the Agno agent.
  </Step>
</Steps>

Additional examples are available in the [cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agent_os/interfaces/agui/).

Custom events created in tools are automatically delivered to AG-UI in the AG-UI custom event format.

**Creating custom events:**

**Yielding from tools:**

Custom events are streamed in real-time to the AG-UI frontend.

See [Custom Events documentation](/basics/agents/running-agents#custom-events) for more details.

* `AGUI` (interface): Wraps an Agno `Agent` or `Team` into an AG-UI compatible FastAPI router.
* `AgentOS.serve`: Serves the FastAPI app (including the AGUI router) with Uvicorn.

`AGUI` mounts protocol-compliant routes on the app.

Main entry point for AG-UI exposure.

### Initialization Parameters

| Parameter | Type              | Default | Description            |
| --------- | ----------------- | ------- | ---------------------- |
| `agent`   | `Optional[Agent]` | `None`  | Agno `Agent` instance. |
| `team`    | `Optional[Team]`  | `None`  | Agno `Team` instance.  |

Provide `agent` or `team`.

| Method       | Parameters               | Return Type | Description                                              |
| ------------ | ------------------------ | ----------- | -------------------------------------------------------- |
| `get_router` | `use_async: bool = True` | `APIRouter` | Returns the AG-UI FastAPI router and attaches endpoints. |

Mounted at the interface's route prefix (root by default):

* `POST /agui`: Main entrypoint. Accepts `RunAgentInput` from `ag-ui-protocol`. Streams AG-UI events.
* `GET /status`: Health/status endpoint for the interface.

Refer to `ag-ui-protocol` docs for payload details.

Use `AgentOS.serve` to run the app with Uvicorn.

| Parameter | Type                  | Default       | Description                            |
| --------- | --------------------- | ------------- | -------------------------------------- |
| `app`     | `Union[str, FastAPI]` | required      | FastAPI app instance or import string. |
| `host`    | `str`                 | `"localhost"` | Host to bind.                          |
| `port`    | `int`                 | `7777`        | Port to bind.                          |
| `reload`  | `bool`                | `False`       | Enable auto-reload for development.    |

See [cookbook examples](https://github.com/agno-agi/agno/tree/main/cookbook/agent_os/interfaces/agui/) for updated interface patterns.

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Run the backend">
    Expose an Agno agent through the AG-UI interface using `AgentOS` and `AGUI`.
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the frontend">
    Use Dojo (`ag-ui`'s frontend) as an advanced, customizable interface for AG-UI agents.

    1. Clone: `git clone https://github.com/ag-ui-protocol/ag-ui.git`
    2. Install dependencies in `/ag-ui/typescript-sdk`: `pnpm install`
    3. Build the Agno package in `/ag-ui/integrations/agno`: `pnpm run build`
    4. Start Dojo following the instructions in the repository.
  </Step>

  <Step title="Chat with the Agno Agent">
    With Dojo running, open `http://localhost:3000` and select the Agno agent.
  </Step>
</Steps>

Additional examples are available in the [cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agent_os/interfaces/agui/).

## Custom Events

Custom events created in tools are automatically delivered to AG-UI in the AG-UI custom event format.

**Creating custom events:**
```

Example 3 (unknown):
```unknown
**Yielding from tools:**
```

---

## Configuration for the Metrics page

**URL:** llms-txt#configuration-for-the-metrics-page

metrics:
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        display_name: <DISPLAY_NAME>
    ...
```

---

## Secondary knowledge base for cross-validation

**URL:** llms-txt#secondary-knowledge-base-for-cross-validation

validation_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_validation",
        uri="tmp/lancedb",
        search_type=SearchType.vector,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## Employee Recruiter

**URL:** llms-txt#employee-recruiter

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/workflows/employee-recruiter

Build an automated employee recruitment workflow that handles resume screening, interview scheduling, and email communication. This multi-agent system processes PDF resumes, evaluates candidates, and coordinates the hiring pipeline.

By building this workflow, you'll understand:

* How to create multi-agent systems for complex business processes
* How to process PDF resumes and extract candidate information
* How to implement conditional logic based on screening scores
* How to coordinate asynchronous streaming for real-time feedback

Build automated recruitment systems, candidate screening platforms, interview scheduling tools, or HR automation workflows.

The workflow automates the complete hiring pipeline through specialized agents:

1. **Screen**: Screening agent analyzes resumes against job requirements and assigns scores
2. **Schedule**: If candidate scores >= 5.0, scheduler agent creates interview appointments
3. **Notify**: Email writer crafts professional invitation and sender delivers it
4. **Cache**: Resume content is cached in session state to avoid re-processing

The workflow uses simulated Zoom scheduling and email tools for demonstration (replace with real tools in production).

```python employee_recruiter.py theme={null}

import asyncio
import io
import random
from datetime import datetime, timedelta
from typing import Any, List

import requests
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel
from pypdf import PdfReader

---

## Basic Agent Tracing

**URL:** llms-txt#basic-agent-tracing

**Contents:**
- Developer Resources

Source: https://docs.agno.com/basics/tracing/usage/basic-agent-tracing

Learn how to trace your agents with Agno

This example shows how to enable tracing for an agent. Once tracing is enabled, all agent runs, model calls, and tool executions are automatically captured and stored in your database.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

## Developer Resources

* View the [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/integrations/observability/trace_to_database.py) for a full example that visualizes the trace tree in CLI.

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Agent Memory

**URL:** llms-txt#agent-memory

**Contents:**
- User Memories

Source: https://docs.agno.com/basics/memory/agent/overview

Memory gives an Agent the ability to recall information about the user.

Memory is a part of the Agent's context that helps it provide the best, most personalized response.

<Tip>
  If the user tells the Agent they like to ski, then future responses can reference this information to provide a more personalized experience.
</Tip>

Here's a simple example of using Memory in an Agent.

```python memory_demo.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.db.postgres import PostgresDb
from rich.pretty import pprint

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(
  db_url=db_url,
  memory_table="user_memories",  # Optionally specify a table name for the memories
)

---

## Search for posts

**URL:** llms-txt#search-for-posts

agent.print_response("Search for recent posts about AI agents", markdown=True)

---

## Postgres for Agent

**URL:** llms-txt#postgres-for-agent

**Contents:**
- Usage
  - Run PgVector
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/postgres/usage/postgres-for-agent

Agno supports using PostgreSQL as a storage backend for Agents using the `PostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

<Snippet file="db-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/postgres/postgres_for_agent.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Optimize

**URL:** llms-txt#optimize

optimized = memory_manager.optimize_memories(
    user_id="user_123",
    strategy=strategy,
    apply=True,
)

---

## Multi Language Team

**URL:** llms-txt#multi-language-team

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code
- What to Expect
- Usage
- Next Steps

Source: https://docs.agno.com/examples/use-cases/teams/multi_language_team

Build a language routing team that automatically detects user language and responds with native speakers. This example demonstrates intelligent routing across multiple AI models specialized for different languages.

By building this team, you'll understand:

* How to create language-specific agents using different AI models
* How to implement intelligent routing based on user input language
* How to use multiple AI providers (OpenAI, Claude, Mistral, DeepSeek) in one team
* How to handle unsupported languages with graceful fallback responses

Build multilingual customer support, international chatbots, translation services, or global help desk systems.

The team uses route mode to direct queries to language-specific agents:

1. **Detect**: Team leader analyzes the language of the user's question
2. **Route**: Directs the query to the appropriate language-specialized agent
3. **Respond**: Language agent answers in the user's native language
4. **Fallback**: Handles unsupported languages with English response

Each language agent uses a different AI model optimized for that language's characteristics.

The team will automatically detect the language of your input and route it to the appropriate language-specific agent. Each agent responds fluently in their designated language using models optimized for that language.

The output shows which agent handled each request. For unsupported languages, you'll receive a polite message in English listing available languages.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* Add more language agents for additional language support
* Modify the routing logic in `instructions` to handle dialects or regional variations
* Experiment with different AI models for each language to optimize quality
* Explore [Teams](/basics/teams/overview) for advanced request distribution

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The team will automatically detect the language of your input and route it to the appropriate language-specific agent. Each agent responds fluently in their designated language using models optimized for that language.

The output shows which agent handled each request. For unsupported languages, you'll receive a polite message in English listing available languages.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Team">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Remote mode (for production deployments)

**URL:** llms-txt#remote-mode-(for-production-deployments)

**Contents:**
- Params
- Developer Resources

knowledge_remote = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="vllm_embeddings_remote",
        embedder=VLLMEmbedder(
            id="intfloat/e5-mistral-7b-instruct",
            dimensions=4096,
            base_url="http://localhost:8000/v1",  # Example endpoint for local development
            api_key="your-api-key",  # Optional
        ),
    ),
    max_results=2,
)
```

| Parameter        | Type                       | Default                             | Description                                    |
| ---------------- | -------------------------- | ----------------------------------- | ---------------------------------------------- |
| `id`             | `str`                      | `"intfloat/e5-mistral-7b-instruct"` | Model identifier (HuggingFace model name)      |
| `dimensions`     | `int`                      | `4096`                              | Embedding vector dimensions                    |
| `base_url`       | `Optional[str]`            | `None`                              | Remote vLLM server URL (enables remote mode)   |
| `api_key`        | `Optional[str]`            | `getenv("VLLM_API_KEY")`            | API key for remote server authentication       |
| `enable_batch`   | `bool`                     | `False`                             | Enable batch processing for multiple texts     |
| `batch_size`     | `int`                      | `10`                                | Number of texts to process per batch           |
| `enforce_eager`  | `bool`                     | `True`                              | Use eager execution mode (local mode)          |
| `vllm_kwargs`    | `Optional[Dict[str, Any]]` | `None`                              | Additional vLLM engine parameters (local mode) |
| `request_params` | `Optional[Dict[str, Any]]` | `None`                              | Additional request parameters (remote mode)    |
| `client_params`  | `Optional[Dict[str, Any]]` | `None`                              | OpenAI client configuration (remote mode)      |

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/embedders/vllm_embedder.py)

---

## Technology trends

**URL:** llms-txt#technology-trends

tech_prompt = """\
Analyze media trends for:
Keywords: artificial intelligence, machine learning, automation
Sources: techcrunch.com, arstechnica.com, wired.com
"""

---

## print("John Doe's memories:")

**URL:** llms-txt#print("john-doe's-memories:")

---

## Async SQLite

**URL:** llms-txt#async-sqlite

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/async-sqlite/overview

Learn to use SQLite asynchronously as a database for your Agents

Agno supports using [Sqlite](https://www.sqlite.org) asynchronously, with the `AsyncSqliteDb` class.

```python async_sqlite_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import AsyncSqliteDb

---

## Get Session Runs

**URL:** llms-txt#get-session-runs

Source: https://docs.agno.com/reference-api/schema/sessions/get-session-runs

get /sessions/{session_id}/runs
Retrieve all runs (executions) for a specific session with optional timestamp filtering. Runs represent individual interactions or executions within a session. Response schema varies based on session type.

---

## Clone the repo and navigate to the demo folder

**URL:** llms-txt#clone-the-repo-and-navigate-to-the-demo-folder

git clone https://github.com/agno-agi/agno.git
cd agno/cookbook/tools/mcp/mcp_toolbox_demo

---

## Human in the Loop

**URL:** llms-txt#human-in-the-loop

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/12-human-in-the-loop

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:

* Add pre-hooks to tools for user confirmation
* Handle user input during tool execution
* Gracefully cancel operations based on user choice

Some practical applications:

* Confirming sensitive operations before execution
* Reviewing API calls before they're made
* Validating data transformations
* Approving automated actions in critical systems

```python human_in_the_loop.py theme={null}
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

Args:
        num_stories (int): Number of stories to retrieve

Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

# Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)

---

## Custom FastAPI App with JWT Middleware

**URL:** llms-txt#custom-fastapi-app-with-jwt-middleware

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/middleware/custom-fastapi-jwt

Custom FastAPI application with JWT middleware for authentication and AgentOS integration

This example demonstrates how to integrate JWT middleware with your custom FastAPI application and then add AgentOS functionality on top.

```python custom_fastapi_jwt.py theme={null}
from datetime import datetime, timedelta, UTC

import jwt
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.middleware import JWTMiddleware
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import FastAPI, Form, HTTPException

---

## Create knowledge search agent with filter awareness

**URL:** llms-txt#create-knowledge-search-agent-with-filter-awareness

web_agent = Agent(
    name="Knowledge Search Agent",
    role="Handle knowledge search",
    knowledge=knowledge,
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=["Always take into account filters"],
)

---

## Initialize the Agent with the knowledge base and filters

**URL:** llms-txt#initialize-the-agent-with-the-knowledge-base-and-filters

**Contents:**
- Usage

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

agent.print_response(
    "Tell me about Jordan Mitchell's experience and skills",
    knowledge_filters={"user_id": "jordan_mitchell"},
    markdown=True,
)

bash  theme={null}
    pip install -U agno surrealdb openai
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root     
    bash Mac theme={null}
      python cookbook/knowledge/filters/vector_dbs/filtering_surrealdb.py
      bash Windows theme={null}
      python cookbook/knowledge/filters/vector_dbs/filtering_surrealdb.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run SurrealDB">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the example">
    <CodeGroup>
```

---

## OpenAI Key Request While Using Other Models

**URL:** llms-txt#openai-key-request-while-using-other-models

**Contents:**
- Quick fix: Configure a Different Model

Source: https://docs.agno.com/faq/openai-key-request-for-other-models

If you see a request for an OpenAI API key but haven't explicitly configured OpenAI, it's because Agno uses OpenAI models by default in several places, including:

* The default model when unspecified in `Agent`
* The default embedder is OpenAIEmbedder with VectorDBs, unless specified

## Quick fix: Configure a Different Model

It is best to specify the model for the agent explicitly, otherwise it would default to `OpenAIChat`.

For example, to use Google's Gemini instead of OpenAI:

```python  theme={null}
from agno.agent import Agent, RunOutput
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-1.5-flash"),
    markdown=True,
)

---

## ag infra up

**URL:** llms-txt#ag-infra-up

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/up

Create resources for the active workspace

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

<ResponseField name="pull" type="bool">
  Pull `--pull` `-p`
</ResponseField>

---

## Travel Agent

**URL:** llms-txt#travel-agent

Source: https://docs.agno.com/examples/use-cases/agents/travel-planner

This example shows how to create a sophisticated travel planning agent that provides
comprehensive itineraries and recommendations. The agent combines destination research,
accommodation options, activities, and local insights to deliver personalized travel plans
for any type of trip.

Example prompts to try:

* "Plan a 5-day cultural exploration trip to Kyoto for a family of 4"
* "Create a romantic weekend getaway in Paris with a \$2000 budget"
* "Organize a 7-day adventure trip to New Zealand for solo travel"
* "Design a tech company offsite in Barcelona for 20 people"
* "Plan a luxury honeymoon in Maldives for 10 days"

```python travel_planner.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

travel_agent = Agent(
    name="Globe Hopper",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[ExaTools()],
    markdown=True,
    description=dedent("""\
        You are Globe Hopper, an elite travel planning expert with decades of experience! ðŸŒ

Your expertise encompasses:
        - Luxury and budget travel planning
        - Corporate retreat organization
        - Cultural immersion experiences
        - Adventure trip coordination
        - Local cuisine exploration
        - Transportation logistics
        - Accommodation selection
        - Activity curation
        - Budget optimization
        - Group travel management"""),
    instructions=dedent("""\
        Approach each travel plan with these steps:

1. Initial Assessment ðŸŽ¯
           - Understand group size and dynamics
           - Note specific dates and duration
           - Consider budget constraints
           - Identify special requirements
           - Account for seasonal factors

2. Destination Research ðŸ”
           - Use Exa to find current information
           - Verify operating hours and availability
           - Check local events and festivals
           - Research weather patterns
           - Identify potential challenges

3. Accommodation Planning ðŸ¨
           - Select locations near key activities
           - Consider group size and preferences
           - Verify amenities and facilities
           - Include backup options
           - Check cancellation policies

4. Activity Curation ðŸŽ¨
           - Balance various interests
           - Include local experiences
           - Consider travel time between venues
           - Add flexible backup options
           - Note booking requirements

5. Logistics Planning ðŸš—
           - Detail transportation options
           - Include transfer times
           - Add local transport tips
           - Consider accessibility
           - Plan for contingencies

6. Budget Breakdown ðŸ’°
           - Itemize major expenses
           - Include estimated costs
           - Add budget-saving tips
           - Note potential hidden costs
           - Suggest money-saving alternatives

Presentation Style:
        - Use clear markdown formatting
        - Present day-by-day itinerary
        - Include maps when relevant
        - Add time estimates for activities
        - Use emojis for better visualization
        - Highlight must-do activities
        - Note advance booking requirements
        - Include local tips and cultural notes"""),
    expected_output=dedent("""\
        # {Destination} Travel Itinerary ðŸŒŽ

## Overview
        - **Dates**: {dates}
        - **Group Size**: {size}
        - **Budget**: {budget}
        - **Trip Style**: {style}

## Accommodation ðŸ¨
        {Detailed accommodation options with pros and cons}

### Day 1
        {Detailed schedule with times and activities}

### Day 2
        {Detailed schedule with times and activities}

[Continue for each day...]

## Budget Breakdown ðŸ’°
        - Accommodation: {cost}
        - Activities: {cost}
        - Transportation: {cost}
        - Food & Drinks: {cost}
        - Miscellaneous: {cost}

## Important Notes â„¹ï¸
        {Key information and tips}

## Booking Requirements ðŸ“‹
        {What needs to be booked in advance}

## Local Tips ðŸ—ºï¸
        {Insider advice and cultural notes}

---
        Created by Globe Hopper
        Last Updated: {current_time}"""),
    add_datetime_to_context=True,
    )

---

## json_mode_response: RunOutput = json_mode_agent.run("New York")

**URL:** llms-txt#json_mode_response:-runoutput-=-json_mode_agent.run("new-york")

---

## Infra Name: social-intel

**URL:** llms-txt#infra-name:-social-intel

env  theme={null}
export OPENAI_API_KEY=sk-your-openai-api-key-here
export X_API_KEY=your-x-api-key
export X_API_SECRET=your-x-api-secret
export X_ACCESS_TOKEN=your-access-token
export X_ACCESS_TOKEN_SECRET=your-access-token-secret
export X_BEARER_TOKEN=your-bearer-token
export EXA_API_KEY=your-exa-api-key
```

Our environment is now ready. Let's start building!

**Examples:**

Example 1 (unknown):
```unknown
5. Set your environment variables:
```

---

## Performance on Agent Response

**URL:** llms-txt#performance-on-agent-response

Source: https://docs.agno.com/basics/evals/performance/usage/performance-simple-response

Example showing how to analyze the runtime and memory usage of an Agent's run, given its response.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Reasoning Agent with Knowledge Tools

**URL:** llms-txt#reasoning-agent-with-knowledge-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/knowledge-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print context management stats from the last message

**URL:** llms-txt#print-context-management-stats-from-the-last-message

**Contents:**
- Usage

if response and response.messages:
    for message in reversed(response.messages):
        if message.provider_data and "context_management" in message.provider_data:
            edits = message.provider_data["context_management"].get("applied_edits", [])
            if edits:
                print(
                    f"\nâœ… Saved: {edits[-1].get('cleared_input_tokens', 0):,} tokens"
                )
                print(f"   Cleared: {edits[-1].get('cleared_tool_uses', 0)} tool uses")
                break

print("\n" + "=" * 60)
bash  theme={null}
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U anthropic agno ddgs
    bash Mac theme={null}
      python anthropic_context_management.py
      bash Windows theme={null}
      python anthropic_context_management.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get AgentOS Metrics

**URL:** llms-txt#get-agentos-metrics

Source: https://docs.agno.com/reference-api/schema/metrics/get-agentos-metrics

get /metrics
Retrieve AgentOS metrics and analytics data for a specified date range. If no date range is specified, returns all available metrics.

---

## PgVector Async

**URL:** llms-txt#pgvector-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/pgvector/usage/async-pgvector-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="run-pgvector.mdx" />

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector.mdx" />

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Filter content during search

**URL:** llms-txt#filter-content-during-search

**Contents:**
- AgentOS Integration
  - Required Setup for AgentOS

results = knowledge.search(
    query="technical documentation",
    filters={"department": "engineering", "version": "2.1"}
)
python  theme={null}
from agno.os import AgentOS
from agno.db.postgres import PostgresDb
from agno.agent import Agent

**Examples:**

Example 1 (unknown):
```unknown
## AgentOS Integration

### Required Setup for AgentOS

When using AgentOS, ContentsDB is mandatory for the Knowledge management interface:
```

---

## Load knowledge base using vector search

**URL:** llms-txt#load-knowledge-base-using-vector-search

vector_db = PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.vector)
knowledge = Knowledge(
    name="Vector Search Knowledge Base",
    vector_db=vector_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

---

## Step 4: Create the complete agent

**URL:** llms-txt#step-4:-create-the-complete-agent

**Contents:**
  - 4e. Spin-up the infrastructure for our project:
  - 4f. Test Your Complete Agent
- Step 5: Test and experiment via AgentOS

social_media_agent = Agent(
    name="Social Media Intelligence Analyst",
    model=model,
    tools=tools,
    instructions=complete_instructions,
    markdown=True,
    show_tool_calls=True,
)

def analyze_brand_sentiment(query: str, tweet_count: int = 20):
    """Execute comprehensive social media intelligence analysis."""
    prompt = f"""
    Conduct comprehensive social media intelligence analysis for: "{query}"

ANALYSIS PARAMETERS:
    - Twitter Analysis: {tweet_count} most recent tweets with engagement metrics
    - Web Intelligence: Related articles, discussions, and broader context via Exa
    - Cross-Platform Synthesis: Correlate social sentiment with web discussions
    - Strategic Focus: Brand positioning, competitive analysis, risk assessment

METHODOLOGY:
    1. Gather direct social media mentions and engagement data
    2. Search for related web discussions and broader context
    3. Analyze sentiment patterns and engagement indicators
    4. Identify cross-platform themes and influence networks
    5. Generate strategic recommendations with evidence backing

Provide comprehensive intelligence report following the structured format.
    """

return social_media_agent.print_response(prompt, stream=True)

if __name__ == "__main__":
    # Test the complete agent
    analyze_brand_sentiment("Agno OR AgnoAGI", tweet_count=25)
bash  theme={null}
ag infra up
bash  theme={null}
python app/social_media_agent.py
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 4e. Spin-up the infrastructure for our project:

Now that we have completed our agent, we can spin-up the infrastructure for our project:
```

Example 2 (unknown):
```unknown
Your [AgentOS](/agent-os/introduction) API is now running. We are ready to start building!

### 4f. Test Your Complete Agent
```

Example 3 (unknown):
```unknown
You should see your agent:

1. **Use X Tools** to gather Twitter data with engagement metrics
2. **Use Exa Tools** to find broader web context
3. **Generate a structured report** following your defined format
4. **Provide strategic recommendations** based on the analysis

## Step 5: Test and experiment via AgentOS

**Why API-first?** The AgentOS infrastructure automatically exposes your agent as a REST API, making it ready for production integration without additional deployment work.

Your agent is automatically available via the AgentOS API. Let's test it!

**Find your API endpoint:**
```

---

## Async Cooperative Team

**URL:** llms-txt#async-cooperative-team

Source: https://docs.agno.com/basics/teams/usage/async-flows/delegate-to-all-members

This example demonstrates a collaborative team of AI agents working together asynchronously to research topics across different platforms.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/async_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Postgres for Team

**URL:** llms-txt#postgres-for-team

**Contents:**
- Usage
  - Run PgVector
- Params
- Developer Resources

Source: https://docs.agno.com/integrations/database/postgres/usage/postgres-for-team

Agno supports using PostgreSQL as a storage backend for Teams using the `PostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

<Snippet file="db-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/postgres/postgres_for_team.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## RAG with LanceDB and SQLite

**URL:** llms-txt#rag-with-lancedb-and-sqlite

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/rag-with-lance-db-and-sqlite

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.ollama import Ollama
from agno.vectordb.lancedb import LanceDb

---

## Check what's been processed and its status

**URL:** llms-txt#check-what's-been-processed-and-its-status

content_list, total_count = knowledge.get_content()
for content in content_list:
    status, message = knowledge.get_content_status(content.id)
    print(f"{content.name}: {status}")

---

## What is Agno?

**URL:** llms-txt#what-is-agno?

**Contents:**
- Example

Source: https://docs.agno.com/introduction

**Agno is an incredibly fast multi-agent framework, runtime and control plane.**

Companies want to build AI products, add AI to existing products and automate workflows with agents. Doing this well takes far more than calling an LLM API. It requires a thoughtfully designed agentic platform.

Agno provides the complete stack to build, run and manage multi-agent systems:

* **Framework**: Build agents, multi-agent teams and workflows with memory, knowledge, state, guardrails, HITL, context compression, MCP, A2A and 100+ toolkits.
* **AgentOS Runtime**: Run your multi-agent system in production with a secure, stateless runtime and ready to use integration endpoints.
* **AgentOS Control Plane**: Test, monitor and manage AgentOS deployments across environments with full operational visibility.

Hereâ€™s an example of an Agent that connects to an MCP server, manages conversation state in a database, is served using a FastAPI application that you can chat using the [AgentOS UI](https://os.agno.com).

```python agno_agent.py lines theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.mcp import MCPTools

---

## Groq

**URL:** llms-txt#groq

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/groq

The Groq model provides access to Groq's high-performance language models.

| Parameter               | Type            | Default                            | Description                                                      |
| ----------------------- | --------------- | ---------------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"llama-3.3-70b-versatile"`        | The id of the Groq model to use                                  |
| `name`                  | `str`           | `"Groq"`                           | The name of the model                                            |
| `provider`              | `str`           | `"Groq"`                           | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                             | The API key for Groq (defaults to GROQ\_API\_KEY env var)        |
| `base_url`              | `str`           | `"https://api.groq.com/openai/v1"` | The base URL for the Groq API                                    |
| `retries`               | `int`           | `0`                                | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                                | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                            | If True, the delay between retries is doubled each time          |

Groq extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Should use parallel_search

**URL:** llms-txt#should-use-parallel_search

agent.print_response(
    "Search for the latest information on 'AI agents and autonomous systems' and summarize the key findings"
)

---

## Metrics

**URL:** llms-txt#metrics

**Contents:**
- Learn more

Source: https://docs.agno.com/basics/sessions/metrics/overview

Learn about run and session metrics.

When you run an agent, team or workflow in Agno, the response you get includes detailed metrics about the run.
These metrics help you understand resource usage (like **token usage** and **duration**), performance, and other aspects of the model and tool calls.

<CardGroup cols={3}>
  <Card title="Agent Metrics" icon="robot" iconType="duotone" href="/basics/sessions/metrics/agent">
    Learn about agent run and session metrics.
  </Card>

<Card title="Team Metrics" icon="users" iconType="duotone" href="/basics/sessions/metrics/team">
    Learn about team run and session metrics.
  </Card>

<Card title="Workflow Metrics" icon="diagram-project" iconType="duotone" href="/basics/sessions/metrics/workflow">
    Learn about workflow sessions.
  </Card>
</CardGroup>

---

## Print the aggregated metrics for the whole session

**URL:** llms-txt#print-the-aggregated-metrics-for-the-whole-session

**Contents:**
- Developer Resources

print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.get_session_metrics().to_dict())
```

You'll see the outputs with following information:

* `input_tokens`: The number of tokens sent to the model.
* `output_tokens`: The number of tokens received from the model.
* `total_tokens`: The sum of `input_tokens` and `output_tokens`.
* `audio_input_tokens`: The number of tokens sent to the model for audio input.
* `audio_output_tokens`: The number of tokens received from the model for audio output.
* `audio_total_tokens`: The sum of `audio_input_tokens` and `audio_output_tokens`.
* `cache_read_tokens`: The number of tokens read from the cache.
* `cache_write_tokens`: The number of tokens written to the cache.
* `reasoning_tokens`: The number of tokens used for reasoning.
* `duration`: The duration of the run in seconds.
* `time_to_first_token`: The time taken until the first token was generated.
* `provider_metrics`: Any provider-specific metrics.

## Developer Resources

* View the [RunOutput schema](/reference/agents/run-response)
* View the [Metrics schema](/reference/agents/metrics)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/other/agent_metrics.py)

---

## In-Memory Storage for Teams

**URL:** llms-txt#in-memory-storage-for-teams

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/in-memory/usage/in-memory-for-team

Example using `InMemoryDb` with teams for multi-agent coordination.

```python  theme={null}
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

---

## Agent with Output Model

**URL:** llms-txt#agent-with-output-model

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/input-output/agent/usage/output-model

This example demonstrates how to use the output\_model parameter to specify a different model for generating the final response, enabling model switching during agent execution.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Knowledge base setup (same as synchronous example)

**URL:** llms-txt#knowledge-base-setup-(same-as-synchronous-example)

embedder = OpenAIEmbedder(id="text-embedding-3-small")
vector_db = Qdrant(collection="thai-recipes", url="http://localhost:6333", embedder=embedder)
knowledge_base = Knowledge(
    vector_db=vector_db,
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

---

## Later run with same session_id

**URL:** llms-txt#later-run-with-same-session_id

**Contents:**
- What Gets Stored
- Multi-User Sessions

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="conversation_123",  # Same ID
    add_history_to_context=True,     # Enable history
)
agent.run("What's my name?")  # Agent remembers "Alice"
python  theme={null}
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
)

**Examples:**

Example 1 (unknown):
```unknown
Need custom naming conventions, caching, or UI-friendly labels? Continue with the [Session Management](/basics/sessions/session-management) guide.

## What Gets Stored

When you configure a database, Agno automatically stores:

* âœ… **Messages** - User inputs and agent responses
* âœ… **Run metadata** - Timestamps, token usage, model info
* âœ… **Session state** - Custom key-value data
* âœ… **Tool calls** - Tool usage and results (optional)
* âœ… **Media** - Images, audio, files (optional)

See [Storage Control](/basics/sessions/persisting-sessions/storage-control) to customize what gets saved.

## Multi-User Sessions

Use `user_id` to track different users:
```

---

## agent.print_response(crypto_prompt, stream=True)

**URL:** llms-txt#agent.print_response(crypto_prompt,-stream=true)

**Contents:**
- Usage
- Customization Options
- Example Prompts

bash  theme={null}
    export OPENAI_API_KEY=xxx
    export EXA_API_KEY=xxx
    export FIRECRAWL_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai exa-py agno firecrawl
    bash Mac theme={null}
      python cookbook/examples/agents/media_trend_analysis_agent.py
      bash Windows theme={null}
      python cookbook/examples/agents/media_trend_analysis_agent.py
      python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Access Dependencies in Team Tool

**URL:** llms-txt#access-dependencies-in-team-tool

Source: https://docs.agno.com/basics/dependencies/team/usage/access-dependencies-in-tool

This example demonstrates how team tools can access dependencies passed to the team, allowing tools to utilize dynamic context like team metrics and current time information while team members collaborate with shared data sources.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Protected route

**URL:** llms-txt#protected-route

@app.get("/protected", dependencies=[Depends(verify_token)])
async def protected_endpoint():
    return {"message": "Access granted"}

---

## Create New Session

**URL:** llms-txt#create-new-session

Source: https://docs.agno.com/reference-api/schema/sessions/create-new-session

post /sessions
Create a new empty session with optional configuration. Useful for pre-creating sessions with specific session_state, metadata, or other properties before running any agent/team/workflow interactions. The session can later be used by providing its session_id in run requests.

---

## Hybrid knowledge base for comprehensive search

**URL:** llms-txt#hybrid-knowledge-base-for-comprehensive-search

hybrid_knowledge = Knowledge(
    vector_db=PgVector(
        table_name="recipes_hybrid",
        db_url=db_url,
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## Install dependencies and build

**URL:** llms-txt#install-dependencies-and-build

**Contents:**
  - 2. Install Python Dependencies
  - 3. Set Environment Variables
- Code Example
- Available Tools

npm install
npm run build
bash  theme={null}
pip install agno mcp openai
bash  theme={null}
export BROWSERBASE_API_KEY=your_browserbase_api_key
export BROWSERBASE_PROJECT_ID=your_browserbase_project_id
export OPENAI_API_KEY=your_openai_api_key
python  theme={null}
import asyncio
from os import environ
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters

async def run_agent(message: str) -> None:
    server_params = StdioServerParameters(
        command="node",
        # Update this path to the location where you cloned the repository
        args=["mcp-server-browserbase/stagehand/dist/index.js"],
        env=environ.copy(),
    )

async with MCPTools(server_params=server_params, timeout_seconds=60) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-5-mini"),
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a web scraping assistant that creates concise reader's digests from Hacker News.

CRITICAL INITIALIZATION RULES - FOLLOW EXACTLY:
                1. NEVER use screenshot tool until AFTER successful navigation
                2. ALWAYS start with stagehand_navigate first
                3. Wait for navigation success message before any other actions
                4. If you see initialization errors, restart with navigation only
                5. Use stagehand_observe and stagehand_extract to explore pages safely

Available tools and safe usage order:
                - stagehand_navigate: Use FIRST to initialize browser
                - stagehand_extract: Use to extract structured data from pages
                - stagehand_observe: Use to find elements and understand page structure
                - stagehand_act: Use to click links and navigate to comments
                - screenshot: Use ONLY after navigation succeeds and page loads

Your goal is to create a comprehensive but concise digest that includes:
                - Top headlines with brief summaries
                - Key themes and trends
                - Notable comments and insights
                - Overall tech news landscape overview

Be methodical, extract structured data, and provide valuable insights.
            """),
            markdown=True,
                    )
        await agent.aprint_response(message, stream=True)

if __name__ == "__main__":
    asyncio.run(
        run_agent(
            "Create a comprehensive Hacker News Reader's Digest from https://news.ycombinator.com"
        )
    )
```

The Stagehand MCP server provides several tools for web automation:

| Tool                 | Purpose                                     | Usage Notes                            |
| -------------------- | ------------------------------------------- | -------------------------------------- |
| `stagehand_navigate` | Navigate to web pages                       | **Use first** for initialization       |
| `stagehand_extract`  | Extract structured data                     | Safe for content extraction            |
| `stagehand_observe`  | Find elements and understand page structure | Good for exploration                   |
| `stagehand_act`      | Interact with page elements                 | Click, type, scroll actions            |
| `screenshot`         | Take screenshots                            | **Use only after** navigation succeeds |

**Examples:**

Example 1 (unknown):
```unknown
### 2. Install Python Dependencies
```

Example 2 (unknown):
```unknown
### 3. Set Environment Variables
```

Example 3 (unknown):
```unknown
## Code Example
```

---

## Deep Knowledge Agent

**URL:** llms-txt#deep-knowledge-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/groq/usage/deep-knowledge

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Automatic reader selection based on file extension

**URL:** llms-txt#automatic-reader-selection-based-on-file-extension

reader = ReaderFactory.get_reader_for_extension(".pdf")  # Returns PDFReader
reader = ReaderFactory.get_reader_for_extension(".csv")  # Returns CSVReader

---

## Cassandra Async

**URL:** llms-txt#cassandra-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/cassandra/usage/async-cassandra-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Cassandra">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Cassandra">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## STREAMING EXAMPLES

**URL:** llms-txt#streaming-examples

---

## Initialize the ZepTools

**URL:** llms-txt#initialize-the-zeptools

zep_tools = ZepTools(user_id="agno", session_id="agno-session", add_instructions=True)

---

## pprint(response.content)

**URL:** llms-txt#pprint(response.content)

**Contents:**
- Usage

agent.print_response("New York")
bash  theme={null}
    export DEEPINFRA_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/deepinfra/json_output.py
      bash Windows theme={null}
      python cookbook/models/deepinfra/json_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Team with Knowledge Base

**URL:** llms-txt#team-with-knowledge-base

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/team-with-knowledge

This example demonstrates how to create a team with knowledge base integration. The team has access to a knowledge base with Agno documentation and can combine this knowledge with web search capabilities.

```python cookbook/examples/teams/knowledge/01_team_with_knowledge.py theme={null}
"""
This example demonstrates how to create a team with knowledge base integration.

The team has access to a knowledge base with Agno documentation and can combine
this knowledge with web search capabilities.
"""

from pathlib import Path

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.lancedb import LanceDb, SearchType

---

## Get Agent Details

**URL:** llms-txt#get-agent-details

Source: https://docs.agno.com/reference-api/schema/agents/get-agent-details

get /agents/{agent_id}
Retrieve detailed configuration and capabilities of a specific agent.

**Returns comprehensive agent information including:**
- Model configuration and provider details
- Complete tool inventory and configurations
- Session management settings
- Knowledge base and memory configurations
- Reasoning capabilities and settings
- System prompts and response formatting options

---

## Deep Research Agent

**URL:** llms-txt#deep-research-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/deep-research-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Database connection

**URL:** llms-txt#database-connection

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

@tool(requires_confirmation=True)
def delete_records(table_name: str, count: int) -> str:
    """Delete records from a database table.

Args:
        table_name: Name of the table
        count: Number of records to delete

Returns:
        str: Confirmation message
    """
    return f"Deleted {count} records from {table_name}"

@tool(requires_confirmation=True)
def send_notification(recipient: str, message: str) -> str:
    """Send a notification to a user.

Args:
        recipient: Email or username of the recipient
        message: Notification message

Returns:
        str: Confirmation message
    """
    return f"Sent notification to {recipient}: {message}"

---

## input=ResearchTopic(

**URL:** llms-txt#input=researchtopic(

---

## Parallel Steps Workflow

**URL:** llms-txt#parallel-steps-workflow

Source: https://docs.agno.com/basics/workflows/usage/parallel-steps-workflow

This example demonstrates **Workflows 2.0** parallel execution for independent tasks that can run simultaneously. Shows how to optimize workflow performance by executing non-dependent steps in parallel, significantly reducing total execution time.

This example demonstrates **Workflows 2.0** parallel execution for independent tasks that
can run simultaneously. Shows how to optimize workflow performance by executing
non-dependent steps in parallel, significantly reducing total execution time.

**When to use**: When you have independent tasks that don't depend on each other's output
but can contribute to the same final goal. Ideal for research from multiple sources,
parallel data processing, or any scenario where tasks can run simultaneously.

```python parallel_steps_workflow.py theme={null}
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.workflow.parallel import Parallel

---

## Agno will automatically detect and use these loggers

**URL:** llms-txt#agno-will-automatically-detect-and-use-these-loggers

agent = Agent()
agent.print_response("Hello from agent!")  # Agent logs will go to agent.log

team = Team(members=[agent])
team.print_response("Hello from team!")  # Team logs will go to team.log

---

## Example 1: Using IN operator

**URL:** llms-txt#example-1:-using-in-operator

print("Using IN operator")
sales_agent.print_response(
    "Describe revenue performance for the region",
    knowledge_filters=[IN("region", ["north_america"])],
    markdown=True,
)

---

## WhatsApp Reasoning Finance Agent

**URL:** llms-txt#whatsapp-reasoning-finance-agent

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/whatsapp/reasoning-agent

WhatsApp agent with advanced reasoning and financial analysis capabilities

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Advanced Reasoning**: ThinkingTools for step-by-step financial analysis
* **Real-time Data**: Stock prices, analyst recommendations, company news
* **Claude Powered**: Superior analytical and reasoning capabilities
* **Structured Output**: Well-formatted tables and financial insights
* **Market Intelligence**: Comprehensive company analysis and recommendations

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add Dependencies to Agent Run

**URL:** llms-txt#add-dependencies-to-agent-run

Source: https://docs.agno.com/basics/dependencies/agent/usage/add-dependencies-run

This example demonstrates how to inject dependencies into agent runs, allowing the agent to access dynamic context like user profiles and current time information for personalized responses.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Image Transcribe Document Agent

**URL:** llms-txt#image-transcribe-document-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/image-transcribe-document-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The agent will now think step-by-step before responding

**URL:** llms-txt#the-agent-will-now-think-step-by-step-before-responding

**Contents:**
- Choosing the Right Approach

reasoning_agent.print_response(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. Include an ASCII diagram of your solution.",
    stream=True,
    show_full_reasoning=True,
)
```

**Learn more:** [Reasoning Agents Guide](/basics/reasoning/reasoning-agents)

## Choosing the Right Approach

Here's how the three approaches compare:

| Approach             | Transparency                       | Best Use Case                  | Model Requirements                |
| -------------------- | ---------------------------------- | ------------------------------ | --------------------------------- |
| **Reasoning Models** | Continuous (full reasoning trace)  | Single-shot complex problems   | Requires reasoning-capable models |
| **Reasoning Tools**  | Structured (explicit step-by-step) | Structured research & analysis | Works with any model              |
| **Reasoning Agents** | Iterative (agent interactions)     | Multi-step tool-based tasks    | Works with any model              |

---

## Async MongoDB for Workflow

**URL:** llms-txt#async-mongodb-for-workflow

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/integrations/database/async-mongo/usage/async-mongodb-for-workflow

Agno supports using MongoDB asynchronously as a storage backend for Workflows, with the `AsyncMongoDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python async_mongodb_for_workflow.py theme={null}
"""
Run `pip install agno openai motor pymongo` to install dependencies.
"""
from agno.agent import Agent
from agno.db.mongo import AsyncMongoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "mongodb://localhost:27017"

db = AsyncMongoDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Loop Steps Workflow

**URL:** llms-txt#loop-steps-workflow

Source: https://docs.agno.com/basics/workflows/usage/loop-steps-workflow

This example demonstrates **Workflows 2.0** loop execution for quality-driven iterative processes.

This example demonstrates **Workflows 2.0** to repeatedly execute steps until specific conditions are met,
ensuring adequate research depth before proceeding to content creation.

**When to use**: When you need iterative refinement, quality assurance, or when the
required output quality can't be guaranteed in a single execution. Ideal for research
gathering, data collection, or any process where "good enough" is determined by content
analysis rather than a fixed number of iterations.

```python loop_steps_workflow.py theme={null}
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Step, Workflow
from agno.workflow.types import StepOutput

---

## What is Reasoning?

**URL:** llms-txt#what-is-reasoning?

**Contents:**
- Why Reasoning Matters
- How Reasoning Works
- Reasoning Models
- Three Approaches to Reasoning
  - 1. Reasoning Models

Source: https://docs.agno.com/basics/reasoning/overview

Reasoning gives Agents the ability to "think" before responding and "analyze" the results of their actions (i.e. tool calls), greatly improving the Agents' ability to solve problems that require sequential tool calls.

Imagine asking a regular AI agent to solve a complex math problem, analyze a scientific paper, or plan a multi-step travel itinerary. Often, it rushes to an answer without fully thinking through the problem. The result? Wrong calculations, incomplete analysis, or illogical plans.

Now imagine an agent that pauses, thinks through the problem step-by-step, validates its reasoning, catches its own mistakes, and only then provides an answer. This is reasoning in action, and it transforms agents from quick responders into careful problem-solvers.

## Why Reasoning Matters

Without reasoning, agents struggle with tasks that require:

* **Multi-step thinking** - Breaking complex problems into logical steps
* **Self-validation** - Checking their own work before responding
* **Error correction** - Catching and fixing mistakes mid-process
* **Strategic planning** - Thinking ahead instead of reacting

**Example:** Ask a normal agent "Which is bigger: 9.11 or 9.9?" and it might incorrectly say 9.11 (comparing digit by digit instead of decimal values). A reasoning agent thinks through the decimal comparison logic first and gets it right.

## How Reasoning Works

<Note>
  The features of reasoning are available both on Agents and Teams.
</Note>

**Chain-of-Thought (CoT):** The model thinks through a problem step-by-step internally, breaking down complex reasoning into logical steps before producing an answer. This is used by reasoning models and reasoning agents.

**ReAct (Reason and Act):** An iterative cycle where the agent alternates between reasoning and taking actions:

1. **Reason** - Think through the problem, plan next steps
2. **Act** - Take action (call a tool, perform calculation)
3. **Observe** - Analyze the results
4. **Repeat** - Continue reasoning based on new information until solved

This pattern is particularly useful with reasoning tools and when agents need to validate assumptions through real-world feedback.

## Three Approaches to Reasoning

Agno gives you three ways to add reasoning to your agents, each suited for different use cases:

### 1. Reasoning Models

**What:** Pre-trained models that natively think before answering (e.g. OpenAI gpt-5, Claude 4.5 Sonnet, Gemini 2.0 Flash Thinking, DeepSeek-R1).

**How it works:** The model generates an internal chain of thought before producing its final response. This happens at the model layer: you simply use the model and reasoning happens automatically.

* Single-shot complex problems (math, coding, physics)
* Problems where you trust the model to handle reasoning internally
* Use cases where you don't need to control the reasoning process

```python o3_mini.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

---

## Filtering on Weaviate

**URL:** llms-txt#filtering-on-weaviate

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-weaviate

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in Weaviate.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

---

## Import the workflows

**URL:** llms-txt#import-the-workflows

from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput, WorkflowRunOutputEvent
from agno.workflow.workflow import Workflow

---

## 4. EDUCATIONAL TUTORING WORKFLOW

**URL:** llms-txt#4.-educational-tutoring-workflow

---

## Portkey

**URL:** llms-txt#portkey

**Contents:**
- Authentication
- Example
- Advanced Configuration
- Params

Source: https://docs.agno.com/integrations/models/gateways/portkey/overview

Learn how to use models through the Portkey AI Gateway in Agno.

Portkey is an AI Gateway that provides a unified interface to access multiple AI providers with advanced features like routing, load balancing, retries, and observability. Use Portkey to build production-ready AI applications with better reliability and cost optimization.

With Portkey, you can:

* Route requests across multiple AI providers
* Implement fallback mechanisms for better reliability
* Monitor and analyze your AI usage
* Cache responses for cost optimization
* Apply rate limiting and usage controls

You need both a Portkey API key and a virtual key for model routing. Get them [from Portkey here](https://app.portkey.ai/).

Use `Portkey` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

## Advanced Configuration

You can configure Portkey with custom routing and retry policies:

<Note> View more examples [here](/integrations/models/gateways/portkey/usage/basic-stream). </Note>

| Parameter     | Type            | Default                       | Description                                                     |
| ------------- | --------------- | ----------------------------- | --------------------------------------------------------------- |
| `id`          | `str`           | `"gpt-4o-mini"`               | The id of the model to use through Portkey                      |
| `name`        | `str`           | `"Portkey"`                   | The name of the model                                           |
| `provider`    | `str`           | `"Portkey"`                   | The provider of the model                                       |
| `api_key`     | `Optional[str]` | `None`                        | The API key for Portkey (defaults to PORTKEY\_API\_KEY env var) |
| `base_url`    | `str`           | `"https://api.portkey.ai/v1"` | The base URL for the Portkey API                                |
| `virtual_key` | `Optional[str]` | `None`                        | The virtual key for the underlying provider                     |
| `trace_id`    | `Optional[str]` | `None`                        | Custom trace ID for request tracking                            |
| `config_id`   | `Optional[str]` | `None`                        | Configuration ID for Portkey routing                            |

`Portkey` also supports the params of [OpenAI](/reference/models/openai).

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Portkey` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

## Advanced Configuration

You can configure Portkey with custom routing and retry policies:
```

---

## Recipe Rag Image

**URL:** llms-txt#recipe-rag-image

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/recipe_rag_image

An agent that uses Llama 4 for multi-modal RAG and OpenAITools to create a visual, step-by-step image manual for a recipe.

```python cookbook/examples/agents/recipe_rag_image.py theme={null}
import asyncio
from pathlib import Path

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge

---

## Setup a team with two members

**URL:** llms-txt#setup-a-team-with-two-members

english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-5-mini"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You can only answer in Spanish",
    model=OpenAIChat(id="gpt-5-mini"),
)

multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[english_agent, spanish_agent],
    respond_directly=True,
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English and Spanish.",
        "Always check the language of the user's input before routing to an agent.",
    ],
)

---

## This is the URL of the MCP server we want to use.

**URL:** llms-txt#this-is-the-url-of-the-mcp-server-we-want-to-use.

server_url = "http://localhost:7777/mcp"

async def run_agent(message: str) -> None:
    async with MCPTools(transport="streamable-http", url=server_url) as mcp_tools:
        agent = Agent(
            model=Claude(id="claude-sonnet-4-0"),
            tools=[mcp_tools],
            markdown=True,
        )
        await agent.aprint_response(input=message, stream=True, markdown=True)

---

## Slow: Search everything

**URL:** llms-txt#slow:-search-everything

results = knowledge.search("deployment process", max_results=10)

---

## Then watch them recall the information (the question below states: "Tell me a 2-sentence story using my name")

**URL:** llms-txt#then-watch-them-recall-the-information-(the-question-below-states:-"tell-me-a-2-sentence-story-using-my-name")

**Contents:**
- Follow up in Spanish
- Usage

## Follow up in Spanish
multi_lingual_q_and_a_team.print_response(
    "CuÃ©ntame una historia de 2 oraciones usando mi nombre real.",
    stream=True,
    session_id=session_id,
)
bash  theme={null}
    pip install agno openai
    `bash  theme={null}
    export OPENAI_API_KEY=****
    </Step>

<Step title="Run the agent">
    `
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (bash):
```bash
python team_history.py
```

---

## Youtube

**URL:** llms-txt#youtube

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/youtube

**YouTubeTools** enable an Agent to access captions and metadata of YouTube videos, when provided with a video URL.

The following example requires the `youtube_transcript_api` library.

The following agent will provide a summary of a YouTube video.

| Param                         | Type        | Default | Description                                                                        |
| ----------------------------- | ----------- | ------- | ---------------------------------------------------------------------------------- |
| `get_video_captions`          | `bool`      | `True`  | Enables the functionality to retrieve video captions.                              |
| `get_video_data`              | `bool`      | `True`  | Enables the functionality to retrieve video metadata and other related data.       |
| `languages`                   | `List[str]` | -       | Specifies the list of languages for which data should be retrieved, if applicable. |
| `enable_get_video_captions`   | `bool`      | `True`  | Enable the get\_video\_captions functionality.                                     |
| `enable_get_video_data`       | `bool`      | `True`  | Enable the get\_video\_data functionality.                                         |
| `enable_get_video_timestamps` | `bool`      | `True`  | Enable the get\_video\_timestamps functionality.                                   |
| `all`                         | `bool`      | `False` | Enable all functionality.                                                          |

| Function                       | Description                                                |
| ------------------------------ | ---------------------------------------------------------- |
| `get_youtube_video_captions`   | This function retrieves the captions of a YouTube video.   |
| `get_youtube_video_data`       | This function retrieves the metadata of a YouTube video.   |
| `get_youtube_video_timestamps` | This function retrieves the timestamps of a YouTube video. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/youtube.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/youtube_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will provide a summary of a YouTube video.
```

---

## Parser Model for Structured Output

**URL:** llms-txt#parser-model-for-structured-output

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/parser-model

This example demonstrates how to use a different parser model for structured output generation, combining Claude for content generation with OpenAI for parsing into structured formats.

```python parser_model.py theme={null}
import random
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.anthropic import Claude
from agno.models.openai import OpenAIChat
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa

class NationalParkAdventure(BaseModel):
    park_name: str = Field(..., description="Name of the national park")
    best_season: str = Field(
        ...,
        description="Optimal time of year to visit this park (e.g., 'Late spring to early fall')",
    )
    signature_attractions: List[str] = Field(
        ...,
        description="Must-see landmarks, viewpoints, or natural features in the park",
    )
    recommended_trails: List[str] = Field(
        ...,
        description="Top hiking trails with difficulty levels (e.g., 'Angel's Landing - Strenuous')",
    )
    wildlife_encounters: List[str] = Field(
        ..., description="Animals visitors are likely to spot, with viewing tips"
    )
    photography_spots: List[str] = Field(
        ...,
        description="Best locations for capturing stunning photos, including sunrise/sunset spots",
    )
    camping_options: List[str] = Field(
        ..., description="Available camping areas, from primitive to RV-friendly sites"
    )
    safety_warnings: List[str] = Field(
        ..., description="Important safety considerations specific to this park"
    )
    hidden_gems: List[str] = Field(
        ..., description="Lesser-known spots or experiences that most visitors miss"
    )
    difficulty_rating: int = Field(
        ...,
        ge=1,
        le=5,
        description="Overall park difficulty for average visitor (1=easy, 5=very challenging)",
    )
    estimated_days: int = Field(
        ...,
        ge=1,
        le=14,
        description="Recommended number of days to properly explore the park",
    )
    special_permits_needed: List[str] = Field(
        default=[],
        description="Any special permits or reservations required for certain activities",
    )

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You help people plan amazing national park adventures and provide detailed park guides.",
    output_schema=NationalParkAdventure,
    parser_model=OpenAIChat(id="gpt-5-mini"),
)

---

## run_response: Iterator[RunOutputEvent] = asyncio.run(agent.arun("Share a 2 sentence horror story", stream=True))

**URL:** llms-txt#run_response:-iterator[runoutputevent]-=-asyncio.run(agent.arun("share-a-2-sentence-horror-story",-stream=true))

---

## Create a team with document processing tools

**URL:** llms-txt#create-a-team-with-document-processing-tools

**Contents:**
- Developer Resources

team = Team(
    members=[member_agent],
    model=Gemini(id="gemini-2.5-pro"),
    tools=[DocumentProcessingTools()],
    name="Document Processing Team",
    instructions=[
        "When files are uploaded, use the extract_text_from_pdf tool to process them.",
        "Analyze the extracted content and provide insights directly in your response.",
    ],
    send_media_to_model=False,
    store_media=True,
)

pdf_content = b"Sample PDF content"
sample_file = File(content=pdf_content)

response = team.run(
    input="Extract text from the uploaded PDF and analyze it.",
    files=[sample_file],
)
```

## Developer Resources

* View [Image Team Examples](/basics/multimodal/team/usage/image-to-text)
* View [Audio Team Examples](/basics/multimodal/team/usage/audio-to-text)
* View [Video Team Examples](/basics/multimodal/team/usage/video-caption-generation)

---

## Research Agent

**URL:** llms-txt#research-agent

research_agent = Agent(
    name="Market Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    instructions="Research current market conditions and news",
)

---

## Agent Same Run Image Analysis

**URL:** llms-txt#agent-same-run-image-analysis

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/agent/usage/agent-same-run-image-analysis

This example demonstrates how to create an agent that generates an image using DALL-E and then analyzes the generated image in the same run, providing insights about the image's contents.

```python agent_same_run_image_analysis.py theme={null}
from agno.agent import Agent
from agno.tools.dalle import DalleTools

---

## Location-Aware Agent Instructions

**URL:** llms-txt#location-aware-agent-instructions

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/context/agent/usage/location-instructions

This example demonstrates how to add location context to agent instructions, enabling the agent to provide location-specific responses and search for local news.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Reasoning Models

**URL:** llms-txt#reasoning-models

**Contents:**
- Examples
  - gpt-5-mini

Source: https://docs.agno.com/basics/reasoning/reasoning-models

Reasoning models are a class of large language models pre-trained to think before they answer. They produce a long internal chain of thought before responding.

Examples of reasoning models include:

* OpenAI o1-pro and gpt-5-mini
* Claude 3.7 sonnet in extended-thinking mode
* Gemini 2.0 flash thinking
* DeepSeek-R1

Reasoning models deeply consider and think through a plan before taking action. Its all about what the model does **before it starts generating a response**. Reasoning models excel at single-shot use-cases. They're perfect for solving hard problems (coding, math, physics) that don't require multiple turns, or calling tools sequentially.

```python gpt_5_mini.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

---

## Claude

**URL:** llms-txt#claude

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/anthropic

The Claude model provides access to Anthropic's Claude models.

| Parameter               | Type                                     | Default                        | Description                                                      |
| ----------------------- | ---------------------------------------- | ------------------------------ | ---------------------------------------------------------------- |
| `id`                    | `str`                                    | `"claude-3-5-sonnet-20241022"` | The id of the Anthropic Claude model to use                      |
| `name`                  | `str`                                    | `"Claude"`                     | The name of the model                                            |
| `provider`              | `str`                                    | `"Anthropic"`                  | The provider of the model                                        |
| `max_tokens`            | `Optional[int]`                          | `4096`                         | Maximum number of tokens to generate in the chat completion      |
| `thinking`              | `Optional[Dict[str, Any]]`               | `None`                         | Configuration for the thinking (reasoning) process               |
| `temperature`           | `Optional[float]`                        | `None`                         | Controls randomness in the model's output                        |
| `stop_sequences`        | `Optional[List[str]]`                    | `None`                         | A list of strings that the model should stop generating text at  |
| `top_p`                 | `Optional[float]`                        | `None`                         | Controls diversity via nucleus sampling                          |
| `top_k`                 | `Optional[int]`                          | `None`                         | Controls diversity via top-k sampling                            |
| `cache_system_prompt`   | `Optional[bool]`                         | `False`                        | Whether to cache the system prompt for improved performance      |
| `extended_cache_time`   | `Optional[bool]`                         | `False`                        | Whether to use extended cache time (1 hour instead of default)   |
| `request_params`        | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters to include in the request                  |
| `mcp_servers`           | `Optional[List[MCPServerConfiguration]]` | `None`                         | List of MCP (Model Context Protocol) server configurations       |
| `api_key`               | `Optional[str]`                          | `None`                         | The API key for authenticating with Anthropic                    |
| `default_headers`       | `Optional[Dict[str, Any]]`               | `None`                         | Default headers to include in all requests                       |
| `client_params`         | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters for client configuration                   |
| `client`                | `Optional[AnthropicClient]`              | `None`                         | A pre-configured instance of the Anthropic client                |
| `async_client`          | `Optional[AsyncAnthropicClient]`         | `None`                         | A pre-configured instance of the async Anthropic client          |
| `retries`               | `int`                                    | `0`                            | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`                                    | `1`                            | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`                                   | `False`                        | If True, the delay between retries is doubled each time          |

---

## Teams

**URL:** llms-txt#teams

**Contents:**
- When should you use Teams?
- Guides
- Developer Resources

Source: https://docs.agno.com/basics/teams/overview

Build autonomous multi-agent systems with Agno Teams.

A Team is a collection of Agents (or other sub-teams) that work together to accomplish tasks.

A `Team` has a list of `members` that can be instances of either `Agent` or `Team`.

A team can be visualized as a tree structure, where the team leader delegates tasks to sub-teams or directly to agents. The top level of the `Team` is called the "team leader".

Below is a minimal example of a language-routing team with two agents and one sub-team.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=0bbb6a84e04b9201746b0ff3495243e3" alt="Team structure" data-og-width="2610" width="2610" data-og-height="1413" height="1413" data-path="images/teams/team-structure-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=d9ca1e24c2099dffd88f2578e270dfa3 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=2a02ea12563bb86e4abb675612f9798a 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=638794030f685b82d26b434bb29e965f 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=6af9acfb6c2a82818deb979daee38365 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=5b3eaa00ec569867483b304a98d6be0d 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=5a153a9a9cd31794ab9f53a82324537f 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c4a60d21db7ffcd568b1d40c6eeb6866" alt="Team structure" data-og-width="2610" width="2610" data-og-height="1413" height="1413" data-path="images/teams/team-structure-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c44983dd9d8421469118ab7754a31852 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=6737f538162e1f95fffc78f71de32ae4 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=53c06595fc0177b71e828771431ff9c7 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=7874c16f789718d6dd7333d40bf36290 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=d83b756d8d1effc8825882a5f39ce86f 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-structure-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=08ae74854289473571437318aee88b55 2500w" />

<Note>
  It is highly recommended to first learn more about [Agents](/basics/agents/overview) before diving into Teams.
</Note>

The team leader delegates tasks to members depending on the role of the members and the nature of the tasks.  See the [Delegation](/basics/teams/delegation) guide for more details.

As with agents, teams support the following features:

* **Model:** Set the model that is used by the "team leader" to delegate tasks to the team members.
* **Instructions:** Instruct the team leader on how to solve problems. The names, descriptions and roles of team members are automatically provided to the team leader.
* **Database:** The Team's session history and state is stored in a database. This enables your team to continue conversations from where they left off, enabling multi-turn, long-running conversations.
* **Reasoning:** Enables the team leader to "think" before responding or delegating tasks to team members, and "analyze" the results of team members' responses.
* **Knowledge:** If the team needs to search for information, you can add a knowledge base to the team. This is accessible to the team leader.
* **Memory:** Gives Teams the ability to store and recall information from previous interactions, allowing them to learn user preferences and personalize their responses.
* **Tools:** If the team leader needs to be able to use tools directly, you can add tools to the team.

<Note>
  If you are migrating from Agno v1.x.x, the `mode` parameter has been deprecated. Please see the [Migration Guide](/how-to/v2-migration#teams) for more details on how to migrate your teams.
</Note>

## When should you use Teams?

When should you use Teams?

The general guideline is to have Agents that are narrow in scope. When you have a complex task that requires a variety of tools or a long list of steps, a Team of single-purpose agents would be a good fit.

In addition, if a single agent's context limit gets easily exceeded, because of the complexity of the task, a Team would address this by keeping a single agent's context small, becaues it only addresses a part of the task.

<CardGroup cols={3}>
  <Card title="Building Teams" icon="wrench" iconType="duotone" href="/basics/teams/building-teams">
    Learn how to build your teams.
  </Card>

<Card title="Running your Team" icon="user-robot" iconType="duotone" href="/basics/teams/running-teams">
    Learn how to run your teams.
  </Card>

<Card title="Debugging Teams" icon="bug" iconType="duotone" href="/basics/teams/debugging-teams">
    Learn how to debug and troubleshoot your teams.
  </Card>
</CardGroup>

## Developer Resources

* View the [`Team` schema reference](/reference/teams/team)
* View [Use-cases](/examples/use-cases/teams/)
* View [Usage Examples](/basics/teams/usage/)
* View a [Teams Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/README.md)

---

## Configure embedder with custom settings

**URL:** llms-txt#configure-embedder-with-custom-settings

embedder = JinaEmbedder(
    id="jina-embeddings-v3",
    dimensions=1024,
    embedding_type="float",
    late_chunking=True,
    batch_size=50,
    timeout=30.0
)

---

## Process only necessary content

**URL:** llms-txt#process-only-necessary-content

**Contents:**
- Advanced Optimizations
  - Use Hybrid Search
  - Add Reranking
  - Optimize Embedder Dimensions

knowledge.add_contents(
    paths=["large_dataset/"],
    include=["*.pdf"],       # Only PDFs
    exclude=["*backup*"],    # Skip backups
    skip_if_exists=True,
    metadata={"batch": "current"}
)
python  theme={null}
from agno.vectordb.pgvector import PgVector, SearchType

vector_db = PgVector(
    table_name="knowledge",
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    search_type=SearchType.hybrid  # Vector + keyword search
)
python  theme={null}
from agno.knowledge.reranker.cohere import CohereReranker

vector_db = PgVector(
    table_name="knowledge",
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    reranker=CohereReranker(
        model="rerank-multilingual-v3.0",
        top_n=10
    )
)
python  theme={null}
from agno.knowledge.embedder.openai import OpenAIEmbedder

**Examples:**

Example 1 (unknown):
```unknown
## Advanced Optimizations

Once you've applied the quick wins above, consider these for further improvements:

### Use Hybrid Search

Combine vector and keyword search for better results:
```

Example 2 (unknown):
```unknown
### Add Reranking

Improve result quality by reranking with Cohere:
```

Example 3 (unknown):
```unknown
### Optimize Embedder Dimensions

Reduce dimensions for faster search (with slight quality trade-off):
```

---

## Define tools to manage our shopping list

**URL:** llms-txt#define-tools-to-manage-our-shopping-list

def add_item(run_context: RunContext, item: str) -> str:
    """Add an item to the shopping list and return confirmation."""
    # Add the item if it's not already in the list
    if item.lower() not in [i.lower() for i in run_context.session_state["shopping_list"]]:
        run_context.session_state["shopping_list"].append(item)  # type: ignore
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"

def remove_item(run_context: RunContext, item: str) -> str:
    """Remove an item from the shopping list by name."""
    # Case-insensitive search
    for i, list_item in enumerate(run_context.session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            run_context.session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

return f"'{item}' was not found in the shopping list"

def list_items(run_context: RunContext) -> str:
    """List all items in the shopping list."""
    shopping_list = run_context.session_state["shopping_list"]

if not shopping_list:
        return "The shopping list is empty."

items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"

---

## Status

**URL:** llms-txt#status

Source: https://docs.agno.com/reference-api/schema/whatsapp/status

---

## External Tool Execution

**URL:** llms-txt#external-tool-execution

Source: https://docs.agno.com/basics/hitl/usage/external-tool-execution

This example demonstrates how to execute tools outside of the agent using external tool execution. This pattern allows you to control tool execution externally while maintaining agent functionality.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Generate unique user ID

**URL:** llms-txt#generate-unique-user-id

user_id = str(uuid4())

---

## Fireworks

**URL:** llms-txt#fireworks

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/fireworks

The Fireworks model provides access to Fireworks' language models.

| Parameter               | Type            | Default                                                | Description                                                         |
| ----------------------- | --------------- | ------------------------------------------------------ | ------------------------------------------------------------------- |
| `id`                    | `str`           | `"accounts/fireworks/models/llama-v3p1-405b-instruct"` | The id of the Fireworks model to use                                |
| `name`                  | `str`           | `"Fireworks"`                                          | The name of the model                                               |
| `provider`              | `str`           | `"Fireworks"`                                          | The provider of the model                                           |
| `api_key`               | `Optional[str]` | `None`                                                 | The API key for Fireworks (defaults to FIREWORKS\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.fireworks.ai/inference/v1"`              | The base URL for the Fireworks API                                  |
| `retries`               | `int`           | `0`                                                    | Number of retries to attempt before raising a ModelProviderError    |
| `delay_between_retries` | `int`           | `1`                                                    | Delay between retries, in seconds                                   |
| `exponential_backoff`   | `bool`          | `False`                                                | If True, the delay between retries is doubled each time             |

Fireworks extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## JSON for Team

**URL:** llms-txt#json-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/json/usage/json-for-team

Agno supports using local JSON files as a storage backend for Teams using the `JsonDb` class.

```python json_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Parallel Workflow

**URL:** llms-txt#parallel-workflow

**Contents:**
- Example
- Handling Session State Data in Parallel Steps
- Developer Resources
- Reference

Source: https://docs.agno.com/basics/workflows/workflow-patterns/parallel-workflow

Independent, concurrent tasks that can execute simultaneously for improved efficiency

**Example Use-Cases**: Multi-source research, parallel analysis, concurrent data processing

Parallel workflows maintain deterministic results while dramatically reducing execution time for independent operations.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=afda5268ab0637c6064ace8edd6f35e5" alt="Workflows parallel steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-parallel-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c153b9934fa98b2b886a9435022b020a 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=17253f58b485bb6180827516f7f947be 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=b067f30b291f2de8cb6a04e208ee61cc 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=a814e829581fb1d7c64e49fa87ca847e 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=1cd456c3e949af82fd6325b3f3865f23 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=52fcc51f72ee6e1df2648612451cae70 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=ec4f6c7c9a6ef76cec8f0866eb1acc5b" alt="Workflows parallel steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-parallel-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=39624cca7177ba0064491bb64c645db2 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c3e5cde671f164b7dd13eb417f5f74db 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=db6672401fdf35e0eb616e72016ec41c 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=831053d087a3bf26e966f8f896ac9b61 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=445e07ea13a74913e67e2a2a9c8e9c5f 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=77083544c8387f660207dfaf645bdaf0 2500w" />

## Handling Session State Data in Parallel Steps

When using custom Python functions in your steps, you can access and update the Worfklow session state via the `run_context` parameter.

If you are performing session state updates in Parallel Steps, be aware that concurrent access to shared state will require coordination to avoid race conditions.

## Developer Resources

* [Parallel Steps Workflow](/basics/workflows/usage/parallel-steps-workflow)

For complete API documentation, see [Parallel Steps Reference](/reference/workflows/parallel-steps).

---

## --- Response Models ---

**URL:** llms-txt#----response-models----

class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )

class SearchResults(BaseModel):
    articles: list[NewsArticle]

class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )

---

## Create a custom logger that writes to a file

**URL:** llms-txt#create-a-custom-logger-that-writes-to-a-file

custom_logger = logging.getLogger("file_logger")

---

## Tool Hooks

**URL:** llms-txt#tool-hooks

**Contents:**
  - Multiple Tool Hooks
- Pre and Post Hooks

Source: https://docs.agno.com/basics/tools/hooks

Learn how to use tool hooks to modify the behavior of a tool.

You can use tool hooks to perform validation, logging, or any other logic before or after a tool is called.

A tool hook is a function that takes a function name, function call, and arguments. Optionally, you can access the `Agent` or `Team` object as well.  Inside the tool hook, you have to call the function call and return the result.

<Note>
  It is important to use exact parameter names when defining a tool hook. `agent`, `team`, `run_context`, `function_name`, `function_call`, and `arguments` are available parameters.
</Note>

You can assign tool hooks on agents and teams.  The tool hooks will be applied to all tool calls made by the agent or team.

You can also get access to the `RunContext` object in the tool hook. Inside the run context, you will find the session state, dependencies, and metadata.

### Multiple Tool Hooks

You can also assign multiple tool hooks at once. They will be applied in the order they are assigned.

You can also assign tool hooks to specific custom tools.

## Pre and Post Hooks

Pre and post hooks let's you modify what happens before and after a tool is called. It is an alternative to tool hooks.

Set the `pre_hook` in the `@tool` decorator to run a function before the tool call.

Set the `post_hook` in the `@tool` decorator to run a function after the tool call.

Here's a demo example of using a `pre_hook`, `post_hook` along with Agent Context.

**Examples:**

Example 1 (unknown):
```unknown
or
```

Example 2 (unknown):
```unknown
You can assign tool hooks on agents and teams.  The tool hooks will be applied to all tool calls made by the agent or team.

For example:
```

Example 3 (unknown):
```unknown
You can also get access to the `RunContext` object in the tool hook. Inside the run context, you will find the session state, dependencies, and metadata.
```

Example 4 (unknown):
```unknown
### Multiple Tool Hooks

You can also assign multiple tool hooks at once. They will be applied in the order they are assigned.
```

---

## Accessing Multiple Previous Steps

**URL:** llms-txt#accessing-multiple-previous-steps

**Contents:**
- Example

Source: https://docs.agno.com/basics/workflows/access-previous-steps

How to access multiple previous steps

Advanced workflows often require data from multiple previous steps beyond just the immediate predecessor. The `StepInput` object provides powerful methods to access any previous step's output by name or retrieve all accumulated content.

```python  theme={null}
def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

# Access original workflow input
    original_topic = step_input.workflow_message or ""

# Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

# Or access ALL previous content
    all_research = step_input.get_all_previous_content()

# Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

## HackerNews Insights
        {hackernews_data[:500]}...

## Web Research Findings  
        {web_data[:500]}...
    """

return StepOutput(
        step_name="comprehensive_report", 
        content=report.strip(), 
        success=True
    )

---

## Load knowledge base using hybrid search

**URL:** llms-txt#load-knowledge-base-using-hybrid-search

hybrid_db = PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.hybrid)
knowledge = Knowledge(
    name="Hybrid Search Knowledge Base",
    vector_db=hybrid_db,
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
)

---

## Update Content

**URL:** llms-txt#update-content

Source: https://docs.agno.com/reference-api/schema/knowledge/update-content

patch /knowledge/content/{content_id}
Update content properties such as name, description, metadata, or processing configuration. Allows modification of existing content without re-uploading.

---

## Alex River is not in the knowledge base, so the Agent should not find any information about him

**URL:** llms-txt#alex-river-is-not-in-the-knowledge-base,-so-the-agent-should-not-find-any-information-about-him

**Contents:**
- Usage

agent.print_response(
    "Do you think Alex Rivera is a good candidate?",
    markdown=True,
)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector openai    
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/knowledge/basic_operations/08_include_exclude_files.py
      bash Windows theme={null}
      python cookbook/knowledge/basic_operations/08_include_exclude_files.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Serpapi

**URL:** llms-txt#serpapi

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/serpapi

**SerpApiTools** enable an Agent to search Google and YouTube for a query.

The following example requires the `google-search-results` library and an API key from [SerpApi](https://serpapi.com/).

The following agent will search Google for the query: "Whats happening in the USA" and share results.

| Parameter               | Type            | Default | Description                                                                     |
| ----------------------- | --------------- | ------- | ------------------------------------------------------------------------------- |
| `api_key`               | `Optional[str]` | `None`  | SerpApi API key. If not provided, will use SERP\_API\_KEY environment variable. |
| `enable_search_google`  | `bool`          | `True`  | Enable Google search functionality.                                             |
| `enable_search_youtube` | `bool`          | `False` | Enable YouTube search functionality.                                            |
| `all`                   | `bool`          | `False` | Enable all available functions in the toolkit.                                  |

| Function         | Description                                                                                                                                                                                                                                                                              |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search_google`  | Search Google using the Serpapi API. Parameters include `query` (str) for the search query and `num_results` (int, default=10) for the number of results. Returns JSON formatted search results with organic results, recipes, shopping results, knowledge graph, and related questions. |
| `search_youtube` | Search YouTube using the Serpapi API. Parameters include `query` (str) for the search query. Returns JSON formatted search results with video results, movie results, and channel results.                                                                                               |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/serpapi.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/serpapi_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search Google for the query: "Whats happening in the USA" and share results.
```

---

## focus_areas=["AI", "Machine Learning"],

**URL:** llms-txt#focus_areas=["ai",-"machine-learning"],

---

## Only thinking, no analysis

**URL:** llms-txt#only-thinking,-no-analysis

ReasoningTools(enable_think=True, enable_analyze=False)

---

## Store Events and Events to Skip in a Workflow

**URL:** llms-txt#store-events-and-events-to-skip-in-a-workflow

**Contents:**
- Key Features:

Source: https://docs.agno.com/basics/workflows/usage/store-events-and-events-to-skip-in-a-workflow

This example demonstrates **Workflows 2.0** event storage capabilities

This example demonstrates **Workflows 2.0** event storage capabilities, showing how to:

1. **Store execution events** for debugging/auditing (`store_events=True`)
2. **Filter noisy events** (`events_to_skip`) to focus on critical workflow milestones
3. **Access stored events** post-execution via `workflow.run_response.events`

* **Selective Storage**: Skip verbose events (e.g., `step_started`) while retaining key milestones.
* **Debugging/Audit**: Capture execution flow for analysis without manual logging.
* **Performance Optimization**: Reduce storage overhead by filtering non-essential events.

```python store_events_and_events_to_skip_in_a_workflow.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.agent import (
    RunContentEvent,
    ToolCallCompletedEvent,
    ToolCallStartedEvent,
)
from agno.run.workflow import WorkflowRunEvent, WorkflowRunOutput
from agno.tools.hackernews import HackerNewsTools
from agno.run.agent import RunEvent
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Media Trend Analysis Agent

**URL:** llms-txt#media-trend-analysis-agent

**Contents:**
  - What It Does
  - Key Features
- Code

Source: https://docs.agno.com/examples/use-cases/agents/media_trend_analysis_agent

The Media Trend Analysis Agent Example demonstrates a sophisticated AI-powered tool designed to analyze media trends, track digital conversations, and provide actionable insights across various online platforms. This agent combines web search capabilities with content scraping to deliver comprehensive trend analysis reports.

This agent specializes in:

* **Trend Identification**: Detects emerging patterns and shifts in media coverage
* **Source Analysis**: Identifies key influencers and authoritative sources
* **Data Extraction**: Gathers information from news sites, social platforms, and digital media
* **Insight Generation**: Provides actionable recommendations based on trend analysis
* **Future Forecasting**: Predicts potential developments based on current patterns

* **Multi-Source Analysis**: Combines Exa search tools with Firecrawl scraping capabilities
* **Intelligent Filtering**: Uses keyword-based searches with date filtering for relevant results
* **Smart Scraping**: Only scrapes content when search results are insufficient
* **Structured Reporting**: Generates comprehensive markdown reports with executive summaries
* **Real-time Data**: Analyzes current trends with configurable time windows

```python cookbook/examples/agents/media_trend_analysis_agent.py theme={null}
"""Please install dependencies using:
pip install openai exa-py agno firecrawl
"""

from datetime import datetime, timedelta
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools
from agno.tools.firecrawl import FirecrawlTools

def calculate_start_date(days: int) -> str:
    """Calculate start date based on number of days."""
    start_date = datetime.now() - timedelta(days=days)
    return start_date.strftime("%Y-%m-%d")

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        ExaTools(start_published_date=calculate_start_date(30), type="keyword"),
        FirecrawlTools(scrape=True),
    ],
    description=dedent("""\
        You are an expert media trend analyst specializing in:
        1. Identifying emerging trends across news and digital platforms
        2. Recognizing pattern changes in media coverage
        3. Providing actionable insights based on data
        4. Forecasting potential future developments
    """),
    instructions=[
        "Analyze the provided topic according to the user's specifications:",
        "1. Use keywords to perform targeted searches",
        "2. Identify key influencers and authoritative sources",
        "3. Extract main themes and recurring patterns",
        "4. Provide actionable recommendations",
        "5. if got sources less then 2, only then scrape them using firecrawl tool, dont crawl it  and use them to generate the report",
        "6. growth rate should be in percentage , and if not possible dont give growth rate",
    ],
    expected_output=dedent("""\
    # Media Trend Analysis Report

## Executive Summary
    {High-level overview of findings and key metrics}

## Trend Analysis
    ### Volume Metrics
    - Peak discussion periods: {dates}
    - Growth rate: {percentage or dont show this}

## Source Analysis
    ### Top Sources
    1. {Source 1}

## Actionable Insights
    1. {Insight 1}
       - Evidence: {data points}
       - Recommended action: {action}

## Future Predictions
    1. {Prediction 1}
       - Supporting evidence: {evidence}

## References
    {Detailed source list with links}
    """),
    markdown=True,
    add_datetime_to_context=True,
)

---

## Speech-to-Text

**URL:** llms-txt#speech-to-text

**Contents:**
- Using OpenAI Whisper (Cloud)

Source: https://docs.agno.com/basics/multimodal/audio/speech-to-text

Learn how to transcribe audio with Agno agents.

Agno agents can transcribe audio files using different tools and models. You can use native capabilities of OpenAI or fully multimodal Gemini models.

<Tip>
  Examples of ways to do Audio to Text (Transcribe) are:

* [Using Gemini model](/basics/multimodal/audio/usage/audio-to-text)
  * [Using OpenAI Model](/basics/multimodal/agent/usage/audio-input-output)
  * [Using `OpenAI Tool`](/integrations/toolkits/models/openai#1-transcribing-audio)
  * [Using `Groq Tool`](/integrations/toolkits/models/groq#1-transcribing-audio)
</Tip>

## Using OpenAI Whisper (Cloud)

The following agent uses OpenAI Whisper API for audio transcription.

```python cookbook/tools/models/openai_tools.py theme={null}
import base64
from pathlib import Path

from agno.agent import Agent
from agno.run.agent import RunOutput
from agno.tools.openai import OpenAITools
from agno.utils.media import download_file, save_base64_data

---

## Audio Input (Upload the file)

**URL:** llms-txt#audio-input-(upload-the-file)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/audio-input-file-upload

```python cookbook/models/google/gemini/audio_input_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

---

## Tool Use

**URL:** llms-txt#tool-use

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Combine for active, recent content

**URL:** llms-txt#combine-for-active,-recent-content

current_content = AND(recent_filter, active_filter)

---

## Jina Reader

**URL:** llms-txt#jina-reader

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/jina-reader

**JinaReaderTools** enable an Agent to perform web search tasks using Jina.

The following example requires the `jina` library.

The following agent will use Jina API to summarize the content of [https://github.com/AgnoAgi](https://github.com/AgnoAgi)

| Parameter              | Type            | Default                | Description                                                                                         |
| ---------------------- | --------------- | ---------------------- | --------------------------------------------------------------------------------------------------- |
| `api_key`              | `Optional[str]` | `None`                 | The API key for authentication purposes. If not provided, uses JINA\_API\_KEY environment variable. |
| `base_url`             | `str`           | `"https://r.jina.ai/"` | The base URL of the API.                                                                            |
| `search_url`           | `str`           | `"https://s.jina.ai/"` | The URL used for search queries.                                                                    |
| `max_content_length`   | `int`           | `10000`                | The maximum length of content allowed.                                                              |
| `timeout`              | `Optional[int]` | `None`                 | Timeout in seconds for API requests.                                                                |
| `search_query_content` | `bool`          | `True`                 | Include content in search query results.                                                            |
| `enable_read_url`      | `bool`          | `True`                 | Enable the read\_url functionality.                                                                 |
| `enable_search_query`  | `bool`          | `False`                | Enable the search\_query functionality.                                                             |
| `all`                  | `bool`          | `False`                | Enable all functionality.                                                                           |

| Function       | Description                                                                                                                                                                                            |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `read_url`     | Reads the content of a specified URL using Jina Reader API. Parameters include `url` for the URL to read. Returns the truncated content or an error message if the request fails.                      |
| `search_query` | Performs a web search using Jina Reader API based on a specified query. Parameters include `query` for the search term. Returns the truncated search results or an error message if the request fails. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/jina_reader.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/jina_reader_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Jina API to summarize the content of [https://github.com/AgnoAgi](https://github.com/AgnoAgi)
```

---

## memory_manager = MemoryManager(model=OpenAIChat(id="gpt-5-mini"))

**URL:** llms-txt#memory_manager-=-memorymanager(model=openaichat(id="gpt-5-mini"))

---

## Firecrawl Reader Async

**URL:** llms-txt#firecrawl-reader-async

**Contents:**
- Code
- Usage
- Params

Source: https://docs.agno.com/basics/knowledge/readers/usage/firecrawl-reader-async

The **Firecrawl Reader** with asynchronous processing uses the Firecrawl API to scrape and crawl web content efficiently, converting it into documents for your knowledge base.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set API Key">
    
  </Step>

<Snippet file="run-pgvector-step.mdx" />

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

<Snippet file="firecrawl-reader-reference.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set API Key">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "[Feature Request] Support json schemas in Gemini client in addition to pydantic base model",

**URL:** llms-txt#"[feature-request]-support-json-schemas-in-gemini-client-in-addition-to-pydantic-base-model",

---

## Batch processing with async

**URL:** llms-txt#batch-processing-with-async

**Contents:**
- Usage in Knowledge

tasks = [reader.async_read(file) for file in file_list]
all_documents = await asyncio.gather(*tasks)
python  theme={null}
from agno.knowledge.reader.pdf_reader import PDFReader

**Examples:**

Example 1 (unknown):
```unknown
## Usage in Knowledge

Readers integrate seamlessly with Agno Knowledge:
```

---

## Audio Streaming Agent

**URL:** llms-txt#audio-streaming-agent

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/audio/usage/audio-streaming

```python  theme={null}
import base64
import wave
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat

---

## The Agent sessions and runs will now be stored in SQLite

**URL:** llms-txt#the-agent-sessions-and-runs-will-now-be-stored-in-sqlite

**Contents:**
- Params
- Developer Resources

agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem?")
agent.print_response("List my messages one by one")

<Snippet file="db-sqlite-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/sqlite/sqlite_for_agent.py)

---

## Response includes source information

**URL:** llms-txt#response-includes-source-information

**Contents:**
- Knowledge Base Architecture
- Benefits of Knowledge-Powered Agents
  - Accuracy and Reliability
  - Scalability and Maintenance
  - Context Awareness
- Getting Started with Knowledge Bases
- Learn More

"Based on section 3.2 of our Return Policy document,
items can be returned within 30 days of purchase..."
python Basic Setup theme={null}
  from agno.knowledge.knowledge import Knowledge
  from agno.vectordb.pgvector import PgVector
  from agno.db.postgres import PostgresDb

# Create a knowledge base
  knowledge = Knowledge(
      contents_db=PostgresDb(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"),
      vector_db=PgVector(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai")
  )

# Add your content
  knowledge.add_content(path="documents/")
  python With Custom Configuration theme={null}
  from agno.knowledge.knowledge import Knowledge
  from agno.vectordb.pgvector import PgVector
  from agno.embedder.openai import OpenAIEmbedder

# Customized knowledge base
  knowledge = Knowledge(
      vector_db=PgVector(
          table_name="company_knowledge",
          embedder=OpenAIEmbedder(model="text-embedding-3-large")
      )
  )
  ```
</CodeGroup>

<CardGroup cols={3}>
  <Card title="Content Types" icon="file-lines" href="/basics/knowledge/content-types">
    Explore different ways to add information to your knowledge base
  </Card>

<Card title="Search & Retrieval" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Learn how agents search and find relevant information
  </Card>

<Card title="Vector Databases" icon="database" href="/basics/vectordb/overview">
    Choose the right storage solution for your knowledge base
  </Card>

<Card title="Performance Tips" icon="gauge" href="/basics/knowledge/performance-tips">
    Optimize your knowledge base for speed and accuracy
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
## Knowledge Base Architecture

Here's how the different pieces work together:

<Steps>
  <Step title="Content Ingestion">
    Raw content is processed through **readers** that understand different file formats (PDF, websites, databases, etc.) and extract meaningful text.
  </Step>

  <Step title="Intelligent Chunking">
    Large documents are broken down into smaller, meaningful pieces using **chunking strategies** that preserve context while enabling precise retrieval.
  </Step>

  <Step title="Embedding Generation">
    Each chunk is converted into a vector embedding that captures its semantic meaning using **embedders** powered by language models.
  </Step>

  <Step title="Vector Storage">
    Embeddings are stored in **vector databases** optimized for similarity search, often with support for hybrid search combining semantic and keyword matching.
  </Step>

  <Step title="Intelligent Retrieval">
    When agents need information, they generate search queries, find similar embeddings, and retrieve the most relevant content chunks.
  </Step>
</Steps>

## Benefits of Knowledge-Powered Agents

### Accuracy and Reliability

* Responses are grounded in your specific information, not generic training data
* Reduced hallucinations because agents reference actual sources
* Up-to-date information that reflects your current state

### Scalability and Maintenance

* Add new information without retraining or modifying code
* Handle unlimited amounts of information without performance degradation
* Easy updates by simply adding new content to the knowledge base

### Context Awareness

* Agents understand your specific domain, terminology, and processes
* Responses are tailored to your organization's context and needs
* Consistent information across all agent interactions

## Getting Started with Knowledge Bases

Ready to build your own knowledge base? The process is straightforward:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Parallel and custom function step streaming on AgentOS

**URL:** llms-txt#parallel-and-custom-function-step-streaming-on-agentos

Source: https://docs.agno.com/basics/workflows/usage/parallel-and-custom-function-step-streaming-agentos

This example demonstrates how to use parallel steps with custom function executors and streaming on AgentOS.

This example demonstrates how to use using steps with custom function
executors, and how to stream their responses using the [AgentOS](/agent-os/introduction).

The agents and teams running inside the custom function step in `Parallel` will also stream their results to the AgentOS.

```python parallel_and_custom_function_step_streaming_agentos.py theme={null}
from typing import AsyncIterator, Union

from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.run.workflow import WorkflowRunOutputEvent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step, StepInput, StepOutput
from agno.workflow.workflow import Workflow

---

## Can also use custom binaries: command="./my-mcp-server"

**URL:** llms-txt#can-also-use-custom-binaries:-command="./my-mcp-server"

mcp_tools = MCPTools(command="uvx mcp-server-git")
await mcp_tools.connect()

try:
    agent = Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[mcp_tools])
    await agent.aprint_response("What is the license for this project?", stream=True)
finally:
    # Always close the connection when done
    await mcp_tools.close()
python  theme={null}
import asyncio
import os

from agno.agent import Agent
from agno.tools.mcp import MultiMCPTools

async def run_agent(message: str) -> None:
    """Run the Airbnb and Google Maps agent with the given message."""

env = {
        **os.environ,
        "GOOGLE_MAPS_API_KEY": os.getenv("GOOGLE_MAPS_API_KEY"),
    }

# Initialize and connect to multiple MCP servers
    mcp_tools = MultiMCPTools(
        commands=[
            "npx -y @openbnb/mcp-server-airbnb --ignore-robots-txt",
            "npx -y @modelcontextprotocol/server-google-maps",
        ],
        env=env,
    )
    await mcp_tools.connect()

try:
        agent = Agent(
            tools=[mcp_tools],
            markdown=True,
        )

await agent.aprint_response(message, stream=True)
    finally:
        # Always close the connection when done
        await mcp_tools.close()

**Examples:**

Example 1 (unknown):
```unknown
You can also use multiple MCP servers at once, with the `MultiMCPTools` class. For example:
```

---

## SentenceTransformers Embedder

**URL:** llms-txt#sentencetransformers-embedder

**Contents:**
- Usage

Source: https://docs.agno.com/basics/knowledge/embedder/sentencetransformers/overview

The `SentenceTransformerEmbedder` class is used to embed text data into vectors using the [SentenceTransformers](https://www.sbert.net/) library.

```python sentence_transformer_embedder.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder

---

## Airbnb MCP agent

**URL:** llms-txt#airbnb-mcp-agent

Source: https://docs.agno.com/basics/tools/mcp/usage/airbnb

Using the [Airbnb MCP server](https://github.com/openbnb-org/mcp-server-airbnb) to create an Agent that can search for Airbnb listings:

---

## User-assistant message pairs from each run

**URL:** llms-txt#user-assistant-message-pairs-from-each-run

messages = agent.get_session_messages()

---

## Structured Input with Pydantic Models

**URL:** llms-txt#structured-input-with-pydantic-models

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/structured-input

This example demonstrates how to use structured Pydantic models as input to agents, enabling type-safe and validated input parameters for complex research tasks.

```python structured_input.py theme={null}
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field

class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)

---

## Create an Agent with the ElevenLabs tool

**URL:** llms-txt#create-an-agent-with-the-elevenlabs-tool

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent = Agent(tools=[
    ElevenLabsTools(
        voice_id="JBFqnCBsd6RMkjVDRZzb", model_id="eleven_multilingual_v2", target_directory="audio_generations"
    )
], name="ElevenLabs Agent")

agent.print_response("Generate a audio summary of the big bang theory", markdown=True)
```

| Parameter                      | Type            | Default                  | Description                                                                                                                                                                    |
| ------------------------------ | --------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `api_key`                      | `str`           | `None`                   | The Eleven Labs API key for authentication                                                                                                                                     |
| `voice_id`                     | `str`           | `JBFqnCBsd6RMkjVDRZzb`   | The voice ID to use for the audio generation                                                                                                                                   |
| `target_directory`             | `Optional[str]` | `None`                   | The directory to save the audio file                                                                                                                                           |
| `model_id`                     | `str`           | `eleven_multilingual_v2` | The model's id to use for the audio generation                                                                                                                                 |
| `output_format`                | `str`           | `mp3_44100_64`           | The output format to use for the audio generation (check out [the docs](https://elevenlabs.io/docs/api-reference/text-to-speech#parameter-output-format) for more information) |
| `enable_text_to_speech`        | `bool`          | `True`                   | Enable the text\_to\_speech functionality.                                                                                                                                     |
| `enable_generate_sound_effect` | `bool`          | `True`                   | Enable the generate\_sound\_effect functionality.                                                                                                                              |
| `enable_get_voices`            | `bool`          | `True`                   | Enable the get\_voices functionality.                                                                                                                                          |
| `all`                          | `bool`          | `False`                  | Enable all functionality.                                                                                                                                                      |

| Function                | Description                                     |
| ----------------------- | ----------------------------------------------- |
| `text_to_speech`        | Convert text to speech                          |
| `generate_sound_effect` | Generate sound effect audio from a text prompt. |
| `get_voices`            | Get the list of voices available                |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/eleven_labs.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/elevenlabs_tools.py)

---

## response: RunOutput = agent.run("New York")

**URL:** llms-txt#response:-runoutput-=-agent.run("new-york")

---

## MongoDB Async

**URL:** llms-txt#mongodb-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/mongodb/usage/async-mongo-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run MongoDB">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run MongoDB">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Reliability with Multiple Tools

**URL:** llms-txt#reliability-with-multiple-tools

Source: https://docs.agno.com/basics/evals/reliability/usage/reliability-with-multiple-tools

Example showing how to assert an Agno Agent is making multiple expected tool calls.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Use filters in agent responses - NOTE: filters must be in a list!

**URL:** llms-txt#use-filters-in-agent-responses---note:-filters-must-be-in-a-list!

**Contents:**
  - Complex Filter Examples

sales_agent.print_response(
    "What were our Q1 sales results?",
    knowledge_filters=[  # â† Must be a list!
        AND(EQ("data_type", "sales"), EQ("quarter", "Q1"))
    ]
)
python  theme={null}
from agno.filters import AND, OR, NOT, EQ, IN, GT

**Examples:**

Example 1 (unknown):
```unknown
### Complex Filter Examples
```

---

## Calculator

**URL:** llms-txt#calculator

**Contents:**
- Example
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/local/calculator

**Calculator** enables an Agent to perform mathematical calculations.

The following agent will calculate the result of `10*5` and then raise it to the power of `2`:

| Function       | Description                                                                              |
| -------------- | ---------------------------------------------------------------------------------------- |
| `add`          | Adds two numbers and returns the result.                                                 |
| `subtract`     | Subtracts the second number from the first and returns the result.                       |
| `multiply`     | Multiplies two numbers and returns the result.                                           |
| `divide`       | Divides the first number by the second and returns the result. Handles division by zero. |
| `exponentiate` | Raises the first number to the power of the second number and returns the result.        |
| `factorial`    | Calculates the factorial of a number and returns the result. Handles negative numbers.   |
| `is_prime`     | Checks if a number is prime and returns the result.                                      |
| `square_root`  | Calculates the square root of a number and returns the result. Handles negative numbers. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/calculator.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/calculator_tools.py)

---

## Create filter expression

**URL:** llms-txt#create-filter-expression

filter_expr = AND(EQ("status", "published"), GT("views", 1000))

---

## Setup your Agent with Automatic User Memory

**URL:** llms-txt#setup-your-agent-with-automatic-user-memory

agent = Agent(
    db=db,
    enable_user_memories=True, # Automatic memory management
)

---

## Agent with User Memory

**URL:** llms-txt#agent-with-user-memory

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/discord/usage/agent-with-user-memory

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## AI Image Transformation Team

**URL:** llms-txt#ai-image-transformation-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/image-to-image-transformation

This example demonstrates how a team can collaborate to transform images using a style advisor to recommend transformations and an image transformer to apply AI-powered changes.

```python cookbook/examples/teams/multimodal/image_to_image_transformation.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.fal import FalTools

style_advisor = Agent(
    name="Style Advisor",
    role="Analyze and recommend artistic styles and transformations",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Analyze the input image and transformation request",
        "Provide style recommendations and enhancement suggestions",
        "Consider artistic elements like composition, lighting, and mood",
    ],
)

image_transformer = Agent(
    name="Image Transformer",
    role="Transform images using AI tools",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[FalTools()],
    instructions=[
        "Use the `image_to_image` tool to generate transformed images",
        "Apply the recommended styles and transformations",
        "Return the image URL as provided without markdown conversion",
    ],
)

---

## Example 1: Basic NanoBanana agent with default settings

**URL:** llms-txt#example-1:-basic-nanobanana-agent-with-default-settings

agent = Agent(tools=[NanoBananaTools()], name="NanoBanana Image Generator")

---

## Create agents with tools that use workflow session state

**URL:** llms-txt#create-agents-with-tools-that-use-workflow-session-state

shopping_assistant = Agent(
    name="Shopping Assistant",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[add_item, remove_item, list_items],
    instructions=[
        "You are a helpful shopping assistant.",
        "You can help users manage their shopping list by adding, removing, and listing items.",
        "Always use the provided tools to interact with the shopping list.",
        "Be friendly and helpful in your responses.",
    ],
)

list_manager = Agent(
    name="List Manager",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[list_items, remove_all_items],
    instructions=[
        "You are a list management specialist.",
        "You can view the current shopping list and clear it when needed.",
        "Always show the current list when asked.",
        "Confirm actions clearly to the user.",
    ],
)

---

## Run agent and return the response as a variable

**URL:** llms-txt#run-agent-and-return-the-response-as-a-variable

response: RunOutput = agent.run("Trending startups and products.")

---

## Example usage with Knowledge

**URL:** llms-txt#example-usage-with-knowledge

**Contents:**
- Usage

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="jina_embeddings",
        embedder=JinaEmbedder(
            late_chunking=True,  # Better handling of long documents
            timeout=30.0,  # Configure request timeout
        ),
    ),
    max_results=2,
)

bash  theme={null}
    export JINA_API_KEY=xxx
    bash  theme={null}
    pip install -U sqlalchemy psycopg pgvector aiohttp requests agno
    bash  theme={null}
    docker run -d \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -e PGDATA=/var/lib/postgresql/data/pgdata \
      -v pgvolume:/var/lib/postgresql/data \
      -p 5532:5432 \
      --name pgvector \
      agnohq/pgvector:16
    bash Mac theme={null}
      python cookbook/knowledge/embedders/jina_embedder.py
      bash Windows theme={null}
      python cookbook/knowledge/embedders/jina_embedder.py 
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run PgVector">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Agent sessions stored in memory

**URL:** llms-txt#agent-sessions-stored-in-memory

**Contents:**
- Developer Resources

agent.print_response("Give me an easy dinner recipe")
```

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/in_memory/in_memory_storage_for_agent.py)

---

## -*- Build container environment

**URL:** llms-txt#-*--build-container-environment

**Contents:**
  - Update the ECS Task Definition
  - Update the ECS Service
- Manually migrate prodution database
- How the migrations directory was created

container_env = {
    ...
    # Migrate database on startup using alembic
    "MIGRATE_DB": ws_settings.prd_db_enabled,
}
...
bash terminal theme={null}
  ag infra patch --env prd --infra aws --name td
  bash shorthand theme={null}
  ag infra patch -e prd -i aws -n td
  bash terminal theme={null}
  ag infra patch --env prd --infra aws --name service
  bash shorthand theme={null}
  ag infra patch -e prd -i aws -n service
  bash  theme={null}
ECS_CLUSTER=ai-app-prd-cluster
TASK_ARN=$(aws ecs list-tasks --cluster ai-app-prd-cluster --query "taskArns[0]" --output text)
CONTAINER_NAME=ai-api-prd

aws ecs execute-command --cluster $ECS_CLUSTER \
    --task $TASK_ARN \
    --container $CONTAINER_NAME \
    --interactive \
    --command "alembic -c db/alembic.ini upgrade head"
bash  theme={null}
docker exec -it ai-api cd db && alembic init migrations
python db/migrations/env.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Update the ECS Task Definition

Because we updated the Environment Variables, we need to update the Task Definition:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

### Update the ECS Service

After updating the task definition, redeploy the production application:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Arxiv

**URL:** llms-txt#arxiv

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/arxiv

**ArxivTools** enable an Agent to search for publications on Arxiv.

The following example requires the `arxiv` and `pypdf` libraries.

The following agent will run seach arXiv for "language models" and print the response.

| Parameter                  | Type   | Default | Description                                                        |
| -------------------------- | ------ | ------- | ------------------------------------------------------------------ |
| `enable_search_arxiv`      | `bool` | `True`  | Enables the functionality to search the arXiv database.            |
| `enable_read_arxiv_papers` | `bool` | `True`  | Allows reading of arXiv papers directly.                           |
| `download_dir`             | `Path` | -       | Specifies the directory path where downloaded files will be saved. |

| Function                                 | Description                                                                                        |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `search_arxiv_and_update_knowledge_base` | This function searches arXiv for a topic, adds the results to the knowledge base and returns them. |
| `search_arxiv`                           | Searches arXiv for a query.                                                                        |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/arxiv.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/arxiv_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will run seach arXiv for "language models" and print the response.
```

---

## Agent with URL Context and Search

**URL:** llms-txt#agent-with-url-context-and-search

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/url-context-with-search

```python cookbook/models/google/gemini/url_context_with_search.py theme={null}
"""Combine URL context with Google Search for comprehensive web analysis.

from agno.agent import Agent
from agno.models.google import Gemini

---

## Basic Streaming

**URL:** llms-txt#basic-streaming

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/cloud/azure-openai/usage/basic-stream

```python cookbook/models/azure/openai/basic_stream.py theme={null}
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.azure import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="gpt-5-mini"), markdown=True)

---

## Open the file once in append-binary mode

**URL:** llms-txt#open-the-file-once-in-append-binary-mode

**Contents:**
- Usage

with wave.open(str(filename), "wb") as wav_file:
    wav_file.setnchannels(CHANNELS)
    wav_file.setsampwidth(SAMPLE_WIDTH)
    wav_file.setframerate(SAMPLE_RATE)

# Iterate over generated audio
    for response in output_stream:
        response_audio = response.response_audio  # type: ignore
        if response_audio:
            if response_audio.transcript:
                print(response_audio.transcript, end="", flush=True)
            if response_audio.content:
                try:
                    pcm_bytes = base64.b64decode(response_audio.content)
                    wav_file.writeframes(pcm_bytes)
                except Exception as e:
                    print(f"Error decoding audio: {e}")
print()
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/agent_basics/multimodal/audio_streaming.py
      bash Windows theme={null}
      python cookbook/agent_basics/multimodal/audio_streaming.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Steps

**URL:** llms-txt#steps

Source: https://docs.agno.com/reference/workflows/steps-step

| Parameter     | Type                  | Default | Description                                                        |
| ------------- | --------------------- | ------- | ------------------------------------------------------------------ |
| `name`        | `Optional[str]`       | `None`  | Name of the steps group for identification                         |
| `description` | `Optional[str]`       | `None`  | Description of the steps group's purpose                           |
| `steps`       | `Optional[List[Any]]` | `[]`    | List of steps to execute sequentially (empty list if not provided) |

---

## Create agent with workflow tools

**URL:** llms-txt#create-agent-with-workflow-tools

**Contents:**
- Common Pattern: Think â†’ Act â†’ Analyze
- Choosing the Right Reasoning Toolkit
- Combining Multiple Reasoning Toolkits
- Configuration Options
  - Enable/Disable Specific Tools

orchestrator = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[WorkflowTools(workflow=research_workflow, add_instructions=True)],
)

orchestrator.print_response(
    "Research climate change impacts on agriculture",
    stream=True,
)
python  theme={null}
from agno.agent import Agent
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.models.openai import OpenAIChat
from agno.tools.knowledge import KnowledgeTools
from agno.tools.memory import MemoryTools
from agno.tools.reasoning import ReasoningTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        ReasoningTools(add_instructions=True),
        KnowledgeTools(
            knowledge=my_knowledge,
            enable_think=False,
            enable_analyze=False,
            add_instructions=False,
        ),
        MemoryTools(
            db=my_db,
            enable_think=False,
            enable_analyze=False,
            add_instructions=False,
        ),
    ],
    instructions="Use reasoning for planning, knowledge for facts, and memory for personalization",
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**How it works:**

1. Agent calls `think()`: "I need to run the research workflow with 'climate change agriculture' as input."
2. Agent calls `run_workflow(input_data="climate change impacts on agriculture")`
3. Workflow executes all steps (search â†’ summarize â†’ fact-check)
4. Agent calls `analyze()`: "Workflow completed successfully. All fact-checks passed."
5. Agent provides final synthesized answer

## Common Pattern: Think â†’ Act â†’ Analyze

All four toolkits follow the same reasoning cycle:

1. **THINK** - Plan what to do, refine approach, brainstorm
2. **ACT** (Domain-Specific)
   * ReasoningTools: Direct reasoning
   * KnowledgeTools: `search_knowledge()`
   * MemoryTools: `get/add/update/delete_memory()`
   * WorkflowTools: `run_workflow()`
3. **ANALYZE** - Evaluate results, decide next action
4. **REPEAT** - Loop back to THINK if needed, or provide answer

This mirrors how humans solve complex problems: we think before acting, evaluate results, and adjust our approach based on what we learn.

## Choosing the Right Reasoning Toolkit

| If you need to...                    | Use                   | Example                                                |
| ------------------------------------ | --------------------- | ------------------------------------------------------ |
| Solve logic puzzles or math problems | `ReasoningTools`      | "Solve: If xÂ² + 5x + 6 = 0, what is x?"                |
| Search through documents             | `KnowledgeTools`      | "Find all mentions of user authentication in our docs" |
| Remember user preferences            | `MemoryTools`         | "Remember that I'm allergic to shellfish"              |
| Orchestrate complex multi-step tasks | `WorkflowTools`       | "Research, write, and fact-check an article"           |
| Combine multiple domains             | Use multiple toolkits | See examples for more patterns                         |

## Combining Multiple Reasoning Toolkits

You can use multiple reasoning toolkits together for powerful multi-domain reasoning. Just remember that tool names must stay unique, so disable overlapping `think`/`analyze` entries (or rename the later ones) to prevent silent overrides:
```

Example 2 (unknown):
```unknown
With this setup:

* `ReasoningTools` supplies the shared `think`/`analyze` scratchpad.
* `KnowledgeTools` still exposes `search_knowledge()` (and any other unique methods) without trying to register duplicate scratchpad functions.
* `MemoryTools` contributes the CRUD memory tools while inheriting the same central thinking loop.

If you need separate scratchpads per domain, create custom wrappers around `think()`/`analyze()` so each toolkit registers uniquely named functions (e.g., `knowledge_think`, `memory_analyze`).

## Configuration Options

### Enable/Disable Specific Tools

You can control which reasoning tools are available:
```

---

## Initialize and connect to the SSE MCP server

**URL:** llms-txt#initialize-and-connect-to-the-sse-mcp-server

mcp_tools = MCPTools(url=server_url, transport="sse")
await mcp_tools.connect()

try:
    agent = Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[mcp_tools])
    await agent.aprint_response("What is the license for this project?", stream=True)
finally:
    # Always close the connection when done
    await mcp_tools.close()
python  theme={null}
from agno.tools.mcp import MCPTools, SSEClientParams

server_params = SSEClientParams(
    url=...,
    headers=...,
    timeout=...,
    sse_read_timeout=...,
)

**Examples:**

Example 1 (unknown):
```unknown
You can also use the `server_params` argument to define the MCP connection. This way you can specify the headers to send to the MCP server with every request, and the timeout values:
```

---

## (Optional) Set up your Cassandra DB

**URL:** llms-txt#(optional)-set-up-your-cassandra-db

**Contents:**
- Cassandra Params
- Developer Resources

session = cluster.connect()
session.execute(
    """
    CREATE KEYSPACE IF NOT EXISTS testkeyspace
    WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
    """
)

knowledge_base = Knowledge(
    vector_db=Cassandra(table_name="recipes", keyspace="testkeyspace", session=session, embedder=MistralEmbedder()),
)

knowledge_base.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

agent = Agent(
    model=MistralChat(provider="mistral-large-latest", api_key=os.getenv("MISTRAL_API_KEY")),
    knowledge=knowledge_base,
)

agent.print_response(
    "What are the health benefits of Khao Niew Dam Piek Maphrao Awn?", markdown=True, show_full_reasoning=True
)
python async_cassandra.py theme={null}
    import asyncio

from agno.agent import Agent
    from agno.knowledge.embedder.mistral import MistralEmbedder
    from agno.knowledge.knowledge import Knowledge
    from agno.models.mistral import MistralChat
    from agno.vectordb.cassandra import Cassandra

try:
        from cassandra.cluster import Cluster  # type: ignore
    except (ImportError, ModuleNotFoundError):
        raise ImportError(
            "Could not import cassandra-driver python package.Please install it with pip install cassandra-driver."
        )

session = cluster.connect()
    session.execute(
        """
        CREATE KEYSPACE IF NOT EXISTS testkeyspace
        WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
        """
    )

knowledge_base = Knowledge(
        vector_db=Cassandra(
            table_name="recipes",
            keyspace="testkeyspace",
            session=session,
            embedder=MistralEmbedder(),
        ),
    )

agent = Agent(
        model=MistralChat(),
        knowledge=knowledge_base,
    )

if __name__ == "__main__":
        asyncio.run(knowledge_base.add_content_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
            )
        )

# Create and use the agent
        asyncio.run(
            agent.aprint_response(
                "What are the health benefits of Khao Niew Dam Piek Maphrao Awn?",
                markdown=True,
            )
        )
    ```

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_cassandra_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/cassandra_db/cassandra_db.py)
* View [Cookbook (Async)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/cassandra_db/async_cassandra_db.py)

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      Cassandra also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Reasoning Team

**URL:** llms-txt#reasoning-team

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code
- What to Expect
- Usage
- Next Steps

Source: https://docs.agno.com/examples/use-cases/teams/reasoning_team

Build a reasoning team that handles complex queries by combining web search, financial data, and transparent analytical thinking. This example demonstrates how reasoning tools make the team's decision-making process visible.

By building this team, you'll understand:

* How to integrate reasoning tools for transparent analytical thinking
* How to combine multiple specialized agents with different data sources
* How to use domain-specific search with Exa for financial information
* How to stream reasoning events to see the team's thought process in real-time

Build financial research platforms, investment analysis tools, market intelligence systems, or decision support applications.

The team coordinates specialized agents with transparent reasoning:

1. **Analyze**: Team leader uses reasoning tools to plan the approach
2. **Search**: Web agent finds general information using DuckDuckGo
3. **Research**: Finance agent retrieves financial data from trusted sources
4. **Synthesize**: Team combines findings with visible reasoning process
5. **Present**: Outputs structured data with supporting logic

Reasoning tools provide transparency into how the team thinks through complex queries.

The team will research companies and stock prices by coordinating between web search and financial data agents. You'll see the reasoning process as the team plans its approach, delegates tasks, and synthesizes information.

The output includes visible thinking through reasoning tools, showing how the team decides which agent to use and how to combine their findings. Financial data is presented in tables with sources from trusted financial news outlets.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* Modify the query to research different types of companies or markets
* Adjust `include_domains` in ExaTools to focus on specific financial sources
* Toggle `show_full_reasoning` to control visibility of the thinking process
* Explore [Reasoning Tools](/basics/tools/reasoning_tools/reasoning-tools) for advanced analytical capabilities

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The team will research companies and stock prices by coordinating between web search and financial data agents. You'll see the reasoning process as the team plans its approach, delegates tasks, and synthesizes information.

The output includes visible thinking through reasoning tools, showing how the team decides which agent to use and how to combine their findings. Financial data is presented in tables with sources from trusted financial news outlets.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Team">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## RunContext

**URL:** llms-txt#runcontext

**Contents:**
- RunContext Attributes

Source: https://docs.agno.com/reference/run/run-context

The `RunContext` is an object that can be referenced in pre- and post-hooks, tools, and other parts of the run.

See [Agent State](/basics/state/agent) for examples of how to use the `RunContext` in your code.

## RunContext Attributes

| Attribute           | Type             | Description                      |
| ------------------- | ---------------- | -------------------------------- |
| `run_id`            | `str`            | Run ID                           |
| `session_id`        | `str`            | Session ID for the run           |
| `user_id`           | `Optional[str]`  | User ID associated with the run  |
| `dependencies`      | `Dict[str, Any]` | Dependencies for the run         |
| `knowledge_filters` | `Dict[str, Any]` | Knowledge filters for the run    |
| `metadata`          | `Dict[str, Any]` | Metadata associated with the run |
| `session_state`     | `Dict[str, Any]` | Session state for the run        |

---

## - Create a new index in Upstash Console with the correct dimension

**URL:** llms-txt#--create-a-new-index-in-upstash-console-with-the-correct-dimension

---

## Define research team

**URL:** llms-txt#define-research-team

research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions="Plan a content schedule based on research",
)

---

## Create and post a tweet

**URL:** llms-txt#create-and-post-a-tweet

agent.print_response("Create a post about AI ethics", markdown=True)

---

## Check optimized memories

**URL:** llms-txt#check-optimized-memories

print("\nAfter optimization:")
memories_after = agent.get_user_memories(user_id=user_id)
print(f"  Memory count: {len(memories_after)}")

---

## JWT Middleware with Cookies

**URL:** llms-txt#jwt-middleware-with-cookies

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/middleware/jwt-cookies

AgentOS with JWT middleware using HTTP-only cookies for secure web authentication

This example demonstrates how to use JWT middleware with AgentOS using HTTP-only cookies instead of Authorization headers.
This approach is more secure for web applications as it prevents XSS attacks.

```python jwt_cookies.py theme={null}
from datetime import UTC, datetime, timedelta

import jwt
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.os.middleware import JWTMiddleware
from agno.os.middleware.jwt import TokenSource
from fastapi import FastAPI, Response

---

## Initialize the AgentOS with the workflows

**URL:** llms-txt#initialize-the-agentos-with-the-workflows

agent_os = AgentOS(
    description="Example OS setup",
    workflows=[streaming_content_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="workflow_with_custom_function_stream:app", reload=True)
```

---

## Image Editing Agent

**URL:** llms-txt#image-editing-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/image-editing

```python cookbook/models/google/gemini/image_editing.py theme={null}
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.media import Image
from agno.models.google import Gemini
from PIL import Image as PILImage

---

## Async MongoDB for Agent

**URL:** llms-txt#async-mongodb-for-agent

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/integrations/database/async-mongo/usage/async-mongodb-for-agent

Agno supports using MongoDB asynchronously as a storage backend for Agents, with the `AsyncMongoDb` class.

You need to provide either `db_url` or `client`. The following example uses `db_url`.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python async_mongodb_for_agent.py theme={null}
"""
Run `pip install agno openai motor pymongo` to install dependencies.
"""
from agno.agent import Agent
from agno.db.mongo import AsyncMongoDb
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Memory Tools

**URL:** llms-txt#memory-tools

**Contents:**
- Example

Source: https://docs.agno.com/basics/tools/reasoning_tools/memory-tools

The `MemoryTools` toolkit enables Agents to manage user memories through create, update, and delete operations. This toolkit integrates with a provided database where memories are stored.

The toolkit implements a "Think â†’ Operate â†’ Analyze" cycle that allows an Agent to:

1. Think through memory management requirements and plan operations
2. Execute memory operations (add, update, delete) on the database
3. Analyze the results to ensure operations completed successfully and meet requirements

This approach gives Agents the ability to persistently store, retrieve, and manage user information, preferences, and context across conversations.

The toolkit includes the following tools:

* `think`: A scratchpad for planning memory operations, brainstorming content, and refining approaches. These thoughts remain internal to the Agent and are not shown to users.
* `get_memories`: Gets a list of memories for the current user from the database.
* `add_memory`: Creates new memories in the database with specified content and optional topics.
* `update_memory`: Modifies existing memories by memory ID, allowing updates to content and topics.
* `delete_memory`: Removes memories from the database by memory ID.
* `analyze`: Evaluates whether memory operations completed successfully and produced the expected results.

Here's an example of how to use the `MemoryTools` toolkit:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.memory import MemoryTools

---

## Analyze individual member metrics

**URL:** llms-txt#analyze-individual-member-metrics

**Contents:**
- Usage

print("=" * 50)
print("TEAM MEMBER MESSAGE METRICS")
print("=" * 50)

if run_output.member_responses:
    for member_response in run_output.member_responses:
        if member_response.messages:
            for message in member_response.messages:
                if message.role == "assistant":
                    if message.content:
                        print(f"ðŸ“ Member Message: {message.content[:100]}...")
                    elif message.tool_calls:
                        print(f"ðŸ”§ Member Tool calls: {message.tool_calls}")

print("-" * 20, "Member Metrics", "-" * 20)
                    pprint(message.metrics)
                    print("-" * 60)
bash  theme={null}
    pip install agno exa_py rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    cookbook/run_pgvector.sh
    bash  theme={null}
    python cookbook/examples/teams/metrics/01_team_metrics.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Example 1: Sleep for 2 seconds

**URL:** llms-txt#example-1:-sleep-for-2-seconds

agent.print_response("Sleep for 2 seconds")

---

## Team Performance with Memory

**URL:** llms-txt#team-performance-with-memory

Source: https://docs.agno.com/basics/evals/performance/usage/performance-team-with-memory

Example showing how to evaluate team performance with memory tracking and growth monitoring.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Check content processing status

**URL:** llms-txt#check-content-processing-status

content_list, total_count = knowledge.get_content()

failed = [c for c in content_list if c.status == "failed"]
if failed:
    print(f"Failed items: {len(failed)}")
    for content in failed:
        status, message = knowledge.get_content_status(content.id)
        print(f"  {content.name}: {message}")

---

## When initializing the knowledge base, we can attach metadata that will be used for filtering

**URL:** llms-txt#when-initializing-the-knowledge-base,-we-can-attach-metadata-that-will-be-used-for-filtering

---

## Async PostgreSQL

**URL:** llms-txt#async-postgresql

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/async-postgres/overview

Learn to use PostgreSQL asynchronously as a database for your Agents

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

```python postgres_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import AsyncPostgresDb

---

## - Complex memory reasoning within the conversation flow

**URL:** llms-txt#--complex-memory-reasoning-within-the-conversation-flow

**Contents:**
- Mitigation Strategy #2: Use a Cheaper Model for Memory Operations

python  theme={null}
from agno.memory import MemoryManager
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
## Mitigation Strategy #2: Use a Cheaper Model for Memory Operations

If you do need agentic memory, use a less expensive model for memory management while keeping a powerful model for conversation:
```

---

## Milvus Hybrid Search

**URL:** llms-txt#milvus-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/milvus/usage/milvus-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Workflow Run Cancellation

**URL:** llms-txt#workflow-run-cancellation

**Contents:**
- Example
- API Endpoint

Source: https://docs.agno.com/execution-control/run-cancellation/workflow-cancel-run

Learn how to cancel a running workflow execution by starting a workflow run in a separate thread and cancelling it from another thread.

This example demonstrates how to cancel a running workflow execution by starting a workflow run in a separate thread and cancelling it from another thread. It shows proper handling of cancelled responses and thread management.

Workflow runs can be cancelled via the AgentOS API:

**Reference:** [Cancel Workflow Run API](/reference-api/schema/workflows/cancel-workflow-run)

**Examples:**

Example 1 (unknown):
```unknown
## API Endpoint

Workflow runs can be cancelled via the AgentOS API:
```

Example 2 (unknown):
```unknown
**Example:**
```

---

## File

**URL:** llms-txt#file

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/local/file

The FileTools toolkit enables Agents to read and write files on the local file system.

The following agent will generate an answer and save it in a file.

| Parameter                   | Type   | Default    | Description                                                                                |
| --------------------------- | ------ | ---------- | ------------------------------------------------------------------------------------------ |
| `base_dir`                  | `Path` | `None`     | Specifies the base directory path for file operations                                      |
| `enable_save_file`          | `bool` | `True`     | Enables functionality to save files                                                        |
| `enable_delete_file`        | `bool` | `False`    | Enables functionality to delete files                                                      |
| `enable_read_file`          | `bool` | `True`     | Enables functionality to read files                                                        |
| `enable_read_file_chunks`   | `bool` | `True`     | Enables functionality to read files in chunks                                              |
| `enable_replace_file_chunk` | `bool` | `True`     | Enables functionality to update files in chunks                                            |
| `enable_list_files`         | `bool` | `True`     | Enables functionality to list files in directories                                         |
| `enable_search_files`       | `bool` | `True`     | Enables functionality to search for files                                                  |
| `all`                       | `bool` | `False`    | Enables all functionality when set to True                                                 |
| `expose_base_directory`     | `bool` | `False`    | Adds 'base\_directory' to the tool responses if set to True                                |
| `max_file_length`           | `int`  | `10000000` | Maximum file length to read in bytes. Reading will fail if the file is larger.             |
| `max_file_lines`            | `int`  | `100000`   | Maximum number of lines to read from a file. Reading will fail if the file has more lines. |
| `line_separator`            | `str`  | `"\n"`     | The separator to use when interacting with chunks.                                         |

| Name                 | Description                                                                                  |
| -------------------- | -------------------------------------------------------------------------------------------- |
| `save_file`          | Saves the contents to a file called `file_name` and returns the file name if successful.     |
| `read_file`          | Reads the contents of the file `file_name` and returns the contents if successful.           |
| `read_file_chunks`   | Reads the contents of the file `file_name` in chunks and returns the contents if successful. |
| `replace_file_chunk` | Partial replace of the contents of the file `file_name`                                      |
| `delete_file`        | Deletes the file `file_name` if successful.                                                  |
| `list_files`         | Returns a list of files in the base directory                                                |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/file.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/file_tools.py)

---

## Filter by single field

**URL:** llms-txt#filter-by-single-field

results = knowledge.search(
    "deployment process",
    filters={"department": "engineering"}
)

---

## LlamaIndex

**URL:** llms-txt#llamaindex

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/llamaindex/usage/llamaindex-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Milvus

**URL:** llms-txt#milvus

Source: https://docs.agno.com/reference/vector-db/milvus

<Snippet file="vector-db-milvus-reference.mdx" />

---

## WorkflowSession

**URL:** llms-txt#workflowsession

**Contents:**
- WorkflowSession Attributes
- WorkflowSession Methods
  - `upsert_run(run: WorkflowRunOutput)`
  - `get_run(run_id: str) -> Optional[WorkflowRunOutput]`
  - `get_workflow_history(num_runs: Optional[int] = None) -> List[Tuple[str, str]]`
  - `get_workflow_history_context(num_runs: Optional[int] = None) -> Optional[str]`
  - `get_messages(...) -> List[Message]`
  - `get_chat_history(last_n_runs: Optional[int] = None) -> List[WorkflowChatInteraction]`
  - `to_dict() -> Dict[str, Any]`
  - `from_dict(data: Mapping[str, Any]) -> Optional[WorkflowSession]`

Source: https://docs.agno.com/reference/workflows/session

## WorkflowSession Attributes

| Parameter       | Type                                | Default  | Description                                                                    |
| --------------- | ----------------------------------- | -------- | ------------------------------------------------------------------------------ |
| `session_id`    | `str`                               | Required | Session UUID - this is the workflow\_session\_id that gets set on agents/teams |
| `user_id`       | `Optional[str]`                     | `None`   | ID of the user interacting with this workflow                                  |
| `workflow_id`   | `Optional[str]`                     | `None`   | ID of the workflow that this session is associated with                        |
| `workflow_name` | `Optional[str]`                     | `None`   | Workflow name                                                                  |
| `runs`          | `Optional[List[WorkflowRunOutput]]` | `None`   | List of all workflow runs in the session                                       |
| `session_data`  | `Optional[Dict[str, Any]]`          | `None`   | Session Data: session\_name, session\_state, images, videos, audio             |
| `workflow_data` | `Optional[Dict[str, Any]]`          | `None`   | Workflow configuration and metadata                                            |
| `metadata`      | `Optional[Dict[str, Any]]`          | `None`   | Metadata stored with this workflow session                                     |
| `created_at`    | `Optional[int]`                     | `None`   | The unix timestamp when this session was created                               |
| `updated_at`    | `Optional[int]`                     | `None`   | The unix timestamp when this session was last updated                          |

## WorkflowSession Methods

### `upsert_run(run: WorkflowRunOutput)`

Adds a WorkflowRunOutput to the runs list. If a run with the same `run_id` already exists, it updates the existing run.

### `get_run(run_id: str) -> Optional[WorkflowRunOutput]`

Retrieves a specific workflow run by its `run_id`.

### `get_workflow_history(num_runs: Optional[int] = None) -> List[Tuple[str, str]]`

Gets workflow history as structured data (input, response pairs).

* `num_runs`: Number of recent runs to include. If None, returns all available history.

Returns a list of tuples containing (input, response) pairs from completed workflow runs only.

### `get_workflow_history_context(num_runs: Optional[int] = None) -> Optional[str]`

Gets formatted workflow history context for steps.

* `num_runs`: Number of recent runs to include. If None, returns all available history.

Returns a formatted string containing the workflow history wrapped in `<workflow_history_context>` tags, suitable for providing context to workflow steps.

### `get_messages(...) -> List[Message]`

Returns the messages belonging to the session that fit the given criteria. Note: Either `agent_id` or `team_id` must be provided, but not both.

* `agent_id` (Optional\[str]): The ID of the agent to get the messages for
* `team_id` (Optional\[str]): The ID of the team to get the messages for
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs
* `limit` (Optional\[int]): The number of messages to return, counting from the latest. Defaults to all messages
* `skip_roles` (Optional\[List\[str]]): Skip messages with these roles
* `skip_statuses` (Optional\[List\[RunStatus]]): Skip messages with these statuses
* `skip_history_messages` (bool): Skip messages that were tagged as history in previous runs. Defaults to True
* `skip_member_messages` (bool): Skip messages created by members of the team. Defaults to True

* `List[Message]`: The messages for the session

### `get_chat_history(last_n_runs: Optional[int] = None) -> List[WorkflowChatInteraction]`

Return a list of WorkflowChatInteraction objects containing the input and output for each run in the session.

* `last_n_runs` (Optional\[int]): Number of recent runs to include. If None, all runs will be considered

* `List[WorkflowChatInteraction]`: The chat history for the session as a list of input/output interactions

### `to_dict() -> Dict[str, Any]`

Converts the WorkflowSession to a dictionary for storage, serializing runs to dictionaries.

### `from_dict(data: Mapping[str, Any]) -> Optional[WorkflowSession]`

Creates a WorkflowSession from a dictionary, deserializing runs from dictionaries back to WorkflowRunOutput objects.

---

## Code Generation

**URL:** llms-txt#code-generation

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/vllm/usage/code-generation

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Libraries">
    
  </Step>

<Step title="Start vLLM server">
    
  </Step>

<Step title="Run Agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start vLLM server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Pre-hooks

**URL:** llms-txt#pre-hooks

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/hooks/pre-hooks

Running a pre-hook is handled automatically during the Agent or Team run. These are the parameters that will be injected:

| Parameter       | Type                       | Default  | Description                                                                  |
| --------------- | -------------------------- | -------- | ---------------------------------------------------------------------------- |
| `agent`         | `Agent`                    | Required | The Agent that is running the pre-hook. Only present in Agent runs.          |
| `team`          | `Team`                     | Required | The Team that is running the pre-hook. Only present in Team runs.            |
| `run_input`     | `RunInput`                 | Required | The input provided to the Agent or Team when invoking the run.               |
| `session`       | `AgentSession`             | Required | The `AgentSession` or `TeamSession` object representing the current session. |
| `session_state` | `Optional[Dict[str, Any]]` | `None`   | The session state of the current session.                                    |
| `dependencies`  | `Optional[Dict[str, Any]]` | `None`   | The dependencies of the current run.                                         |
| `metadata`      | `Optional[Dict[str, Any]]` | `None`   | The metadata of the current run.                                             |
| `user_id`       | `Optional[str]`            | `None`   | The contextual user ID, if any.                                              |
| `debug_mode`    | `Optional[bool]`           | `None`   | Whether the debug mode is enabled.                                           |

---

## Use Cases

**URL:** llms-txt#use-cases

**Contents:**
- Getting Started
- Use Cases

Source: https://docs.agno.com/examples/use-cases/overview

Explore Agno's use cases showcasing everything from single-agent tasks to sophisticated multi-agent workflows.

Welcome to Agno's use cases! Here you'll discover use cases showcasing everything from single-agent tasks to sophisticated multi-agent workflows. You can either:

* Run the examples individually
* Clone the entire [Agno cookbook](https://github.com/agno-agi/agno/tree/main/cookbook)

Have an interesting example to share? Please consider [contributing](https://github.com/agno-agi/agno-docs) to our growing collection.

If you're just getting started, follow the [Getting Started](/examples/getting-started) guide for a step-by-step tutorial. The examples build on each other, introducing new concepts and capabilities progressively.

Build real-world applications with Agno.

<CardGroup cols={3}>
  <Card title="Simple Agents" icon="user-astronaut" iconType="duotone" href="/examples/use-cases/agents">
    Simple agents for web scraping, data processing, financial analysis, etc.
  </Card>

<Card title="Multi-Agent Teams" icon="people-group" iconType="duotone" href="/examples/use-cases/teams/">
    Multi-agent teams that collaborate to solve tasks.
  </Card>

<Card title="Advanced Workflows" icon="diagram-project" iconType="duotone" href="/examples/use-cases/workflows/">
    Advanced workflows for creating blog posts, investment reports, etc.
  </Card>
</CardGroup>

---

## DuckDuckGo

**URL:** llms-txt#duckduckgo

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/duckduckgo

**DuckDuckGo** enables an Agent to search the web for information.

The following example requires the `ddgs` library. To install DuckDuckGo, run the following command:

| Parameter           | Type            | Default | Description                                           |
| ------------------- | --------------- | ------- | ----------------------------------------------------- |
| `enable_search`     | `bool`          | `True`  | Enable DuckDuckGo search function.                    |
| `enable_news`       | `bool`          | `True`  | Enable DuckDuckGo news function.                      |
| `all`               | `bool`          | `False` | Enable all available functions in the toolkit.        |
| `modifier`          | `Optional[str]` | `None`  | A modifier to be used in the search request.          |
| `fixed_max_results` | `Optional[int]` | `None`  | A fixed number of maximum results.                    |
| `proxy`             | `Optional[str]` | `None`  | Proxy to be used in the search request.               |
| `timeout`           | `Optional[int]` | `10`    | The maximum number of seconds to wait for a response. |
| `verify_ssl`        | `bool`          | `True`  | Whether to verify SSL certificates.                   |

| Function            | Description                                                                                                                                                                             |
| ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `duckduckgo_search` | Search DuckDuckGo for a query. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum results. Returns JSON formatted search results.     |
| `duckduckgo_news`   | Get the latest news from DuckDuckGo. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum results. Returns JSON formatted news results. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/duckduckgo.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/duckduckgo_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Print workflow-level metrics

**URL:** llms-txt#print-workflow-level-metrics

print("Workflow Metrics")
if response.metrics:
    pprint(response.metrics.to_dict())

---

## Agent gets only hotel management tools - focused!

**URL:** llms-txt#agent-gets-only-hotel-management-tools---focused!

**Contents:**
- Advanced Usage
  - Multiple Toolsets
  - Custom Authentication and Parameters
  - Manual Connection Management
- Toolkit Params
- Toolkit Functions
- Demo Examples
- Developer Resources

tools = MCPToolbox(url="http://127.0.0.1:5001", toolsets=["hotel-management"])  # 3 tools
python cookbook/tools/mcp/mcp_toolbox_for_db.py theme={null}
import asyncio
from textwrap import dedent
from agno.agent import Agent
from agno.tools.mcp_toolbox import MCPToolbox

url = "http://127.0.0.1:5001"

async def run_agent(message: str = None) -> None:
    """Run an interactive CLI for the Hotel agent with the given message."""

async with MCPToolbox(
        url=url, toolsets=["hotel-management", "booking-system"]
    ) as db_tools:
        print(db_tools.functions)  # Print available tools for debugging
        agent = Agent(
            tools=[db_tools],
            instructions=dedent(
                """ \
                You're a helpful hotel assistant. You handle hotel searching, booking and
                cancellations. When the user searches for a hotel, mention it's name, id,
                location and price tier. Always mention hotel ids while performing any
                searches. This is very important for any operations. For any bookings or
                cancellations, please provide the appropriate confirmation. Be sure to
                update checkin or checkout dates if mentioned by the user.
                Don't ask for confirmations from the user.
            """
            ),
            markdown=True,
            show_tool_calls=True,
            add_history_to_messages=True,
            debug_mode=True,
        )

await agent.acli_app(message=message, stream=True)

if __name__ == "__main__":
    asyncio.run(run_agent(message=None))
python  theme={null}
async def production_example():
    async with MCPToolbox(url=url) as toolbox:
        # Load with authentication and bound parameters
        hotel_tools = await toolbox.load_toolset(
            "hotel-management",
            auth_token_getters={"hotel_api": lambda: "your-hotel-api-key"},
            bound_params={"region": "us-east-1"},
        )

booking_tools = await toolbox.load_toolset(
            "booking-system",
            auth_token_getters={"booking_api": lambda: "your-booking-api-key"},
            bound_params={"environment": "production"},
        )

# Use individual tools instead of the toolbox
        all_tools = hotel_tools + booking_tools[:2]  # First 2 booking tools only
        
        agent = Agent(tools=all_tools, instructions="Hotel management with auth.")
        await agent.aprint_response("Book a hotel for tonight")
python  theme={null}
async def manual_connection_example():
    # Initialize without auto-connection
    toolbox = MCPToolbox(url=url, toolsets=["hotel-management"])
    
    try:
        await toolbox.connect()
        agent = Agent(
            tools=[toolbox],
            instructions="Hotel search assistant.",
            markdown=True
        )
        await agent.aprint_response("Show me hotels in Basel")
    finally:
        await toolbox.close()  # Always clean up
```

| Parameter   | Type                       | Default             | Description                                                                |
| ----------- | -------------------------- | ------------------- | -------------------------------------------------------------------------- |
| `url`       | `str`                      | -                   | Base URL for the toolbox service (automatically appends "/mcp" if missing) |
| `toolsets`  | `Optional[List[str]]`      | `None`              | List of toolset names to filter tools by. Cannot be used with `tool_name`. |
| `tool_name` | `Optional[str]`            | `None`              | Single tool name to load. Cannot be used with `toolsets`.                  |
| `headers`   | `Optional[Dict[str, Any]]` | `None`              | HTTP headers for toolbox client requests                                   |
| `transport` | `str`                      | `"streamable-http"` | MCP transport protocol. Options: `"stdio"`, `"sse"`, `"streamable-http"`   |

<Note>
  Only one of `toolsets` or `tool_name` can be specified. The implementation validates this and raises a `ValueError` if both are provided.
</Note>

| Function                                                                                            | Description                                                    |
| --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- |
| `async connect()`                                                                                   | Initialize and connect to both MCP server and toolbox client   |
| `async load_tool(tool_name, auth_token_getters={}, bound_params={})`                                | Load a single tool by name with optional authentication        |
| `async load_toolset(toolset_name, auth_token_getters={}, bound_params={}, strict=False)`            | Load all tools from a specific toolset                         |
| `async load_multiple_toolsets(toolset_names, auth_token_getters={}, bound_params={}, strict=False)` | Load tools from multiple toolsets                              |
| `async load_toolset_safe(toolset_name)`                                                             | Safely load a toolset and return tool names for error handling |
| `get_client()`                                                                                      | Get the underlying ToolboxClient instance                      |
| `async close()`                                                                                     | Close both toolbox client and MCP client connections           |

The complete demo includes multiple working patterns:

* **[Basic Agent](https://github.com/agno-agi/agno/blob/main/cookbook/tools/mcp/mcp_toolbox_demo/agent.py)**: Simple hotel assistant with toolset filtering
* **[AgentOS Integration](https://github.com/agno-agi/agno/blob/main/cookbook/tools/mcp/mcp_toolbox_demo/agent_os.py)**: Integration with AgentOS control plane
* **[Workflow Integration](https://github.com/agno-agi/agno/blob/main/cookbook/tools/mcp/mcp_toolbox_demo/hotel_management_workflows.py)**: Using MCPToolbox in Agno workflows
* **[Type-Safe Agent](https://github.com/agno-agi/agno/blob/main/cookbook/tools/mcp/mcp_toolbox_demo/hotel_management_typesafe.py)**: Implementation with Pydantic models

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/mcp_toolbox.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/mcp/mcp_toolbox_demo)

For more information about MCP Toolbox for Databases, visit the [official documentation](https://googleapis.github.io/genai-toolbox/getting-started/introduction/).

**Examples:**

Example 1 (unknown):
```unknown
**The flow:**

1. MCP Toolbox Server exposes 50+ database tools
2. MCPToolbox connects and loads ALL tools internally
3. Filters to only the `hotel-management` toolset (3 tools)
4. Agent sees only the 3 relevant tools and stays focused

## Advanced Usage

### Multiple Toolsets

Load tools from multiple related toolsets:
```

Example 2 (unknown):
```unknown
### Custom Authentication and Parameters

For production scenarios with authentication:
```

Example 3 (unknown):
```unknown
### Manual Connection Management

For explicit control over connections:
```

---

## Setup your Agent with the Database

**URL:** llms-txt#setup-your-agent-with-the-database

**Contents:**
- Params
- Developer Resources

agent = Agent(db=db)
```

<Snippet file="db-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/postgres/postgres_for_agent.py)

---

## 1. CUSTOMER SUPPORT WORKFLOW

**URL:** llms-txt#1.-customer-support-workflow

---

## Image Ocr With Structured Output

**URL:** llms-txt#image-ocr-with-structured-output

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/image-ocr-with-structured-output

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## -*- Secrets for production database

**URL:** llms-txt#-*--secrets-for-production-database

prd_db_secret = SecretsManager(
    ...
    # Create secret from workspace/secrets/prd_db_secrets.yml
    secret_files=[infra_settings.infra_root.joinpath("infra/secrets/prd_db_secrets.yml")],
)
python FastApi theme={null}
  prd_fastapi = FastApi(
      ...
      aws_secrets=[prd_secret],
      ...
  )
  python RDS theme={null}
  prd_db = DbInstance(
      ...
      aws_secret=prd_db_secret,
      ...
  )
  ```
</CodeGroup>

Production resources can also read secrets using yaml files but we highly recommend using [AWS Secrets](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html).

**Examples:**

Example 1 (unknown):
```unknown
Read the secret in production apps using:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Change State On Run

**URL:** llms-txt#change-state-on-run

Source: https://docs.agno.com/basics/state/agent/usage/change-state-on-run

This example demonstrates how to manage session state across different runs for different users. It shows how session state persists within the same session but is isolated between different sessions and users.

<Steps>
  <Step title="Create a Python file">
    Create a file called `change_state_on_run.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Distributed Search with Infinity Reranker

**URL:** llms-txt#distributed-search-with-infinity-reranker

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/distributed-infinity-search

This example demonstrates how multiple agents coordinate to perform distributed search using Infinity reranker for high-performance ranking across team members.

```python cookbook/examples/teams/search_coordination/03_distributed_infinity_search.py theme={null}
"""
This example demonstrates how multiple agents coordinate to perform distributed
search using Infinity reranker for high-performance ranking across team members.

Team Composition:
- Primary Searcher: Performs initial broad search with infinity reranking
- Secondary Searcher: Performs targeted search on specific topics
- Cross-Reference Validator: Validates information across different sources
- Result Synthesizer: Combines and ranks all results for final response

Setup:
1. Install dependencies: `pip install agno anthropic infinity-client lancedb`
2. Set up Infinity Server:
   \`\`\`bash
   pip install "infinity-emb[all]"
   infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
   \`\`\`
3. Export ANTHROPIC_API_KEY
4. Run this script
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker.infinity import InfinityReranker
from agno.models.anthropic import Claude
from agno.team.team import Team
from agno.vectordb.lancedb import LanceDb, SearchType

---

## 1. Create a new bot with BotFather on Telegram. https://core.telegram.org/bots/features#creating-a-new-bot

**URL:** llms-txt#1.-create-a-new-bot-with-botfather-on-telegram.-https://core.telegram.org/bots/features#creating-a-new-bot

---

## Replace with your own connection string, and notice the `async_` prefix

**URL:** llms-txt#replace-with-your-own-connection-string,-and-notice-the-`async_`-prefix

db_url = "postgresql+psycopg_async://ai:ai@localhost:5532/ai"

---

## Couchbase Async

**URL:** llms-txt#couchbase-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/couchbase/usage/async-couchbase-db

```python cookbook/knowledge/vector_db/couchbase_db/async_couchbase_db.py theme={null}
import asyncio
import os
import time

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.couchbase import CouchbaseSearch
from couchbase.auth import PasswordAuthenticator
from couchbase.management.search import SearchIndex
from couchbase.options import ClusterOptions, KnownConfigProfiles

---

## DynamoDB for Agent

**URL:** llms-txt#dynamodb-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/dynamodb/usage/dynamodb-for-agent

Agno supports using DynamoDB as a storage backend for Agents using the `DynamoDb` class.

You need to provide `aws_access_key_id` and `aws_secret_access_key` parameters to the `DynamoDb` class.

```python dynamo_for_agent.py theme={null}
from agno.db.dynamo import DynamoDb

---

## Agent that uses JSON mode

**URL:** llms-txt#agent-that-uses-json-mode

json_mode_agent = Agent(
    model=Perplexity(id="sonar-pro"),
    description="You write movie scripts.",
    output_schema=MovieScript,
    markdown=True,
)

---

## Condition and Parallel Steps Workflow

**URL:** llms-txt#condition-and-parallel-steps-workflow

Source: https://docs.agno.com/basics/workflows/usage/condition-and-parallel-steps-stream

This example demonstrates **Workflows 2.0** advanced pattern combining conditional execution with parallel processing.

This example shows how to create sophisticated workflows where multiple
conditions evaluate simultaneously, each potentially triggering different research strategies
based on comprehensive content analysis.

**When to use**: When you need comprehensive, multi-dimensional content analysis where
different aspects of the input may trigger different specialized research pipelines
simultaneously. Ideal for adaptive research workflows that can leverage multiple sources
based on various content characteristics.

```python condition_and_parallel_steps_stream.py theme={null}
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.condition import Condition
from agno.workflow.parallel import Parallel
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

---

## Zoom

**URL:** llms-txt#zoom

**Contents:**
- Prerequisites
- Example Usage

Source: https://docs.agno.com/integrations/toolkits/social/zoom

**Zoom** enables an Agent to interact with Zoom, allowing it to schedule meetings, manage recordings, and handle various meeting-related operations through the Zoom API. The toolkit uses Zoom's Server-to-Server OAuth authentication for secure API access.

The Zoom toolkit requires the following setup:

1. Install required dependencies:

2. Set up Server-to-Server OAuth app in Zoom Marketplace:

* Go to [Zoom Marketplace](https://marketplace.zoom.us/)
   * Click "Develop" â†’ "Build App"
   * Choose "Server-to-Server OAuth" app type
   * Configure the app with required scopes:
     * `/meeting:write:admin`
     * `/meeting:read:admin`
     * `/recording:read:admin`
   * Note your Account ID, Client ID, and Client Secret

3. Set up environment variables:

```python  theme={null}
from agno.agent import Agent
from agno.tools.zoom import ZoomTools

**Examples:**

Example 1 (unknown):
```unknown
2. Set up Server-to-Server OAuth app in Zoom Marketplace:

   * Go to [Zoom Marketplace](https://marketplace.zoom.us/)
   * Click "Develop" â†’ "Build App"
   * Choose "Server-to-Server OAuth" app type
   * Configure the app with required scopes:
     * `/meeting:write:admin`
     * `/meeting:read:admin`
     * `/recording:read:admin`
   * Note your Account ID, Client ID, and Client Secret

3. Set up environment variables:
```

Example 2 (unknown):
```unknown
## Example Usage
```

---

## Second call - will answer directly from history

**URL:** llms-txt#second-call---will-answer-directly-from-history

workflow.print_response(
    "What was Rocky's personality?", stream=True
)

---

## Brandfetch

**URL:** llms-txt#brandfetch

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/brandfetch

BrandfetchTools provides access to brand data and logo information through the Brandfetch API.

The following agent can search for brand information and retrieve brand data:

| Parameter                     | Type              | Default                          | Description                                                   |
| ----------------------------- | ----------------- | -------------------------------- | ------------------------------------------------------------- |
| `api_key`                     | `Optional[str]`   | `None`                           | Brandfetch API key. Uses BRANDFETCH\_API\_KEY if not set.     |
| `client_id`                   | `Optional[str]`   | `None`                           | Brandfetch Client ID for search. Uses BRANDFETCH\_CLIENT\_ID. |
| `base_url`                    | `str`             | `"https://api.brandfetch.io/v2"` | Brandfetch API base URL.                                      |
| `timeout`                     | `Optional[float]` | `20.0`                           | Request timeout in seconds.                                   |
| `enable_search_by_identifier` | `bool`            | `True`                           | Enable searching brands by domain/identifier.                 |
| `enable_search_by_brand`      | `bool`            | `False`                          | Enable searching brands by name.                              |
| `async_tools`                 | `bool`            | `False`                          | Enable async versions of tools.                               |

| Function                | Description                                               |
| ----------------------- | --------------------------------------------------------- |
| `search_by_identifier`  | Search for brand data using domain or company identifier. |
| `search_by_brand`       | Search for brands by name (requires client\_id).          |
| `asearch_by_identifier` | Async version of search by identifier.                    |
| `asearch_by_brand`      | Async version of search by brand name.                    |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/brandfetch.py)
* [Brandfetch API Documentation](https://docs.brandfetch.com/)

---

## Memory Optimization

**URL:** llms-txt#memory-optimization

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/memory-optimization

```python memory-optimization.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.memory import MemoryManager, SummarizeStrategy
from agno.memory.strategies.types import MemoryOptimizationStrategyType
from agno.models.openai import OpenAIChat

db_file = "tmp/memory_summarize_strategy.db"
db = SqliteDb(db_file=db_file)

---

## agent.print_response(

**URL:** llms-txt#agent.print_response(

---

## Docx Reader

**URL:** llms-txt#docx-reader

Source: https://docs.agno.com/reference/knowledge/reader/docx

DocxReader is a reader class that allows you to read data from Docx files.

<Snippet file="docx-reader-reference.mdx" />

---

## Create an agent with ApifyTools

**URL:** llms-txt#create-an-agent-with-apifytools

agent = Agent(
    tools=[
        ApifyTools(
            actors=["apify/rag-web-browser"],  # Specify which Apify Actors to use, use multiple ones if needed
            apify_api_token="your_apify_api_key"  # Or set the APIFY_API_TOKEN environment variable 
        )
    ],
        markdown=True
)

---

## or

**URL:** llms-txt#or

**Contents:**
- Usage

vector_db.delete_by_metadata({"doc_type": "recipe_book"})
bash  theme={null}
    pip install -U weaviate-client pypdf openai agno
    bash Weaviate Cloud theme={null}
      # 1. Create account at https://console.weaviate.cloud/
      # 2. Create a cluster and copy the "REST endpoint" and "Admin" API Key
      # 3. Set environment variables:
      export WCD_URL="your-cluster-url" 
      export WCD_API_KEY="your-api-key"
      # 4. Set local=False in the code
      bash Local Development theme={null}
      # 1. Install Docker from https://docs.docker.com/get-docker/
      # 2. Run Weaviate locally:
      docker run -d \
          -p 8080:8080 \
          -p 50051:50051 \
          --name weaviate \
          cr.weaviate.io/semitechnologies/weaviate:1.28.4
      # 3. Set local=True in the code
      bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/knowledge/vector_db/weaviate_db/weaviate_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/weaviate_db/weaviate_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Weaviate">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Set environment variables">
```

---

## Image Agent with Bytes

**URL:** llms-txt#image-agent-with-bytes

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/dashscope/usage/image-agent-bytes

```python cookbook/models/dashscope/image_agent_bytes.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.dashscope import DashScope
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=DashScope(id="qwen-vl-plus"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg",
    output_path=str(image_path),
)

---

## Bitbucket

**URL:** llms-txt#bitbucket

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/bitbucket

BitbucketTools enables agents to interact with Bitbucket repositories for managing code, pull requests, and issues.

The following agent can manage Bitbucket repositories:

| Parameter     | Type            | Default               | Description                                                  |
| ------------- | --------------- | --------------------- | ------------------------------------------------------------ |
| `server_url`  | `str`           | `"api.bitbucket.org"` | Bitbucket server URL (for Bitbucket Server instances).       |
| `username`    | `Optional[str]` | `None`                | Bitbucket username. Uses BITBUCKET\_USERNAME if not set.     |
| `password`    | `Optional[str]` | `None`                | Bitbucket app password. Uses BITBUCKET\_PASSWORD if not set. |
| `token`       | `Optional[str]` | `None`                | Access token. Uses BITBUCKET\_TOKEN if not set.              |
| `workspace`   | `Optional[str]` | `None`                | Bitbucket workspace name (required).                         |
| `repo_slug`   | `Optional[str]` | `None`                | Repository slug name (required).                             |
| `api_version` | `str`           | `"2.0"`               | Bitbucket API version to use.                                |

| Function                   | Description                                             |
| -------------------------- | ------------------------------------------------------- |
| `get_issue`                | Get details of a specific issue by ID.                  |
| `get_issues`               | List all issues in the repository.                      |
| `create_issue`             | Create a new issue in the repository.                   |
| `update_issue`             | Update an existing issue.                               |
| `get_pull_request`         | Get details of a specific pull request.                 |
| `get_pull_requests`        | List all pull requests in the repository.               |
| `create_pull_request`      | Create a new pull request.                              |
| `update_pull_request`      | Update an existing pull request.                        |
| `get_pull_request_diff`    | Get the diff/changes of a pull request.                 |
| `get_pull_request_commits` | Get commits associated with a pull request.             |
| `get_repository_info`      | Get detailed information about the repository.          |
| `get_branches`             | List all branches in the repository.                    |
| `get_commits`              | List commits in the repository.                         |
| `get_file_content`         | Get the content of a specific file from the repository. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/bitbucket.py)
* [Bitbucket API Documentation](https://developer.atlassian.com/bitbucket/api/2/reference/)

---

## Movie Recommender

**URL:** llms-txt#movie-recommender

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/movie-recommender

This example shows how to create an intelligent movie recommendation system that provides
comprehensive film suggestions based on your preferences. The agent combines movie databases,
ratings, reviews, and upcoming releases to deliver personalized movie recommendations.

Example prompts to try:

* "Suggest thriller movies similar to Inception and Shutter Island"
* "What are the top-rated comedy movies from the last 2 years?"
* "Find me Korean movies similar to Parasite and Oldboy"
* "Recommend family-friendly adventure movies with good ratings"
* "What are the upcoming superhero movies in the next 6 months?"

```python movie_recommender.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

movie_recommendation_agent = Agent(
    name="PopcornPal",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
        You are PopcornPal, a passionate and knowledgeable film curator with expertise in cinema worldwide! ðŸŽ¥

Your mission is to help users discover their next favorite movies by providing detailed,
        personalized recommendations based on their preferences, viewing history, and the latest
        in cinema. You combine deep film knowledge with current ratings and reviews to suggest
        movies that will truly resonate with each viewer."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:
        1. Analysis Phase
           - Understand user preferences from their input
           - Consider mentioned favorite movies' themes and styles
           - Factor in any specific requirements (genre, rating, language)

2. Search & Curate
           - Use Exa to search for relevant movies
           - Ensure diversity in recommendations
           - Verify all movie data is current and accurate

3. Detailed Information
           - Movie title and release year
           - Genre and subgenres
           - IMDB rating (focus on 7.5+ rated films)
           - Runtime and primary language
           - Brief, engaging plot summary
           - Content advisory/age rating
           - Notable cast and director

4. Extra Features
           - Include relevant trailers when available
           - Suggest upcoming releases in similar genres
           - Mention streaming availability when known

Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar movies together
        - Add emoji indicators for genres (ðŸŽ­ ðŸŽ¬ ðŸŽª)
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
    """),
    markdown=True,
    add_datetime_to_context=True,
    )

---

## Supabase MCP agent

**URL:** llms-txt#supabase-mcp-agent

Source: https://docs.agno.com/basics/tools/mcp/usage/supabase

Using the [Supabase MCP server](https://github.com/supabase-community/supabase-mcp) to create an Agent that can create projects, database schemas, edge functions, and more:

```python  theme={null}
"""ðŸ”‘ Supabase MCP Agent - Showcase Supabase MCP Capabilities

This example demonstrates how to use the Supabase MCP server to create projects, database schemas, edge functions, and more.

Setup:
1. Install Python dependencies: `pip install agno mcp-sdk`
2. Create a Supabase Access Token: https://supabase.com/dashboard/account/tokens and set it as the SUPABASE_ACCESS_TOKEN environment variable.
"""

import asyncio
import os
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from agno.tools.reasoning import ReasoningTools
from agno.utils.log import log_error, log_exception, log_info

async def run_agent(task: str) -> None:
    token = os.getenv("SUPABASE_ACCESS_TOKEN")
    if not token:
        log_error("SUPABASE_ACCESS_TOKEN environment variable not set.")
        return

npx_cmd = "npx.cmd" if os.name == "nt" else "npx"

try:
        async with MCPTools(
            f"{npx_cmd} -y @supabase/mcp-server-supabase@latest --access-token={token}"
        ) as mcp:
            instructions = dedent(f"""
                You are an expert Supabase MCP architect. Given the project description:
                {task}

Automatically perform the following steps :
                1. Plan the entire database schema based on the project description.
                2. Call `list_organizations` and select the first organization in the response.
                3. Use `get_cost(type='project')` to estimate project creation cost and mention the cost in your response.
                4. Create a new Supabase project with `create_project`, passing the confirmed cost ID.
                5. Poll project status with `get_project` until the status is `ACTIVE_HEALTHY`.
                6. Analyze the project requirements and propose a complete, normalized SQL schema (tables,  columns, data types, indexes, constraints, triggers, and functions) as DDL statements.
                7. Apply the schema using `apply_migration`, naming the migration `initial_schema`.
                8. Validate the deployed schema via `list_tables` and `list_extensions`.
                8. Deploy a simple health-check edge function with `deploy_edge_function`.
                9. Retrieve and print the project URL (`get_project_url`) and anon key (`get_anon_key`).
            """)
            agent = Agent(
                model=OpenAIChat(id="o4-mini"),
                instructions=instructions,
                tools=[mcp, ReasoningTools(add_instructions=True)],
                markdown=True,
            )

log_info(f"Running Supabase project agent for: {task}")
            await agent.aprint_response(
                message=task,
                stream=True,
                show_full_reasoning=True,
            )
    except Exception as e:
        log_exception(f"Unexpected error: {e}")

if __name__ == "__main__":
    demo_description = (
        "Develop a cloud-based SaaS platform with AI-powered task suggestions, calendar syncing, predictive prioritization, "
        "team collaboration, and project analytics."
    )
    asyncio.run(run_agent(demo_description))

---

## hackernews_agent.print_response(

**URL:** llms-txt#hackernews_agent.print_response(

---

## Check memory count for a user

**URL:** llms-txt#check-memory-count-for-a-user

memories = agent.get_user_memories(user_id="user_123")
print(f"User has {len(memories)} memories")

---

## Basic Team

**URL:** llms-txt#basic-team

Source: https://docs.agno.com/basics/teams/usage/basic-flows/basic-team

This example demonstrates a simple team of AI agents working together to research topics across different platforms.

The team consists of three specialized agents:

1. **HackerNews Researcher** - Uses HackerNews API to find and analyze relevant HackerNews posts
2. **Article Reader** - Reads articles from URLs

The team leader coordinates the agents by:

* Giving each agent a specific task
* Providing clear instructions for each agent
* Collecting and summarizing the results from each agent

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Accuracy with Database Logging

**URL:** llms-txt#accuracy-with-database-logging

Source: https://docs.agno.com/basics/evals/accuracy/usage/accuracy-db-logging

Example showing how to store evaluation results in the database for tracking and analysis.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print session metrics

**URL:** llms-txt#print-session-metrics

**Contents:**
- Developer Resources

print("Session Metrics")
pprint(workflow.get_session_metrics().to_dict())
```

You'll see the outputs with following information:

**Workflow-level metrics:**

* `duration`: Total workflow execution time in seconds (from start to finish, including orchestration overhead)
* `steps`: Dictionary mapping step names to their individual step metrics

**Step-level metrics:**

* `step_name`: Name of the step
* `executor_type`: Type of executor ("agent", "team", or "function")
* `executor_name`: Name of the executor
* `metrics`: Execution metrics including tokens, duration, and model information (see [Metrics schema](/reference/agents/metrics))

* Aggregates step-level metrics (tokens, duration) across all runs in the session
* Includes only agent/team execution time, not workflow orchestration overhead

## Developer Resources

* View the [Workflow schema](/reference/workflows/workflow)
* View the [Session schema](/reference/workflows/session)
* View the [WorkflowRunOutput schema](/reference/workflows/run-output)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/session/)

---

## ContentsDB is required for AgentOS Knowledge page

**URL:** llms-txt#contentsdb-is-required-for-agentos-knowledge-page

contents_db = PostgresDb(
    db_url="postgresql+psycopg://user:pass@localhost:5432/db"
)

vector_db = PgVector(table_name="vectors", db_url="http://my-postgress:5432")

knowledge = Knowledge(
    vector_db=vector_db,
    contents_db=contents_db  # Must be provided for AgentOS
)

knowledge_agent = Agent(
    name="Knowledge Agent",
    knowledge=knowledge
)

---

## Change Session State on Run

**URL:** llms-txt#change-session-state-on-run

Source: https://docs.agno.com/basics/state/team/usage/change-state-on-run

This example demonstrates how to set and manage session state for different users and sessions. It shows how session state can be passed during runs and persists across multiple interactions within the same session.

<Steps>
  <Step title="Create a Python file">
    Create a file `change_state_on_run.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the team">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Note: Gemini may not properly interpret OpenAI's message history format

**URL:** llms-txt#note:-gemini-may-not-properly-interpret-openai's-message-history-format

**Contents:**
- Learn More

* [All supported models](/basics/models/overview)
* [Environment variables setup](/faq/environment-variables)

---

## Setup basic agents, teams and workflows

**URL:** llms-txt#setup-basic-agents,-teams-and-workflows

basic_agent = Agent(
    name="Basic Agent",
    db=db,
    enable_session_summaries=True,
    enable_user_memories=True,
    add_history_to_context=True,
    num_history_runs=3,
    add_datetime_to_context=True,
    markdown=True,
)
basic_team = Team(
    id="basic-team",
    name="Basic Team",
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    members=[basic_agent],
    enable_user_memories=True,
)
basic_workflow = Workflow(
    id="basic-workflow",
    name="Basic Workflow",
    description="Just a simple workflow",
    db=db,
    steps=[
        Step(
            name="step1",
            description="Just a simple step",
            agent=basic_agent,
        )
    ],
)
basic_knowledge = Knowledge(
    name="Basic Knowledge",
    description="A basic knowledge base",
    contents_db=db,
    vector_db=PgVector(db_url="postgresql+psycopg://ai:ai@localhost:5532/ai", table_name="vectors"),
)

---

## Agentic User Input with Control Flow

**URL:** llms-txt#agentic-user-input-with-control-flow

Source: https://docs.agno.com/basics/hitl/usage/agentic-user-input

This example demonstrates how to use UserControlFlowTools to allow agents to dynamically request user input when they need additional information to complete tasks.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 2: Invoke a specific Lambda function

**URL:** llms-txt#example-2:-invoke-a-specific-lambda-function

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Invoke the 'hello-world' Lambda function with an empty payload", markdown=True)
```

| Parameter                | Type   | Default       | Description                                         |
| ------------------------ | ------ | ------------- | --------------------------------------------------- |
| `region_name`            | `str`  | `"us-east-1"` | AWS region name where Lambda functions are located. |
| `enable_list_functions`  | `bool` | `True`        | Enable the list\_functions functionality.           |
| `enable_invoke_function` | `bool` | `True`        | Enable the invoke\_function functionality.          |
| `all`                    | `bool` | `False`       | Enable all functionality.                           |

| Function          | Description                                                                                                           |
| ----------------- | --------------------------------------------------------------------------------------------------------------------- |
| `list_functions`  | Lists all Lambda functions available in the AWS account.                                                              |
| `invoke_function` | Invokes a specific Lambda function with an optional payload. Takes `function_name` and optional `payload` parameters. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/aws_lambda.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/aws_lambda_tools.py)

---

## Define workflow with steps

**URL:** llms-txt#define-workflow-with-steps

workflow = Workflow(
    name="Content Creation Workflow",
    steps=[
        Step(name="Research Step", team=research_team),
        Step(name="Content Planning Step", executor=custom_content_planning_function),
    ]
)

---

## Request: C -> B -> A -> Your Route

**URL:** llms-txt#request:-c-->-b-->-a-->-your-route

---

## Print aggregated team leader metrics

**URL:** llms-txt#print-aggregated-team-leader-metrics

print("---" * 5, "Aggregated Metrics of Team", "---" * 5)
pprint(run_response.metrics)

---

## Multimodal Agents

**URL:** llms-txt#multimodal-agents

**Contents:**
- Multimodal inputs to an agent
  - Image Agent
  - Audio Agent

Source: https://docs.agno.com/basics/multimodal/agent/overview

Learn how to create multimodal agents in Agno.

Agno agents support text, image, audio, video and files inputs and can generate text, image, audio, video and files as output.

For a complete overview of multimodal support, please checkout the [multimodal](/basics/multimodal/overview) documentation.

<Tip>
  Not all models support multimodal inputs and outputs.
  To see which models support multimodal inputs and outputs, please checkout the [compatibility matrix](/basics/models/compatibility).
</Tip>

## Multimodal inputs to an agent

Let's create an agent that can understand images and make tool calls as needed

See [Image as input](/basics/multimodal/images/image-input) for more details.

```python audio_agent.py theme={null}
import base64

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
Run the agent:
```

Example 2 (unknown):
```unknown
See [Image as input](/basics/multimodal/images/image-input) for more details.

### Audio Agent
```

---

## ScrapeGraph

**URL:** llms-txt#scrapegraph

**Contents:**
- Prerequisites
- Example
  - Raw HTML Scraping

Source: https://docs.agno.com/integrations/toolkits/web-scrape/scrapegraph

ScrapeGraphTools enable an Agent to extract structured data from webpages, convert content to markdown, and retrieve raw HTML content.

**ScrapeGraphTools** enable an Agent to extract structured data from webpages, convert content to markdown, and retrieve raw HTML content using the ScrapeGraphAI API.

The toolkit provides 5 core capabilities:

1. **smartscraper**: Extract structured data using natural language prompts
2. **markdownify**: Convert web pages to markdown format
3. **searchscraper**: Search the web and extract information
4. **crawl**: Crawl websites with structured data extraction
5. **scrape**: Get raw HTML content from websites *(NEW!)*

The scrape method is particularly useful when you need:

* Complete HTML source code
* Raw content for further processing
* HTML structure analysis
* Content that needs to be parsed differently

All methods support heavy JavaScript rendering when needed.

The following examples require the `scrapegraph-py` library.

Optionally, if your ScrapeGraph configuration or specific models require an API key, set the `SGAI_API_KEY` environment variable:

The following agent will extract structured data from a website using the smartscraper tool:

### Raw HTML Scraping

Get complete HTML content from websites for custom processing:

```python cookbook/tools/scrapegraph_tools.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Optionally, if your ScrapeGraph configuration or specific models require an API key, set the `SGAI_API_KEY` environment variable:
```

Example 2 (unknown):
```unknown
## Example

The following agent will extract structured data from a website using the smartscraper tool:
```

Example 3 (unknown):
```unknown
### Raw HTML Scraping

Get complete HTML content from websites for custom processing:
```

---

## List Evaluation Runs

**URL:** llms-txt#list-evaluation-runs

Source: https://docs.agno.com/reference-api/schema/evals/list-evaluation-runs

get /eval-runs
Retrieve paginated evaluation runs with filtering and sorting options. Filter by agent, team, workflow, model, or evaluation type.

---

## Prompt Injection Guardrail for Teams

**URL:** llms-txt#prompt-injection-guardrail-for-teams

Source: https://docs.agno.com/basics/guardrails/usage/team/prompt-injection

This example demonstrates how to use Agno's built-in prompt injection guardrail with a Team to stop injection attempts.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create agents for research

**URL:** llms-txt#create-agents-for-research

research_agent = Agent(
    name="Research Agent",
    role="Research specialist",
    tools=[HackerNewsTools(), DuckDuckGoTools()],
    instructions="You are a research specialist. Research the given topic thoroughly.",
    markdown=True,
)

analysis_agent = Agent(
    name="Analysis Agent",
    role="Data analyst",
    instructions="You are a data analyst. Analyze and summarize research findings.",
    markdown=True,
)

content_agent = Agent(
    name="Content Agent",
    role="Content creator",
    instructions="You are a content creator. Create engaging content based on research.",
    markdown=True,
)

---

## Azure OpenAI o1

**URL:** llms-txt#azure-openai-o1

Source: https://docs.agno.com/basics/reasoning/usage/models/azure-openai/o1

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set your Azure OpenAI credentials">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set your Azure OpenAI credentials">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Async Structured Output Streaming

**URL:** llms-txt#async-structured-output-streaming

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/async-structured-output-streaming

This example demonstrates async structured output streaming from a team using Pydantic models to ensure structured responses while streaming, providing both real-time output and validated data structures asynchronously.

```python cookbook/examples/teams/structured_input_output/05_async_structured_output_streaming.py theme={null}
"""
This example demonstrates async structured output streaming from a team.

The team uses Pydantic models to ensure structured responses while streaming,
providing both real-time output and validated data structures asynchronously.
"""

import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.exa import ExaTools
from agno.utils.pprint import apprint_run_response
from pydantic import BaseModel

class StockAnalysis(BaseModel):
    """Stock analysis data structure."""

symbol: str
    company_name: str
    analysis: str

class CompanyAnalysis(BaseModel):
    """Company analysis data structure."""

company_name: str
    analysis: str

class StockReport(BaseModel):
    """Final stock report data structure."""

symbol: str
    company_name: str
    analysis: str

---

## Agent with Media

**URL:** llms-txt#agent-with-media

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/discord/usage/agent-with-media

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Gemini Embedder

**URL:** llms-txt#gemini-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/gemini/usage/gemini-embedder

```python  theme={null}
from agno.knowledge.embedder.google import GeminiEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = GeminiEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Read the image file content as bytes

**URL:** llms-txt#read-the-image-file-content-as-bytes

**Contents:**
- Usage

image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)

bash  theme={null}
    export XAI_API_KEY=xxx
    bash  theme={null}
    pip install -U xai ddgs agno
    bash Mac theme={null}
      python cookbook/models/xai/image_agent_bytes.py
      bash Windows theme={null}
      python cookbook/models/xai/image_agent_bytes.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create custom FastAPI app

**URL:** llms-txt#create-custom-fastapi-app

app = FastAPI(
    title="Example Custom App",
    version="1.0.0",
)

---

## Send a message to a Space in Webex

**URL:** llms-txt#send-a-message-to-a-space-in-webex

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "Send a funny ice-breaking message to the webex Welcome space", markdown=True
)
```

| Parameter             | Type   | Default | Description                                                                                             |
| --------------------- | ------ | ------- | ------------------------------------------------------------------------------------------------------- |
| `access_token`        | `str`  | `None`  | Webex access token for authentication. If not provided, uses WEBEX\_ACCESS\_TOKEN environment variable. |
| `enable_send_message` | `bool` | `True`  | Enable sending messages to Webex spaces.                                                                |
| `enable_list_rooms`   | `bool` | `True`  | Enable listing Webex spaces/rooms.                                                                      |
| `all`                 | `bool` | `False` | Enable all functionality.                                                                               |

| Function       | Description                                                                                                     |
| -------------- | --------------------------------------------------------------------------------------------------------------- |
| `send_message` | Sends a message to a Webex room. Parameters: `room_id` (str) for the target room, `text` (str) for the message. |
| `list_rooms`   | Lists all available Webex rooms/spaces with their details including ID, title, type, and visibility settings.   |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/webex.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/webex_tools.py)

---

## Configure quick prompts for the Chat interface (per agent)

**URL:** llms-txt#configure-quick-prompts-for-the-chat-interface-(per-agent)

chat:
  quick_prompts:
    marketing-agent:
      - "What can you do?"
      - "How is our latest post working?"
      - "Tell me about our active marketing campaigns"

---

## Delete Session

**URL:** llms-txt#delete-session

Source: https://docs.agno.com/reference-api/schema/sessions/delete-session

delete /sessions/{session_id}
Permanently delete a specific session and all its associated runs. This action cannot be undone and will remove all conversation history.

---

## Create agent and team

**URL:** llms-txt#create-agent-and-team

agent = Agent()
team = Team(members=[agent])

---

## Agent State

**URL:** llms-txt#agent-state

Source: https://docs.agno.com/examples/getting-started/07-agent-state

This example shows how to create an agent that maintains state across interactions. It demonstrates a simple counter mechanism, but this pattern can be extended to more complex state management like maintaining conversation context, user preferences, or tracking multi-step processes.

Example prompts to try:

* "Increment the counter 3 times and tell me the final count"
* "What's our current count? Add 2 more to it"
* "Let's increment the counter 5 times, but tell me each step"
* "Add 4 to our count and remind me where we started"
* "Increase the counter twice and summarize our journey"

<Steps>
  <Step title="Create a Python file">
    Create a file `agent_state.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

  <Step title="Run the agent">
```

---

## Async Team Events Monitoring

**URL:** llms-txt#async-team-events-monitoring

Source: https://docs.agno.com/basics/teams/usage/async-flows/stream-events

This example demonstrates how to handle and monitor team events asynchronously, capturing various events during async team execution including tool calls, run states, and content generation.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/streaming" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Consume the streaming events and get the final response

**URL:** llms-txt#consume-the-streaming-events-and-get-the-final-response

**Contents:**
- Usage

run_response = None
for event_or_response in stream_generator:
    # The last item in the stream is the final TeamRunOutput
    run_response = event_or_response

assert isinstance(run_response.content, StockReport)
print(
    f"âœ… Response content is correctly typed as StockReport: {type(run_response.content)}"
)
print(f"âœ… Stock Symbol: {run_response.content.symbol}")
print(f"âœ… Company Name: {run_response.content.company_name}")
bash  theme={null}
    pip install agno exa_py pydantic
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/04_structured_output_streaming.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Wikipedia Reader Async

**URL:** llms-txt#wikipedia-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/wikipedia-reader-async

The **Wikipedia Reader** with asynchronous processing allows you to search and read Wikipedia articles efficiently, converting them into vector embeddings for your knowledge base.

```python examples/basics/knowledge/readers/wikipedia_reader_async.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.arxiv_reader import ArxivReader
from agno.knowledge.reader.wikipedia_reader import WikipediaReader
from agno.vectordb.pgvector import PgVector

---

## -----------------------------------------------------------------------------------

**URL:** llms-txt#-----------------------------------------------------------------------------------

---

## Get the most recent income statement for Apple

**URL:** llms-txt#get-the-most-recent-income-statement-for-apple

**Contents:**
- Toolkit Params
- Toolkit Functions
- Rate Limits and Usage
- Developer Resources

agent.print_response("Get the most recent income statement for AAPL and highlight key metrics")
```

For more examples, see the [Financial Datasets Examples](/examples/basics/tools/others/financial_datasets).

\| Parameter | Type            | Default | Description                                                                             |
\| --------- | --------------- | ------- | --------------------------------------------------------------------------------------- | --- |
\| `api_key` | `Optional[str]` | `None`  | Optional API key. If not provided, uses FINANCIAL\_DATASETS\_API\_KEY environment variable |     |

| Function                      | Description                                                                                                     |
| ----------------------------- | --------------------------------------------------------------------------------------------------------------- |
| `get_income_statements`       | Get income statements for a company with options for annual, quarterly, or trailing twelve months (ttm) periods |
| `get_balance_sheets`          | Get balance sheets for a company with period options                                                            |
| `get_cash_flow_statements`    | Get cash flow statements for a company                                                                          |
| `get_company_info`            | Get company information including business description, sector, and industry                                    |
| `get_crypto_prices`           | Get cryptocurrency prices with configurable time intervals                                                      |
| `get_earnings`                | Get earnings reports with EPS estimates, actuals, and revenue data                                              |
| `get_financial_metrics`       | Get key financial metrics and ratios for a company                                                              |
| `get_insider_trades`          | Get data on insider buying and selling activity                                                                 |
| `get_institutional_ownership` | Get information about institutional investors and their positions                                               |
| `get_news`                    | Get market news, optionally filtered by company                                                                 |
| `get_stock_prices`            | Get historical stock prices with configurable time intervals                                                    |
| `search_tickers`              | Search for stock tickers based on a query string                                                                |
| `get_sec_filings`             | Get SEC filings with optional filtering by form type (10-K, 10-Q, etc.)                                         |
| `get_segmented_financials`    | Get segmented financial data by product category and geographic region                                          |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Rate Limits and Usage

The Financial Datasets API may have usage limits based on your subscription tier. Please refer to their documentation for specific rate limit information.

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/financial_datasets.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/financial_datasets_tools.py)

---

## Condition evaluator function

**URL:** llms-txt#condition-evaluator-function

def is_tech_topic(step_input) -> bool:
    """Check if the topic is tech-related and needs specialized research"""
    message = step_input.input.lower() if step_input.input else ""
    tech_keywords = [
        "ai",
        "machine learning",
        "technology",
        "software",
        "programming",
        "tech",
        "startup",
        "blockchain",
    ]
    return any(keyword in message for keyword in tech_keywords)

---

## YouTube Reader

**URL:** llms-txt#youtube-reader

Source: https://docs.agno.com/reference/knowledge/reader/youtube

YouTubeReader is a reader class that allows you to read transcript from YouTube videos.

<Snippet file="youtube-reader-reference.mdx" />

---

## Print team leader session metrics

**URL:** llms-txt#print-team-leader-session-metrics

print("---" * 5, "Session Metrics", "---" * 5)
pprint(team.get_session_metrics().to_dict())

---

## Step 1: Initialize knowledge base with documents and metadata

**URL:** llms-txt#step-1:-initialize-knowledge-base-with-documents-and-metadata

---

## When loading the knowledge base, we can attach metadata that will be used for filtering

**URL:** llms-txt#when-loading-the-knowledge-base,-we-can-attach-metadata-that-will-be-used-for-filtering

---

## Agent Infra AWS

**URL:** llms-txt#agent-infra-aws

**Contents:**
  - Local environment
  - AWS environment
  - How to get started

Source: https://docs.agno.com/templates/agent-infra-aws/introduction

The Agent Infra AWS template provides a standardized codebase for running AgentOS on AWS. This template uses `agno-infra` SDK to manage the local and AWS deployment.

### Local environment

Runs both AgentOS and PostgreSQL using Docker.

Ideal for development, testing, and verifying your setup before deploying to the cloud.

AgentOS is deployed on ECS while the image is built and pushed to AWS ECR.

PostgreSQL is provisioned via RDS for reliable, managed database hosting.

The template automates infrastructure creation and deployment steps using agno-infra.

### How to get started

* Clone the template repository and install the dependencies
* Start with the local setup to ensure everything works end-to-end.
* Move to AWS deployment using the provided guides once your local environment is ready.

<Snippet file="setup.mdx" />

<Snippet file="create-agent-infra-aws-codebase.mdx" />

You can also clone the template directory and follow the instructions in the template documentation.

<CodeGroup>
  
</CodeGroup>

After creating your codebase, the next step is to get it up and running locally using docker.

---

## Asynchronous Agent

**URL:** llms-txt#asynchronous-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/meta/usage/async-basic

```python cookbook/models/meta/llama/async_basic.py theme={null}
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"),
    markdown=True
)

---

## Define two completely different workflows as Steps

**URL:** llms-txt#define-two-completely-different-workflows-as-steps

image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[
        Step(name="generate_image", agent=image_generator),
        Step(name="describe_image", agent=image_describer),
    ],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[
        Step(name="generate_video", agent=video_generator),
        Step(name="describe_video", agent=video_describer),
    ],
)

def media_sequence_selector(step_input) -> List[Step]:
    """Route to appropriate media generation pipeline"""
    if not step_input.input:
        return [image_sequence]

message_lower = step_input.input.lower()

if "video" in message_lower:
        return [video_sequence]
    elif "image" in message_lower:
        return [image_sequence]
    else:
        return [image_sequence]  # Default

---

## Example: Ask the agent to run a SQL query

**URL:** llms-txt#example:-ask-the-agent-to-run-a-sql-query

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("""
Please run a SQL query to get all users from the users table
who signed up in the last 30 days
""")
```

| Name           | Type                              | Default  | Description                                    |
| -------------- | --------------------------------- | -------- | ---------------------------------------------- |
| `connection`   | `Optional[PgConnection[DictRow]]` | `None`   | Optional existing psycopg connection object.   |
| `db_name`      | `Optional[str]`                   | `None`   | Optional name of the database to connect to.   |
| `user`         | `Optional[str]`                   | `None`   | Optional username for database authentication. |
| `password`     | `Optional[str]`                   | `None`   | Optional password for database authentication. |
| `host`         | `Optional[str]`                   | `None`   | Optional host for the database connection.     |
| `port`         | `Optional[int]`                   | `None`   | Optional port for the database connection.     |
| `table_schema` | `str`                             | `public` | Schema name to search for tables.              |

| Function               | Description                                                                                                                                                                                                                                                                            |
| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `show_tables`          | Retrieves and displays a list of tables in the database. Returns the list of tables.                                                                                                                                                                                                   |
| `describe_table`       | Describes the structure of a specified table by returning its columns, data types, and nullability. Parameters include `table` (str) to specify the table name. Returns the table description.                                                                                         |
| `summarize_table`      | Summarizes a table by computing aggregates such as min, max, average, standard deviation, and non-null counts for numeric columns, or unique values and average length for text columns. Parameters include `table` (str) to specify the table name. Returns the summary of the table. |
| `inspect_query`        | Inspects an SQL query by returning the query plan using EXPLAIN. Parameters include `query` (str) to specify the SQL query. Returns the query plan.                                                                                                                                    |
| `export_table_to_path` | Exports a specified table in CSV format to a given path. Parameters include `table` (str) to specify the table name and `path` (str) to specify where to save the file. Returns the result of the export operation.                                                                    |
| `run_query`            | Executes a read-only SQL query and returns the result. Parameters include `query` (str) to specify the SQL query. Returns the result of the query execution.                                                                                                                           |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/postgres.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/postgres_tools.py)

---

## Agent with PDF Input (URL)

**URL:** llms-txt#agent-with-pdf-input-(url)

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/google/usage/pdf-input-url

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Model

**URL:** llms-txt#model

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/model

The Model class is the base class for all models in Agno. It provides common functionality and parameters that are inherited by specific model implementations like OpenAIChat, Claude, etc.

| Parameter               | Type                              | Default  | Description                                                                             |
| ----------------------- | --------------------------------- | -------- | --------------------------------------------------------------------------------------- |
| `id`                    | `str`                             | Required | The id/name of the model to use                                                         |
| `name`                  | `Optional[str]`                   | `None`   | The display name of the model                                                           |
| `provider`              | `Optional[str]`                   | `None`   | The provider of the model                                                               |
| `frequency_penalty`     | `Optional[float]`                 | `None`   | Penalizes new tokens based on their frequency in the text so far                        |
| `presence_penalty`      | `Optional[float]`                 | `None`   | Penalizes new tokens based on whether they appear in the text so far                    |
| `response_format`       | `Optional[str]`                   | `None`   | The format of the response                                                              |
| `seed`                  | `Optional[int]`                   | `None`   | Random seed for deterministic sampling                                                  |
| `stop`                  | `Optional[Union[str, List[str]]]` | `None`   | Up to 4 sequences where the API will stop generating further tokens                     |
| `stream`                | `bool`                            | `True`   | Whether to stream the response                                                          |
| `temperature`           | `Optional[float]`                 | `None`   | Controls randomness in the model's output                                               |
| `top_p`                 | `Optional[float]`                 | `None`   | Controls diversity via nucleus sampling                                                 |
| `max_tokens`            | `Optional[int]`                   | `None`   | Maximum number of tokens to generate                                                    |
| `request_params`        | `Optional[Dict[str, Any]]`        | `None`   | Additional parameters to include in the request                                         |
| `cache_response`        | `bool`                            | `False`  | Enable caching of model responses to avoid redundant API calls                          |
| `cache_ttl`             | `Optional[int]`                   | `None`   | Time-to-live for cached model responses, in seconds. If None, cache never expires       |
| `cache_dir`             | `Optional[str]`                   | `None`   | Directory path for storing cached model responses. If None, uses default cache location |
| `retries`               | `int`                             | `0`      | Number of retries to attempt before raising a ModelProviderError                        |
| `delay_between_retries` | `int`                             | `1`      | Delay between retries, in seconds                                                       |
| `exponential_backoff`   | `bool`                            | `False`  | If True, the delay between retries is doubled each time                                 |

---

## Create a team with these agents

**URL:** llms-txt#create-a-team-with-these-agents

content_team = Team(
    name="Content Team",
    members=[researcher, writer],
    instructions="You are a team of researchers and writers that work together to create high-quality content.",
    model=OpenAIChat(id="gpt-4o"),
    show_members_responses=True,
)

---

## Apply user-specific filtering

**URL:** llms-txt#apply-user-specific-filtering

**Contents:**
  - Time-Based Filtering

user_filter = get_user_filter("john_doe", "engineering")
agent.print_response(
    "Show me the latest project updates",
    knowledge_filters=[user_filter]  # â† List wrapper required
)
python  theme={null}
from datetime import datetime
from agno.filters import AND, NOT, EQ, GT

current_year = datetime.now().year

**Examples:**

Example 1 (unknown):
```unknown
### Time-Based Filtering

Filter by recency or date ranges:
```

---

## images=[Image(url="YOUR_IMAGE_URL")],

**URL:** llms-txt#images=[image(url="your_image_url")],

---

## Set your API key and run the basic agent

**URL:** llms-txt#set-your-api-key-and-run-the-basic-agent

**Contents:**
- Verification

export OPENAI_API_KEY="your_openai_api_key"
uv run agent.py
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This starts a PostgreSQL database with sample hotel data and an MCP Toolbox server that exposes database operations as filtered tools.

## Verification

To verify that your docker/podman setup is working correctly, you can check the database connection:
```

---

## Setup MongoDb

**URL:** llms-txt#setup-mongodb

**Contents:**
- Usage

db_url = "mongodb://localhost:27017"

db = MongoDb(db_url=db_url)

agent = Agent(
    db=db,
    enable_user_memories=True,
)

agent.print_response("My name is John Doe and I like to play basketball on the weekends.")
agent.print_response("What's do I do in weekends?")
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai pymongo
    bash Mac/Linux theme={null}
      python mem-mongodb-memory.py
      bash Windows theme={null}
      python mem-mongodb-memory.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set environment variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Missing required fields

**URL:** llms-txt#missing-required-fields

curl ... -F 'knowledge_filters={"op": "EQ", "key": "status"}'

---

## Access Run Context and Session State in Condition Evaluator Function

**URL:** llms-txt#access-run-context-and-session-state-in-condition-evaluator-function

Source: https://docs.agno.com/basics/state/workflows/usage/access-session-state-in-condition-evaluator-function

This example demonstrates how to access the run context in the evaluator function of a condition step

1. How to use `run_context` in a Condition evaluator function
2. Reading and modifying `run_context.session_state` based on condition logic
3. Accessing `user_id` and `session_id` from `run_context.session_state`
4. Making conditional decisions based on `run_context.session_state`

<Steps>
  <Step title="Create a Python file">
    Create a file named `access_session_state_in_condition_evaluator_function.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Step title="Setup venv">
    <Snippet file="create-venv-step.mdx" />
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

<Step title="Run the workflow">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Setup venv">
    <Snippet file="create-venv-step.mdx" />
  </Step>

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

  <Step title="Run the workflow">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Configure model to use File Search

**URL:** llms-txt#configure-model-to-use-file-search

model.file_search_store_names = [store.name]

---

## Add memories for Jane Doe

**URL:** llms-txt#add-memories-for-jane-doe

jane_doe_id = "jane_doe@example.com"
print(f"\nUser: {jane_doe_id}")
memory_id_1 = memory.add_user_memory(
    memory=UserMemory(memory="The user's name is Jane Doe", topics=["name"]),
    user_id=jane_doe_id,
)
memory_id_2 = memory.add_user_memory(
    memory=UserMemory(memory="She likes to play tennis", topics=["hobbies"]),
    user_id=jane_doe_id,
)
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

---

## Agent Input as Message Object

**URL:** llms-txt#agent-input-as-message-object

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/input-output/agent/usage/input-as-message

This example demonstrates how to provide input to an agent using the Message object format for structured multimodal content.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Example usage with diverse queries

**URL:** llms-txt#example-usage-with-diverse-queries

agent_team.print_response(
    input="Summarize analyst recommendations and share the latest news for NVDA",
    stream=True,
)
agent_team.print_response(
    input="What's the market outlook and financial performance of AI semiconductor companies?",
    stream=True,
)
agent_team.print_response(
    input="Analyze recent developments and financial performance of TSLA",
    stream=True,
)

---

## Configure Redis connection

**URL:** llms-txt#configure-redis-connection

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
INDEX_NAME = os.getenv("REDIS_INDEX", "agno_cookbook_vectors")

---

## Instrument Agno with Maxim for automatic tracing and logging

**URL:** llms-txt#instrument-agno-with-maxim-for-automatic-tracing-and-logging

instrument_agno(Maxim().logger())

---

## ChromaDb

**URL:** llms-txt#chromadb

Source: https://docs.agno.com/reference/vector-db/chromadb

<Snippet file="vector-db-chromadb-reference.mdx" />

---

## Adding Dependencies to Team Context

**URL:** llms-txt#adding-dependencies-to-team-context

Source: https://docs.agno.com/basics/dependencies/team/usage/add-dependencies-to-context

This example demonstrates how to add dependencies directly to the team context. Unlike adding dependencies per run, this approach makes the dependency functions available to all team runs by default, providing consistent access to contextual information across all interactions.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## RunOutput

**URL:** llms-txt#runoutput

**Contents:**
- RunOutput Attributes
- RunOutputEvent Types and Attributes
  - Base RunOutputEvent Attributes
  - RunStartedEvent
  - RunContentEvent
  - RunContentCompletedEvent
  - IntermediateRunContentEvent
  - RunCompletedEvent
  - RunPausedEvent
  - RunContinuedEvent

Source: https://docs.agno.com/reference/agents/run-response

## RunOutput Attributes

| Attribute             | Type                                | Default             | Description                                                      |
| --------------------- | ----------------------------------- | ------------------- | ---------------------------------------------------------------- |
| `run_id`              | `Optional[str]`                     | `None`              | Run ID                                                           |
| `agent_id`            | `Optional[str]`                     | `None`              | Agent ID for the run                                             |
| `agent_name`          | `Optional[str]`                     | `None`              | Agent name for the run                                           |
| `session_id`          | `Optional[str]`                     | `None`              | Session ID for the run                                           |
| `parent_run_id`       | `Optional[str]`                     | `None`              | Parent run ID                                                    |
| `workflow_id`         | `Optional[str]`                     | `None`              | Workflow ID if this run is part of a workflow                    |
| `user_id`             | `Optional[str]`                     | `None`              | User ID associated with the run                                  |
| `content`             | `Optional[Any]`                     | `None`              | Content of the response                                          |
| `content_type`        | `str`                               | `"str"`             | Specifies the data type of the content                           |
| `reasoning_content`   | `Optional[str]`                     | `None`              | Any reasoning content the model produced                         |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`              | List of reasoning steps                                          |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`              | List of reasoning messages                                       |
| `model`               | `Optional[str]`                     | `None`              | The model used in the run                                        |
| `model_provider`      | `Optional[str]`                     | `None`              | The model provider used in the run                               |
| `messages`            | `Optional[List[Message]]`           | `None`              | A list of messages included in the response                      |
| `metrics`             | `Optional[Metrics]`                 | `None`              | Usage metrics of the run                                         |
| `additional_input`    | `Optional[List[Message]]`           | `None`              | Additional input messages                                        |
| `tools`               | `Optional[List[ToolExecution]]`     | `None`              | List of tool executions                                          |
| `images`              | `Optional[List[Image]]`             | `None`              | List of images attached to the response                          |
| `videos`              | `Optional[List[Video]]`             | `None`              | List of videos attached to the response                          |
| `audio`               | `Optional[List[Audio]]`             | `None`              | List of audio snippets attached to the response                  |
| `files`               | `Optional[List[File]]`              | `None`              | List of files attached to the response                           |
| `response_audio`      | `Optional[Audio]`                   | `None`              | The model's raw response in audio                                |
| `input`               | `Optional[RunInput]`                | `None`              | Input media and messages from user                               |
| `citations`           | `Optional[Citations]`               | `None`              | Any citations used in the response                               |
| `model_provider_data` | `Optional[Any]`                     | `None`              | Model provider specific metadata                                 |
| `references`          | `Optional[List[MessageReferences]]` | `None`              | References used in the response                                  |
| `metadata`            | `Optional[Dict[str, Any]]`          | `None`              | Metadata associated with the run                                 |
| `created_at`          | `int`                               | Current timestamp   | Unix timestamp of the response creation                          |
| `events`              | `Optional[List[RunOutputEvent]]`    | `None`              | List of events that occurred during the run                      |
| `status`              | `RunStatus`                         | `RunStatus.running` | Status of the run (running, completed, paused, cancelled, error) |
| `workflow_step_id`    | `Optional[str]`                     | `None`              | Workflow step ID (foreign key relationship)                      |

## RunOutputEvent Types and Attributes

### Base RunOutputEvent Attributes

All events inherit from `BaseAgentRunEvent` which provides these common attributes:

| Attribute         | Type                            | Default           | Description                                      |
| ----------------- | ------------------------------- | ----------------- | ------------------------------------------------ |
| `created_at`      | `int`                           | Current timestamp | Unix timestamp of the event creation             |
| `event`           | `str`                           | Event type value  | The type of event                                |
| `agent_id`        | `str`                           | `""`              | ID of the agent generating the event             |
| `agent_name`      | `str`                           | `""`              | Name of the agent generating the event           |
| `run_id`          | `Optional[str]`                 | `None`            | ID of the current run                            |
| `session_id`      | `Optional[str]`                 | `None`            | ID of the current session                        |
| `workflow_id`     | `Optional[str]`                 | `None`            | ID of the workflow if part of workflow execution |
| `workflow_run_id` | `Optional[str]`                 | `None`            | ID of the workflow run                           |
| `step_id`         | `Optional[str]`                 | `None`            | ID of the workflow step                          |
| `step_name`       | `Optional[str]`                 | `None`            | Name of the workflow step                        |
| `step_index`      | `Optional[int]`                 | `None`            | Index of the workflow step                       |
| `tools`           | `Optional[List[ToolExecution]]` | `None`            | Tools associated with this event                 |
| `content`         | `Optional[Any]`                 | `None`            | For backwards compatibility                      |

| Attribute        | Type  | Default        | Description               |
| ---------------- | ----- | -------------- | ------------------------- |
| `event`          | `str` | `"RunStarted"` | Event type                |
| `model`          | `str` | `""`           | The model being used      |
| `model_provider` | `str` | `""`           | The provider of the model |

| Attribute             | Type                                | Default        | Description                      |
| --------------------- | ----------------------------------- | -------------- | -------------------------------- |
| `event`               | `str`                               | `"RunContent"` | Event type                       |
| `content`             | `Optional[Any]`                     | `None`         | The content of the response      |
| `content_type`        | `str`                               | `"str"`        | Type of the content              |
| `reasoning_content`   | `Optional[str]`                     | `None`         | Reasoning content produced       |
| `citations`           | `Optional[Citations]`               | `None`         | Citations used in the response   |
| `model_provider_data` | `Optional[Any]`                     | `None`         | Model provider specific metadata |
| `response_audio`      | `Optional[Audio]`                   | `None`         | Model's audio response           |
| `image`               | `Optional[Image]`                   | `None`         | Image attached to the response   |
| `references`          | `Optional[List[MessageReferences]]` | `None`         | References used in the response  |
| `additional_input`    | `Optional[List[Message]]`           | `None`         | Additional input messages        |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`         | Reasoning steps                  |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`         | Reasoning messages               |

### RunContentCompletedEvent

| Attribute | Type  | Default                 | Description |
| --------- | ----- | ----------------------- | ----------- |
| `event`   | `str` | `"RunContentCompleted"` | Event type  |

### IntermediateRunContentEvent

| Attribute      | Type            | Default                    | Description                          |
| -------------- | --------------- | -------------------------- | ------------------------------------ |
| `event`        | `str`           | `"RunIntermediateContent"` | Event type                           |
| `content`      | `Optional[Any]` | `None`                     | Intermediate content of the response |
| `content_type` | `str`           | `"str"`                    | Type of the content                  |

### RunCompletedEvent

| Attribute             | Type                                | Default          | Description                             |
| --------------------- | ----------------------------------- | ---------------- | --------------------------------------- |
| `event`               | `str`                               | `"RunCompleted"` | Event type                              |
| `content`             | `Optional[Any]`                     | `None`           | Final content of the response           |
| `content_type`        | `str`                               | `"str"`          | Type of the content                     |
| `reasoning_content`   | `Optional[str]`                     | `None`           | Reasoning content produced              |
| `citations`           | `Optional[Citations]`               | `None`           | Citations used in the response          |
| `model_provider_data` | `Optional[Any]`                     | `None`           | Model provider specific metadata        |
| `images`              | `Optional[List[Image]]`             | `None`           | Images attached to the response         |
| `videos`              | `Optional[List[Video]]`             | `None`           | Videos attached to the response         |
| `audio`               | `Optional[List[Audio]]`             | `None`           | Audio snippets attached to the response |
| `response_audio`      | `Optional[Audio]`                   | `None`           | Model's audio response                  |
| `references`          | `Optional[List[MessageReferences]]` | `None`           | References used in the response         |
| `additional_input`    | `Optional[List[Message]]`           | `None`           | Additional input messages               |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`           | Reasoning steps                         |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`           | Reasoning messages                      |
| `metadata`            | `Optional[Dict[str, Any]]`          | `None`           | Additional metadata                     |
| `metrics`             | `Optional[Metrics]`                 | `None`           | Usage metrics                           |

| Attribute | Type                            | Default       | Description                     |
| --------- | ------------------------------- | ------------- | ------------------------------- |
| `event`   | `str`                           | `"RunPaused"` | Event type                      |
| `tools`   | `Optional[List[ToolExecution]]` | `None`        | Tools that require confirmation |

### RunContinuedEvent

| Attribute | Type  | Default          | Description |
| --------- | ----- | ---------------- | ----------- |
| `event`   | `str` | `"RunContinued"` | Event type  |

| Attribute | Type            | Default      | Description   |
| --------- | --------------- | ------------ | ------------- |
| `event`   | `str`           | `"RunError"` | Event type    |
| `content` | `Optional[str]` | `None`       | Error message |

### RunCancelledEvent

| Attribute | Type            | Default          | Description             |
| --------- | --------------- | ---------------- | ----------------------- |
| `event`   | `str`           | `"RunCancelled"` | Event type              |
| `reason`  | `Optional[str]` | `None`           | Reason for cancellation |

### PreHookStartedEvent

| Attribute       | Type                 | Default            | Description                         |
| --------------- | -------------------- | ------------------ | ----------------------------------- |
| `event`         | `str`                | `"PreHookStarted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`      | `None`             | Name of the pre-hook being executed |
| `run_input`     | `Optional[RunInput]` | `None`             | The run input passed to the hook    |

### PreHookCompletedEvent

| Attribute       | Type                 | Default              | Description                         |
| --------------- | -------------------- | -------------------- | ----------------------------------- |
| `event`         | `str`                | `"PreHookCompleted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`      | `None`               | Name of the pre-hook that completed |
| `run_input`     | `Optional[RunInput]` | `None`               | The run input passed to the hook    |

### PostHookStartedEvent

| Attribute        | Type            | Default             | Description                          |
| ---------------- | --------------- | ------------------- | ------------------------------------ |
| `event`          | `str`           | `"PostHookStarted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`              | Name of the post-hook being executed |

### PostHookCompletedEvent

| Attribute        | Type            | Default               | Description                          |
| ---------------- | --------------- | --------------------- | ------------------------------------ |
| `event`          | `str`           | `"PostHookCompleted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`                | Name of the post-hook that completed |

### ReasoningStartedEvent

| Attribute | Type  | Default              | Description |
| --------- | ----- | -------------------- | ----------- |
| `event`   | `str` | `"ReasoningStarted"` | Event type  |

### ReasoningStepEvent

| Attribute           | Type            | Default           | Description                   |
| ------------------- | --------------- | ----------------- | ----------------------------- |
| `event`             | `str`           | `"ReasoningStep"` | Event type                    |
| `content`           | `Optional[Any]` | `None`            | Content of the reasoning step |
| `content_type`      | `str`           | `"str"`           | Type of the content           |
| `reasoning_content` | `str`           | `""`              | Detailed reasoning content    |

### ReasoningCompletedEvent

| Attribute      | Type            | Default                | Description                   |
| -------------- | --------------- | ---------------------- | ----------------------------- |
| `event`        | `str`           | `"ReasoningCompleted"` | Event type                    |
| `content`      | `Optional[Any]` | `None`                 | Content of the reasoning step |
| `content_type` | `str`           | `"str"`                | Type of the content           |

### ToolCallStartedEvent

| Attribute | Type                      | Default             | Description           |
| --------- | ------------------------- | ------------------- | --------------------- |
| `event`   | `str`                     | `"ToolCallStarted"` | Event type            |
| `tool`    | `Optional[ToolExecution]` | `None`              | The tool being called |

### ToolCallCompletedEvent

| Attribute | Type                      | Default               | Description                 |
| --------- | ------------------------- | --------------------- | --------------------------- |
| `event`   | `str`                     | `"ToolCallCompleted"` | Event type                  |
| `tool`    | `Optional[ToolExecution]` | `None`                | The tool that was called    |
| `content` | `Optional[Any]`           | `None`                | Result of the tool call     |
| `images`  | `Optional[List[Image]]`   | `None`                | Images produced by the tool |
| `videos`  | `Optional[List[Video]]`   | `None`                | Videos produced by the tool |
| `audio`   | `Optional[List[Audio]]`   | `None`                | Audio produced by the tool  |

### MemoryUpdateStartedEvent

| Attribute | Type  | Default                 | Description |
| --------- | ----- | ----------------------- | ----------- |
| `event`   | `str` | `"MemoryUpdateStarted"` | Event type  |

### MemoryUpdateCompletedEvent

| Attribute | Type  | Default                   | Description |
| --------- | ----- | ------------------------- | ----------- |
| `event`   | `str` | `"MemoryUpdateCompleted"` | Event type  |

### SessionSummaryStartedEvent

| Attribute | Type  | Default                   | Description |
| --------- | ----- | ------------------------- | ----------- |
| `event`   | `str` | `"SessionSummaryStarted"` | Event type  |

### SessionSummaryCompletedEvent

| Attribute         | Type                       | Default                     | Description                   |
| ----------------- | -------------------------- | --------------------------- | ----------------------------- |
| `event`           | `str`                      | `"SessionSummaryCompleted"` | Event type                    |
| `session_summary` | `Optional[SessionSummary]` | `None`                      | The generated session summary |

### ParserModelResponseStartedEvent

| Attribute | Type  | Default                        | Description |
| --------- | ----- | ------------------------------ | ----------- |
| `event`   | `str` | `"ParserModelResponseStarted"` | Event type  |

### ParserModelResponseCompletedEvent

| Attribute | Type  | Default                          | Description |
| --------- | ----- | -------------------------------- | ----------- |
| `event`   | `str` | `"ParserModelResponseCompleted"` | Event type  |

### OutputModelResponseStartedEvent

| Attribute | Type  | Default                        | Description |
| --------- | ----- | ------------------------------ | ----------- |
| `event`   | `str` | `"OutputModelResponseStarted"` | Event type  |

### OutputModelResponseCompletedEvent

| Attribute | Type  | Default                          | Description |
| --------- | ----- | -------------------------------- | ----------- |
| `event`   | `str` | `"OutputModelResponseCompleted"` | Event type  |

| Attribute | Type  | Default         | Description |
| --------- | ----- | --------------- | ----------- |
| `event`   | `str` | `"CustomEvent"` | Event type  |

---

## Access Run Context and Session State in Router Selector Function

**URL:** llms-txt#access-run-context-and-session-state-in-router-selector-function

Source: https://docs.agno.com/basics/state/workflows/usage/access-session-state-in-router-selector-function

This example demonstrates how to access the run context in the selector function of a router step

1. Using `run_context.session_state` in a Router selector function
2. Making routing decisions based on session state data
3. Accessing user preferences and history from `run_context.session_state`
4. Dynamically selecting different agents based on user context

<Steps>
  <Step title="Create a Python file">
    Create a file named `access_session_state_in_router_selector_function.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Step title="Setup venv">
    <Snippet file="create-venv-step.mdx" />
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

<Step title="Run the workflow">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Setup venv">
    <Snippet file="create-venv-step.mdx" />
  </Step>

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set OpenAI Key">
    <Snippet file="set-openai-key.mdx" />
  </Step>

  <Step title="Run the workflow">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Tool Result Caching

**URL:** llms-txt#tool-result-caching

**Contents:**
- On Toolkit
- On @tool

Source: https://docs.agno.com/basics/tools/caching

Learn how to cache tool results in Agno.

Tool result caching is designed to avoid unnecessary recomputation by storing the results of function calls on disk.
This is useful during development and testing to speed up the development process, avoid rate limiting, and reduce costs.

<Check>
  This is supported for all Agno Toolkits
</Check>

Pass `cache_results=True` to the Toolkit constructor to enable caching for that Toolkit.

Pass `cache_results=True` to the `@tool` decorator to enable caching for that tool.

**Examples:**

Example 1 (unknown):
```unknown
## On @tool

Pass `cache_results=True` to the `@tool` decorator to enable caching for that tool.
```

---

## Disable batching for immediate trace visibility during development

**URL:** llms-txt#disable-batching-for-immediate-trace-visibility-during-development

Traceloop.init(app_name="agno_dev", disable_batch=True)

---

## Performance Evals

**URL:** llms-txt#performance-evals

**Contents:**
- Basic Example
- Tool Usage Performance
- Performance with asyncronous functions

Source: https://docs.agno.com/basics/evals/performance/overview

Performance evals measure the latency and memory footprint of an Agent or Team.

<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=ed1f5ecf303b83d454af05ae6e3a14d7" data-og-width="1528" data-og-height="748" data-path="images/evals/performance_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=037bf59b601e8c9b8cf5779ca66cd302 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=3c79dff6bf7eca3de5abad855b0598a0 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=d4fd86aa92eecfba7a32c9608a23abb3 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=59b2683492b2a8473b66800c0bb13129 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=a1ee1c1d8b8e01abdbabbe6e49fbac60 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=2efdebf30a5a05f1303f4baaa840db11 2500w" />
</Frame>

## Tool Usage Performance

Compare how tools affects your agent's performance:

## Performance with asyncronous functions

Evaluate agent performance with asyncronous functions:

```python async_performance.py theme={null}

"""This example shows how to run a Performance evaluation on an async function."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=ed1f5ecf303b83d454af05ae6e3a14d7" data-og-width="1528" data-og-height="748" data-path="images/evals/performance_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=037bf59b601e8c9b8cf5779ca66cd302 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=3c79dff6bf7eca3de5abad855b0598a0 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=d4fd86aa92eecfba7a32c9608a23abb3 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=59b2683492b2a8473b66800c0bb13129 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=a1ee1c1d8b8e01abdbabbe6e49fbac60 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/performance_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=2efdebf30a5a05f1303f4baaa840db11 2500w" />
</Frame>

## Tool Usage Performance

Compare how tools affects your agent's performance:
```

Example 2 (unknown):
```unknown
## Performance with asyncronous functions

Evaluate agent performance with asyncronous functions:
```

---

## All logging will now use the custom agent logger by default

**URL:** llms-txt#all-logging-will-now-use-the-custom-agent-logger-by-default

log_info("Using custom loggers!")

---

## Define how the agent should analyze the data

**URL:** llms-txt#define-how-the-agent-should-analyze-the-data

**Contents:**
  - 3c. Define the Intelligence Synthesis

analysis_framework = dedent("""
    ANALYSIS FRAMEWORK:
    - Classify sentiment as Positive/Negative/Neutral/Mixed with detailed reasoning
    - Weight analysis by engagement volume and author influence (verified accounts = 1.5x)
    - Identify engagement patterns: viral advocacy, controversy, influence concentration
    - Extract cross-platform themes and recurring discussion points
""")

print("Analysis framework defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3c. Define the Intelligence Synthesis
```

---

## Initialize AsyncSqliteDb

**URL:** llms-txt#initialize-asyncsqlitedb

**Contents:**
- Params
- Developer Resources

db = AsyncSqliteDb(db_file="agent_storage.db")

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    add_datetime_to_context=True,
)

asyncio.run(agent.aprint_response("How many people live in Canada?"))
asyncio.run(agent.aprint_response("What is their national anthem called?"))

<Snippet file="db-async-sqlite-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/sqlite/async_sqlite/async_sqlite_for_agent.py)

---

## Agent with strict_output=False (guided mode)

**URL:** llms-txt#agent-with-strict_output=false-(guided-mode)

guided_output_agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini", strict_output=False),
    description="You write movie scripts.",
    output_schema=MovieScript,
)

---

## --- Execution function ---

**URL:** llms-txt#----execution-function----

async def startup_validation_execution(
    workflow: Workflow,
    execution_input: WorkflowExecutionInput,
    startup_idea: str,
    **kwargs: Any,
) -> str:
    """Execute the complete startup idea validation workflow"""

# Get inputs
    message: str = execution_input.input
    idea: str = startup_idea

if not idea:
        return "No startup idea provided"

print(f"Starting startup idea validation for: {idea}")
    print(f"Validation request: {message}")

# Phase 1: Idea Clarification
    print("\nPHASE 1: IDEA CLARIFICATION & REFINEMENT")
    print("=" * 60)

clarification_prompt = f"""
    {message}

Please analyze and refine the following startup idea:

Evaluate:
    1. The originality of this idea compared to existing solutions
    2. Define a clear mission statement for this startup
    3. Outline specific, measurable objectives
    Provide insights on how to strengthen and focus the core concept.
    """

print("Analyzing and refining the startup concept...")

try:
        clarification_result = await idea_clarifier_agent.arun(clarification_prompt)
        idea_clarification = clarification_result.content

print("Idea clarification completed")
        print(f"Mission: {idea_clarification.mission[:100]}...")

except Exception as e:
        return f"Failed to clarify idea: {str(e)}"

# Phase 2: Market Research
    print("\nPHASE 2: MARKET RESEARCH & ANALYSIS")
    print("=" * 60)

market_research_prompt = f"""
    Based on the refined startup idea and clarification below, conduct comprehensive market research:
    STARTUP IDEA: {idea}
    ORIGINALITY: {idea_clarification.originality}
    MISSION: {idea_clarification.mission}
    OBJECTIVES: {idea_clarification.objectives}
    Please research and provide:
    1. Total Addressable Market (TAM) - overall market size
    2. Serviceable Available Market (SAM) - portion you could serve
    3. Serviceable Obtainable Market (SOM) - realistic market share
    4. Target customer segments with detailed characteristics
    Use web search to find current market data and trends.
    """

print("Researching market size and customer segments...")

try:
        market_result = await market_research_agent.arun(market_research_prompt)
        market_research = market_result.content

print("Market research completed")
        print(f"TAM: {market_research.total_addressable_market[:100]}...")

except Exception as e:
        return f"Failed to complete market research: {str(e)}"

# Phase 3: Competitor Analysis
    print("\nPHASE 3: COMPETITIVE LANDSCAPE ANALYSIS")
    print("=" * 60)

competitor_prompt = f"""
    Based on the startup idea and market research below, analyze the competitive landscape:
    STARTUP IDEA: {idea}
    TAM: {market_research.total_addressable_market}
    SAM: {market_research.serviceable_available_market}
    SOM: {market_research.serviceable_obtainable_market}
    TARGET SEGMENTS: {market_research.target_customer_segments}
    Please research and provide:
    1. Identify direct and indirect competitors
    2. SWOT analysis for each major competitor
    3. Assessment of startup's potential competitive positioning
    4. Market gaps and opportunities
    Use web search to find current competitor information.
    """

print("Analyzing competitive landscape...")

try:
        competitor_result = await competitor_analysis_agent.arun(competitor_prompt)
        competitor_analysis = competitor_result.content

print("Competitor analysis completed")
        print(f"Positioning: {competitor_analysis.positioning[:100]}...")

except Exception as e:
        return f"Failed to complete competitor analysis: {str(e)}"

# Phase 4: Final Validation Report
    print("\nPHASE 4: COMPREHENSIVE VALIDATION REPORT")
    print("=" * 60)

report_prompt = f"""
    Synthesize all the research and analysis into a comprehensive startup validation report:

IDEA CLARIFICATION:
    - Originality: {idea_clarification.originality}
    - Mission: {idea_clarification.mission}
    - Objectives: {idea_clarification.objectives}
    MARKET RESEARCH:
    - TAM: {market_research.total_addressable_market}
    - SAM: {market_research.serviceable_available_market}
    - SOM: {market_research.serviceable_obtainable_market}
    - Target Segments: {market_research.target_customer_segments}
    COMPETITOR ANALYSIS:
    - Competitors: {competitor_analysis.competitors}
    - SWOT: {competitor_analysis.swot_analysis}
    - Positioning: {competitor_analysis.positioning}
    Create a professional validation report with:
    1. Executive summary
    2. Idea assessment (strengths/weaknesses)
    3. Market opportunity analysis
    4. Competitive landscape overview
    5. Strategic recommendations
    6. Specific next steps for the entrepreneur
    """

print("Generating comprehensive validation report...")

try:
        final_result = await report_agent.arun(report_prompt)
        validation_report = final_result.content

print("Validation report completed")

except Exception as e:
        return f"Failed to generate final report: {str(e)}"

# Final summary
    summary = f"""
    STARTUP IDEA VALIDATION COMPLETED!
    Validation Summary:
    â€¢ Startup Idea: {idea}
    â€¢ Idea Clarification: Completed
    â€¢ Market Research: Completed
    â€¢ Competitor Analysis: Completed
    â€¢ Final Report: Generated

Key Market Insights:
    â€¢ TAM: {market_research.total_addressable_market[:150]}...
    â€¢ Target Segments: {market_research.target_customer_segments[:150]}...

Competitive Positioning:
    {competitor_analysis.positioning[:200]}...

COMPREHENSIVE VALIDATION REPORT:

## Executive Summary
    {validation_report.executive_summary}

## Idea Assessment
    {validation_report.idea_assessment}

## Market Opportunity
    {validation_report.market_opportunity}

## Competitive Landscape
    {validation_report.competitive_landscape}

## Strategic Recommendations
    {validation_report.recommendations}

## Next Steps
    {validation_report.next_steps}

Disclaimer: This validation is for informational purposes only. Conduct additional due diligence before making investment decisions.
    """

---

## Ask about the conversation

**URL:** llms-txt#ask-about-the-conversation

**Contents:**
- Usage

agent.print_response(
    "What have we been talking about, do you know my name?", stream=True
)

bash  theme={null}
    export PERPLEXITY_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/models/perplexity/memory.py
      bash Windows theme={null}
      python cookbook/models/perplexity/memory.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Team Input as Image List

**URL:** llms-txt#team-input-as-image-list

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/input-output/team/usage/input-as-list

This example demonstrates how to pass input to a team as a list containing both text and images. The team processes multimodal input including text descriptions and image URLs for analysis.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Run workflow and display events

**URL:** llms-txt#run-workflow-and-display-events

**Contents:**
  - Async Streaming

for event in workflow.run(
    "What is Python?",
    stream=True,
    stream_events=True,
):
    event_name = event.event if hasattr(event, "event") else type(event).__name__
    print(f"  â†’ {event_name}")
Python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Async Streaming

The `Workflow.arun(stream=True)` returns an async iterator of `WorkflowRunOutputEvent` objects instead of a single response.
So for example, if you want to stream the response, you can do the following:
```

---

## -*- Make tool call

**URL:** llms-txt#-*--make-tool-call

agent.print_response("What is the weather in nyc?", stream=True)

---

## Getting Started

**URL:** llms-txt#getting-started

**Contents:**
- What You'll Build
- Prerequisites
- Step 1: Set Up Your Knowledge Base

Source: https://docs.agno.com/basics/knowledge/getting-started

Build your first knowledge-powered agent in three simple steps with this hands-on tutorial.

Ready to build your first intelligent agent? This guide will walk you through creating a knowledge-powered agent that can answer questions about your documents in just a few minutes.

By the end of this tutorial, you'll have an agent that can:

* Read and understand your documents or website content
* Answer specific questions based on that information
* Provide sources for its responses
* Search intelligently without you having to specify what to look for

<Steps>
  <Step title="Install Agno">
    
  </Step>

<Step title="Set up your API key">

<Note>This tutorial uses OpenAI, but Agno supports [many other models](/basics/models/overview).</Note>
  </Step>
</Steps>

## Step 1: Set Up Your Knowledge Base

First, let's create a knowledge base with a vector database to store your information:

```python knowledge_agent.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Set up your API key">
```

Example 2 (unknown):
```unknown
<Note>This tutorial uses OpenAI, but Agno supports [many other models](/basics/models/overview).</Note>
  </Step>
</Steps>

## Step 1: Set Up Your Knowledge Base

First, let's create a knowledge base with a vector database to store your information:
```

---

## âœ… Choose one approach

**URL:** llms-txt#âœ…-choose-one-approach

agent = Agent(db=db, enable_user_memories=True)  # Automatic

---

## Get embeddings directly

**URL:** llms-txt#get-embeddings-directly

embeddings = VLLMEmbedder(
    id="intfloat/e5-mistral-7b-instruct",
    dimensions=4096,
    enforce_eager=True,
    vllm_kwargs={
        "disable_sliding_window": True,
        "max_model_len": 4096,
    },
).get_embedding("The quick brown fox jumps over the lazy dog.")

print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

---

## Upstash Vector Database

**URL:** llms-txt#upstash-vector-database

Source: https://docs.agno.com/integrations/vectordb/upstash/overview

Learn how to use Upstash as a vector database for your Knowledge Base

---

## Distributed RAG with LanceDB

**URL:** llms-txt#distributed-rag-with-lancedb

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/distributed-rag-lancedb

This example demonstrates how multiple specialized agents coordinate to provide comprehensive RAG responses using distributed knowledge bases and specialized retrieval strategies with LanceDB. The team includes primary retrieval, context expansion, answer synthesis, and quality validation.

```python cookbook/examples/teams/distributed_rag/02_distributed_rag_lancedb.py theme={null}
"""
This example demonstrates how multiple specialized agents coordinate to provide
comprehensive RAG responses using distributed knowledge bases and specialized
retrieval strategies with LanceDB.

Team Composition:
- Primary Retriever: Handles primary document retrieval from main knowledge base
- Context Expander: Expands context by finding related information
- Answer Synthesizer: Synthesizes retrieved information into comprehensive answers
- Quality Validator: Validates answer quality and suggests improvements

Setup:
1. Run: `pip install openai lancedb tantivy pypdf sqlalchemy agno`
2. Run this script to see distributed RAG in action
"""

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.vectordb.lancedb import LanceDb, SearchType

---

## Step 3: Instrument and execute

**URL:** llms-txt#step-3:-instrument-and-execute

with instrument_agno("openai"):
    response = agent.run("Retrieve the latest news about the stock market.")
    print(response.content)
```

Now go to the [Atla dashboard](https://app.atla-ai.com/app/) and view the traces created by your agent. You can visualize the execution flow, monitor performance, and debug issues directly from the Atla dashboard.

<Frame caption="Atla Agent run trace">
  <img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=83334fabbdbc6d69fd6c322568a79910" style={{ borderRadius: '10px', width: '100%', maxWidth: '800px' }} alt="atla-trace" data-og-width="1482" width="1482" data-og-height="853" height="853" data-path="images/atla-trace-summary.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c16251e3e7934ba377fcc96c87fb9c94 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=30b4c98898ac04641de69533b0e9b2b3 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=846aa70151fd7874c52b08db17fb25ac 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0ddcdd1f24323b0cfe1f38af5bc56b03 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=6f8a77a8d026dfc3533084b788e3caf0 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e9e30bd974ffc954415f1a9e0a5931d3 2500w" />
</Frame>

---

## print(chunk.content)

**URL:** llms-txt#print(chunk.content)

---

## Load the document

**URL:** llms-txt#load-the-document

raw_documents = TextLoader(str(state_of_the_union), encoding="utf-8").load()

---

## the `step_input`.

**URL:** llms-txt#the-`step_input`.

class CustomContentPlanning:
    def __call__(self, step_input: StepInput) -> StepOutput:
        """
        Custom function that does intelligent content planning with context awareness
        """
        message = step_input.input
        previous_step_content = step_input.previous_step_content

# Create intelligent planning prompt
        planning_prompt = f"""
            STRATEGIC CONTENT PLANNING REQUEST:
            Core Topic: {message}
            Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}
            Planning Requirements:
            1. Create a comprehensive content strategy based on the research
            2. Leverage the research findings effectively
            3. Identify content formats and channels
            4. Provide timeline and priority recommendations
            5. Include engagement and distribution strategies
            Please create a detailed, actionable content plan.
        """

try:
            response = content_planner.run(planning_prompt)

enhanced_content = f"""
                ## Strategic Content Plan
                **Planning Topic:** {message}
                **Research Integration:** {"âœ“ Research-based" if previous_step_content else "âœ— No research foundation"}
                **Content Strategy:**
                {response.content}
                **Custom Planning Enhancements:**
                - Research Integration: {"High" if previous_step_content else "Baseline"}
                - Strategic Alignment: Optimized for multi-channel distribution
                - Execution Ready: Detailed action items included
            """.strip()

return StepOutput(content=enhanced_content)

except Exception as e:
            return StepOutput(
                content=f"Custom content planning failed: {str(e)}",
                success=False,
            )

---

## Website Tools

**URL:** llms-txt#website-tools

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/web-scrape/website

**WebsiteTools** enable an Agent to parse a website and add its contents to the knowledge base.

The following example requires the `beautifulsoup4` library.

The following agent will read the contents of a website and add it to the knowledge base.

| Parameter   | Type        | Default | Description                                                                                                            |
| ----------- | ----------- | ------- | ---------------------------------------------------------------------------------------------------------------------- |
| `knowledge` | `Knowledge` | -       | The knowledge base associated with the website, containing various data and resources linked to the website's content. |

| Function                        | Description                                                                                                                                                                                                          |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `add_website_to_knowledge_base` | This function adds a website's content to the knowledge base. **NOTE:** The website must start with `https://` and should be a valid website. Use this function to get information about products from the internet. |
| `read_url`                      | This function reads a URL and returns the contents.                                                                                                                                                                  |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/website.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/website_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will read the contents of a website and add it to the knowledge base.
```

---

## SQL

**URL:** llms-txt#sql

**Contents:**
- Prerequisites
  - PostgreSQL
  - MySQL
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/database/sql

The SQLTools toolkit enables an Agent to run SQL queries and interact with databases.

**SQLTools** enable an Agent to run SQL queries and interact with databases.

The following example requires the `sqlalchemy` library and a database URL.

You will also need to install the appropriate Python adapter for the specific database you intend to use.

For PostgreSQL, you can install the `psycopg2-binary` adapter:

For MySQL, you can install the `mysqlclient` adapter:

The `mysqlclient` adapter may have additional system-level dependencies. Please consult the [official installation guide](https://github.com/PyMySQL/mysqlclient/blob/main/README.md#install) for more details.

You will also need a database. The following example uses a Postgres database running in a Docker container.

The following agent will run a SQL query to list all tables in the database and describe the contents of one of the tables.

| Parameter               | Type             | Default | Description                                                                 |
| ----------------------- | ---------------- | ------- | --------------------------------------------------------------------------- |
| `db_url`                | `str`            | `None`  | The URL for connecting to the database.                                     |
| `db_engine`             | `Engine`         | `None`  | The database engine used for connections and operations.                    |
| `user`                  | `str`            | `None`  | The username for database authentication.                                   |
| `password`              | `str`            | `None`  | The password for database authentication.                                   |
| `host`                  | `str`            | `None`  | The hostname or IP address of the database server.                          |
| `port`                  | `int`            | `None`  | The port number on which the database server is listening.                  |
| `schema`                | `str`            | `None`  | The specific schema within the database to use.                             |
| `dialect`               | `str`            | `None`  | The SQL dialect used by the database.                                       |
| `tables`                | `Dict[str, Any]` | `None`  | A dictionary mapping table names to their respective metadata or structure. |
| `enable_list_tables`    | `bool`           | `True`  | Enables the functionality to list all tables in the database.               |
| `enable_describe_table` | `bool`           | `True`  | Enables the functionality to describe the schema of a specific table.       |
| `enable_run_sql_query`  | `bool`           | `True`  | Enables the functionality to execute SQL queries directly.                  |
| `all`                   | `bool`           | `False` | Enables all functionality when set to True.                                 |

| Function         | Description                               |
| ---------------- | ----------------------------------------- |
| `list_tables`    | Lists all tables in the database.         |
| `describe_table` | Describes the schema of a specific table. |
| `run_sql_query`  | Executes SQL queries directly.            |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/sql.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/sql_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
You will also need to install the appropriate Python adapter for the specific database you intend to use.

### PostgreSQL

For PostgreSQL, you can install the `psycopg2-binary` adapter:
```

Example 2 (unknown):
```unknown
### MySQL

For MySQL, you can install the `mysqlclient` adapter:
```

Example 3 (unknown):
```unknown
The `mysqlclient` adapter may have additional system-level dependencies. Please consult the [official installation guide](https://github.com/PyMySQL/mysqlclient/blob/main/README.md#install) for more details.

You will also need a database. The following example uses a Postgres database running in a Docker container.
```

Example 4 (unknown):
```unknown
## Example

The following agent will run a SQL query to list all tables in the database and describe the contents of one of the tables.
```

---

## Access Dependencies in Tool

**URL:** llms-txt#access-dependencies-in-tool

Source: https://docs.agno.com/basics/dependencies/agent/usage/access-dependencies-in-tool

This example demonstrates how tools can access dependencies passed to the agent, allowing tools to utilize dynamic context like user profiles and current time information for enhanced functionality.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Dependencies with Teams

**URL:** llms-txt#dependencies-with-teams

**Contents:**
- Learn more

Source: https://docs.agno.com/basics/dependencies/team/overview

Learn how to use dependencies in your teams.

**Dependencies** are a way to inject variables into your Team context. The `dependencies` parameter accepts a dictionary containing functions or static variables that are automatically resolved before the team runs.

<Note>
  You can use dependencies to inject memories, dynamic few-shot examples, "retrieved" documents, etc.
</Note>

<Check>
  Dependencies are automatically resolved when the team is run.
</Check>

<Tip>
  You can set `dependencies` on `Team` initialization, or pass it to the `run()`, `arun()`, `print_response()` and `aprint_response()` methods. Use `add_dependencies_to_context=True` to automatically add all dependencies to the user message instead of referencing them in instructions.
</Tip>

<CardGroup cols={2}>
  <Card title="Dependencies Overview" icon="book" href="/basics/dependencies/overview">
    Learn the fundamentals of dependency injection
  </Card>

<Card title="Reference Dependencies" icon="link" href="/basics/dependencies/team/usage/reference-dependencies">
    Reference dependencies in instructions
  </Card>

<Card title="Access Dependencies in Tool" icon="wrench" href="/basics/dependencies/team/usage/access-dependencies-in-tool">
    Use RunContext to access dependencies in tools
  </Card>

<Card title="Team Schema" icon="file-code" href="/reference/teams/team">
    View the complete Team API reference
  </Card>
</CardGroup>

---

## Create team with custom tool and agent members

**URL:** llms-txt#create-team-with-custom-tool-and-agent-members

team = Team(name="Q & A team", members=[web_agent], tools=[answer_from_known_questions])

---

## Updating an Agent's Tools

**URL:** llms-txt#updating-an-agent's-tools

**Contents:**
- Agent Example

Source: https://docs.agno.com/basics/tools/attaching-tools

Learn how to add/update tools on Agents and Teams after they have been created.

Tools can be added to Agents and Teams post-creation. This gives you the flexibility to add tools to an existing Agent or Team instance after initialization, which is useful for dynamic tool management or when you need to conditionally add tools based on runtime requirements.
The whole collection of tools available to an Agent or Team can also be updated by using the `set_tools` call. Note that this will remove any other tools already assigned to your Agent or Team and override it with the list of tools provided to `set_tools`.

Create your own tool, for example `get_weather`. Then call `add_tool` to attach it to your Agent.

---

## Create and configure the agent

**URL:** llms-txt#create-and-configure-the-agent

agent = Agent(model=OpenAIChat(id="gpt-5-mini"), markdown=True, debug_mode=True)

---

## How team delegation works

**URL:** llms-txt#how-team-delegation-works

**Contents:**
- Members respond directly

Source: https://docs.agno.com/basics/teams/delegation

How tasks are delegated to team members.

A `Team` internally has a team-leader "agent" that delegates tasks and requests to team members.  When you call `run` or `arun` on a team, the team leader uses a model to determine which member to delegate the task to.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=2ee1ac8bf7774125e30b2f8a8ba5e0e5" alt="Team delegation flow" data-og-width="4353" width="4353" data-og-height="1233" height="1233" data-path="images/teams/team-delegate-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c916681a232cce0726b37ff11231199c 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=b969c1ebdafcf0a396919831b66e5705 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=71f8372710764574ec6c1a90077e2b32 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=74343688fc677499b5cbf5c4561aec65 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=8d9a72eaad258842d959f0a29124dba0 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=cc4d6396915b59c137aadd29eab708c3 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=75634aa1f580bf358dafee8c7e114d85" alt="Team delegation flow" data-og-width="4353" width="4353" data-og-height="1233" height="1233" data-path="images/teams/team-delegate-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=7c700427d1047a065738303c582321da 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=e1df59a26d24fc7b73b489264700bed1 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=9fc3130ac1b144e34ef120ca627b6a9d 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=e4670efd70404a5dc612e89a81e01850 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=550dac3bfceea50fa94481a8562b5851 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=4d0a1bbeba92f6b5a4d5f02d2d57084f 2500w" />

1. The team receives user input
2. The team leader analyzes the input and decides how to break it down into subtasks
3. The team leader delegates specific tasks to appropriate team members.
4. Team members complete their assigned tasks and return their results
5. The team leader then either delegates to more team members, or synthesizes all outputs into a final, cohesive response to return to the user

<Note>
  Delegating to members is done by the team leader deciding to use a **tool**, namely the `delegate_task_to_member` tool.

When running the team asynchronously (using `arun`), and the team leader decides to delegate to multiple members at once, these members will run **concurrently**. Behind the scenes this is just concurrent execution of the `delegate_task_to_member` tool.
</Note>

There are various ways to control how the team delegates tasks to members:

* **How do I return the response of members directly?** â†’ See [Members respond directly](#members-respond-directly)
* **How do I send my user input directly to the members without the team leader synthesizing it?** â†’ See [Send input directly to members](#send-input-directly-to-members)
* **How do I make sure the team leader delegates the task to all members at the same time?** â†’ See [Delegate tasks to all members](#delegate-tasks-to-all-members-simultaneously)

## Members respond directly

By default, the team leader processes responses from members and synthesizes them into a single cohesive response.

Set `respond_directly=True` to return member responses directly without team leader synthesis.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=a855fa91b944dceb3789755b812fe5c2" alt="Team direct response flow" data-og-width="3741" width="3741" data-og-height="906" height="906" data-path="images/teams/team-direct-response-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=27e35955cb85dca4bc71dc1d1dfbbf29 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=387ba410b479f59b05223337345e5718 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=dce5fcec7a3430097af30350ac8b5553 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=3d998291345023788b76061af03957d9 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=af7a76b687b05506b17216021005ba6b 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=915198cecad6c4257b3d4e83fc07cf91 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=d9e2544a995856e07934634987e04b19" alt="Team direct response flow" data-og-width="3741" width="3741" data-og-height="906" height="906" data-path="images/teams/team-direct-response-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=173929609381c8ea1e48f47a139f9b67 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=8824c3afb76ad9cdabb91aee3e90349d 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c8e7b3a1a78a34a3433bd5a8cffabb1c 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=76bee7fe4ec738d466d3c4eeac29aff5 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=61b1e5c2dd13a193a9c06eb2635f3bf5 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-direct-response-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=f97f8c756fbdfab1a16a00f3c79f6c81 2500w" />

<Tip>
  Use this feature with `determine_input_for_members=False` to create a **passthrough pattern** team â€” the team leader is effectively bypassed and all communication is directly with a team member.
</Tip>

**Example:** Create a language router that directs questions to language-specific agents:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team

english_agent = Agent(
    name="English Agent",
    role="You can only answer in English",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "You must only respond in English",
    ],
)

japanese_agent = Agent(
    name="Japanese Agent",
    role="You can only answer in Japanese",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "You must only respond in Japanese",
    ],
)
multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat("gpt-4.5-preview"),
    respond_directly=True,
    members=[
        english_agent,
        japanese_agent,
    ],
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English and Japanese. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    show_members_responses=True,
)

---

## BAAI/bge-reranker-v2-m3

**URL:** llms-txt#baai/bge-reranker-v2-m3

---

## Async Basic

**URL:** llms-txt#async-basic

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/async-basic

```python cookbook/models/openai/responses/async_basic.py theme={null}
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-5-mini"), markdown=True)

---

## Agent with Structured Outputs Streaming

**URL:** llms-txt#agent-with-structured-outputs-streaming

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/structured-output-stream

```python structured_output_stream.py theme={null}
from typing import Dict, List

from agno.agent import Agent
from agno.models.anthropic import Claude
from pydantic import BaseModel, Field

class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )

---

## Condition steps workflow

**URL:** llms-txt#condition-steps-workflow

Source: https://docs.agno.com/basics/workflows/usage/condition-steps-workflow-stream

This example demonstrates how to use conditional steps in a workflow.

This example demonstrates **Workflows 2.0** conditional execution pattern. Shows how to conditionally execute steps based on content analysis,
providing intelligent selection of steps based on the actual data being processed.

**When to use**: When you need intelligent selection of steps based on content analysis rather than
simple input parameters or some other business logic. Ideal for quality gates, content-specific processing, or
adaptive workflows that respond to intermediate results.

```python condition_steps_workflow_stream.py theme={null}
from agno.agent.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.workflow.condition import Condition
from agno.workflow.step import Step
from agno.workflow.types import StepInput
from agno.workflow.workflow import Workflow

---

## AWS Bedrock Claude

**URL:** llms-txt#aws-bedrock-claude

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/bedrock-claude

The AWS Bedrock Claude model provides access to Anthropic's Claude models hosted on AWS Bedrock.

| Parameter               | Type                       | Default                                       | Description                                                          |
| ----------------------- | -------------------------- | --------------------------------------------- | -------------------------------------------------------------------- |
| `id`                    | `str`                      | `"anthropic.claude-3-5-sonnet-20241022-v2:0"` | The id of the AWS Bedrock Claude model to use                        |
| `name`                  | `str`                      | `"BedrockClaude"`                             | The name of the model                                                |
| `provider`              | `str`                      | `"AWS"`                                       | The provider of the model                                            |
| `max_tokens`            | `Optional[int]`            | `4096`                                        | Maximum number of tokens to generate in the chat completion          |
| `thinking`              | `Optional[Dict[str, Any]]` | `None`                                        | Configuration for the thinking (reasoning) process                   |
| `temperature`           | `Optional[float]`          | `None`                                        | Controls randomness in the model's output                            |
| `stop_sequences`        | `Optional[List[str]]`      | `None`                                        | A list of strings that the model should stop generating text at      |
| `top_p`                 | `Optional[float]`          | `None`                                        | Controls diversity via nucleus sampling                              |
| `top_k`                 | `Optional[int]`            | `None`                                        | Controls diversity via top-k sampling                                |
| `cache_system_prompt`   | `Optional[bool]`           | `False`                                       | Whether to cache the system prompt for improved performance          |
| `extended_cache_time`   | `Optional[bool]`           | `False`                                       | Whether to use extended cache time (1 hour instead of default)       |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`                                        | Additional parameters to include in the request                      |
| `aws_region`            | `Optional[str]`            | `None`                                        | The AWS region to use (defaults to AWS\_REGION env var)              |
| `aws_access_key_id`     | `Optional[str]`            | `None`                                        | AWS access key ID (defaults to AWS\_ACCESS\_KEY\_ID env var)         |
| `aws_secret_access_key` | `Optional[str]`            | `None`                                        | AWS secret access key (defaults to AWS\_SECRET\_ACCESS\_KEY env var) |
| `aws_session_token`     | `Optional[str]`            | `None`                                        | AWS session token (defaults to AWS\_SESSION\_TOKEN env var)          |
| `aws_profile`           | `Optional[str]`            | `None`                                        | AWS profile to use (defaults to AWS\_PROFILE env var)                |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`                                        | Additional parameters for client configuration                       |
| `retries`               | `int`                      | `0`                                           | Number of retries to attempt before raising a ModelProviderError     |
| `delay_between_retries` | `int`                      | `1`                                           | Delay between retries, in seconds                                    |
| `exponential_backoff`   | `bool`                     | `False`                                       | If True, the delay between retries is doubled each time              |

---

## List Sessions

**URL:** llms-txt#list-sessions

Source: https://docs.agno.com/reference-api/schema/sessions/list-sessions

get /sessions
Retrieve paginated list of sessions with filtering and sorting options. Supports filtering by session type (agent, team, workflow), component, user, and name. Sessions represent conversation histories and execution contexts.

---

## Understands intent and context

**URL:** llms-txt#understands-intent-and-context

user: "Can I send back this item I bought last month?"

---

## Model Context Protocol (MCP)

**URL:** llms-txt#model-context-protocol-(mcp)

Source: https://docs.agno.com/basics/tools/mcp/overview

Learn how to use MCP with Agno to enable your agents to interact with external systems through a standardized interface.

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) enables Agents to interact with external systems through a standardized interface.
You can connect your Agents to any MCP server, using Agno's MCP integration.

Below is a simple example shows how to connect an Agent to the Agno MCP server:

```python  theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.mcp import MCPTools

---

## With agentic memory: (10 Ã— 500) + (7 Ã— 5,000) = 40,000 tokens

**URL:** llms-txt#with-agentic-memory:-(10-Ã—-500)-+-(7-Ã—-5,000)-=-40,000-tokens

---

## Initialize your Agent passing the database

**URL:** llms-txt#initialize-your-agent-passing-the-database

**Contents:**
  - Adding session history to the context

agent = Agent(db=db)
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SQLiteDb

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  It's recommended to add an `id` for better management and easier identification of your database. You can add it to your database configuration like this:
</Note>

<Tip>
  **When should I use Storage on my Agents?**

  Agents are ephemeral by default. They won't remember previous conversations.

  But in production environments, you will often need to continue the same session across multiple requests. Storage is the way to persist the session history and state in a database, enabling you and your users to pick up where you left off.

  Storage also lets us inspect and evaluate Agent sessions, extract few-shot examples and build internal monitoring tools. In general, it lets you **keep track of the data**, to build better Agents.
</Tip>

### Adding session history to the context

Agents backed by a database persist their [sessions](/basics/sessions/overview) to it. You can then load this persisted history into the context to maintain continuity across sessions.
```

---

## Understanding Server Parameters

**URL:** llms-txt#understanding-server-parameters

Source: https://docs.agno.com/basics/tools/mcp/server-params

Understanding how to configure the server parameters for the MCPTools and MultiMCPTools classes

The recommended way to configure `MCPTools` is to use the `command` or `url` parameters.

Alternatively, you can use the `server_params` parameter with `MCPTools` to configure the connection to the MCP server in more detail.

When using the **stdio** transport, the `server_params` parameter should be an instance of `StdioServerParameters`. It contains the following keys:

* `command`: The command to run the MCP server.
  * Use `npx` for mcp servers that can be installed via npm (or `node` if running on Windows).
  * Use `uvx` for mcp servers that can be installed via uvx.
  * Use custom binary executables (e.g., `./my-server`, `../usr/local/bin/my-server`, or binaries in your PATH).
* `args`: The arguments to pass to the MCP server.
* `env`: Optional environment variables to pass to the MCP server. Remember to include all current environment variables in the `env` dictionary. If `env` is not provided, the current environment variables will be used.
  e.g.

When using the **SSE** transport, the `server_params` parameter should be an instance of `SSEClientParams`. It contains the following fields:

* `url`: The URL of the MCP server.
* `headers`: Headers to pass to the MCP server (optional).
* `timeout`: Timeout for the connection to the MCP server (optional).
* `sse_read_timeout`: Timeout for the SSE connection itself (optional).

When using the **Streamable HTTP** transport, the `server_params` parameter should be an instance of `StreamableHTTPClientParams`. It contains the following fields:

* `url`: The URL of the MCP server.
* `headers`: Headers to pass to the MCP server (optional).
* `timeout`: Timeout for the connection to the MCP server (optional).
* `sse_read_timeout`: how long (in seconds) the client will wait for a new event before disconnecting. All other HTTP operations are controlled by `timeout` (optional).
* `terminate_on_close`: Whether to terminate the connection when the client is closed (optional).

---

## target_audience="Developers",

**URL:** llms-txt#target_audience="developers",

---

## Image File Input Agent

**URL:** llms-txt#image-file-input-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/image-file-input-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Overriding Routes

**URL:** llms-txt#overriding-routes

**Contents:**
- When to Use
- Configuration Options
- Example

Source: https://docs.agno.com/agent-os/custom-fastapi/override-routes

Learn how to override AgentOS routes with your own custom routes when conflicts occur

When integrating your custom FastAPI application with AgentOS, route conflicts can occur if both your app and AgentOS define the same endpoint paths. For example, both might define a `/health` endpoint or a root `/` route.

AgentOS provides the `on_route_conflict` parameter to control how these conflicts are resolved, allowing you to choose whether your custom routes or AgentOS routes take precedence.

Override routes when you need:

* **Custom health checks**: Replace AgentOS's `/health` endpoint with your own monitoring logic
* **Branded landing pages**: Serve a custom homepage at `/` instead of the default AgentOS interface
* **Custom authentication**: Implement your own auth endpoints that conflict with AgentOS defaults
* **API versioning**: Control which version of an endpoint is exposed
* **Custom error handlers**: Define specialized error handling for specific routes

## Configuration Options

AgentOS provides two values for the `on_route_conflict` parameter for handling route conflicts:

| Option                       | Custom Routes | AgentOS Routes             | Warnings Logged |
| ---------------------------- | ------------- | -------------------------- | --------------- |
| `preserve_base_app`          | âœ“ Preserved   | âœ— Skipped (conflicts only) | Yes             |
| `preserve_agentos` (default) | âœ— Overridden  | âœ“ Preserved                | Yes             |

<Note>
  Non-conflicting routes from both your app and AgentOS are always included regardless of the mode.
</Note>

This example demonstrates using `on_route_conflict="preserve_base_app"` to preserve custom routes for the home page and health endpoint.

```python override_routes.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.anthropic import Claude
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from fastapi import FastAPI

---

## Custom health endpoint (conflicts with AgentOS health route)

**URL:** llms-txt#custom-health-endpoint-(conflicts-with-agentos-health-route)

@app.get("/health")
async def get_custom_health():
    return {"status": "custom_ok", "note": "This is your custom health endpoint"}

---

## Initialize knowledge base

**URL:** llms-txt#initialize-knowledge-base

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

knowledge = Knowledge()
knowledge.load_documents("./docs/")

agent = Agent(
    instructions=[
        "You are a knowledge assistant that helps find and analyze information",
        "Search through the knowledge base to answer questions",
        "Provide detailed analysis and reasoning about the information found",
        "Always cite your sources and explain your reasoning",
    ],
    tools=[KnowledgeTools(knowledge=knowledge)],
)

agent.print_response("What are the best practices for API design?", stream=True)
```

| Parameter           | Type            | Default | Description                                   |
| ------------------- | --------------- | ------- | --------------------------------------------- |
| `knowledge`         | `Knowledge`     | `None`  | Knowledge base instance (required).           |
| `enable_think`      | `bool`          | `True`  | Enable reasoning capabilities.                |
| `enable_search`     | `bool`          | `True`  | Enable knowledge search functionality.        |
| `enable_analyze`    | `bool`          | `True`  | Enable knowledge analysis capabilities.       |
| `instructions`      | `Optional[str]` | `None`  | Custom instructions for knowledge operations. |
| `add_instructions`  | `bool`          | `True`  | Whether to add instructions to the agent.     |
| `add_few_shot`      | `bool`          | `False` | Whether to include few-shot examples.         |
| `few_shot_examples` | `Optional[str]` | `None`  | Custom few-shot examples.                     |

| Function  | Description                                                |
| --------- | ---------------------------------------------------------- |
| `think`   | Apply reasoning to knowledge-based problems and questions. |
| `search`  | Search the knowledge base for relevant information.        |
| `analyze` | Perform detailed analysis of knowledge base content.       |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/knowledge.py)
* [Agno Knowledge Framework](https://docs.agno.com/knowledge)
* [Vector Database Integration](https://docs.agno.com/vector-db)

---

## Result: Filter ignored, warning logged

**URL:** llms-txt#result:-filter-ignored,-warning-logged

**Contents:**
  - Client-Side Validation
- Next Steps

python  theme={null}
def validate_and_send_filter(filter_expr, message):
    """Validate filter before sending to API."""
    try:
        # Test serialization
        filter_dict = filter_expr.to_dict()
        filter_json = json.dumps(filter_dict)
        
        # Verify it's valid JSON
        json.loads(filter_json)
        
        # Send request
        return send_filtered_agent_request(message, filter_expr)
        
    except (AttributeError, TypeError, json.JSONDecodeError) as e:
        print(f"Filter validation failed: {e}")
        return None
```

<CardGroup cols={2}>
  <Card title="Advanced Filtering Guide" icon="filter" href="/basics/knowledge/filters/advanced-filtering">
    Learn about filter expressions and metadata design in detail
  </Card>

<Card title="Agent OS API" icon="code" href="/agent-os/api">
    Explore the full Agent OS API reference
  </Card>

<Card title="Knowledge Bases" icon="database" href="/basics/knowledge/knowledge-bases">
    Understand knowledge base architecture and setup
  </Card>

<Card title="Search & Retrieval" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Optimize your search strategies
  </Card>

<Card title="Filter Expressions Example" icon="code" href="/basics/knowledge/filters/usage/filter-expressions">
    See working examples of filter expressions via API
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  When filters fail to parse, the search proceeds **without filters** rather than throwing an error. Always verify your filter JSON is valid and check server logs if results seem unfiltered.
</Warning>

### Client-Side Validation

Add validation before sending requests:
```

---

## AgentSession

**URL:** llms-txt#agentsession

**Contents:**
- AgentSession Attributes
- AgentSession Methods
  - `upsert_run(run: RunOutput)`
  - `get_run(run_id: str) -> Optional[RunOutput]`
  - `get_messages(...) -> List[Message]`
  - `get_session_summary() -> Optional[SessionSummary]`
  - `get_chat_history(...) -> List[Message]`

Source: https://docs.agno.com/reference/agents/session

## AgentSession Attributes

| Parameter      | Type                        | Default  | Description                                                        |
| -------------- | --------------------------- | -------- | ------------------------------------------------------------------ |
| `session_id`   | `str`                       | Required | Session UUID                                                       |
| `agent_id`     | `Optional[str]`             | `None`   | ID of the agent that this session is associated with               |
| `team_id`      | `Optional[str]`             | `None`   | ID of the team that this session is associated with                |
| `user_id`      | `Optional[str]`             | `None`   | ID of the user interacting with this agent                         |
| `workflow_id`  | `Optional[str]`             | `None`   | ID of the workflow that this session is associated with            |
| `session_data` | `Optional[Dict[str, Any]]`  | `None`   | Session Data: session\_name, session\_state, images, videos, audio |
| `metadata`     | `Optional[Dict[str, Any]]`  | `None`   | Metadata stored with this agent                                    |
| `agent_data`   | `Optional[Dict[str, Any]]`  | `None`   | Agent Data: agent\_id, name and model                              |
| `runs`         | `Optional[List[RunOutput]]` | `None`   | List of all runs in the session                                    |
| `summary`      | `Optional[SessionSummary]`  | `None`   | Summary of the session                                             |
| `created_at`   | `Optional[int]`             | `None`   | The unix timestamp when this session was created                   |
| `updated_at`   | `Optional[int]`             | `None`   | The unix timestamp when this session was last updated              |

## AgentSession Methods

### `upsert_run(run: RunOutput)`

Adds a RunOutput to the runs list. If a run with the same `run_id` already exists, it updates the existing run.

### `get_run(run_id: str) -> Optional[RunOutput]`

Retrieves a specific run by its `run_id`.

### `get_messages(...) -> List[Message]`

Returns the messages belonging to the session that fit the given criteria.

* `agent_id` (Optional\[str]): The id of the agent to get the messages from
* `team_id` (Optional\[str]): The id of the team to get the messages from
* `last_n_runs` (Optional\[int]): The number of runs to return messages from, counting from the latest. Defaults to all runs
* `limit` (Optional\[int]): The number of messages to return, counting from the latest. Defaults to all messages
* `skip_roles` (Optional\[List\[str]]): Skip messages with these roles
* `skip_statuses` (Optional\[List\[RunStatus]]): Skip messages with these statuses
* `skip_history_messages` (bool): Skip messages that were tagged as history in previous runs. Defaults to True

* `List[Message]`: The messages for the session

### `get_session_summary() -> Optional[SessionSummary]`

Get the session summary for the session

### `get_chat_history(...) -> List[Message]`

Get the chat history (user and assistant messages) for the session. Use `get_messages()` for more filtering options.

* `last_n_runs` (Optional\[int]): Number of recent runs to include. If None, all runs will be considered

* `List[Message]`: The chat history for the session

---

## In-Memory Storage for Workflows

**URL:** llms-txt#in-memory-storage-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/in-memory/usage/in-memory-for-workflow

Example using `InMemoryDb` with workflows for multi-step processes.

```python  theme={null}
from agno.agent import Agent
from agno.db.in_memory import InMemoryDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## VoyageAI Embedder

**URL:** llms-txt#voyageai-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/voyageai/usage/voyageai-embedder

```python  theme={null}
from agno.knowledge.embedder.voyageai import VoyageAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = VoyageAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Run the team and capture metrics

**URL:** llms-txt#run-the-team-and-capture-metrics

run_output = team.run("What is the stock price of NVDA")
pprint_run_response(run_output, markdown=True)

---

## Use with Knowledge

**URL:** llms-txt#use-with-knowledge

**Contents:**
- Parameters
- Developer Resources

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="vllm_embeddings",
        embedder=embedder,
    ),
)
```

| Parameter        | Type                       | Default                             | Description                                    |
| ---------------- | -------------------------- | ----------------------------------- | ---------------------------------------------- |
| `id`             | `str`                      | `"intfloat/e5-mistral-7b-instruct"` | Model identifier (HuggingFace model name)      |
| `dimensions`     | `int`                      | `4096`                              | Embedding vector dimensions                    |
| `base_url`       | `Optional[str]`            | `None`                              | Remote vLLM server URL (enables remote mode)   |
| `api_key`        | `Optional[str]`            | `getenv("VLLM_API_KEY")`            | API key for remote server authentication       |
| `enable_batch`   | `bool`                     | `False`                             | Enable batch processing for multiple texts     |
| `batch_size`     | `int`                      | `10`                                | Number of texts to process per batch           |
| `enforce_eager`  | `bool`                     | `True`                              | Use eager execution mode (local mode)          |
| `vllm_kwargs`    | `Optional[Dict[str, Any]]` | `None`                              | Additional vLLM engine parameters (local mode) |
| `request_params` | `Optional[Dict[str, Any]]` | `None`                              | Additional request parameters (remote mode)    |
| `client_params`  | `Optional[Dict[str, Any]]` | `None`                              | OpenAI client configuration (remote mode)      |

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/embedders/vllm_embedder.py)

---

## --- PDF utility ---

**URL:** llms-txt#----pdf-utility----

def extract_text_from_pdf(url: str) -> str:
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        reader = PdfReader(io.BytesIO(resp.content))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        print(f"Error extracting PDF from {url}: {e}")
        return ""

---

## Team with Memory Manager

**URL:** llms-txt#team-with-memory-manager

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/team/usage/team-with-memory-manager

This example demonstrates how to use persistent memory with a team. After each run, user memories are created and updated, allowing the team to remember information about users across sessions and provide personalized experiences.

```python cookbook/examples/teams/memory/01_team_with_memory_manager.py theme={null}
"""
This example shows you how to use persistent memory with an Agent.

After each run, user memories are created/updated.

To enable this, set `enable_user_memories=True` in the Agent config.
"""

from uuid import uuid4

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager  # noqa: F401
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url)

session_id = str(uuid4())
john_doe_id = "john_doe@example.com"

---

## Fourth call - will answer directly from history

**URL:** llms-txt#fourth-call---will-answer-directly-from-history

**Contents:**
- Developer Resources

workflow.print_response(
    "Compare Rocky and Luna", stream=True
)
```

## Developer Resources

Explore the different examples in [Conversational Workflows Examples](/basics/workflows/usage/conversational-workflow-with-conditional-step) section for more details.

---

## Upload Content

**URL:** llms-txt#upload-content

Source: https://docs.agno.com/reference-api/schema/knowledge/upload-content

post /knowledge/content
Upload content to the knowledge base. Supports file uploads, text content, or URLs. Content is processed asynchronously in the background. Supports custom readers and chunking strategies.

---

## CompressionManager

**URL:** llms-txt#compressionmanager

**Contents:**
- CompressionManager Attributes
- CompressionManager Methods
  - `should_compress(messages: List[Message]) -> bool`
  - `compress(messages: List[Message]) -> None`
  - `acompress(messages: List[Message]) -> None`

Source: https://docs.agno.com/reference/compression/compression-manager

The `CompressionManager` is responsible for compressing tool call results to save context space while preserving critical information.

## CompressionManager Attributes

| Parameter                         | Type              | Default | Description                                                                               |
| --------------------------------- | ----------------- | ------- | ----------------------------------------------------------------------------------------- |
| `model`                           | `Optional[Model]` | `None`  | Model used for compression                                                                |
| `compress_tool_results`           | `bool`            | `True`  | Flag to enable tool result compression                                                    |
| `compress_tool_results_limit`     | `int`             | `3`     | Number of uncompressed tool results before compression triggers                           |
| `compress_tool_call_instructions` | `Optional[str]`   | `None`  | Custom prompt for compression. If not provided, uses the default compression prompt       |
| `stats`                           | `Dict[str, Any]`  | `{}`    | Tracks compression statistics (`messages_compressed`, `original_size`, `compressed_size`) |

## CompressionManager Methods

### `should_compress(messages: List[Message]) -> bool`

Checks whether compression should be triggered based on the number of uncompressed tool results.

* `messages`: List of messages to check

* `bool`: True if the number of uncompressed tool results meets or exceeds `compress_tool_results_limit`

### `compress(messages: List[Message]) -> None`

Compresses all uncompressed tool results.

* `messages`: List of messages containing tool results to compress

* `None`: Modifies messages in place, setting `compressed_content` on each tool message

### `acompress(messages: List[Message]) -> None`

Compresses all uncompressed tool results asynchronously.

* `messages`: List of messages containing tool results to compress

* `None`: Modifies messages in place, setting `compressed_content` on each tool message

---

## Load infrastructure secrets

**URL:** llms-txt#load-infrastructure-secrets

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / "infra" / "secrets" / ".env")

---

## Create a new agent and make sure it pursues the conversation

**URL:** llms-txt#create-a-new-agent-and-make-sure-it-pursues-the-conversation

**Contents:**
- Prerequisites
- Params
- Developer Resources

agent2 = Agent(
    db=db,
    session_id=agent1.session_id,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
    debug_mode=False,
)

agent2.print_response("What's the name of the country we discussed?")
agent2.print_response("What is that country's national sport?")
```

<Snippet file="gcs-auth-storage.mdx" />

<Snippet file="db-gcs-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/gcs/gcs_json_for_agent.py)

---

## Google Maps

**URL:** llms-txt#google-maps

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Rate Limits
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/google-maps

Tools for interacting with Google Maps services including place search, directions, geocoding, and more

**GoogleMapTools** enable an Agent to interact with various Google Maps services for location-based operations including place search, directions, geocoding, and more.

The following example requires the `googlemaps` library and an API key which can be obtained from the [Google Cloud Console](https://console.cloud.google.com/projectselector2/google/maps-apis/credentials).

You'll need to enable the following APIs in your Google Cloud Console:

* Places API
* Directions API
* Geocoding API
* Address Validation API
* Distance Matrix API
* Elevation API
* Time Zone API

Basic usage of the Google Maps toolkit:

For more examples, see the [Google Maps Tools Examples](/examples/basics/tools/others/google_maps).

| Parameter | Type            | Default | Description                                                                         |
| --------- | --------------- | ------- | ----------------------------------------------------------------------------------- |
| `key`     | `Optional[str]` | `None`  | Optional API key. If not provided, uses GOOGLE\_MAPS\_API\_KEY environment variable |

| Function              | Description                                                                                                                                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search_places`       | Search for places using Google Maps Places API. Parameters: `query` (str) for the search query. Returns stringified JSON with place details including name, address, phone, website, rating, and hours.   |
| `get_directions`      | Get directions between locations. Parameters: `origin` (str), `destination` (str), optional `mode` (str) for travel mode, optional `avoid` (List\[str]) for features to avoid. Returns route information. |
| `validate_address`    | Validate an address. Parameters: `address` (str), optional `region_code` (str), optional `locality` (str). Returns address validation results.                                                            |
| `geocode_address`     | Convert address to coordinates. Parameters: `address` (str), optional `region` (str). Returns location information with coordinates.                                                                      |
| `reverse_geocode`     | Convert coordinates to address. Parameters: `lat` (float), `lng` (float), optional `result_type` and `location_type` (List\[str]). Returns address information.                                           |
| `get_distance_matrix` | Calculate distances between locations. Parameters: `origins` (List\[str]), `destinations` (List\[str]), optional `mode` (str) and `avoid` (List\[str]). Returns distance and duration matrix.             |
| `get_elevation`       | Get elevation for a location. Parameters: `lat` (float), `lng` (float). Returns elevation data.                                                                                                           |
| `get_timezone`        | Get timezone for a location. Parameters: `lat` (float), `lng` (float), optional `timestamp` (datetime). Returns timezone information.                                                                     |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

Google Maps APIs have usage limits and quotas that vary by service and billing plan. Please refer to the [Google Maps Platform pricing](https://cloud.google.com/maps-platform/pricing) for details.

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/google_maps.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/google_maps_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
You'll need to enable the following APIs in your Google Cloud Console:

* Places API
* Directions API
* Geocoding API
* Address Validation API
* Distance Matrix API
* Elevation API
* Time Zone API

## Example

Basic usage of the Google Maps toolkit:
```

---

## Finance Agent: Gets financial data using YFinance tools

**URL:** llms-txt#finance-agent:-gets-financial-data-using-yfinance-tools

finance_agent = Agent(
    name="Finance Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools()],
    instructions="Use tables to display data",
    markdown=True,
)

---

## Add PDF content synchronously

**URL:** llms-txt#add-pdf-content-synchronously

knowledge.add_content(
    path="cookbook/knowledge/testing_resources/cv_1.pdf",
    reader=PDFReader(),
)

---

## DynamoDB Workflow Storage

**URL:** llms-txt#dynamodb-workflow-storage

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/dynamodb/usage/dynamodb-for-workflow

Agno supports using DynamoDB as a storage backend for Workflows using the `DynamoDb` class.

You need to provide `aws_access_key_id` and `aws_secret_access_key` parameters to the `DynamoDb` class.

```python dynamo_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.dynamodb import DynamoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Information Gatherer Agent - Specialized in comprehensive information retrieval

**URL:** llms-txt#information-gatherer-agent---specialized-in-comprehensive-information-retrieval

information_gatherer = Agent(
    name="Information Gatherer",
    model=Claude(id="claude-sonnet-4-20250514"),
    role="Gather comprehensive information from knowledge sources",
    knowledge=knowledge,
    search_knowledge=True,
    tools=[ReasoningTools(add_instructions=True)],
    instructions=[
        "Search the knowledge base thoroughly for all relevant information.",
        "Use reasoning tools to plan your search strategy.",
        "Gather comprehensive context and supporting details.",
        "Document all sources and evidence found.",
    ],
    markdown=True,
)

---

## Set up knowledge base

**URL:** llms-txt#set-up-knowledge-base

knowledge = Knowledge(
    contents_db=PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5432/db",
        knowledge_table="knowledge_contents",
    ),
    vector_db=PgVector(db_url="postgresql+psycopg://ai:ai@localhost:5432/ai"),
)

---

## Redis for Agent

**URL:** llms-txt#redis-for-agent

**Contents:**
- Usage
  - Run Redis

Source: https://docs.agno.com/integrations/database/redis/usage/redis-for-agent

Agno supports using Redis as a storage backend for Agents using the `RedisDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Redis** on port **6379** using:

```python redis_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.redis import RedisDb
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Create sales dashboard

**URL:** llms-txt#create-sales-dashboard

prompt = (
    "Create a sales dashboard for January 2026 with:\n"
    "Sales data for 5 reps:\n"
    "- Alice: 24 deals, $385K revenue, 65% close rate\n"
    "- Bob: 19 deals, $298K revenue, 58% close rate\n"
    "- Carol: 31 deals, $467K revenue, 72% close rate\n"
    "- David: 22 deals, $356K revenue, 61% close rate\n"
    "- Emma: 27 deals, $412K revenue, 68% close rate\n\n"
    "Include:\n"
    "1. Table with all metrics\n"
    "2. Total revenue calculation\n"
    "3. Bar chart showing revenue by rep\n"
    "4. Quota attainment (quota: $350K per rep)\n"
    "5. Conditional formatting (green if above quota, red if below)\n"
    "Save as 'sales_dashboard.xlsx'"
)

response = excel_agent.run(prompt)
print(response.content)

---

## The first Agent will create a Memory about the user name here:

**URL:** llms-txt#the-first-agent-will-create-a-memory-about-the-user-name-here:

agent_1.print_response("Hi! My name is John Doe")

---

## ag infra down

**URL:** llms-txt#ag-infra-down

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/down

Delete resources for active infra

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

---

## Compatibility Overview

**URL:** llms-txt#compatibility-overview

**Contents:**
- Core Features
  - Multimodal Support
- Developer Resources

Source: https://docs.agno.com/basics/models/compatibility

Understand which features are supported across different model providers in Agno.

Agno provides comprehensive support across all major model providers, ensuring consistent functionality regardless of which models you choose. This page outlines the features and capabilities supported by each provider.

All models on Agno supports:

* Streaming responses
* Tool calling
* Structured outputs
* Async execution

<Note>
  HuggingFace supports tool calling through the Agno framework, but not for streaming
  responses.
</Note>

<Note>
  Perplexity supports tool calling through the Agno framework, but their models don't
  natively support tool calls in a straightforward way. This means tool usage may
  be less reliable compared to other providers.
</Note>

<Note>
  Vercel V0 doesn't support native structured output, but does support `use_json_mode=True`.
</Note>

### Multimodal Support

| Agno Supported Models | Image Input | Audio Input | Audio Responses | Video Input | File Upload |
| --------------------- | :---------: | :---------: | :-------------: | :---------: | :---------: |
| AIMLAPI               |      âœ…      |             |                 |             |             |
| Anthropic Claude      |      âœ…      |             |                 |             |      âœ…      |
| AWS Bedrock           |      âœ…      |             |                 |             |      âœ…      |
| AWS Bedrock Claude    |      âœ…      |             |                 |             |      âœ…      |
| Azure AI Foundry      |      âœ…      |             |                 |             |             |
| Azure OpenAI          |      âœ…      |             |                 |             |             |
| Cerebras              |             |             |                 |             |             |
| Cerebras OpenAI       |             |             |                 |             |             |
| Cohere                |      âœ…      |             |                 |             |             |
| CometAPI              |      âœ…      |             |                 |             |             |
| DashScope             |      âœ…      |             |                 |             |             |
| DeepInfra             |             |             |                 |             |             |
| DeepSeek              |             |             |                 |             |             |
| Fireworks             |             |             |                 |             |             |
| Gemini                |      âœ…      |      âœ…      |                 |      âœ…      |      âœ…      |
| Groq                  |      âœ…      |             |                 |             |             |
| HuggingFace           |      âœ…      |             |                 |             |             |
| IBM WatsonX           |      âœ…      |             |                 |             |             |
| InternLM              |             |             |                 |             |             |
| LangDB                |      âœ…      |      âœ…      |                 |             |             |
| LiteLLM               |      âœ…      |      âœ…      |                 |             |             |
| LiteLLMOpenAI         |             |      âœ…      |                 |             |             |
| LlamaCpp              |             |             |                 |             |             |
| LM Studio             |      âœ…      |             |                 |             |             |
| Llama                 |      âœ…      |             |                 |             |             |
| LlamaOpenAI           |      âœ…      |             |                 |             |             |
| Mistral               |      âœ…      |             |                 |             |             |
| Nebius                |             |             |                 |             |             |
| Nexus                 |             |             |                 |             |             |
| Nvidia                |             |             |                 |             |             |
| Ollama                |      âœ…      |             |                 |             |             |
| OpenAIChat            |      âœ…      |      âœ…      |        âœ…        |             |             |
| OpenAIResponses       |      âœ…      |      âœ…      |        âœ…        |             |      âœ…      |
| OpenRouter            |             |             |                 |             |             |
| Perplexity            |             |             |                 |             |             |
| Portkey               |             |             |                 |             |             |
| Requesty              |             |             |                 |             |             |
| Sambanova             |             |             |                 |             |             |
| Siliconflow           |             |             |                 |             |             |
| Together              |      âœ…      |             |                 |             |             |
| Vercel V0             |             |             |                 |             |             |
| VLLM                  |             |             |                 |             |             |
| Vertex AI Claude      |      âœ…      |             |                 |             |             |
| XAI                   |      âœ…      |             |                 |             |             |

## Developer Resources

* View [All Model Providers](/integrations/models/model-index)

---

## HuggingFace

**URL:** llms-txt#huggingface

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/huggingface

The HuggingFace model provides access to models hosted on the HuggingFace Hub.

| Parameter               | Type              | Default                                         | Description                                                      |
| ----------------------- | ----------------- | ----------------------------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`             | `"microsoft/DialoGPT-medium"`                   | The id of the Hugging Face model to use                          |
| `name`                  | `str`             | `"HuggingFace"`                                 | The name of the model                                            |
| `provider`              | `str`             | `"HuggingFace"`                                 | The provider of the model                                        |
| `api_key`               | `Optional[str]`   | `None`                                          | The API key for Hugging Face (defaults to HF\_TOKEN env var)     |
| `base_url`              | `str`             | `"https://api-inference.huggingface.co/models"` | The base URL for Hugging Face Inference API                      |
| `wait_for_model`        | `bool`            | `True`                                          | Whether to wait for the model to load if it's cold               |
| `use_cache`             | `bool`            | `True`                                          | Whether to use caching for faster inference                      |
| `max_tokens`            | `Optional[int]`   | `None`                                          | Maximum number of tokens to generate                             |
| `temperature`           | `Optional[float]` | `None`                                          | Controls randomness in the model's output                        |
| `top_p`                 | `Optional[float]` | `None`                                          | Controls diversity via nucleus sampling                          |
| `repetition_penalty`    | `Optional[float]` | `None`                                          | Penalty for repeating tokens (higher values reduce repetition)   |
| `retries`               | `int`             | `0`                                             | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`             | `1`                                             | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`            | `False`                                         | If True, the delay between retries is doubled each time          |

---

## SurrealDB Vector Database

**URL:** llms-txt#surrealdb-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/surrealdb/overview

Learn how to use SurrealDB as a vector database for your Knowledge Base

```python agent_with_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

**Examples:**

Example 1 (unknown):
```unknown
or
```

Example 2 (unknown):
```unknown
## Example
```

---

## List Memories

**URL:** llms-txt#list-memories

Source: https://docs.agno.com/reference-api/schema/memory/list-memories

get /memories
Retrieve paginated list of user memories with filtering and search capabilities. Filter by user, agent, team, topics, or search within memory content.

---

## AWS Claude

**URL:** llms-txt#aws-claude

**Contents:**
- Authentication
  - Method 1: API Key Authentication (Recommended)
  - Method 2: Access Key and Secret Key
  - Method 3: Boto3 Session

Source: https://docs.agno.com/integrations/models/cloud/aws-claude/overview

Learn how to use AWS Claude models in Agno.

Use Claude models through AWS Bedrock. This provides a native Claude integration optimized for AWS infrastructure.

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `anthropic.claude-sonnet-4-20250514-v1:0` model is their most capable model (Claude Sonnet 4).
* `anthropic.claude-3-5-sonnet-20241022-v2:0` model is good for most use-cases and supports image input.
* `anthropic.claude-3-5-haiku-20241022-v2:0` model is their fastest model.

AWS Claude supports three authentication methods:

### Method 1: API Key Authentication (Recommended)

Use the AWS Bedrock API key for simplified authentication:

Or pass them directly to the model:

### Method 2: Access Key and Secret Key

Set your `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_REGION` environment variables.

Get your keys from [here](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home).

Or pass them directly:

### Method 3: Boto3 Session

Use a pre-configured boto3 Session for advanced authentication scenarios:

```python  theme={null}
from boto3.session import Session
from agno.agent import Agent
from agno.models.aws import Claude

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Or pass them directly to the model:
```

Example 3 (unknown):
```unknown
### Method 2: Access Key and Secret Key

Set your `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_REGION` environment variables.

Get your keys from [here](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home).

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create coordinated RAG team

**URL:** llms-txt#create-coordinated-rag-team

**Contents:**
- Usage

coordinated_rag_team = Team(
    name="Coordinated RAG Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[knowledge_searcher, content_analyzer, response_synthesizer],
    instructions=[
        "Work together to provide comprehensive responses using the knowledge base.",
        "Knowledge Searcher: First search for relevant information thoroughly.",
        "Content Analyzer: Then analyze and organize the retrieved content.",
        "Response Synthesizer: Finally create a well-structured response with sources.",
        "Ensure all responses include proper citations and are factually accurate.",
    ],
    show_members_responses=True,
    markdown=True,
)

def main():
    """Run the coordinated agentic RAG team example."""
    print("ðŸ¤– Coordinated Agentic RAG Team Demo")
    print("=" * 50)

# Example query that benefits from coordinated search and analysis
    query = "What are Agents and how do they work with tools and knowledge?"

# Run the coordinated team
    coordinated_rag_team.print_response(
        query, stream=True
    )

if __name__ == "__main__":
    main()
bash  theme={null}
    pip install agno cohere lancedb tantivy sqlalchemy
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export CO_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/search_coordination/01_coordinated_agentic_rag.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## First run - this will create the cache

**URL:** llms-txt#first-run---this-will-create-the-cache

response = agent.run(
    "Explain the difference between REST and GraphQL APIs with examples"
)
if response and response.metrics:
    print(f"First run cache write tokens = {response.metrics.cache_write_tokens}")

---

## Initialize Langtrace

**URL:** llms-txt#initialize-langtrace

---

## Pre-hooks and Post-hooks

**URL:** llms-txt#pre-hooks-and-post-hooks

**Contents:**
- When Hooks Are Triggered
- Pre-hooks
  - Common Use Cases
  - Basic Example

Source: https://docs.agno.com/basics/hooks/overview

Learn about using pre-hooks and post-hooks with your agents.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.0" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.0">v2.1.0</Tooltip>
</Badge>

You can use hooks on agents and teams to do work before or after the main execution of the run.

Use cases for hooks include:

* Security guardrails (e.g. PII detection, prompt injection defense)
* Input validation
* Output validation
* Data preprocessing (e.g. normalizing input data)
* Data postprocessing (e.g. adding additional context to the output)
* Logging (e.g. logging the duration of the run)
* Debugging (e.g. debugging the run)

## When Hooks Are Triggered

Hooks execute at specific points in the Agent/Team run lifecycle:

* **Pre-hooks**: Execute immediately after the current session is loaded, **before** any processing begins. They run before the model context is prepared and before any LLM execution begins, i.e. any modifications to the input, session state, or dependencies will be applied before LLM execution.

* **Post-hooks**: Execute **after** the Agent/Team generates a response and the output is prepared, but **before** the response is returned to the user. In streaming responses, they run after each chunk of the response is generated.

Pre-hooks execute at the very beginning of your Agent run, giving you complete control over what reaches the LLM.

They're perfect for implementing input validation, security checks, or any data preprocessing against the input your Agent receives.

**Security Guardrails**

* Detect and prevent PII (Personally Identifiable Information) from reaching the LLM.
* Defend against prompt injection and jailbreak attempts.
* Filter NSFW or inappropriate content.
* See the [Guardrails](/basics/guardrails/overview) documentation for more details.

* Validate format, length, content or any other property of the input.
* Remove or mask sensitive information.
* Normalize input data.

**Data Preprocessing**

* Transform input format or structure.
* Enrich input with additional context.
* Apply any other business logic before sending the input to the LLM.

Let's create a simple pre-hook that validates the input length and raises an error if it's too long:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.exceptions import CheckTrigger, InputCheckError
from agno.run.agent import RunInput

---

## ClickUp

**URL:** llms-txt#clickup

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/clickup

ClickUpTools enables agents to interact with ClickUp workspaces for project management and task organization.

The following agent can manage ClickUp tasks and projects:

| Parameter         | Type            | Default | Description                                                 |
| ----------------- | --------------- | ------- | ----------------------------------------------------------- |
| `api_key`         | `Optional[str]` | `None`  | ClickUp API key. Uses CLICKUP\_API\_KEY if not set.         |
| `master_space_id` | `Optional[str]` | `None`  | Default space ID to use. Uses MASTER\_SPACE\_ID if not set. |

| Function      | Description                                                  |
| ------------- | ------------------------------------------------------------ |
| `list_tasks`  | List tasks with optional filtering by status, assignee, etc. |
| `create_task` | Create a new task in a specified list.                       |
| `get_task`    | Get detailed information about a specific task.              |
| `update_task` | Update an existing task's properties.                        |
| `delete_task` | Delete a task from ClickUp.                                  |
| `list_spaces` | List all spaces accessible to the user.                      |
| `list_lists`  | List all lists within a space or folder.                     |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/clickup.py)
* [ClickUp API Documentation](https://clickup.com/api/)

---

## Notion MCP agent

**URL:** llms-txt#notion-mcp-agent

Source: https://docs.agno.com/basics/tools/mcp/usage/notion

Using the [Notion MCP server](https://github.com/makenotion/notion-mcp-server) to create an Agent that can create, update and search for Notion pages:

---

## Fal

**URL:** llms-txt#fal

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/fal

**FalTools** enable an Agent to perform media generation tasks.

The following example requires the `fal_client` library and an API key which can be obtained from [Fal](https://fal.ai/).

The following agent will use FAL to generate any video requested by the user.

| Parameter               | Type   | Default | Description                                |
| ----------------------- | ------ | ------- | ------------------------------------------ |
| `api_key`               | `str`  | `None`  | API key for authentication purposes.       |
| `model`                 | `str`  | `None`  | The model to use for the media generation. |
| `enable_generate_media` | `bool` | `True`  | Enable the generate\_media functionality.  |
| `enable_image_to_image` | `bool` | `True`  | Enable the image\_to\_image functionality. |
| `all`                   | `bool` | `False` | Enable all functionality.                  |

| Function         | Description                                                    |
| ---------------- | -------------------------------------------------------------- |
| `generate_media` | Generate either images or videos depending on the user prompt. |
| `image_to_image` | Transform an input image based on a text prompt.               |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/fal.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/fal_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will use FAL to generate any video requested by the user.
```

---

## Memory with SQLite

**URL:** llms-txt#memory-with-sqlite

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/sqlite-memory

```python mem-sqlite-memory.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## Agentic Chunking

**URL:** llms-txt#agentic-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/agentic

Agentic chunking is an intelligent method of splitting documents into smaller chunks by using a model to determine natural breakpoints in the text.
Rather than splitting text at fixed character counts, it analyzes the content to find semantically meaningful boundaries like paragraph breaks and topic transitions.

<Snippet file="chunking-agentic.mdx" />

---

## Create workflow with loop containing parallel steps

**URL:** llms-txt#create-workflow-with-loop-containing-parallel-steps

workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
    )
```

This was a synchronous streaming example of this pattern. To checkout async and non-streaming versions, see the cookbooks-

* [Loop with Parallel Steps Workflow (sync)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps.py)
* [Loop with Parallel Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps.py)
* [Loop with Parallel Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/async/loop_with_parallel_steps_stream.py)

---

## Memory Search

**URL:** llms-txt#memory-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/memory-search

How to search for user memories using different retrieval methods.

* `last_n`: Retrieves the last n memories
* `first_n`: Retrieves the first n memories
* `agentic`: Retrieves memories using agentic search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Get Workflow Details

**URL:** llms-txt#get-workflow-details

Source: https://docs.agno.com/reference-api/schema/workflows/get-workflow-details

get /workflows/{workflow_id}
Retrieve detailed configuration and step information for a specific workflow.

---

## Enable History for Specific Steps

**URL:** llms-txt#enable-history-for-specific-steps

Source: https://docs.agno.com/basics/chat-history/workflow/usage/enable-history-for-step

This example demonstrates a workflow with history enabled for a specific step.

This example shows how to use the `add_workflow_history` flag to add workflow history to a specific step in the workflow.
In this case we have a workflow with three steps.

* The first step is a research specialist that gathers information on topics.
* The second step is a content creator that writes engaging content.
* The third step is a content publisher that prepares the content for publication.

---

## JWT Middleware

**URL:** llms-txt#jwt-middleware

**Contents:**
- Token Sources
- Parameter Injection
- Security Features
- Excluded Routes
- Configuration Options
  - Examples
  - External Resources

Source: https://docs.agno.com/agent-os/middleware/jwt

Secure your AgentOS application with JWT token validation

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.0" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.0">v2.1.0</Tooltip>
</Badge>

Authenticate and authorize requests to your AgentOS application using JWT tokens. The middleware extracts tokens from Authorization headers or cookies, validates them, and automatically injects user\_id, session\_id, and custom claims into your endpoints.

The JWT middleware provides two main features:

1. **Token Validation**: Validates JWT tokens and handles authentication
2. **Parameter Injection**: Automatically injects user\_id, session\_id, and custom claims into endpoint parameters

The middleware supports three token sources:

<Tabs>
  <Tab title="Authorization Header">
    Extract JWT from `Authorization: Bearer <token>` header.

<Tab title="HTTP-Only Cookies">
    Extract JWT from HTTP-only cookies for web applications.

<Tab title="Both Sources">
    Try both header and cookie (header takes precedence).

## Parameter Injection

The middleware automatically injects JWT claims into AgentOS endpoints. The following parameters are extracted from tokens and injected into requests:

* `user_id` - User identifier from token claims
* `session_id` - Session identifier from token claims
* `dependencies` - Custom claims for agent tools
* `session_state` - Custom claims for session management

For example, the `/agents/{agent_id}/runs` endpoint automatically uses `user_id`, `session_id`, `dependencies`, and `session_state` from the JWT token when available.

* Automatically using the `user_id` and `session_id` from your JWT token when running an agent
* Automatically filtering sessions retrieved from `/sessions` endpoints by `user_id` (where applicable)
* Automatically injecting `dependencies` from claims in your JWT token into the agent run, which then is available on tools called by your agent

View the [full example](/agent-os/usage/middleware/jwt-middleware) for more details.

<Note>
  Use strong secret keys, store them securely (not in code), and enable validation in production.
</Note>

**Token Validation**: When `validate=True`, the middleware:

* Verifies JWT signature using the secret key
* Checks token expiration (`exp` claim)
* Returns 401 errors for invalid/expired tokens

<Tip>
  **HTTP-Only Cookies**: When using cookies:

* Set `httponly=True` to prevent JavaScript access (XSS protection)
  * Set `secure=True` for HTTPS-only transmission
  * Set `samesite="strict"` for CSRF protection
</Tip>

Skip middleware for specific routes:

## Configuration Options

| Parameter              | Description                                                                                  | Default                                                                  |
| ---------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| `secret_key`           | Secret key for JWT verification                                                              | Optional, will use `JWT_SECRET_KEY` environment variable if not provided |
| `algorithm`            | JWT algorithm (HS256, RS256, etc.)                                                           | "HS256"                                                                  |
| `token_source`         | Where to extract token from. `HEADER`, `COOKIE`, or `BOTH`.                                  | `TokenSource.HEADER`                                                     |
| `token_header_key`     | Key to use for the Authorization header (only used when token\_source is `HEADER` or `BOTH`) | "Authorization"                                                          |
| `cookie_name`          | Cookie name when using cookies (only used when token\_source is `COOKIE` or `BOTH`)          | "access\_token"                                                          |
| `validate`             | Enable token validation                                                                      | `True`                                                                   |
| `excluded_route_paths` | Routes to skip middleware (useful for health checks, etc.)                                   | `None`                                                                   |
| `scopes_claim`         | JWT claim for scopes                                                                         | `None`                                                                   |
| `user_id_claim`        | JWT claim for user ID                                                                        | "sub"                                                                    |
| `session_id_claim`     | JWT claim for session ID                                                                     | "session\_id"                                                            |
| `dependencies_claims`  | List of additional claims to extract for `dependencies` parameter                            | `[]`                                                                     |
| `session_state_claims` | List of additional claims to extract for `session_state` parameter                           | `[]`                                                                     |

<CardGroup cols={2}>
  <Card title="JWT with Headers" icon="shield" href="/agent-os/usage/middleware/jwt-middleware">
    JWT authentication using Authorization headers for API clients.
  </Card>

<Card title="JWT with Cookies" icon="cookie" href="/agent-os/usage/middleware/jwt-cookies">
    JWT authentication using HTTP-only cookies for web applications.
  </Card>

<Card title="Custom FastAPI + JWT" icon="code" href="/agent-os/usage/middleware/custom-fastapi-jwt">
    Custom FastAPI app with JWT middleware and AgentOS integration.
  </Card>
</CardGroup>

### External Resources

<CardGroup cols={2}>
  <Card title="PyJWT Documentation" icon="book" href="https://pyjwt.readthedocs.io/">
    Official PyJWT library documentation for JWT encoding and decoding.
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
## Token Sources

The middleware supports three token sources:

<Tabs>
  <Tab title="Authorization Header">
    Extract JWT from `Authorization: Bearer <token>` header.
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="HTTP-Only Cookies">
    Extract JWT from HTTP-only cookies for web applications.
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Both Sources">
    Try both header and cookie (header takes precedence).
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

## Parameter Injection

The middleware automatically injects JWT claims into AgentOS endpoints. The following parameters are extracted from tokens and injected into requests:

* `user_id` - User identifier from token claims
* `session_id` - Session identifier from token claims
* `dependencies` - Custom claims for agent tools
* `session_state` - Custom claims for session management

For example, the `/agents/{agent_id}/runs` endpoint automatically uses `user_id`, `session_id`, `dependencies`, and `session_state` from the JWT token when available.

This is useful for:

* Automatically using the `user_id` and `session_id` from your JWT token when running an agent
* Automatically filtering sessions retrieved from `/sessions` endpoints by `user_id` (where applicable)
* Automatically injecting `dependencies` from claims in your JWT token into the agent run, which then is available on tools called by your agent

View the [full example](/agent-os/usage/middleware/jwt-middleware) for more details.

## Security Features

<Note>
  Use strong secret keys, store them securely (not in code), and enable validation in production.
</Note>

**Token Validation**: When `validate=True`, the middleware:

* Verifies JWT signature using the secret key
* Checks token expiration (`exp` claim)
* Returns 401 errors for invalid/expired tokens

<Tip>
  **HTTP-Only Cookies**: When using cookies:

  * Set `httponly=True` to prevent JavaScript access (XSS protection)
  * Set `secure=True` for HTTPS-only transmission
  * Set `samesite="strict"` for CSRF protection
</Tip>

## Excluded Routes

Skip middleware for specific routes:
```

---

## Find documents with high priority scores

**URL:** llms-txt#find-documents-with-high-priority-scores

GT("priority_score", 8.0)

---

## Custom API

**URL:** llms-txt#custom-api

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/custom-api

**CustomApiTools** enable an Agent to make HTTP requests to any external API with customizable authentication and parameters.

The following example requires the `requests` library.

The following agent will use CustomApiTools to make API calls to the Dog CEO API.

| Parameter             | Type             | Default | Description                                 |
| --------------------- | ---------------- | ------- | ------------------------------------------- |
| `base_url`            | `str`            | `None`  | Base URL for API calls                      |
| `username`            | `str`            | `None`  | Username for basic authentication           |
| `password`            | `str`            | `None`  | Password for basic authentication           |
| `api_key`             | `str`            | `None`  | API key for bearer token authentication     |
| `headers`             | `Dict[str, str]` | `None`  | Default headers to include in requests      |
| `verify_ssl`          | `bool`           | `True`  | Whether to verify SSL certificates          |
| `timeout`             | `int`            | `30`    | Request timeout in seconds                  |
| `enable_make_request` | `bool`           | `True`  | Enables functionality to make HTTP requests |
| `all`                 | `bool`           | `False` | Enables all functionality when set to True  |

| Function       | Description                                                                                                                                |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `make_request` | Makes an HTTP request to the API. Takes method (GET, POST, etc.), endpoint, and optional params, data, headers, and json\_data parameters. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/api.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/custom_api_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use CustomApiTools to make API calls to the Dog CEO API.
```

Example 2 (unknown):
```unknown
## Toolkit Params

| Parameter             | Type             | Default | Description                                 |
| --------------------- | ---------------- | ------- | ------------------------------------------- |
| `base_url`            | `str`            | `None`  | Base URL for API calls                      |
| `username`            | `str`            | `None`  | Username for basic authentication           |
| `password`            | `str`            | `None`  | Password for basic authentication           |
| `api_key`             | `str`            | `None`  | API key for bearer token authentication     |
| `headers`             | `Dict[str, str]` | `None`  | Default headers to include in requests      |
| `verify_ssl`          | `bool`           | `True`  | Whether to verify SSL certificates          |
| `timeout`             | `int`            | `30`    | Request timeout in seconds                  |
| `enable_make_request` | `bool`           | `True`  | Enables functionality to make HTTP requests |
| `all`                 | `bool`           | `False` | Enables all functionality when set to True  |

## Toolkit Functions

| Function       | Description                                                                                                                                |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `make_request` | Makes an HTTP request to the API. Takes method (GET, POST, etc.), endpoint, and optional params, data, headers, and json\_data parameters. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/api.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/custom_api_tools.py)
```

---

## Session State for Multiple Users

**URL:** llms-txt#session-state-for-multiple-users

Source: https://docs.agno.com/basics/state/agent/usage/session-state-multiple-users

This example demonstrates how to maintain separate session state for multiple users in a multi-user environment, with each user having their own shopping lists and sessions.

<Steps>
  <Step title="Create a Python file">
    Create a file called `session_state_multiple_users.py`
  </Step>

<Step title="Add code to file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="set-openai-key.mdx" />

<Step title="Run the agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="set-openai-key.mdx" />

  <Step title="Run the agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Setup infra for new users

**URL:** llms-txt#setup-infra-for-new-users

Source: https://docs.agno.com/templates/infra-management/new-users

Follow these steps to setup an existing infra:

<Steps>
  <Step title="Clone git repository">
    Clone the git repo and `cd` into the infra directory

</CodeGroup>
  </Step>

<Step title="Create and activate a virtual environment">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Install agno">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Copy secrets">
    Copy `infra/example_secrets` to `infra/secrets`

</CodeGroup>
  </Step>

<Step title="Start infra">
    <Note>
      Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) if needed.
    </Note>

</CodeGroup>
  </Step>

<Step title="Stop infra">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create and activate a virtual environment">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install agno">
    <CodeGroup>
```

---

## List of models available in the AgentOS

**URL:** llms-txt#list-of-models-available-in-the-agentos

available_models:
  - <MODEL_STRING>
  ...

---

## Change this if your Postgres container is running elsewhere

**URL:** llms-txt#change-this-if-your-postgres-container-is-running-elsewhere

DB_URL = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=VLLM(id="microsoft/Phi-3-mini-128k-instruct"),
    db=PostgresDb(db_url=DB_URL),
    enable_user_memories=True,
    enable_session_summaries=True,
)

---

## macOS with Homebrew

**URL:** llms-txt#macos-with-homebrew

**Contents:**
  - Download a Model
  - Start the Server
- Example
- Configuration
- Params
- Server Configuration
  - Common Server Options
  - Model Options
- Performance Optimization
  - Hardware Acceleration

brew install llama.cpp
bash start server theme={null}
llama-server -hf ggml-org/gpt-oss-20b-GGUF --ctx-size 0 --jinja -ub 2048 -b 2048
python agent.py theme={null}
  from agno.agent import Agent
  from agno.models.llama_cpp import LlamaCpp

agent = Agent(
      model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"),
      markdown=True
  )

# Print the response in the terminal
  agent.print_response("Share a 2 sentence horror story.")
  python custom_config.py theme={null}
  from agno.agent import Agent
  from agno.models.llama_cpp import LlamaCpp

# Custom server configuration
  agent = Agent(
      model=LlamaCpp(
          id="your-custom-model",
          base_url="http://localhost:8080/v1",  # Custom server URL
      ),
      markdown=True
  )
  bash gpu acceleration theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Download a Model

Download a model in GGUF format following the [llama.cpp model download guide](https://github.com/ggerganov/llama.cpp#obtaining-and-using-the-facebook-llama-2-model). For the examples below, we use `ggml-org/gpt-oss-20b-GGUF`.

### Start the Server

Start the LlamaCpp server with your model:
```

Example 2 (unknown):
```unknown
This starts the server at `http://127.0.0.1:8080` with an OpenAI Chat compatible endpoints

## Example

After starting the LlamaCpp server, use the `LlamaCpp` model class to access it:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

## Configuration

The `LlamaCpp` model supports customizing the server URL and model ID:

<CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

<Note> View more examples [here](/integrations/models/local/llama-cpp/usage/basic). </Note>

## Params

| Parameter     | Type              | Default                   | Description                                                  |
| ------------- | ----------------- | ------------------------- | ------------------------------------------------------------ |
| `id`          | `str`             | `"llama-cpp"`             | The identifier for the Llama.cpp model                       |
| `name`        | `str`             | `"LlamaCpp"`              | The name of the model                                        |
| `provider`    | `str`             | `"LlamaCpp"`              | The provider of the model                                    |
| `base_url`    | `str`             | `"http://localhost:8080"` | The base URL for the Llama.cpp server                        |
| `api_key`     | `Optional[str]`   | `None`                    | The API key (usually not needed for local Llama.cpp)         |
| `chat_format` | `Optional[str]`   | `None`                    | The chat format to use (e.g., "chatml", "llama-2", "alpaca") |
| `n_ctx`       | `Optional[int]`   | `None`                    | The context window size                                      |
| `temperature` | `Optional[float]` | `None`                    | Sampling temperature (0.0 to 2.0)                            |
| `top_p`       | `Optional[float]` | `None`                    | Top-p sampling parameter                                     |
| `top_k`       | `Optional[int]`   | `None`                    | Top-k sampling parameter                                     |

`LlamaCpp` is a subclass of the [OpenAILike](/integrations/models/openai-like) class and has access to the same params.

## Server Configuration

The LlamaCpp server supports many configuration options:

### Common Server Options

* `--ctx-size`: Context size (0 for unlimited)
* `--batch-size`, `-b`: Batch size for prompt processing
* `--ubatch-size`, `-ub`: Physical batch size for prompt processing
* `--threads`, `-t`: Number of threads to use
* `--host`: IP address to listen on (default: 127.0.0.1)
* `--port`: Port to listen on (default: 8080)

### Model Options

* `--model`, `-m`: Model file path
* `--hf-repo`: HuggingFace model repository
* `--jinja`: Use Jinja templating for chat formatting

For a complete list of server options, run `llama-server --help`.

## Performance Optimization

### Hardware Acceleration

LlamaCpp supports various acceleration backends:
```

---

## Combine all instruction components

**URL:** llms-txt#combine-all-instruction-components

**Contents:**
- Step 4: Create the Complete Agent
  - 4a. Create the Agent

complete_instructions = f"""
You are a Senior Social Media Intelligence Analyst specializing in cross-platform
brand monitoring and strategic analysis.

{data_collection_strategy}

{intelligence_synthesis}

{analysis_principles}
"""

print(f"Complete instructions created: {len(complete_instructions)} characters")
python  theme={null}
from agno.agent import Agent

**Examples:**

Example 1 (unknown):
```unknown
## Step 4: Create the Complete Agent

Now let's put all the pieces together - model, tools, and instructions - to create your complete social media intelligence agent.

### 4a. Create the Agent
```

---

## Create workflow with workflow_session_state

**URL:** llms-txt#create-workflow-with-workflow_session_state

**Contents:**
  - 3. `run_context` as a parameter for custom python functions step in workflow
- Key Benefits
- Useful Links

shopping_workflow = Workflow(
    name="Shopping List Workflow",
    db=db,
    steps=[manage_items_step, view_list_step],
    session_state={"shopping_list": []},
)

if __name__ == "__main__":
    # Example 1: Add items to the shopping list
    print("=== Example 1: Adding Items ===")
    shopping_workflow.print_response(
        input="Please add milk, bread, and eggs to my shopping list."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

# Example 2: Add more items and view list
    print("\n=== Example 2: Adding More Items ===")
    shopping_workflow.print_response(
        input="Add apples and bananas to the list, then show me the complete list."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

# Example 3: Remove items
    print("\n=== Example 3: Removing Items ===")
    shopping_workflow.print_response(
        input="Remove bread from the list and show me what's left."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

# Example 4: Clear the entire list
    print("\n=== Example 4: Clearing List ===")
    shopping_workflow.print_response(
        input="Clear the entire shopping list and confirm it's empty."
    )
    print("Final workflow session state:", shopping_workflow.get_session_state())
python  theme={null}
  from agno.run import RunContext

def custom_function_step(step_input: StepInput, run_context: RunContext):
      """Update the workflow session state"""
      run_context.session_state["test"] = test_1
  python  theme={null}
from agno.run import RunContext

def evaluator_function(step_input: StepInput, run_context: RunContext):
    return run_context.session_state["test"] == "test_1"

condition_step = Condition(
    name="condition_step",
    evaluator=evaluator_function,
    steps=[step_1, step_2],
)
python  theme={null}
from agno.run import RunContext

def selector_function(step_input: StepInput, run_context: RunContext):
    return run_context.session_state["test"] == "test_1"

router_step = Router(
    name="router_step",
    selector=selector_function,
    choices=[step_1, step_2],
)
```

See example of [Session State in Condition Evaluator Function](/basics/state/workflows/usage/access-session-state-in-condition-evaluator-function)
and [Session State in Router Selector Function](/basics/state/workflows/usage/access-session-state-in-router-selector-function) for more details.

**Persistent State Management**

* Data persists across all workflow steps and components
* Enables complex, stateful workflows with memory
* Supports deterministic execution with consistent state

**Cross-Component Coordination**

* Agents, teams, and functions share the same state object
* Enables sophisticated collaboration patterns
* Maintains data consistency across workflow execution

**Flexible Data Structure**

* Use any Python data structure (dictionaries, lists, objects)
* Structure data to match your workflow requirements
* Access and modify state through standard Python operations

<Note>
  The `run_context` object, containing the session state, is automatically passed to all agents and teams within a
  workflow, enabling seamless collaboration and data sharing between different
  components without manual state management.
</Note>

<CardGroup cols={3}>
  <Card title="Agent Examples" icon="user" href="https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_06_advanced_basics/_04_shared_session_state/shared_session_state_with_agent.py">
    See how agents interact with shared session state
  </Card>

<Card title="Team Examples" icon="users" href="https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_06_advanced_basics/_04_shared_session_state/shared_session_state_with_team.py">
    Learn how teams coordinate using shared state
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
See the [RunContext schema](/reference/run/run-context) for more information.

### 3. `run_context` as a parameter for custom python functions step in workflow

You can add the `run_context` parameter to the Python functions you use as custom steps.

The `run_context` object will be automatically injected when running the function.

You can use it to read and modify the session state, via `run_context.session_state`.

<Note>
  On the function of the custom python function step for a workflow
```

Example 2 (unknown):
```unknown
</Note>

See [examples](/basics/state/workflows/usage/access-session-state-in-custom-python-function-step) for more details.

The `run_context` is also available as a parameter in the evaluator and selector functions of the `Condition` and `Router` steps:
```

Example 3 (unknown):
```unknown

```

---

## Agent with Structured Output

**URL:** llms-txt#agent-with-structured-output

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/perplexity/usage/structured-output

```python cookbook/models/perplexity/structured_output.py theme={null}
from typing import List

from agno.agent import Agent, RunOutput  # noqa
from agno.models.perplexity import Perplexity
from pydantic import BaseModel, Field

class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

---

## Conditional Workflow

**URL:** llms-txt#conditional-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/basics/workflows/workflow-patterns/conditional-workflow

Deterministic branching based on input analysis or business rules

**Example Use-Cases**: Content type routing, topic-specific processing, quality-based decisions

Conditional workflows provide predictable branching logic while maintaining deterministic execution paths.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=7bc060741f060c43747d9866246d0587" alt="Workflows condition steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-condition-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=051009cf50418538acbc49a9c690cdf8 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5f8e6a2ed1301cf1d4edda7804c5ec08 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=65a6ba82ef0a22a0927439644a6c7912 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f5d676c0bf82f2045e61f31126b66a42 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=408f8ed2a78755b289c7a0b4e07d6f0e 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=548ca991ffb6e9e5d7669bf37e98fed2 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=de3fa0bc3fc9b4079e7dd3596d6e589a" alt="Workflows condition steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-condition-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=3b0eb6ed78b037dd85346647665f373e 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=9c5e9785043d807c36b6ee130fde63ef 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5044466baae4103aadf462fb81b9be60 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=d67a6cb9bb25245259a4001be56f7d91 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=ff169ab64d750cfb21073305fdedd213 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=0f44b53b13a00c51b26314825d605013 2500w" />

## Developer Resources

* [Condition Steps Workflow](/basics/workflows/usage/condition-steps-workflow-stream)
* [Condition with List of Steps](/basics/workflows/usage/condition-with-list-of-steps)

For complete API documentation, see [Condition Steps Reference](/reference/workflows/conditional-steps).

---

## Audio Streaming

**URL:** llms-txt#audio-streaming

Source: https://docs.agno.com/basics/multimodal/agent/usage/audio-streaming

This example demonstrates how to use Agno agents to generate streaming audio responses using OpenAI's GPT-4o audio preview model.

```python  theme={null}
import base64
import wave
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.openai import OpenAIChat

---

## Replicate

**URL:** llms-txt#replicate

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/replicate

**ReplicateTools** enables an Agent to generate media using the [Replicate platform](https://replicate.com/).

The following example requires the `replicate` library. To install the Replicate client, run the following command:

The following agent will use Replicate to generate images or videos requested by the user.

| Parameter               | Type   | Default            | Description                                                          |
| ----------------------- | ------ | ------------------ | -------------------------------------------------------------------- |
| `api_key`               | `str`  | `None`             | If you want to manually supply the Replicate API key.                |
| `model`                 | `str`  | `minimax/video-01` | The replicate model to use. Find out more on the Replicate platform. |
| `enable_generate_media` | `bool` | `True`             | Enable the generate\_media functionality.                            |
| `all`                   | `bool` | `False`            | Enable all functionality.                                            |

| Function         | Description                                                                         |
| ---------------- | ----------------------------------------------------------------------------------- |
| `generate_media` | Generate either an image or a video from a prompt. The output depends on the model. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/replicate.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/replicate_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
The following example requires the `replicate` library. To install the Replicate client, run the following command:
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Replicate to generate images or videos requested by the user.
```

---

## Ollama DeepSeek R1

**URL:** llms-txt#ollama-deepseek-r1

Source: https://docs.agno.com/basics/reasoning/usage/models/ollama/ollama

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Install Ollama">
    Follow the [installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install Ollama">
    Follow the [installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Cohere Reranker

**URL:** llms-txt#cohere-reranker

Source: https://docs.agno.com/reference/knowledge/reranker/cohere

<Snippet file="reranker-cohere-params.mdx" />

---

## Filter by agent

**URL:** llms-txt#filter-by-agent

traces, count = db.get_traces(agent_id=agent.id)

---

## - "Show me the most recent story from Hacker News"

**URL:** llms-txt#--"show-me-the-most-recent-story-from-hacker-news"

---

## Adding Dependencies to Team Run

**URL:** llms-txt#adding-dependencies-to-team-run

Source: https://docs.agno.com/basics/dependencies/team/usage/add-dependencies-run

This example demonstrates how to add dependencies to a specific team run. Dependencies are functions that provide contextual information (like user profiles and current context) that get passed to the team during execution for personalized responses.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Session by ID

**URL:** llms-txt#get-session-by-id

Source: https://docs.agno.com/reference-api/schema/sessions/get-session-by-id

get /sessions/{session_id}
Retrieve detailed information about a specific session including metadata, configuration, and run history. Response schema varies based on session type (agent, team, or workflow).

---

## user_id=john_doe_id,

**URL:** llms-txt#user_id=john_doe_id,

---

## Jira

**URL:** llms-txt#jira

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/jira

**JiraTools** enable an Agent to perform Jira tasks.

The following example requires the `jira` library and auth credentials.

The following agent will use Jira API to search for issues in a project.

| Parameter              | Type   | Default | Description                                                                                                                   |
| ---------------------- | ------ | ------- | ----------------------------------------------------------------------------------------------------------------------------- |
| `server_url`           | `str`  | `""`    | The URL of the JIRA server, retrieved from the environment variable `JIRA_SERVER_URL`. Default is an empty string if not set. |
| `username`             | `str`  | `None`  | The JIRA username for authentication, retrieved from the environment variable `JIRA_USERNAME`. Default is None if not set.    |
| `password`             | `str`  | `None`  | The JIRA password for authentication, retrieved from the environment variable `JIRA_PASSWORD`. Default is None if not set.    |
| `token`                | `str`  | `None`  | The JIRA API token for authentication, retrieved from the environment variable `JIRA_TOKEN`. Default is None if not set.      |
| `enable_get_issue`     | `bool` | `True`  | Enable the get\_issue functionality.                                                                                          |
| `enable_create_issue`  | `bool` | `True`  | Enable the create\_issue functionality.                                                                                       |
| `enable_search_issues` | `bool` | `True`  | Enable the search\_issues functionality.                                                                                      |
| `enable_add_comment`   | `bool` | `True`  | Enable the add\_comment functionality.                                                                                        |
| `all`                  | `bool` | `False` | Enable all functionality.                                                                                                     |

| Function        | Description                                                                                                                                                                                                                                                                                                                                |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `get_issue`     | Retrieves issue details from JIRA. Parameters include:<br />- `issue_key`: the key of the issue to retrieve<br />Returns a JSON string containing issue details or an error message.                                                                                                                                                       |
| `create_issue`  | Creates a new issue in JIRA. Parameters include:<br />- `project_key`: the project in which to create the issue<br />- `summary`: the issue summary<br />- `description`: the issue description<br />- `issuetype`: the type of issue (default is "Task")<br />Returns a JSON string with the new issue's key and URL or an error message. |
| `search_issues` | Searches for issues using a JQL query in JIRA. Parameters include:<br />- `jql_str`: the JQL query string<br />- `max_results`: the maximum number of results to return (default is 50)<br />Returns a JSON string containing a list of dictionaries with issue details or an error message.                                               |
| `add_comment`   | Adds a comment to an issue in JIRA. Parameters include:<br />- `issue_key`: the key of the issue<br />- `comment`: the comment text<br />Returns a JSON string indicating success or an error message.                                                                                                                                     |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/jira.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/jira_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will use Jira API to search for issues in a project.
```

---

## Team Metrics Analysis

**URL:** llms-txt#team-metrics-analysis

**Contents:**
- Code

Source: https://docs.agno.com/basics/sessions/metrics/usage/team-metrics

This example demonstrates how to access and analyze comprehensive team metrics including message-level metrics, session metrics, and member-specific performance data.

```python cookbook/examples/teams/metrics/01_team_metrics.py theme={null}
"""
This example demonstrates how to access and analyze team metrics.

Shows how to retrieve detailed metrics for team execution, including
message-level metrics, session metrics, and member-specific metrics.

Prerequisites:
1. Run: cookbook/run_pgvector.sh (to start PostgreSQL)
2. Ensure PostgreSQL is running on localhost:5532
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.exa import ExaTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

---

## Get a response

**URL:** llms-txt#get-a-response

**Contents:**
  - Using Hugging Face Models
  - Configuration Options
  - Examples

agent.print_response("Share a 2 sentence horror story")
python  theme={null}
from agno.agent import Agent
from agno.models.litellm import LiteLLM

agent = Agent(
    model=LiteLLM(
        id="huggingface/mistralai/Mistral-7B-Instruct-v0.2",
        top_p=0.95,
    ),
    markdown=True,
)

agent.print_response("What's happening in France?")
```

### Configuration Options

The `LiteLLM` class accepts the following parameters:

| Parameter        | Type                       | Description                                                                               | Default      |
| ---------------- | -------------------------- | ----------------------------------------------------------------------------------------- | ------------ |
| `id`             | str                        | Model identifier (e.g., "gpt-5-mini" or "huggingface/mistralai/Mistral-7B-Instruct-v0.2") | "gpt-5-mini" |
| `name`           | str                        | Display name for the model                                                                | "LiteLLM"    |
| `provider`       | str                        | Provider name                                                                             | "LiteLLM"    |
| `api_key`        | Optional\[str]             | API key (falls back to LITELLM\_API\_KEY environment variable)                            | None         |
| `api_base`       | Optional\[str]             | Base URL for API requests                                                                 | None         |
| `max_tokens`     | Optional\[int]             | Maximum tokens in the response                                                            | None         |
| `temperature`    | float                      | Sampling temperature                                                                      | 0.7          |
| `top_p`          | float                      | Top-p sampling value                                                                      | 1.0          |
| `request_params` | Optional\[Dict\[str, Any]] | Additional request parameters                                                             | None         |

<Note> View more examples [here](/integrations/models/gateways/litellm/usage/basic-stream). </Note>

**Examples:**

Example 1 (unknown):
```unknown
### Using Hugging Face Models

LiteLLM can also work with Hugging Face models:
```

---

## Get audio files from storage/audio directory

**URL:** llms-txt#get-audio-files-from-storage/audio-directory

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agno_root_dir = Path(__file__).parent.parent.parent.resolve()
audio_storage_dir = agno_root_dir.joinpath("storage/audio")
if not audio_storage_dir.exists():
    audio_storage_dir.mkdir(exist_ok=True, parents=True)

agent = Agent(
    name="Transcription Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[MLXTranscribeTools(base_dir=audio_storage_dir)],
    instructions=[
        "To transcribe an audio file, use the `transcribe` tool with the name of the audio file as the argument.",
        "You can find all available audio files using the `read_files` tool.",
    ],
    markdown=True,
)

agent.print_response("Summarize the reid hoffman ted talk, split into sections", stream=True)
```

| Parameter                         | Type                           | Default                                  | Description                                  |
| --------------------------------- | ------------------------------ | ---------------------------------------- | -------------------------------------------- |
| `base_dir`                        | `Path`                         | `Path.cwd()`                             | Base directory for audio files               |
| `enable_read_files_in_base_dir`   | `bool`                         | `True`                                   | Whether to register the read\_files function |
| `path_or_hf_repo`                 | `str`                          | `"mlx-community/whisper-large-v3-turbo"` | Path or HuggingFace repo for the model       |
| `verbose`                         | `bool`                         | `None`                                   | Enable verbose output                        |
| `temperature`                     | `float` or `Tuple[float, ...]` | `None`                                   | Temperature for sampling                     |
| `compression_ratio_threshold`     | `float`                        | `None`                                   | Compression ratio threshold                  |
| `logprob_threshold`               | `float`                        | `None`                                   | Log probability threshold                    |
| `no_speech_threshold`             | `float`                        | `None`                                   | No speech threshold                          |
| `condition_on_previous_text`      | `bool`                         | `None`                                   | Whether to condition on previous text        |
| `initial_prompt`                  | `str`                          | `None`                                   | Initial prompt for transcription             |
| `word_timestamps`                 | `bool`                         | `None`                                   | Enable word-level timestamps                 |
| `prepend_punctuations`            | `str`                          | `None`                                   | Punctuations to prepend                      |
| `append_punctuations`             | `str`                          | `None`                                   | Punctuations to append                       |
| `clip_timestamps`                 | `str` or `List[float]`         | `None`                                   | Clip timestamps                              |
| `hallucination_silence_threshold` | `float`                        | `None`                                   | Hallucination silence threshold              |
| `decode_options`                  | `dict`                         | `None`                                   | Additional decoding options                  |

| Function     | Description                                 |
| ------------ | ------------------------------------------- |
| `transcribe` | Transcribes an audio file using MLX Whisper |
| `read_files` | Lists all audio files in the base directory |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/mlx_transcribe.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/mlx_transcribe_tools.py)

---

## Print the embeddings and their dimensions

**URL:** llms-txt#print-the-embeddings-and-their-dimensions

print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

---

## Image Generation

**URL:** llms-txt#image-generation

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/14-image-generation

This example shows how to create an AI agent that generates images using DALL-E.
You can use this agent to create various types of images, from realistic photos to artistic
illustrations and creative concepts.

Example prompts to try:

* "Create a surreal painting of a floating city in the clouds at sunset"
* "Generate a photorealistic image of a cozy coffee shop interior"
* "Design a cute cartoon mascot for a tech startup"
* "Create an artistic portrait of a cyberpunk samurai"

```python image_generation.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.dalle import DalleTools

---

## Image As Input

**URL:** llms-txt#image-as-input

**Contents:**
- Developer Resources

Source: https://docs.agno.com/basics/multimodal/images/image-input

Learn how to use image as input with Agno agents.

Agno supports images as input to agents and teams.  Take a look at the [compatibility matrix](/basics/models/compatibility#multimodal-support) to see which models support images as input.

Let's create an agent that can understand images and make tool calls as needed

## Developer Resources

* View more [Examples](/basics/multimodal/images/usage/image-to-text)

---

## 4. Create output directory

**URL:** llms-txt#4.-create-output-directory

output_dir = Path(output_dir)
output_dir.mkdir(parents=True, exist_ok=True)

---

## Get Trace Statistics by Session

**URL:** llms-txt#get-trace-statistics-by-session

Source: https://docs.agno.com/reference-api/schema/tracing/get-trace-session-stats

get /trace_session_stats
Retrieve aggregated trace statistics grouped by session ID with pagination.

**Provides insights into:**
- Total traces per session
- First and last trace timestamps per session
- Associated user and agent information

**Filtering Options:**
- By user ID
- By agent ID
- By team ID
- By workflow ID
- By time range

**Use Cases:**
- Monitor session-level activity
- Track conversation flows
- Identify high-activity sessions
- Analyze user engagement patterns

---

## Create web search agent for supplementary information

**URL:** llms-txt#create-web-search-agent-for-supplementary-information

web_agent = Agent(
    name="Web Search Agent",
    role="Handle web search requests",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
)

---

## Filtering on MilvusDB

**URL:** llms-txt#filtering-on-milvusdb

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-milvus-db

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in MilvusDB.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.milvus import Milvus

---

## Google BigQuery

**URL:** llms-txt#google-bigquery

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/database/google-bigquery

GoogleBigQueryTools enables agents to interact with Google BigQuery for large-scale data analysis and SQL queries.

The following agent can query and analyze BigQuery datasets:

| Parameter               | Type            | Default | Description                                           |
| ----------------------- | --------------- | ------- | ----------------------------------------------------- |
| `dataset`               | `str`           | `None`  | BigQuery dataset name (required).                     |
| `project`               | `Optional[str]` | `None`  | Google Cloud project ID. Uses GOOGLE\_CLOUD\_PROJECT. |
| `location`              | `Optional[str]` | `None`  | BigQuery location. Uses GOOGLE\_CLOUD\_LOCATION.      |
| `credentials`           | `Optional[Any]` | `None`  | Google Cloud credentials object.                      |
| `enable_list_tables`    | `bool`          | `True`  | Enable table listing functionality.                   |
| `enable_describe_table` | `bool`          | `True`  | Enable table description functionality.               |
| `enable_run_sql_query`  | `bool`          | `True`  | Enable SQL query execution functionality.             |

| Function         | Description                                             |
| ---------------- | ------------------------------------------------------- |
| `list_tables`    | List all tables in the specified BigQuery dataset.      |
| `describe_table` | Get detailed schema information about a specific table. |
| `run_sql_query`  | Execute SQL queries on BigQuery datasets.               |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/google_bigquery.py)
* [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
* [BigQuery SQL Reference](https://cloud.google.com/bigquery/docs/reference/standard-sql/)

---

## Async Accuracy Evaluation

**URL:** llms-txt#async-accuracy-evaluation

Source: https://docs.agno.com/basics/evals/accuracy/usage/accuracy-async

Example showing how to run accuracy evaluations asynchronously for better performance.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run the Agent. This will store a session in our "my_session_table"

**URL:** llms-txt#run-the-agent.-this-will-store-a-session-in-our-"my_session_table"

**Contents:**
- Retrieving sessions

agent.print_response("What is the capital of France?")
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SQLiteDb

**Examples:**

Example 1 (unknown):
```unknown
## Retrieving sessions

You can manually retrieve stored sessions using the `get_session` method. This also works for `Teams` and `Workflows`:
```

---

## Reliability with Single Tool

**URL:** llms-txt#reliability-with-single-tool

Source: https://docs.agno.com/basics/evals/reliability/usage/basic

Example showing how to assert an Agent is making the expected tool calls.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## List All Teams

**URL:** llms-txt#list-all-teams

Source: https://docs.agno.com/reference-api/schema/teams/list-all-teams

get /teams
Retrieve a comprehensive list of all teams configured in this OS instance.

**Returns team information including:**
- Team metadata (ID, name, description, execution mode)
- Model configuration for team coordination
- Team member roster with roles and capabilities
- Knowledge sharing and memory configurations

---

## Step with custom function streaming on AgentOS

**URL:** llms-txt#step-with-custom-function-streaming-on-agentos

Source: https://docs.agno.com/basics/workflows/usage/step-with-function-streaming-agentos

This example demonstrates how to use named steps with custom function executors and streaming on AgentOS.

This example demonstrates how to use Step objects with custom function executors, and how to stream their responses using the [AgentOS](/agent-os/introduction).

The agent and team running inside the custom function step can also stream their results directly to the AgentOS.

```python step_with_function_streaming_agentos.py theme={null}
from typing import AsyncIterator, Union

from agno.agent.agent import Agent
from agno.db.in_memory import InMemoryDb

---

## Financial Analysis Agent

**URL:** llms-txt#financial-analysis-agent

finance_agent = Agent(
    name="Financial Analyst",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools(stock_price=True, company_info=True)],
    instructions="Perform quantitative financial analysis",
)

---

## Print team member message metrics

**URL:** llms-txt#print-team-member-message-metrics

**Contents:**
- Developer Resources

print("---" * 5, "Team Member Message Metrics", "---" * 5)
if run_response.member_responses:
    for member_response in run_response.member_responses:
        if member_response.messages:
            for message in member_response.messages:
                if message.role == "assistant":
                    if message.content:
                        print(f"Member Message: {message.content}")
                    elif message.tool_calls:
                        print(f"Member Tool calls: {message.tool_calls}")
                    print("---" * 5, "Member Metrics", "---" * 5)
                    pprint(message.metrics)
                    print("---" * 20)
```

You'll see the outputs with following information:

* `input_tokens`: The number of tokens sent to the model.
* `output_tokens`: The number of tokens received from the model.
* `total_tokens`: The sum of `input_tokens` and `output_tokens`.
* `audio_input_tokens`: The number of tokens sent to the model for audio input.
* `audio_output_tokens`: The number of tokens received from the model for audio output.
* `audio_total_tokens`: The sum of `audio_input_tokens` and `audio_output_tokens`.
* `cache_read_tokens`: The number of tokens read from the cache.
* `cache_write_tokens`: The number of tokens written to the cache.
* `reasoning_tokens`: The number of tokens used for reasoning.
* `duration`: The duration of the run in seconds.
* `time_to_first_token`: The time taken until the first token was generated.
* `provider_metrics`: Any provider-specific metrics.

## Developer Resources

* View the [TeamRunOutput schema](/reference/teams/team-response)
* View the [Metrics schema](/reference/agents/metrics)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/metrics/01_team_metrics.py)

---

## SurrealDB Async

**URL:** llms-txt#surrealdb-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/surrealdb/usage/async-surreal-db

```python cookbook/knowledge/vector_db/surrealdb/async_surreal_db.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import AsyncSurreal

SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

---

## Create supplier profile request

**URL:** llms-txt#create-supplier-profile-request

supplier_request = SupplierProfile(
    supplier_name="Your Company Name",
    supplier_homepage_url="https://yourcompany.com",
    user_email="your.email@company.com",
)

---

## Async Team with Tools

**URL:** llms-txt#async-team-with-tools

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/usage/async-team-with-tools

This example demonstrates how to create an async team with various tools for information gathering using multiple agents with different tools (Wikipedia, DuckDuckGo, AgentQL) to gather comprehensive information asynchronously.

```python cookbook/examples/teams/tools/03_async_team_with_tools.py theme={null}
"""
This example demonstrates how to create an async team with various tools for information gathering.

The team uses multiple agents with different tools (Wikipedia, DuckDuckGo, AgentQL) to
gather comprehensive information about a company asynchronously.
"""

import asyncio
from uuid import uuid4

from agno.agent.agent import Agent
from agno.models.anthropic import Claude
from agno.models.mistral import MistralChat
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.agentql import AgentQLTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.wikipedia import WikipediaTools

---

## Delete operations examples

**URL:** llms-txt#delete-operations-examples

vector_db = knowledge.vector_db
vector_db.delete_by_name("Recipes")

---

## Video Input (Local File Upload)

**URL:** llms-txt#video-input-(local-file-upload)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/video-input-local-file-upload

```python cookbook/models/google/gemini/video_input_local_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

---

## Add from URL to the knowledge base

**URL:** llms-txt#add-from-url-to-the-knowledge-base

**Contents:**
- Custom knowledge retrieval
- Knowledge storage
  - Contents database
  - Vector databases
  - Adding contents
- Example: Agentic RAG Agent
- Developer Resources

asyncio.run(
    knowledge.add_content_async(
        name="Recipes",
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
        metadata={"user_tag": "Recipes from website"},
    )
)

agent = Agent(
    name="My Agent",
    description="Agno 2.0 Agent Implementation",
    knowledge=knowledge,
    search_knowledge=True,
)

agent.print_response(
    "How do I make chicken and galangal in coconut milk soup?",
    markdown=True,
)
python  theme={null}
def knowledge_retriever(agent: Agent, query: str, num_documents: Optional[int], **kwargs) -> Optional[list[dict]]:
  ...
python  theme={null}
def knowledge_retriever(agent: Agent, query: str, num_documents: Optional[int], **kwargs) -> Optional[list[dict]]:
  ...

agent = Agent(
    knowledge_retriever=knowledge_retriever,
    search_knowledge=True,
)
python  theme={null}
...
knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=vector_db,
    contents_db=contents_db,
)

asyncio.run(
    knowledge.add_content_async(
        name="CV",
        path="cookbook/knowledge/testing_resources/cv_1.pdf",
        metadata={"user_tag": "Engineering Candidates"},
    )
)
bash  theme={null}
    docker run -d \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -e PGDATA=/var/lib/postgresql/data/pgdata \
      -v pgvolume:/var/lib/postgresql/data \
      -p 5532:5432 \
      --name pgvector \
      agnohq/pgvector:16
    bash Mac theme={null}
      pip install -U pgvector pypdf psycopg sqlalchemy
      bash Windows theme={null}
      pip install -U pgvector pypdf psycopg sqlalchemy
      python agentic_rag.py theme={null}
    import asyncio
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    from agno.knowledge.knowledge import Knowledge
    from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(
        db_url=db_url,
        knowledge_table="knowledge_contents",
    )

knowledge = Knowledge(
        contents_db=db,
        vector_db=PgVector(
            table_name="recipes",
            db_url=db_url,
        )
    )

agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        db=db,
        knowledge=knowledge,
        markdown=True,
    )
    if __name__ == "__main__":
        asyncio.run(
            knowledge.add_content_async(
                name="Recipes",
                url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
                metadata={"user_tag": "Recipes from website"}
            )
        )
        # Create and use the agent
        asyncio.run(
            agent.aprint_response(
                "How do I make chicken and galangal in coconut milk soup?",
                markdown=True,
            )
        )
    python  theme={null}
    python agentic_rag.py
    ```
  </Step>
</Steps>

## Developer Resources

* View the [Agent schema](/reference/agents/agent)
* View the [Knowledge schema](/reference/knowledge/knowledge)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/)

**Examples:**

Example 1 (unknown):
```unknown
We can give our agent access to the knowledge base in the following ways:

* We can set `search_knowledge=True` to add a `search_knowledge_base()` tool to the Agent. `search_knowledge` is `True` **by default** if you add `knowledge` to an Agent.
* We can set `add_knowledge_to_context=True` to automatically add references from the knowledge base to the Agent's context, based in your user message. This is the traditional RAG approach.

## Custom knowledge retrieval

If you need complete control over the knowledge base search, you can pass your own `knowledge_retriever` function with the following signature:
```

Example 2 (unknown):
```unknown
Example of how to configure an agent with a custom retriever:
```

Example 3 (unknown):
```unknown
This function is called during `search_knowledge_base()` and is used by the Agent to retrieve references from the knowledge base.

<Tip>
  Async retrievers are supported. Simply create an async function and pass it to
  the `knowledge_retriever` parameter.
</Tip>

## Knowledge storage

Knowledge content is tracked in a "Contents DB" and vectorized and stored in a "Vector DB".

### Contents database

The Contents DB is a database that stores the name, description, metadata and other information for any content you add to the knowledge base.

Below is the schema for the Contents DB:

| Field            | Type   | Description                                                                                         |
| ---------------- | ------ | --------------------------------------------------------------------------------------------------- |
| `id`             | `str`  | The unique identifier for the knowledge content.                                                    |
| `name`           | `str`  | The name of the knowledge content.                                                                  |
| `description`    | `str`  | The description of the knowledge content.                                                           |
| `metadata`       | `dict` | The metadata for the knowledge content.                                                             |
| `type`           | `str`  | The type of the knowledge content.                                                                  |
| `size`           | `int`  | The size of the knowledge content. Applicable only to files.                                        |
| `linked_to`      | `str`  | The ID of the knowledge content that this content is linked to.                                     |
| `access_count`   | `int`  | The number of times this content has been accessed.                                                 |
| `status`         | `str`  | The status of the knowledge content.                                                                |
| `status_message` | `str`  | The message associated with the status of the knowledge content.                                    |
| `created_at`     | `int`  | The timestamp when the knowledge content was created.                                               |
| `updated_at`     | `int`  | The timestamp when the knowledge content was last updated.                                          |
| `external_id`    | `str`  | The external ID of the knowledge content. Used when external vector stores are used, like LightRAG. |

This data is best displayed on the [knowledge page of the AgentOS UI](https://os.agno.com/knowledge).

### Vector databases

Vector databases offer the best solution for retrieving relevant results from dense information quickly.

### Adding contents

The typical way content is processed when being added to the knowledge base is:

<Steps>
  <Step title="Parse the content">
    A reader is used to parse the content based on the type of content that is
    being inserted
  </Step>

  <Step title="Chunk the information">
    The content is broken down into smaller chunks to ensure our search query
    returns only relevant results.
  </Step>

  <Step title="Embed each chunk">
    The chunks are converted into embedding vectors and stored in a vector
    database.
  </Step>
</Steps>

For example, to add a PDF to the knowledge base:
```

Example 4 (unknown):
```unknown
<Tip>
  See more details on [Loading the Knowledge
  Base](/basics/knowledge/overview#loading-the-knowledge).
</Tip>

<Note>
  Knowledge filters are currently supported on the following knowledge base
  types: <b>PDF</b>, <b>PDF\_URL</b>, <b>Text</b>, <b>JSON</b>, and <b>DOCX</b>.
  For more details, see the [Knowledge Filters
  documentation](/basics/knowledge/filters/overview).
</Note>

## Example: Agentic RAG Agent

Let's build a **RAG Agent** that answers questions from a PDF.

<Steps>
  <Step title="Set up the database">
    Let's use `Postgres` as both our contents and vector databases.

    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Postgres** on port **5532** using:
```

---

## Option B: Manually add cultural knowledge without a model

**URL:** llms-txt#option-b:-manually-add-cultural-knowledge-without-a-model

**Contents:**
- Storage: Where Culture Lives
  - Manual Culture Retrieval

response_format = CulturalKnowledge(
    name="Response Format Standard",
    summary="Keep responses concise, scannable, and runnable-first",
    categories=["communication", "ux"],
    content=(
        "- Lead with minimal runnable snippet\n"
        "- Use numbered steps for procedures\n"
        "- End with validation checklist"
    ),
    notes=["Derived from user feedback"],
)

culture_manager.add_cultural_knowledge(response_format)
python  theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

db = PostgresDb(
    db_url="postgresql://user:password@localhost:5432/my_database",
    cultural_knowledge_table="my_culture_table",  # Custom table name
)

agent = Agent(
    db=db, 
    add_culture_to_context=True,
    update_cultural_knowledge=True,
)
python  theme={null}
from agno.culture.manager import CultureManager
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="agno.db")
culture_manager = CultureManager(db=db)

**Examples:**

Example 1 (unknown):
```unknown
**Best for:** Seeding initial organizational principles, onboarding standards, or brand guidelines that all agents should follow.

## Storage: Where Culture Lives

Cultural knowledge is stored in the database you connect to your agent. Agno supports all major database systems: Postgres, SQLite, MongoDB, and more. Check the [Database documentation](/basics/database/overview) for the full list.

By default, cultural knowledge is stored in the `agno_cultural_knowledge` table (or collection for document databases). A custom table name can also be configured. If this table doesn't exist, Agno creates it automatically.
```

Example 2 (unknown):
```unknown
### Manual Culture Retrieval

You can manually retrieve cultural knowledge using the `CultureManager`:
```

---

## SingleStore Async

**URL:** llms-txt#singlestore-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/singlestore/usage/async-singlestore-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run SingleStore">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run SingleStore">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Example: Generate Fibonacci numbers

**URL:** llms-txt#example:-generate-fibonacci-numbers

**Contents:**
- Toolkit Params
- Toolkit Functions
  - Code Execution
  - File Operations
  - Filesystem Operations
  - Command Execution
  - Internet Access
  - Sandbox Management
- Developer Resources

agent.print_response(
    "Write Python code to generate the first 10 Fibonacci numbers and calculate their sum and average"
)

| Parameter         | Type   | Default | Description                                               |
| ----------------- | ------ | ------- | --------------------------------------------------------- |
| `api_key`         | `str`  | `None`  | E2B API key. If not provided, uses E2B\_API\_KEY env var. |
| `timeout`         | `int`  | `300`   | Timeout in seconds for the sandbox (default: 5 minutes)   |
| `sandbox_options` | `dict` | `None`  | Additional options to pass to the Sandbox constructor     |

| Function          | Description                                    |
| ----------------- | ---------------------------------------------- |
| `run_python_code` | Run Python code in the E2B sandbox environment |

| Function                     | Description                                             |
| ---------------------------- | ------------------------------------------------------- |
| `upload_file`                | Upload a file to the sandbox                            |
| `download_png_result`        | Add a PNG image result as an Image object to the agent  |
| `download_chart_data`        | Extract chart data from an interactive chart in results |
| `download_file_from_sandbox` | Download a file from the sandbox to the local system    |

### Filesystem Operations

| Function             | Description                                            |
| -------------------- | ------------------------------------------------------ |
| `list_files`         | List files and directories in a path in the sandbox    |
| `read_file_content`  | Read the content of a file from the sandbox            |
| `write_file_content` | Write text content to a file in the sandbox            |
| `watch_directory`    | Watch a directory for changes for a specified duration |

### Command Execution

| Function                  | Description                                    |
| ------------------------- | ---------------------------------------------- |
| `run_command`             | Run a shell command in the sandbox environment |
| `stream_command`          | Run a shell command and stream its output      |
| `run_background_command`  | Run a shell command in the background          |
| `kill_background_command` | Kill a background command                      |

| Function         | Description                                             |
| ---------------- | ------------------------------------------------------- |
| `get_public_url` | Get a public URL for a service running in the sandbox   |
| `run_server`     | Start a server in the sandbox and return its public URL |

### Sandbox Management

| Function                 | Description                           |
| ------------------------ | ------------------------------------- |
| `set_sandbox_timeout`    | Update the timeout for the sandbox    |
| `get_sandbox_status`     | Get the current status of the sandbox |
| `shutdown_sandbox`       | Shutdown the sandbox immediately      |
| `list_running_sandboxes` | List all running sandboxes            |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/e2b.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/e2b_tools.py)

---

## Agent Using Multimodal Tool Response in Runs

**URL:** llms-txt#agent-using-multimodal-tool-response-in-runs

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/agent/usage/agent-using-multimodal-tool-response-in-runs

This example demonstrates how to create an agent that uses DALL-E to generate images and maintains conversation history across multiple runs, allowing the agent to remember previous interactions and images generated.

```python agent_using_multimodal_tool_response_in_runs.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.tools.dalle import DalleTools

---

## Setup your Database

**URL:** llms-txt#setup-your-database

db = SingleStoreDb(db_url=db_url)

---

## Database configuration for metrics storage

**URL:** llms-txt#database-configuration-for-metrics-storage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
db = PostgresDb(db_url=db_url, session_table="team_metrics_sessions")

---

## Run: `wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4` to download a sample video

**URL:** llms-txt#run:-`wget-https://storage.googleapis.com/generativeai-downloads/images/greatredspot.mp4`-to-download-a-sample-video

video_path = Path(__file__).parent.joinpath("samplevideo.mp4")
video_file = None
remote_file_name = f"files/{video_path.stem.lower().replace('_', '')}"
try:
    video_file = model.get_client().files.get(name=remote_file_name)
except Exception as e:
    logger.info(f"Error getting file {video_path.stem}: {e}")
    pass

---

## Mistral Small

**URL:** llms-txt#mistral-small

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/mistral-small

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Retrieve and display generated videos

**URL:** llms-txt#retrieve-and-display-generated-videos

run_response = video_agent.get_last_run_output()
if run_response and run_response.videos:
    for video in run_response.videos:
        print(f"Generated video URL: {video.url}")

---

## store only important events

**URL:** llms-txt#store-only-important-events

production_workflow = Workflow(
    name="Production Workflow",
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.step_started,
        WorkflowRunEvent.parallel_execution_started,
        # keep step_completed and workflow_completed
    ],
    steps=[...]
)

---

## Recursive Chunking

**URL:** llms-txt#recursive-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/recursive

Recursive chunking is a method of splitting documents into smaller chunks by recursively applying a chunking strategy.
This is useful when you want to process large documents in smaller, manageable pieces.

<Snippet file="chunking-recursive.mdx" />

---

## Secure deployment pipeline

**URL:** llms-txt#secure-deployment-pipeline

workflow = Workflow(
    name="Secure Deployment Pipeline",
    steps=[
        Step(name="Security Scan", agent=security_scanner),
        Step(name="Security Gate", executor=security_gate),  # May stop here
        Step(name="Deploy Code", agent=code_deployer),       # Only if secure
        Step(name="Setup Monitoring", agent=monitoring_agent), # Only if deployed
    ]
)

---

## Context Engineering

**URL:** llms-txt#context-engineering

**Contents:**
- System message context
  - System message Parameters
  - How the system message is built
  - Set the system message directly
- User message context
  - Additional user message context
- Chat history
- Managing Tool Calls

Source: https://docs.agno.com/basics/context/team/overview

Learn how to write prompts and other context engineering techniques for your teams.

Context engineering is the process of designing and controlling the information (context) that is sent to language models to guide their behavior and outputs.
In practice, building context comes down to one question: "Which information is most likely to achieve the desired outcome?"

Effective context engineering is an iterative process: refining the system message, trying out different descriptions and instructions, and using features such as schemas, delegation, and tool integrations.

The context of an Agno team consists of the following:

* **System message**: The system message is the main context that is sent to the team, including all additional context
* **User message**: The user message is the message that is sent to the team.
* **Chat history**: The chat history is the history of the conversation between the team and the user.
* **Additional input**: Any few-shot examples or other additional input that is added to the context.

## System message context

The following are some key parameters that are used to create the system message:

1. **Description**: A description that guides the overall behaviour of the team.
2. **Instructions**: A list of precise, task-specific instructions on how to achieve its goal.
3. **Expected Output**: A description of the expected output from the Team.
4. **Members**: Information about team members, their roles, and capabilities.

The system message is built from the teamâ€™s description, instructions, member details, and other settings. A team leaderâ€™s system message additionally includes delegation rules and coordination guidelines. For example:

Will produce the following system message:

### System message Parameters

The Team creates a default system message that can be customized using the following parameters:

| Parameter                          | Type        | Default | Description                                                                                                                                                                |
| ---------------------------------- | ----------- | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `description`                      | `str`       | `None`  | A description of the Team that is added to the start of the system message.                                                                                                |
| `instructions`                     | `List[str]` | `None`  | List of instructions added to the system prompt in `<instructions>` tags. Default instructions are also created depending on values for `markdown`, `expected_output` etc. |
| `additional_context`               | `str`       | `None`  | Additional context added to the end of the system message.                                                                                                                 |
| `expected_output`                  | `str`       | `None`  | Provide the expected output from the Team. This is added to the end of the system message.                                                                                 |
| `markdown`                         | `bool`      | `False` | Add an instruction to format the output using markdown.                                                                                                                    |
| `add_datetime_to_context`          | `bool`      | `False` | If True, add the current datetime to the prompt to give the team a sense of time. This allows for relative times like "tomorrow" to be used in the prompt                  |
| `add_name_to_context`              | `bool`      | `False` | If True, add the name of the team to the context.                                                                                                                          |
| `add_location_to_context`          | `bool`      | `False` | If True, add the location of the team to the context. This allows for location-aware responses and local context.                                                          |
| `timezone_identifier`              | `str`       | `None`  | Allows for custom timezone for datetime instructions following the TZ Database format (e.g. "Etc/UTC")                                                                     |
| `add_member_tools_to_context`      | `bool`      | `True`  | If True, add the tools available to team members to the context.                                                                                                           |
| `add_session_summary_to_context`   | `bool`      | `False` | If True, add the session summary to the context. See [sessions](/basics/sessions/overview) for more information.                                                           |
| `add_memories_to_context`          | `bool`      | `False` | If True, add the user memories to the context. See [memory](/basics/memory/overview) for more information.                                                                 |
| `add_dependencies_to_context`      | `bool`      | `False` | If True, add the dependencies to the context. See [dependencies](/basics/dependencies/overview) for more information.                                                      |
| `add_session_state_to_context`     | `bool`      | `False` | If True, add the session state to the context. See [state](/basics/state/overview) for more information.                                                                   |
| `add_knowledge_to_context`         | `bool`      | `False` | If True, add retrieved knowledge to the context, to enable RAG. See [knowledge](/basics/knowledge/overview) for more information.                                          |
| `enable_agentic_knowledge_filters` | `bool`      | `False` | If True, let the team choose the knowledge filters. See [knowledge](/basics/knowledge/filters/overview) for more information.                                              |
| `system_message`                   | `str`       | `None`  | Override the default system message.                                                                                                                                       |
| `respond_directly`                 | `bool`      | `False` | If True, the team leader won't process responses from members and instead will return them directly.                                                                       |
| `delegate_to_all_members`          | `bool`      | `False` | If True, the team leader will delegate the task to all members simultaneously, instead of one by one. When running async (using `arun`) members will run concurrently.     |
| `determine_input_for_members`      | `bool`      | `True`  | Set to false if you want to send the run input directly to the member agents.                                                                                              |
| `share_member_interactions`        | `bool`      | `False` | If True, send all previous member interactions to members.                                                                                                                 |
| `get_member_information_tool`      | `bool`      | `False` | If True, add a tool to get information about the team members.                                                                                                             |

See the full [Team reference](/reference/teams/team) for more information.

### How the system message is built

Lets take the following example team:

Below is the system message that will be built:

<Tip>
  This example is exhaustive and illustrates what is possible with the system message, however in practice you would only use some of these settings.
</Tip>

#### Additional Context

You can add additional context to the end of the system message using the `additional_context` parameter.

Here, `additional_context` adds a note to the system message indicating that the team can access specific database tables.

#### Team Member Information

The member information is automatically injected into the system message. This includes the member ID, name, role, and tools.
You can optionally minimize this by setting `add_member_tools_to_context` to False, which removes the member tools from the system message.

You can also give the team leader a tool to get information about the team members.

#### Tool Instructions

If you are using a [Toolkit](/integrations/toolkits/overview) on your team, you can add tool instructions to the system message using the `instructions` parameter:

These instructions are injected into the system message after the `<additional_information>` tags.

#### Agentic Memories

If you have `enable_agentic_memory` set to `True` on your team, the team gets the ability to create/update user memories using tools.

This adds the following to the system message:

#### Agentic Knowledge Filters

If you have knowledge enabled on your team, you can let the team choose the knowledge filters using the `enable_agentic_knowledge_filters` parameter.

This will add the following to the system message:

Learn about agentic knowledge filters in more detail in the [knowledge filters](/basics/knowledge/filters/overview) section.

### Set the system message directly

You can manually set the system message using the `system_message` parameter. This will ignore all other settings and use the system message you provide.

## User message context

The `input` sent to the `Team.run()` or `Team.print_response()` is used as the user message.

See [dependencies](/basics/dependencies/overview) for how to do dependency injection for your user message.

### Additional user message context

By default, the user message is built using the `input` sent to the `Team.run()` or `Team.print_response()` functions.

The following team parameters configure how the user message is built:

* `add_knowledge_to_context`
* `add_dependencies_to_context`

The user message that is sent to the model will look like this:

If you have database storage enabled on your team, session history is automatically stored (see [sessions](/basics/sessions/overview)).

You can now add the history of the conversation to the context using `add_history_to_context`.

This will add the history of the conversation to the context, which can be used to provide context for the next message.

See more details on [sessions](/basics/chat-history/overview).

<Note>All team member runs are added to the team session history.</Note>

## Managing Tool Calls

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.1" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.1">v2.2.1</Tooltip>
</Badge>

The `max_tool_calls_from_history` parameter can be used to add only the `n` most recent tool calls from history to the context.

This helps manage context size and reduce token costs during team runs.

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

web_agent = Agent(
    name="Web Researcher",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a web researcher. Search the web for comprehensive information on given topics.",
    tools=[DuckDuckGoTools()],
)

team = Team(
    members=[web_agent],
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file="tmp/filter_history_tool_calls_team.db"),
    add_history_to_context=True,
    max_tool_calls_from_history=5,  # Keep only last 5 tool calls in context
    show_members_responses=True,
)

team.print_response("Search for AI news")
team.print_response("Search for crypto trends")
team.print_response("Search for tech stocks")

**Examples:**

Example 1 (unknown):
```unknown
Will produce the following system message:
```

Example 2 (unknown):
```unknown
### System message Parameters

The Team creates a default system message that can be customized using the following parameters:

| Parameter                          | Type        | Default | Description                                                                                                                                                                |
| ---------------------------------- | ----------- | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `description`                      | `str`       | `None`  | A description of the Team that is added to the start of the system message.                                                                                                |
| `instructions`                     | `List[str]` | `None`  | List of instructions added to the system prompt in `<instructions>` tags. Default instructions are also created depending on values for `markdown`, `expected_output` etc. |
| `additional_context`               | `str`       | `None`  | Additional context added to the end of the system message.                                                                                                                 |
| `expected_output`                  | `str`       | `None`  | Provide the expected output from the Team. This is added to the end of the system message.                                                                                 |
| `markdown`                         | `bool`      | `False` | Add an instruction to format the output using markdown.                                                                                                                    |
| `add_datetime_to_context`          | `bool`      | `False` | If True, add the current datetime to the prompt to give the team a sense of time. This allows for relative times like "tomorrow" to be used in the prompt                  |
| `add_name_to_context`              | `bool`      | `False` | If True, add the name of the team to the context.                                                                                                                          |
| `add_location_to_context`          | `bool`      | `False` | If True, add the location of the team to the context. This allows for location-aware responses and local context.                                                          |
| `timezone_identifier`              | `str`       | `None`  | Allows for custom timezone for datetime instructions following the TZ Database format (e.g. "Etc/UTC")                                                                     |
| `add_member_tools_to_context`      | `bool`      | `True`  | If True, add the tools available to team members to the context.                                                                                                           |
| `add_session_summary_to_context`   | `bool`      | `False` | If True, add the session summary to the context. See [sessions](/basics/sessions/overview) for more information.                                                           |
| `add_memories_to_context`          | `bool`      | `False` | If True, add the user memories to the context. See [memory](/basics/memory/overview) for more information.                                                                 |
| `add_dependencies_to_context`      | `bool`      | `False` | If True, add the dependencies to the context. See [dependencies](/basics/dependencies/overview) for more information.                                                      |
| `add_session_state_to_context`     | `bool`      | `False` | If True, add the session state to the context. See [state](/basics/state/overview) for more information.                                                                   |
| `add_knowledge_to_context`         | `bool`      | `False` | If True, add retrieved knowledge to the context, to enable RAG. See [knowledge](/basics/knowledge/overview) for more information.                                          |
| `enable_agentic_knowledge_filters` | `bool`      | `False` | If True, let the team choose the knowledge filters. See [knowledge](/basics/knowledge/filters/overview) for more information.                                              |
| `system_message`                   | `str`       | `None`  | Override the default system message.                                                                                                                                       |
| `respond_directly`                 | `bool`      | `False` | If True, the team leader won't process responses from members and instead will return them directly.                                                                       |
| `delegate_to_all_members`          | `bool`      | `False` | If True, the team leader will delegate the task to all members simultaneously, instead of one by one. When running async (using `arun`) members will run concurrently.     |
| `determine_input_for_members`      | `bool`      | `True`  | Set to false if you want to send the run input directly to the member agents.                                                                                              |
| `share_member_interactions`        | `bool`      | `False` | If True, send all previous member interactions to members.                                                                                                                 |
| `get_member_information_tool`      | `bool`      | `False` | If True, add a tool to get information about the team members.                                                                                                             |

See the full [Team reference](/reference/teams/team) for more information.

### How the system message is built

Lets take the following example team:
```

Example 3 (unknown):
```unknown
Below is the system message that will be built:
```

Example 4 (unknown):
```unknown
<Tip>
  This example is exhaustive and illustrates what is possible with the system message, however in practice you would only use some of these settings.
</Tip>

#### Additional Context

You can add additional context to the end of the system message using the `additional_context` parameter.

Here, `additional_context` adds a note to the system message indicating that the team can access specific database tables.
```

---

## Create the social media analysis agent

**URL:** llms-txt#create-the-social-media-analysis-agent

**Contents:**
- Usage

social_media_agent = Agent(
    name="Social Media Analyst",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        XTools(
            include_post_metrics=True,
            wait_on_rate_limit=True,
        )
    ],
    instructions="""
    You are a senior Brand Intelligence Analyst with a specialty in social-media listening  on the X (Twitter) platform.  
    Your job is to transform raw tweet content and engagement metrics into an executive-ready intelligence report that helps product, marketing, and support teams  make data-driven decisions.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CORE RESPONSIBILITIES
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. Retrieve tweets with X tools that you have access to and analyze both the text and metrics such as likes, retweets, replies.
    2. Classify every tweet as Positive / Negative / Neutral / Mixed, capturing the reasoning (e.g., praise for feature X, complaint about bugs, etc.).
    3. Detect patterns in engagement metrics to surface:
       â€¢ Viral advocacy (high likes & retweets, low replies)
       â€¢ Controversy (low likes, high replies)
       â€¢ Influence concentration (verified or high-reach accounts driving sentiment)
    4. Extract thematic clusters and recurring keywords covering:
       â€¢ Feature praise / pain points  
       â€¢ UX / performance issues  
       â€¢ Customer-service interactions  
       â€¢ Pricing & ROI perceptions  
       â€¢ Competitor mentions & comparisons  
       â€¢ Emerging use-cases & adoption barriers
    5. Produce actionable, prioritized recommendations (Immediate, Short-term, Long-term) that address the issues and pain points.
    6. Supply a response strategy: which posts to engage, suggested tone & template,    influencer outreach, and community-building ideas.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    DELIVERABLE FORMAT (markdown)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ### 1 Â· Executive Snapshot
    â€¢ Brand-health score (1-10)  
    â€¢ Net sentiment ( % positive â€“ % negative )  
    â€¢ Top 3 positive & negative drivers  
    â€¢ Red-flag issues that need urgent attention

### 2 Â· Quantitative Dashboard
    | Sentiment | #Posts | % | Avg Likes | Avg Retweets | Avg Replies | Notes |
    |-----------|-------:|---:|----------:|-------------:|------------:|------|
    ( fill table )

### 3 Â· Key Themes & Representative Quotes
    For each major theme list: description, sentiment trend, excerpted tweets (truncated),  and key metrics.

### 4 Â· Competitive & Market Signals
    â€¢ Competitors referenced, sentiment vs. Agno  
    â€¢ Feature gaps users mention  
    â€¢ Market positioning insights

### 5 Â· Risk Analysis
    â€¢ Potential crises / viral negativity  
    â€¢ Churn indicators  
    â€¢ Trust & security concerns

### 6 Â· Opportunity Landscape
    â€¢ Features or updates that delight users  
    â€¢ Advocacy moments & influencer opportunities  
    â€¢ Untapped use-cases highlighted by the community

### 7 Â· Strategic Recommendations
    **Immediate (â‰¤48 h)** â€“ urgent fixes or comms  
    **Short-term (1-2 wks)** â€“ quick wins & tests  
    **Long-term (1-3 mo)** â€“ roadmap & positioning

### 8 Â· Response Playbook
    For high-impact posts list: tweet-id/url, suggested response, recommended responder (e. g., support, PM, exec), and goal (defuse, amplify, learn).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ASSESSMENT & REASONING GUIDELINES
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Weigh sentiment by engagement volume & author influence (verified == Ã—1.5 weight).  
    â€¢ Use reply-to-like ratio > 0.5 as controversy flag.  
    â€¢ Highlight any coordinated or bot-like behaviour.  
    â€¢ Use the tools provided to you to get the data you need.

Remember: your insights will directly inform the product strategy, customer-experience efforts, and brand reputation.  Be objective, evidence-backed, and solution-oriented.
""",
    markdown=True,
)

social_media_agent.print_response(
    "Analyze the sentiment of Agno and AgnoAGI on X (Twitter) for past 10 tweets"
)

bash  theme={null}
    export OPENAI_API_KEY=xxx
    export X_BEARER_TOKEN=xxx
    export X_CONSUMER_KEY=xxx
    export X_CONSUMER_SECRET=xxx
    export X_ACCESS_TOKEN=xxx
    export X_ACCESS_TOKEN_SECRET=xxx
    bash  theme={null}
    pip install -U agno openai tweepy
    bash Mac theme={null}
      python cookbook/examples/agents/social_media_agent.py
      bash Windows theme={null}
      python cookbook/examples/agents/social_media_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## AWS SES

**URL:** llms-txt#aws-ses

**Contents:**
- Prerequisites

Source: https://docs.agno.com/integrations/toolkits/others/aws-ses

**AWSSESTool** enables an Agent to send emails using Amazon Simple Email Service (SES).

The following example requires the `boto3` library and valid AWS credentials. You can install `boto3` via pip:

You must also configure your AWS credentials so that the SDK can authenticate to SES. The easiest way is via the AWS CLI:

```shell  theme={null}
aws configure

**Examples:**

Example 1 (unknown):
```unknown
You must also configure your AWS credentials so that the SDK can authenticate to SES. The easiest way is via the AWS CLI:
```

---

## -*- Print memories and session summary

**URL:** llms-txt#-*--print-memories-and-session-summary

if agent.db:
    pprint(agent.db.get_user_memories(user_id="test_user"))
    pprint(
        agent.db.get_session(
            session_id="test_session", session_type=SessionType.AGENT
        ).summary  # type: ignore
    )

---

## VectorDB controls search strategy; Knowledge controls retrieval size

**URL:** llms-txt#vectordb-controls-search-strategy;-knowledge-controls-retrieval-size

**Contents:**
- Making Search Work Better
  - Add Rich Metadata
  - Use Descriptive Filenames
  - Structure Content Logically
  - Test with Real Queries
  - Analyze What's Being Retrieved
- Advanced Search Features
  - Custom Retrieval Logic for Agents
  - Search with Filtering

vector_db = PgVector(table_name="embeddings_table", db_url=db_url, search_type=SearchType.hybrid)
knowledge = Knowledge(vector_db=vector_db, max_results=5)
python  theme={null}
knowledge.add_content(
    path="policies/",
    metadata={"type": "policy", "department": "HR", "audience": "employees"},
)
python  theme={null}
"hr_employee_handbook_2024.pdf"  # âœ… Clear and descriptive
"document1.pdf"                  # âŒ Generic and unhelpful
python  theme={null}
test_queries = [
    "What's our vacation policy?",
    "How do I submit expenses?",
    "Remote work guidelines",
]

for q in test_queries:
    results = knowledge.search(q)
    print(q, "->", results[0].content[:100] + "..." if results else "No results")
python  theme={null}
async def my_retriever(query: str, num_documents: int = 5, filters: dict | None = None, **kwargs):
    # Example: reformulate query, then search with metadata filters
    refined = query.replace("vacation", "paid time off")
    docs = await knowledge.async_search(refined, max_results=num_documents, filters=filters)
    return [d.to_dict() for d in docs]

agent.knowledge_retriever = my_retriever
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Making Search Work Better

### Add Rich Metadata

Metadata helps filter and organize results:
```

Example 2 (unknown):
```unknown
### Use Descriptive Filenames

File names can help with search relevance in some backends:
```

Example 3 (unknown):
```unknown
### Structure Content Logically

Well-organized content searches better:

* Use clear headings and sections
* Include relevant terminology naturally (don't keyword-stuff)
* Add summaries at the top of long documents
* Cross-reference related topics

### Test with Real Queries

The best way to know if search is working? Try it with actual questions:
```

Example 4 (unknown):
```unknown
### Analyze What's Being Retrieved

Ask yourself:

* Are results actually relevant to the query?
* Is important information missing from results?
* Are results in a sensible order? (If not, try adding a reranker)
* Should you adjust chunk sizes or metadata?

## Advanced Search Features

### Custom Retrieval Logic for Agents

Provide a `knowledge_retriever` callable to implement your own decisioning (e.g., reformulation, follow-up searches, domain rules). The agent will call this when fetching documents.
```

---

## OpenAI

**URL:** llms-txt#openai

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/openai

The OpenAIChat model provides access to OpenAI models like GPT-4o.

| Parameter               | Type                                               | Default        | Description                                                                        |
| ----------------------- | -------------------------------------------------- | -------------- | ---------------------------------------------------------------------------------- |
| `id`                    | `str`                                              | `"gpt-4o"`     | The id of the OpenAI model to use                                                  |
| `name`                  | `str`                                              | `"OpenAIChat"` | The name of the model                                                              |
| `provider`              | `str`                                              | `"OpenAI"`     | The provider of the model                                                          |
| `store`                 | `Optional[bool]`                                   | `None`         | Whether to store the conversation for training purposes                            |
| `reasoning_effort`      | `Optional[str]`                                    | `None`         | The reasoning effort level for o1 models ("low", "medium", "high")                 |
| `verbosity`             | `Optional[Literal["low", "medium", "high"]]`       | `None`         | Controls verbosity level of reasoning models                                       |
| `metadata`              | `Optional[Dict[str, Any]]`                         | `None`         | Developer-defined metadata to associate with the completion                        |
| `frequency_penalty`     | `Optional[float]`                                  | `None`         | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |
| `logit_bias`            | `Optional[Any]`                                    | `None`         | Modifies the likelihood of specified tokens appearing in the completion            |
| `logprobs`              | `Optional[bool]`                                   | `None`         | Whether to return log probabilities of the output tokens                           |
| `top_logprobs`          | `Optional[int]`                                    | `None`         | Number of most likely tokens to return log probabilities for (0 to 20)             |
| `max_tokens`            | `Optional[int]`                                    | `None`         | Maximum number of tokens to generate (deprecated, use max\_completion\_tokens)     |
| `max_completion_tokens` | `Optional[int]`                                    | `None`         | Maximum number of completion tokens to generate                                    |
| `modalities`            | `Optional[List[str]]`                              | `None`         | List of modalities to use ("text" and/or "audio")                                  |
| `audio`                 | `Optional[Dict[str, Any]]`                         | `None`         | Audio configuration (e.g., `{"voice": "alloy", "format": "wav"}`)                  |
| `presence_penalty`      | `Optional[float]`                                  | `None`         | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |
| `seed`                  | `Optional[int]`                                    | `None`         | Random seed for deterministic sampling                                             |
| `stop`                  | `Optional[Union[str, List[str]]]`                  | `None`         | Up to 4 sequences where the API will stop generating further tokens                |
| `temperature`           | `Optional[float]`                                  | `None`         | Controls randomness in the model's output (0.0 to 2.0)                             |
| `user`                  | `Optional[str]`                                    | `None`         | A unique identifier representing your end-user                                     |
| `top_p`                 | `Optional[float]`                                  | `None`         | Controls diversity via nucleus sampling (0.0 to 1.0)                               |
| `service_tier`          | `Optional[str]`                                    | `None`         | Service tier to use ("auto", "default", "flex", "priority")                        |
| `strict_output`         | `bool`                                             | `True`         | Controls schema adherence for structured outputs                                   |
| `extra_headers`         | `Optional[Any]`                                    | `None`         | Additional headers to include in requests                                          |
| `extra_query`           | `Optional[Any]`                                    | `None`         | Additional query parameters to include in requests                                 |
| `extra_body`            | `Optional[Any]`                                    | `None`         | Additional body parameters to include in requests                                  |
| `request_params`        | `Optional[Dict[str, Any]]`                         | `None`         | Additional parameters to include in the request                                    |
| `role_map`              | `Optional[Dict[str, str]]`                         | `None`         | Mapping of message roles to OpenAI roles                                           |
| `api_key`               | `Optional[str]`                                    | `None`         | The API key for authenticating with OpenAI (defaults to OPENAI\_API\_KEY env var)  |
| `organization`          | `Optional[str]`                                    | `None`         | The organization ID to use for requests                                            |
| `base_url`              | `Optional[Union[str, httpx.URL]]`                  | `None`         | The base URL for the OpenAI API                                                    |
| `timeout`               | `Optional[float]`                                  | `None`         | Request timeout in seconds                                                         |
| `max_retries`           | `Optional[int]`                                    | `None`         | Maximum number of retries for failed requests                                      |
| `default_headers`       | `Optional[Any]`                                    | `None`         | Default headers to include in all requests                                         |
| `default_query`         | `Optional[Any]`                                    | `None`         | Default query parameters to include in all requests                                |
| `http_client`           | `Optional[Union[httpx.Client, httpx.AsyncClient]]` | `None`         | HTTP client instance for making requests                                           |
| `client_params`         | `Optional[Dict[str, Any]]`                         | `None`         | Additional parameters for client configuration                                     |
| `retries`               | `int`                                              | `0`            | Number of retries to attempt before raising a ModelProviderError                   |
| `delay_between_retries` | `int`                                              | `1`            | Delay between retries, in seconds                                                  |
| `exponential_backoff`   | `bool`                                             | `False`        | If True, the delay between retries is doubled each time                            |

---

## Azure Cosmos DB MongoDB vCore Vector Database

**URL:** llms-txt#azure-cosmos-db-mongodb-vcore-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/azure_cosmos_mongodb/overview

Learn how to use Azure Cosmos DB MongoDB vCore as a vector database for your Knowledge Base

Follow the instructions in the [Azure Cosmos DB Setup Guide](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore) to get the connection string.

Install MongoDB packages:

```python agent_with_knowledge.py theme={null}
import urllib.parse
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.mongodb import MongoVectorDb

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Upload file and create cache

**URL:** llms-txt#upload-file-and-create-cache

txt_file = client.files.upload(file="large_document.txt")
cache = client.caches.create(
    model="gemini-2.0-flash-001",
    config={
        "system_instruction": "You are an expert at analyzing transcripts.",
        "contents": [txt_file],
        "ttl": "300s",
    },
)

---

## Initialize AgentOps

**URL:** llms-txt#initialize-agentops

---

## Trello

**URL:** llms-txt#trello

**Contents:**
- Prerequisites
- Example
- Toolkit Functions
- Toolkit Params
  - Board Filter Options for `list_boards`
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/trello

Agno TrelloTools helps to integrate Trello functionalities into your agents, enabling management of boards, lists, and cards.

The following examples require the `trello` library and Trello API credentials which can be obtained by following Trello's developer documentation.

Set the following environment variables:

The following agent will create a board called `ai-agent` and inside it create list called `todo` and `doing` and inside each of them create card called `create agent`.

| Function          | Description                                                   |
| ----------------- | ------------------------------------------------------------- |
| `create_card`     | Creates a new card in a specified board and list.             |
| `get_board_lists` | Retrieves all lists on a specified Trello board.              |
| `move_card`       | Moves a card to a different list.                             |
| `get_cards`       | Retrieves all cards from a specified list.                    |
| `create_board`    | Creates a new Trello board.                                   |
| `create_list`     | Creates a new list on a specified board.                      |
| `list_boards`     | Lists all Trello boards accessible by the authenticated user. |

| Parameter    | Type            | Default | Description                                                                        |
| ------------ | --------------- | ------- | ---------------------------------------------------------------------------------- |
| `api_key`    | `Optional[str]` | `None`  | Trello API key. If not provided, uses TRELLO\_API\_KEY environment variable.       |
| `api_secret` | `Optional[str]` | `None`  | Trello API secret. If not provided, uses TRELLO\_API\_SECRET environment variable. |
| `token`      | `Optional[str]` | `None`  | Trello token. If not provided, uses TRELLO\_TOKEN environment variable.            |

### Board Filter Options for `list_boards`

The `list_boards` function accepts a `board_filter` argument with the following options:

* `all` (default)
* `open`
* `closed`
* `organization`
* `public`
* `starred`

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/trello.py)
* View [Cookbook Example](https://github.com/agno-agi/agno/tree/main/cookbook/tools/trello_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
Set the following environment variables:
```

Example 2 (unknown):
```unknown
## Example

The following agent will create a board called `ai-agent` and inside it create list called `todo` and `doing` and inside each of them create card called `create agent`.
```

---

## Apify

**URL:** llms-txt#apify

**Contents:**
- What is Apify?
- Prerequisites
- Basic Usage

Source: https://docs.agno.com/integrations/toolkits/others/apify

This guide demonstrates how to integrate and use [Apify](https://apify.com/actors) Actors within the Agno framework to enhance your AI agents with web scraping, crawling, data extraction, and web automation capabilities.

[Apify](https://apify.com/) is a platform that provides:

* Data collection services for AI Agents, specializing in extracting data from social media, search engines, online maps, e-commerce sites, travel portals, or general websites
* A marketplace of ready-to-use Actors (specialized tools) for various data tasks
* Infrastructure to run and monetize our own AI Agents

1. Sign up for an [Apify account](https://console.apify.com/sign-up)
2. Obtain your Apify API token (can be obtained from [Apify](https://docs.apify.com/platform/integrations/api))
3. Install the required packages:

The Agno framework makes it easy to integrate Apify Actors into your agents. Here's a simple example:

```python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

**Examples:**

Example 1 (unknown):
```unknown
## Basic Usage

The Agno framework makes it easy to integrate Apify Actors into your agents. Here's a simple example:
```

---

## StopAgentRun

**URL:** llms-txt#stopagentrun

**Contents:**
- Constructor
  - Parameters
- When to Use
- Behavior
- See Also

Source: https://docs.agno.com/reference/tools/stop-agent-run

API reference for the StopAgentRun exception used to exit the tool call loop and complete the agent run.

The `StopAgentRun` exception allows you to exit the model execution loop and end the agent run. When raised from a tool function, the agent immediately exits the tool call loop, and the run status is set to `COMPLETED`. All session state, messages, tool calls, and tool results up to that point are stored in the database.

<Note>
  This does **not** cancel the agent run. It completes the run gracefully after exiting the tool call loop.
</Note>

<ResponseField name="exc" type="str" required>
  The reason for stopping execution. This message is logged and can be used for debugging or user feedback.
</ResponseField>

<ResponseField name="user_message" type="Union[str, Message]" optional>
  An optional message to display to the user about why execution stopped.
</ResponseField>

<ResponseField name="agent_message" type="Union[str, Message]" optional>
  An optional message from the agent's perspective about the stop.
</ResponseField>

<ResponseField name="messages" type="List[Union[dict, Message]]" optional>
  An optional list of messages to add to the conversation history before stopping.
</ResponseField>

Use `StopAgentRun` when:

* **Critical errors**: An error occurs that cannot be recovered from
* **Security triggers**: A security threshold or policy is violated
* **Task completion**: The task is fully completed and no further tool calls are needed
* **Resource limits**: A resource limit (time, cost, API calls) is reached
* **Manual intervention needed**: The situation requires human review or approval
* **Early termination**: You want to exit the tool call loop based on business logic

When `StopAgentRun` is raised:

1. The tool call loop exits immediately
2. No further tool calls are executed
3. The run status is set to `COMPLETED`
4. All session state is saved to the database
5. All messages and tool calls up to that point are preserved
6. The optional `user_message` or `agent_message` can be displayed to the user

* [RetryAgentRun Exception](/reference/tools/retry-agent-run)
* [Exceptions & Retries Guide](/basics/tools/exceptions)

---

## Pipedream Google Calendar

**URL:** llms-txt#pipedream-google-calendar

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/mcp/usage/pipedream-google-calendar

This example shows how to use the Google Calendar Pipedream MCP server with Agno Agents.

---

## Async Team Streaming

**URL:** llms-txt#async-team-streaming

Source: https://docs.agno.com/basics/teams/usage/async-flows/basic-streaming

This example demonstrates asynchronous streaming responses from a team using specialized agents with financial tools to provide real-time stock information with async streaming output.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/streaming" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Ask a question that would likely trigger tool use

**URL:** llms-txt#ask-a-question-that-would-likely-trigger-tool-use

**Contents:**
- Usage

openai_agent.print_response("What is happening in France?")
bash  theme={null}
    export LITELLM_API_KEY=xxx
    bash  theme={null}
    pip install -U litellm openai agno ddgs
    bash Mac theme={null}
      python cookbook/models/litellm/tool_use.py
      bash Windows theme={null}
      python cookbook/models/litellm/tool_use.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ContentsDB automatically stores all the fields from the schema above

**URL:** llms-txt#contentsdb-automatically-stores-all-the-fields-from-the-schema-above

---

## DEMO FUNCTIONS USING CLI

**URL:** llms-txt#demo-functions-using-cli

---

## Initialize Zoom tools with credentials

**URL:** llms-txt#initialize-zoom-tools-with-credentials

zoom_tools = ZoomTools(
    account_id="your_account_id",
    client_id="your_client_id",
    client_secret="your_client_secret"
)

---

## Async Coordinated Team

**URL:** llms-txt#async-coordinated-team

Source: https://docs.agno.com/basics/teams/usage/async-flows/basic-team

This example demonstrates a simple team of AI agents working together to research topics across different platforms.

The team consists of three specialized agents:

1. **HackerNews Researcher** - Uses HackerNews API to find and analyze relevant HackerNews posts
2. **Article Reader** - Reads articles from URLs

The team leader coordinates the agents by:

* Giving each agent a specific task
* Providing clear instructions for each agent
* Collecting and summarizing the results from each agent

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/async_flows" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Instantiate with specific configuration

**URL:** llms-txt#instantiate-with-specific-configuration

**Contents:**
- Streaming execution with custom function step on AgentOS:
- Developer Resources

content_planning_step = Step(
    name="Content Planning Step",
    executor=CustomExecutor(max_retries=5, use_cache=False),
)
python  theme={null}
class CustomExecutor:
    async def __call__(self, step_input: StepInput) -> StepOutput:
        # 1. Custom preprocessing
        # 2. Call agents/teams as needed
        # 3. Custom postprocessing
        return StepOutput(content=enhanced_content)

content_planning_step = Step(
    name="Content Planning Step",
    executor=CustomExecutor(),
)
python custom_function_step_async_stream.py theme={null}
content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
    db=InMemoryDb(),
)

async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness.

Note: This function calls content_planner.arun() internally, and all events
    from that agent call will automatically get workflow context injected by
    the workflow execution system - no manual intervention required!
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

# Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

Core Topic: {message}

Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

Please create a detailed, actionable content plan.
    """

try:
        response_iterator = content_planner.arun(
            planning_prompt, stream=True, stream_events=True
        )
        async for event in response_iterator:
            yield event

response = content_planner.get_last_run_output()

enhanced_content = f"""
            ## Strategic Content Plan

**Planning Topic:** {message}

**Research Integration:** {"âœ“ Research-based" if previous_step_content else "âœ— No research foundation"}

**Content Strategy:**
            {response.content}

**Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

yield StepOutput(content=enhanced_content)

except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )
```

<Note>
  Streaming in case of a class-based executor also works the same way by defining the `__call__` method to yield the events.
</Note>

## Developer Resources

* [Step with a Custom Function](/basics/workflows/usage/step-with-function)
* [Step with a Custom Function with Streaming on AgentOS](/basics/workflows/usage/step-with-function-streaming-agentos)
* [Parallel and custom function step streaming on AgentOS](/basics/workflows/usage/parallel-steps-workflow)

**Examples:**

Example 1 (unknown):
```unknown
Also supports async execution by defining the `__call__` method to be an async function.
```

Example 2 (unknown):
```unknown
For a detailed example see [Class-based Executor](/basics/workflows/usage/class-based-executor).

## Streaming execution with custom function step on AgentOS:

If you are running an agent or team within the custom function step, you can enable streaming on the [AgentOS chat page](/agent-os/introduction#chat-page) by setting `stream=True` and `stream_events=True` when calling `run()` or `arun()` and yielding the events.

<Note>
  Using the AgentOS, runs will be asynchronous and responses will be streamed.
  This means you must keep the custom function step asynchronous, by using `.arun()` instead of `.run()` to run your Agents or Teams.
</Note>
```

---

## Embed sentence in database

**URL:** llms-txt#embed-sentence-in-database

embeddings = GeminiEmbedder().get_embedding("The quick brown fox jumps over the lazy dog.")

---

## Output Transformation Post-Hook

**URL:** llms-txt#output-transformation-post-hook

**Contents:**
- Code
  - Key Takeaways
  - Important Disclaimer
  - Questions to Consider Next
- Usage

Source: https://docs.agno.com/basics/hooks/usage/team/output-transformation-post-hook

This example demonstrates how to use a post-hook to transform the output of an Team, before it is returned to the user.

This example shows how to:

1. Transform team responses by updating RunOutput.content
2. Add formatting, structure, and additional information
3. Enhance the user experience through content modification

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Expensive model for main conversations

**URL:** llms-txt#expensive-model-for-main-conversations

**Contents:**
- Mitigation Strategy #3: Guide Memory Behavior with Instructions
- Mitigation Strategy #4: Implement Memory Pruning

agent = Agent(
    db=db,
    model=OpenAIChat(id="gpt-4o"),
    memory_manager=memory_manager,
    enable_agentic_memory=True
)
python  theme={null}
agent = Agent(
    db=db,
    enable_agentic_memory=True,
    instructions=[
        "Only update memories when users share significant new information.",
        "Don't create memories for casual conversation or temporary states.",
        "Batch multiple memory updates together when possible."
    ]
)
python  theme={null}
from datetime import datetime, timedelta

def prune_old_memories(db, user_id, days=90):
    """Remove memories older than 90 days"""
    cutoff_timestamp = int((datetime.now() - timedelta(days=days)).timestamp())
    
    memories = db.get_user_memories(user_id=user_id)
    for memory in memories:
        if memory.updated_at and memory.updated_at < cutoff_timestamp:
            db.delete_user_memory(memory_id=memory.memory_id)

**Examples:**

Example 1 (unknown):
```unknown
This approach can reduce memory-related costs by 98% while maintaining conversation quality.

## Mitigation Strategy #3: Guide Memory Behavior with Instructions

Add explicit instructions to prevent frivolous memory updates:
```

Example 2 (unknown):
```unknown
## Mitigation Strategy #4: Implement Memory Pruning

Prevent memory bloat by periodically cleaning up old or irrelevant memories:
```

---

## Video Caption Generation Team

**URL:** llms-txt#video-caption-generation-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/video-caption-generation

This example demonstrates how a team can collaborate to process videos and generate captions by extracting audio, transcribing it, and embedding captions back into the video.

```python cookbook/examples/teams/multimodal/video_caption_generation.py theme={null}
"""Please install dependencies using:
pip install openai moviepy ffmpeg
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.moviepy_video import MoviePyVideoTools
from agno.tools.openai import OpenAITools

video_processor = Agent(
    name="Video Processor",
    role="Handle video processing and audio extraction",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[MoviePyVideoTools(process_video=True, generate_captions=True)],
    instructions=[
        "Extract audio from videos for processing",
        "Handle video file operations efficiently",
    ],
)

caption_generator = Agent(
    name="Caption Generator",
    role="Generate and embed captions in videos",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[MoviePyVideoTools(embed_captions=True), OpenAITools()],
    instructions=[
        "Transcribe audio to create accurate captions",
        "Generate SRT format captions with proper timing",
        "Embed captions seamlessly into videos",
    ],
)

---

## Choose the AI model for your agent

**URL:** llms-txt#choose-the-ai-model-for-your-agent

model = OpenAIChat(id="gpt-5-mini")
print(f"Model selected: {model.id}")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can now test our model setup:
```

---

## Chat History Management

**URL:** llms-txt#chat-history-management

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/chat-history/agent/usage/chat-history

This example demonstrates how to manage and retrieve chat history in agent conversations, enabling access to previous conversation messages and context.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Setup PostgreSQL">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/session" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create a team for collaborative image-to-text processing

**URL:** llms-txt#create-a-team-for-collaborative-image-to-text-processing

**Contents:**
- Usage

image_team = Team(
    name="Image Story Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[image_analyzer, creative_writer],
    instructions=[
        "Work together to create compelling fiction stories from images.",
        "Image Analyst: First analyze the image for visual details and context.",
        "Creative Writer: Transform the analysis into engaging fiction narratives.",
        "Ensure the story captures the essence and mood of the image.",
    ],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")
image_team.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
bash  theme={null}
    pip install agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    # Add a sample.jpg image file in the same directory as the script
    bash  theme={null}
    python cookbook/examples/teams/multimodal/image_to_text.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Add sample image">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Sentence Transformer

**URL:** llms-txt#sentence-transformer

Source: https://docs.agno.com/reference/knowledge/embedder/sentence-transformer

Sentence Transformer Embedder is a class that allows you to embed documents using Hugging Face's sentence-transformers library, providing access to a wide range of open-source embedding models that can run locally.

<Snippet file="embedder-sentence-transformer-reference.mdx" />

---

## Configure Redis connection (from environment variables if available, otherwise use local defaults)

**URL:** llms-txt#configure-redis-connection-(from-environment-variables-if-available,-otherwise-use-local-defaults)

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
INDEX_NAME = os.getenv("REDIS_INDEX", "agno_cookbook_vectors")

---

## 2. Configure contents database

**URL:** llms-txt#2.-configure-contents-database

contents_db = PostgresDb(
        db_url="postgresql+psycopg://ai:ai@localhost:5432/db",
        knowledge_table="knowledge_contents",
    ),

---

## Streaming Agent Responses

**URL:** llms-txt#streaming-agent-responses

Source: https://docs.agno.com/basics/agents/usage/streaming

Learn how to stream your agent's responses

In this example, we will see how to stream the responses from an agent.

This is useful when you want to process or show the response in real-time, as it's generated.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create an agent with SingleStore db

**URL:** llms-txt#create-an-agent-with-singlestore-db

**Contents:**
- Params
- Developer Resources

agent = Agent(
    db=db,
    tools=[DuckDuckGoTools()],
    add_history_to_context=True,
)
agent.print_response("How many people live in Canada?")
agent.print_response("What is their national anthem called?")

<Snippet file="db-singlestore-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/singlestore/singlestore_for_agent.py)

---

## Initialize OpenLIT without OTLP endpoint for console output

**URL:** llms-txt#initialize-openlit-without-otlp-endpoint-for-console-output

---

## Add a simple endpoint to set the JWT authentication cookie

**URL:** llms-txt#add-a-simple-endpoint-to-set-the-jwt-authentication-cookie

@app.get("/set-auth-cookie")
async def set_auth_cookie(response: Response):
    """
    Endpoint to set the JWT authentication cookie.
    In a real application, this would be done after successful login.
    """
    # Create a test JWT token
    payload = {
        "sub": "cookie_user_789",
        "session_id": "cookie_session_123",
        "name": "Jane Smith",
        "email": "jane.smith@example.com",
        "roles": ["user", "premium"],
        "org": "Example Corp",
        "exp": datetime.now(UTC) + timedelta(hours=24),
        "iat": datetime.now(UTC),
    }

token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")

# Set HTTP-only cookie (more secure than localStorage for JWT storage)
    response.set_cookie(
        key="auth_token",
        value=token,
        httponly=True,  # Prevents access from JavaScript (XSS protection)
        secure=True,  # Only send over HTTPS in production
        samesite="strict",  # CSRF protection
        max_age=24 * 60 * 60,  # 24 hours
    )

return {
        "message": "Authentication cookie set successfully",
        "cookie_name": "auth_token",
        "expires_in": "24 hours",
        "security_features": ["httponly", "secure", "samesite=strict"],
        "instructions": "Now you can make authenticated requests without Authorization headers",
    }

---

## Define the research agents

**URL:** llms-txt#define-the-research-agents

hackernews_agent = Agent(
    name="HackerNews Researcher",
    instructions="You are a researcher specializing in finding the latest tech news and discussions from Hacker News. Focus on startup trends, programming topics, and tech industry insights.",
    tools=[HackerNewsTools()],
)

web_agent = Agent(
    name="Web Researcher",
    instructions="You are a comprehensive web researcher. Search across multiple sources including news sites, blogs, and official documentation to gather detailed information.",
    tools=[DuckDuckGoTools()],
)

content_agent = Agent(
    name="Content Publisher",
    instructions="You are a content creator who takes research data and creates engaging, well-structured articles. Format the content with proper headings, bullet points, and clear conclusions.",
)

---

## Setup your Workflow

**URL:** llms-txt#setup-your-workflow

**Contents:**
- Asynchronous Database Support
- Supported databases
- Troubleshooting

workflow = Workflow(db=db, ...)
python async_postgres_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import AsyncPostgresDb

db_url = "postgresql+psycopg_async://ai:ai@localhost:5532/ai"

db = AsyncPostgresDb(db_url=db_url)

agent = Agent(db=db)
```

## Supported databases

Databases currently supported for storage:

<CardGroup cols={3}>
  <Card title="DynamoDB" icon="database" iconType="duotone" href="/integrations/database/dynamodb/overview">
    Amazon's NoSQL database service
  </Card>

<Card title="FireStore" icon="database" iconType="duotone" href="/integrations/database/firestore/overview">
    Google's NoSQL document database
  </Card>

<Card title="JSON" icon="file-code" iconType="duotone" href="/integrations/database/json/overview">
    Simple file-based JSON storage
  </Card>

<Card title="JSON on GCS" icon="cloud" iconType="duotone" href="/integrations/database/gcs/overview">
    JSON storage on Google Cloud Storage
  </Card>

<Card title="MongoDB" icon="database" iconType="duotone" href="/integrations/database/mongo/overview">
    Popular NoSQL document database
  </Card>

<Card title="MySQL" icon="database" iconType="duotone" href="/integrations/database/mysql/overview">
    Widely-used relational database
  </Card>

<Card title="Neon" icon="database" iconType="duotone" href="/integrations/database/neon/overview">
    Serverless PostgreSQL platform
  </Card>

<Card title="PostgreSQL" icon="database" iconType="duotone" href="/integrations/database/postgres/overview">
    Advanced open-source relational database
  </Card>

<Card title="Redis" icon="database" iconType="duotone" href="/integrations/database/redis/overview">
    In-memory data structure store
  </Card>

<Card title="SurrealDB" icon="database" iconType="duotone" href="/integrations/database/surrealdb/overview">
    Open source multi-modal database
  </Card>

<Card title="SingleStore" icon="database" iconType="duotone" href="/integrations/database/singlestore/overview">
    Real-time analytics database
  </Card>

<Card title="SQLite" icon="database" iconType="duotone" href="/integrations/database/sqlite/overview">
    Lightweight embedded database
  </Card>

<Card title="Supabase" icon="database" iconType="duotone" href="/integrations/database/supabase/overview">
    Open source Firebase alternative
  </Card>
</CardGroup>

<Note>
  [In-Memory](/integrations/database/in-memory/overview) databases are also supported. This is not recommended for production, but perfect for demos and testing.
</Note>

You can see a detailed list of [examples](/integrations/database) for all supported databases.

<Tabs>
  <Tab title="`MissingGreenlet` exception">
    If you are seeing `sqlalchemy.exc.MissingGreenlet: greenlet_spawn has not been called ...`, it means you are using a synchronous `db_engine` when using `AsyncPostgresDb` or `AsyncSqliteDb`.

You can create an asynchronous database engine by using the `create_async_engine` function from `sqlalchemy.ext.asyncio`.
  </Tab>

<Tab title="`AsyncContextNotStarted` exception">
    If you are seeing `sqlalchemy.ext.asyncio.exc.AsyncContextNotStarted: AsyncConnection context ...` error, it means you are using an asynchronous `db_engine` with `PostgresDb` or `SqliteDb`.

You can create a synchronous database engine by using the `create_engine` function from `sqlalchemy`.
  </Tab>
</Tabs>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Learn more about [Teams](/basics/teams/overview) and
  [Workflows](/basics/workflows/overview), Agno abstractions to build
  multi-agent systems.
</Note>

## Asynchronous Database Support

Agno supports asynchronous database operations when using the `AsyncPostgresDb` and `AsyncSqliteDb` classes.

For example:
```

---

## pprint(structured_output_response.content)

**URL:** llms-txt#pprint(structured_output_response.content)

**Contents:**
- Usage

json_mode_agent.print_response("New York")

bash  theme={null}
    export PERPLEXITY_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai
    bash Mac theme={null}
      python cookbook/models/perplexity/structured_output.py
      bash Windows theme={null}
      python cookbook/models/perplexity/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Ollama

**URL:** llms-txt#ollama

**Contents:**
- Key Features
- Parameters

Source: https://docs.agno.com/reference/models/ollama

The Ollama model provides access to open source models, both locally-hosted and via **Ollama Cloud**.

**Local Usage**: Run models on your own hardware using the Ollama client. Perfect for development, privacy-sensitive workloads, and when you want full control over your infrastructure.

**Cloud Usage**: Access cloud-hosted models via [Ollama Cloud](https://ollama.com) with an API key for scalable, production-ready deployments. No local setup required - simply set your `OLLAMA_API_KEY` and start using powerful models instantly.

* **Dual Deployment Options**: Choose between local hosting for privacy and control, or cloud hosting for scalability
* **Seamless Switching**: Easy transition between local and cloud deployments with minimal code changes
* **Auto-configuration**: When using an API key, the host automatically defaults to Ollama Cloud
* **Wide Model Support**: Access to extensive library of open-source models including GPT-OSS, Llama, Qwen, DeepSeek, and Phi models

| Parameter               | Type                          | Default                    | Description                                                      |
| ----------------------- | ----------------------------- | -------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`                         | `"llama3.2"`               | The name of the Ollama model to use                              |
| `name`                  | `str`                         | `"Ollama"`                 | The name of the model                                            |
| `provider`              | `str`                         | `"Ollama"`                 | The provider of the model                                        |
| `host`                  | `str`                         | `"http://localhost:11434"` | The host URL for the Ollama server                               |
| `timeout`               | `Optional[int]`               | `None`                     | Request timeout in seconds                                       |
| `format`                | `Optional[str]`               | `None`                     | The format to return the response in (e.g., "json")              |
| `options`               | `Optional[Dict[str, Any]]`    | `None`                     | Additional model options (temperature, top\_p, etc.)             |
| `keep_alive`            | `Optional[Union[float, str]]` | `None`                     | How long to keep the model loaded (e.g., "5m", 3600 seconds)     |
| `template`              | `Optional[str]`               | `None`                     | The prompt template to use                                       |
| `system`                | `Optional[str]`               | `None`                     | System message to use                                            |
| `raw`                   | `Optional[bool]`              | `None`                     | Whether to return raw response without formatting                |
| `stream`                | `bool`                        | `True`                     | Whether to stream the response                                   |
| `retries`               | `int`                         | `0`                        | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`                         | `1`                        | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`                        | `False`                    | If True, the delay between retries is doubled each time          |

---

## Couchbase

**URL:** llms-txt#couchbase

Source: https://docs.agno.com/reference/vector-db/couchbase

<Snippet file="vector-db-couchbase-reference.mdx" />

---

## Custom Lifespan

**URL:** llms-txt#custom-lifespan

**Contents:**
- FastAPI Lifespan
- Common Use Cases
- Example

Source: https://docs.agno.com/agent-os/lifespan

Customize the lifespan of your AgentOS app to handle startup and shutdown logic.

You will often want to run code before your AgentOS app starts or before it shuts down.

This can be done by providing a custom **lifespan function** via the `lifespan` parameter.

This is how a lifespan function looks like:

The custom lifespan function you provide will be used as the **lifespan context manager** for the FastAPI app used by your AgentOS. Remember to decorate it with `@asynccontextmanager` as shown in the examples.

<Tip>
  See the [FastAPI documentation](https://fastapi.tiangolo.com/advanced/events/#lifespan-events) for more information about the lifespan context manager.
</Tip>

If you are using a custom FastAPI app, you don't need to worry about overwriting its lifespan.

The lifespan you provide will wrap the existing lifespan of the app, letting you combine all.

Lifespan control is useful to handle typical startup and shutdown tasks, such as:

* **Resource Initialization**: databases, third party services, caches... or anything else needed by your app.
* **Cleanup**: Close connections, store data or release resources before shut down.
* **Health Checks**: Verify dependencies are available before serving requests
* **Background Tasks**: Start/stop background processes

<Steps>
  <Step title="Code">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Setup PostgreSQL Database">
    
  </Step>

<Step title="Run Example with Python">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## FastAPI Lifespan

The custom lifespan function you provide will be used as the **lifespan context manager** for the FastAPI app used by your AgentOS. Remember to decorate it with `@asynccontextmanager` as shown in the examples.

<Tip>
  See the [FastAPI documentation](https://fastapi.tiangolo.com/advanced/events/#lifespan-events) for more information about the lifespan context manager.
</Tip>

If you are using a custom FastAPI app, you don't need to worry about overwriting its lifespan.

The lifespan you provide will wrap the existing lifespan of the app, letting you combine all.

## Common Use Cases

Lifespan control is useful to handle typical startup and shutdown tasks, such as:

* **Resource Initialization**: databases, third party services, caches... or anything else needed by your app.
* **Cleanup**: Close connections, store data or release resources before shut down.
* **Health Checks**: Verify dependencies are available before serving requests
* **Background Tasks**: Start/stop background processes

## Example

<Steps>
  <Step title="Code">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

---

## Vector Search

**URL:** llms-txt#vector-search

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/vector-search

```python vector_search.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## First call - will run the workflow (new topic)

**URL:** llms-txt#first-call---will-run-the-workflow-(new-topic)

workflow.print_response(
    "Tell me a story about a dog named Rocky", stream=True
)

---

## Team Example

**URL:** llms-txt#team-example

**Contents:**
- Developer Resources

Create a list of tools, and assign them to your Team with `set_tools`

<Tip>
  * The `add_tool` method allows you to dynamically extend an Agent's or a Team's capabilities. This is particularly useful when you want to add tools based on user input or other runtime conditions.
  * The `set_tool` method allows you to override an Agent's or a Team's capabilities. Note that this will remove any existing tools previously assigned to your Agent or Team.
</Tip>

## Developer Resources

* [Creating your own tools](/basics/tools/creating-tools/overview) - Learn how to create custom tools
* [Available Toolkits](/integrations/toolkits/overview) - Explore pre-built toolkits
* [Selecting Tools](/basics/tools/selecting-tools) - Learn how to filter tools in toolkits

---

## Basic Agent with Streaming

**URL:** llms-txt#basic-agent-with-streaming

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/dashscope/usage/basic-stream

```python cookbook/models/dashscope/basic_stream.py theme={null}
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.dashscope import DashScope

agent = Agent(model=DashScope(id="qwen-plus", temperature=0.5), markdown=True)

---

## What are Tools?

**URL:** llms-txt#what-are-tools?

Source: https://docs.agno.com/basics/tools/overview

Tools are functions your Agno Agents can use to get things done.

Tools are what make Agents and Teams capable of real-world action. While using LLMs directly you can only generate text responses, Agents and Teams equipped with tools can interact with external systems and perform practical actions.

Some examples of actions that can be performed with tools are: searching the web, running SQL, sending an email or calling APIs.

Agno comes with 120+ pre-built toolkits, which you can use to give your Agents all kind of abilities. You can also write your own tools, to give your Agents even more capabilities. The general syntax is:

```python  theme={null}
import random

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool

def get_weather(city: str) -> str:
    """Get the weather for the given city.
    
    Args:
        city (str): The city to get the weather for.
    """

# In a real implementation, this would call a weather API
    weather_conditions = ["sunny", "cloudy", "rainy", "snowy", "windy"]
    random_weather = random.choice(weather_conditions)

return f"The weather in {city} is {random_weather}."

---

## AgentOSConfig

**URL:** llms-txt#agentosconfig

**Contents:**
- Using a YAML Configuration File

Source: https://docs.agno.com/reference/agent-os/configuration

<Snippet file="agent-os-configuration-reference.mdx" />

## Using a YAML Configuration File

You can also provide your AgentOS configuration via a YAML file.

You can define all the previously mentioned configuration options in the file:

```yaml  theme={null}

---

## Create steps

**URL:** llms-txt#create-steps

manage_items_step = Step(
    name="manage_items",
    description="Help manage shopping list items (add/remove)",
    agent=shopping_assistant,
)

view_list_step = Step(
    name="view_list",
    description="View and manage the complete shopping list",
    agent=list_manager,
)

---

## Startup Analyst Agent

**URL:** llms-txt#startup-analyst-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/startup-analyst-agent

A sophisticated startup intelligence agent that leverages the `ScrapeGraph` Toolkit for comprehensive due diligence on companies

* Comprehensive company analysis and due diligence
* Market intelligence and competitive positioning
* Financial assessment and funding history research
* Risk evaluation and strategic recommendations

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create a knowledge base with the ArXiv documents

**URL:** llms-txt#create-a-knowledge-base-with-the-arxiv-documents

knowledge = Knowledge(
    # Table name: ai.arxiv_documents
    vector_db=PgVector(
        table_name="arxiv_documents",
        db_url=db_url,
    ),
)

---

## Secondary knowledge base for context expansion

**URL:** llms-txt#secondary-knowledge-base-for-context-expansion

context_knowledge = Knowledge(
    vector_db=LanceDb(
        table_name="recipes_context",
        uri="tmp/lancedb",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## User Input Required Stream Async

**URL:** llms-txt#user-input-required-stream-async

Source: https://docs.agno.com/basics/hitl/usage/user-input-required-stream-async

This example demonstrates how to use the requires_user_input parameter with async streaming responses. It shows how to collect specific user input fields in an asynchronous environment while maintaining real-time streaming.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize the agent with a tech-savvy personality and clear instructions

**URL:** llms-txt#initialize-the-agent-with-a-tech-savvy-personality-and-clear-instructions

agent = Agent(
    description="A Tech News Assistant that fetches and summarizes Hacker News stories",
    instructions=dedent("""\
        You are an enthusiastic Tech Reporter

Your responsibilities:
        - Present Hacker News stories in an engaging and informative way
        - Provide clear summaries of the information you gather

Style guide:
        - Use emoji to make your responses more engaging
        - Keep your summaries concise but informative
        - End with a friendly tech-themed sign-off\
    """),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

---

## Execute Workflow

**URL:** llms-txt#execute-workflow

Source: https://docs.agno.com/reference-api/schema/workflows/execute-workflow

post /workflows/{workflow_id}/runs
Execute a workflow with the provided input data. Workflows can run in streaming or batch mode.

**Execution Modes:**
- **Streaming (`stream=true`)**: Real-time step-by-step execution updates via SSE
- **Non-Streaming (`stream=false`)**: Complete workflow execution with final result

**Workflow Execution Process:**
1. Input validation against workflow schema
2. Sequential or parallel step execution based on workflow design
3. Data flow between steps with transformation
4. Error handling and automatic retries where configured
5. Final result compilation and response

**Session Management:**
Workflows support session continuity for stateful execution across multiple runs.

---

## LightRAG

**URL:** llms-txt#lightrag

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/lightrag/usage/lightrag-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## PDF Password Reader

**URL:** llms-txt#pdf-password-reader

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/pdf-password-reader

The **PDF Password Reader** handles password-protected PDF files, allowing you to process secure documents and convert them into searchable knowledge bases.

```python examples/basics/knowledge/readers/pdf_reader_password.py theme={null}
from agno.agent import Agent
from agno.knowledge.content import ContentAuth
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import download_file
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
download_file(
    "https://agno-public.s3.us-east-1.amazonaws.com/recipes/ThaiRecipes_protected.pdf",
    "ThaiRecipes_protected.pdf",
)

---

## Split the document into chunks

**URL:** llms-txt#split-the-document-into-chunks

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)

---

## Bring Your Own FastAPI App

**URL:** llms-txt#bring-your-own-fastapi-app

**Contents:**
- Quick Start

Source: https://docs.agno.com/agent-os/custom-fastapi/overview

Learn how to use your own FastAPI app in your AgentOS

AgentOS is built on FastAPI, which means you can easily integrate your existing FastAPI applications or add custom routes and routers to extend your agent's capabilities.

The simplest way to bring your own FastAPI app is to pass it to the AgentOS constructor:

```python  theme={null}
from fastapi import FastAPI
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os import AgentOS

---

## Accuracy with Given Answer

**URL:** llms-txt#accuracy-with-given-answer

Source: https://docs.agno.com/basics/evals/accuracy/usage/accuracy-with-given-answer

Example showing how to evaluate the accuracy of an Agno Agent's response with a given answer.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add content with rich metadata

**URL:** llms-txt#add-content-with-rich-metadata

knowledge.add_content(
    path="sales_report_q1.csv",
    metadata={
        "data_type": "sales",
        "quarter": "Q1",
        "year": 2024,
        "region": "north_america",
        "currency": "USD"
    }
)

---

## Async Streaming Agent

**URL:** llms-txt#async-streaming-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/xai/usage/basic-async-stream

```python cookbook/models/xai/basic_async_stream.py theme={null}
import asyncio
from typing import Iterator

from agno.agent import Agent, RunOutputEvent
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-3"), markdown=True)

---

## -*- FastAPI running on ECS

**URL:** llms-txt#-*--fastapi-running-on-ecs

prd_fastapi = FastApi(
    ...
    # To enable HTTPS, create an ACM certificate and add the ARN below:
    load_balancer_enable_https=True,
    load_balancer_certificate_arn="arn:aws:acm:us-east-1:497891874516:certificate/6598c24a-d4fc-4f17-8ee0-0d3906eb705f",
    ...
)
bash terminal theme={null}
  ag infra up --env prd --infra aws --name listener
  bash shorthand theme={null}
  ag infra up -e prd -i aws -n listener
  bash terminal theme={null}
  ag infra patch --env prd --infra aws --name listener
  bash shorthand theme={null}
  ag infra patch -e prd -i aws -n listener
  ```
</CodeGroup>

After this, all HTTP requests should redirect to HTTPS automatically.

**Examples:**

Example 1 (unknown):
```unknown
4. Create new Loadbalancer Listeners

Create new listeners for the loadbalancer to pickup the HTTPs configuration.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note>The certificate should be `Issued` before applying it.</Note>

After this, `https` should be working on your custom domain.

5. Update existing listeners to redirect HTTP to HTTPS

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## *******************************

**URL:** llms-txt#*******************************

**Contents:**
  - Screenshots
- Best Practices
- Troubleshooting

documents_knowledge = Knowledge(
    vector_db=PgVector(
        db_url=db_url,
        table_name="agno_knowledge_vectors",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
    contents_db=documents_db,
)

faq_knowledge = Knowledge(
    vector_db=PgVector(
        db_url=db_url,
        table_name="agno_faq_vectors",
        search_type=SearchType.hybrid,
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
    contents_db=faq_db,
)

agent_os = AgentOS(
    description="Example app with AgentOS Knowledge",
    # Add the knowledge bases to AgentOS
    knowledge=[documents_knowledge, faq_knowledge],
)

app = agent_os.get_app()

if __name__ == "__main__":
    documents_knowledge.add_content(
        name="Agno Docs", url="https://docs.agno.com/llms-full.txt", skip_if_exists=True
    )
    faq_knowledge.add_content(
        name="Agno FAQ",
        text_content=dedent("""
            What is Agno?
            Agno is a framework for building agents.
            Use it to build multi-agent systems with memory, knowledge,
            human in the loop and MCP support.
        """),
        skip_if_exists=True,
    )
    # Run your AgentOS
    # You can test your AgentOS at: http://localhost:7777/

agent_os.serve(app="agentos_knowledge:app")
```

The screenshots below show how you can access and manage your different Knowledge bases through the AgentOS interface:

<img src="https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=0ec9015b0454161b4505c8b144130ef1" alt="llm-app-aidev-run" data-og-width="1717" width="1717" data-og-height="396" height="396" data-path="images/agno_knowledge_db.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?w=280&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=eb00fe44299cec9451cd6cdebc44e9f2 280w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?w=560&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=e796c3147bf36d23c2c93280821b5bb0 560w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?w=840&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=7f9c0d49415976200f06a318928e2f92 840w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?w=1100&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=6cda6da6c4467279f58650cdd46af23c 1100w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?w=1650&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=b7c1869a66bc161041f3c342ecdb4ed8 1650w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_knowledge_db.png?w=2500&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=3542d2be09ad321719c1e86ec916debf 2500w" />

<img src="https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=e26a27eb3a5d369931e8f5abe45c2b58" alt="llm-app-aidev-run" data-og-width="1717" width="1717" data-og-height="396" height="396" data-path="images/agno_faq_db.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?w=280&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=0ed5c972cbc1480a8c3ed68f1c9a9c7a 280w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?w=560&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=d937593a927f68669063c6c2242963cb 560w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?w=840&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=69803d582a2a7aff6eb5123178dfc177 840w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?w=1100&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=cba7bff20213a1af3f2b09ad983de581 1100w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?w=1650&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=ab48eaf2440479a1af0507dcf5fed53c 1650w, https://mintcdn.com/agno-v2/d1Mr9QfTpi3u63B7/images/agno_faq_db.png?w=2500&fit=max&auto=format&n=d1Mr9QfTpi3u63B7&q=85&s=1f0ed266857a1fd2c74008f312c32ee2 2500w" />

* **Separate Knowledge by Domain**: Create separate Knowledge bases for different topics (e.g., technical docs, FAQs, policies)
* **Consistent Naming**: Use descriptive names for your Knowledge bases that reflect their content
* **Regular Updates**: Keep your Knowledge bases current by regularly adding new content and removing outdated information
* **Monitor Performance**: Use different table names for vector storage to avoid conflicts
* **Content Organization**: Use the `name` parameter when adding content to make it easily identifiable
* **Use metadata for filtering and searching**: Add metadata to your content to make it easier to find and filter

<AccordionGroup>
  <Accordion title="Knowledge base not appearing in AgentOS interface">
    Ensure your knowledge base is properly added to the `knowledge` parameter when creating your AgentOS instance.
    Also make sure to attach a `contents_db` to your Knowledge instance.
  </Accordion>

<Accordion title="Database connection errors">
    Verify your PostgreSQL connection string and ensure the database is running and accessible.
  </Accordion>

<Accordion title="Content not being found in searches">
    Check that your content has been properly embedded by verifying entries in your vector database table.
  </Accordion>
</AccordionGroup>

---

## InMemoryDb

**URL:** llms-txt#inmemorydb

Source: https://docs.agno.com/reference/storage/in_memory

`InMemoryDb` is a class that implements the Db interface using an in-memory database. It provides lightweight, in-memory storage for agent/team sessions.

<Snippet file="db-in-memory-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Create a Recipe Expert Agent with knowledge of Thai recipes

**URL:** llms-txt#create-a-recipe-expert-agent-with-knowledge-of-thai-recipes

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=dedent("""\
        You are a passionate and knowledgeable Thai cuisine expert! ðŸ§‘â€ðŸ³
        Think of yourself as a combination of a warm, encouraging cooking instructor,
        a Thai food historian, and a cultural ambassador.

Follow these steps when answering questions:
        1. If the user asks a about Thai cuisine, ALWAYS search your knowledge base for authentic Thai recipes and cooking information
        2. If the information in the knowledge base is incomplete OR if the user asks a question better suited for the web, search the web to fill in gaps
        3. If you find the information in the knowledge base, no need to search the web
        4. Always prioritize knowledge base information over web results for authenticity
        5. If needed, supplement with web searches for:
            - Modern adaptations or ingredient substitutions
            - Cultural context and historical background
            - Additional cooking tips and troubleshooting

Communication style:
        1. Start each response with a relevant cooking emoji
        2. Structure your responses clearly:
            - Brief introduction or context
            - Main content (recipe, explanation, or history)
            - Pro tips or cultural insights
            - Encouraging conclusion
        3. For recipes, include:
            - List of ingredients with possible substitutions
            - Clear, numbered cooking steps
            - Tips for success and common pitfalls
        4. Use friendly, encouraging language

Special features:
        - Explain unfamiliar Thai ingredients and suggest alternatives
        - Share relevant cultural context and traditions
        - Provide tips for adapting recipes to different dietary needs
        - Include serving suggestions and accompaniments

End each response with an uplifting sign-off like:
        - 'Happy cooking! à¸‚à¸­à¹ƒà¸«à¹‰à¸­à¸£à¹ˆà¸­à¸¢ (Enjoy your meal)!'
        - 'May your Thai cooking adventure bring joy!'
        - 'Enjoy your homemade Thai feast!'

Remember:
        - Always verify recipe authenticity with the knowledge base
        - Clearly indicate when information comes from web sources
        - Be encouraging and supportive of home cooks at all skill levels\
    """),
    knowledge=knowledge,
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "How do I make chicken and galangal in coconut milk soup", stream=True
)
agent.print_response("What is the history of Thai curry?", stream=True)
agent.print_response("What ingredients do I need for Pad Thai?", stream=True)

---

## PgVector Hybrid Search

**URL:** llms-txt#pgvector-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/pgvector/usage/pgvector-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="run-pgvector.mdx" />

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector.mdx" />

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 1: Generate a basic image with default settings

**URL:** llms-txt#example-1:-generate-a-basic-image-with-default-settings

agent.print_response("Generate an image of a futuristic city with flying cars and tall skyscrapers", markdown=True)

---

## Entertainment trends

**URL:** llms-txt#entertainment-trends

entertainment_prompt = """\
Analyze media trends for:
Keywords: streaming, gaming, social media
Sources: variety.com, polygon.com, theverge.com
"""
```

---

## Agent Team

**URL:** llms-txt#agent-team

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/17-agent-team

This example shows how to create a powerful team of AI agents working together to provide comprehensive financial analysis and news reporting. The team consists of:

1. Web Agent: Searches and analyzes latest news
2. Finance Agent: Analyzes financial data and market trends
3. Lead Editor: Coordinates and combines insights from both agents

Example prompts to try:

* "What's the latest news and financial performance of Apple (AAPL)?"
* "Analyze the impact of AI developments on NVIDIA's stock (NVDA)"
* "How are EV manufacturers performing? Focus on Tesla (TSLA) and Rivian (RIVN)"
* "What's the market outlook for semiconductor companies like AMD and Intel?"
* "Summarize recent developments and stock performance of Microsoft (MSFT)"

```python agent_team.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.exa import ExaTools

web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions=dedent("""\
        You are an experienced web researcher and news analyst! ðŸ”

Follow these steps when searching for information:
        1. Start with the most recent and relevant sources
        2. Cross-reference information from multiple sources
        3. Prioritize reputable news outlets and official sources
        4. Always cite your sources with links
        5. Focus on market-moving news and significant developments

Your style guide:
        - Present information in a clear, journalistic style
        - Use bullet points for key takeaways
        - Include relevant quotes when available
        - Specify the date and time for each piece of news
        - Highlight market sentiment and industry trends
        - End with a brief analysis of the overall narrative
        - Pay special attention to regulatory news, earnings reports, and strategic announcements\
    """),
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        ExaTools(
            include_domains=["trendlyne.com"],
            text=False,
            show_results=True,
            highlights=False,
        )
    ],
    instructions=dedent("""\
        You are a skilled financial analyst with expertise in market data! ðŸ“Š

Follow these steps when analyzing financial data:
        1. Start with the latest stock price, trading volume, and daily range
        2. Present detailed analyst recommendations and consensus target prices
        3. Include key metrics: P/E ratio, market cap, 52-week range
        4. Analyze trading patterns and volume trends
        5. Compare performance against relevant sector indices

Your style guide:
        - Use tables for structured data presentation
        - Include clear headers for each data section
        - Add brief explanations for technical terms
        - Highlight notable changes with emojis (ðŸ“ˆ ðŸ“‰)
        - Use bullet points for quick insights
        - Compare current values with historical averages
        - End with a data-driven financial outlook\
    """),
    markdown=True,
)

agent_team = Team(
    members=[web_agent, finance_agent],
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=dedent("""\
        You are the lead editor of a prestigious financial news desk! ðŸ“°

Your role:
        1. Coordinate between the web researcher and financial analyst
        2. Combine their findings into a compelling narrative
        3. Ensure all information is properly sourced and verified
        4. Present a balanced view of both news and data
        5. Highlight key risks and opportunities

Your style guide:
        - Start with an attention-grabbing headline
        - Begin with a powerful executive summary
        - Present financial data first, followed by news context
        - Use clear section breaks between different types of information
        - Include relevant charts or tables when available
        - Add 'Market Sentiment' section with current mood
        - Include a 'Key Takeaways' section at the end
        - End with 'Risk Factors' when appropriate
        - Sign off with 'Market Watch Team' and the current date\
    """),
    add_datetime_to_context=True,
    markdown=True,
    show_members_responses=False,
)

---

## Evaluate the accuracy of the Team's responses

**URL:** llms-txt#evaluate-the-accuracy-of-the-team's-responses

**Contents:**
- Accuracy with Number Comparison
- Usage
- Track Evals in your AgentOS

evaluation = AccuracyEval(
    name="Multi Language Team",
    model=OpenAIChat(id="o4-mini"),
    team=multi_language_team,
    input="Comment allez-vous?",
    expected_output="I can only answer in the following languages: English and Spanish.",
    num_iterations=1,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8

python accuracy_comparison.py theme={null}
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    name="Number Comparison Evaluation",
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[CalculatorTools()],
        instructions="You must use the calculator tools for comparisons.",
    ),
    input="9.11 and 9.9 -- which is bigger?",
    expected_output="9.9",
    additional_guidelines="Its ok for the output to include additional text or information relevant to the comparison.",
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8

bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python accuracy.py
      bash Windows theme={null}
      python accuracy.py
      python evals_demo.py theme={null}

"""Simple example creating a evals and using the AgentOS."""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.calculator import CalculatorTools

**Examples:**

Example 1 (unknown):
```unknown
## Accuracy with Number Comparison

This example demonstrates evaluating an agent's ability to make correct numerical comparisons, which can be tricky for LLMs when dealing with decimal numbers:
```

Example 2 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run your Accuracy Eval Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Data Validator Agent - Specialized in data quality validation

**URL:** llms-txt#data-validator-agent---specialized-in-data-quality-validation

data_validator = Agent(
    name="Data Validator",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Validate retrieved data quality and relevance",
    instructions=[
        "Assess the quality and relevance of retrieved information.",
        "Check for consistency across different search results.",
        "Identify the most reliable and accurate information.",
        "Filter out any irrelevant or low-quality content.",
        "Ensure data integrity and relevance to the user's query.",
    ],
    markdown=True,
)

---

## Create AgentOS with Slack interface for the workflow

**URL:** llms-txt#create-agentos-with-slack-interface-for-the-workflow

**Contents:**
- Usage

agent_os = AgentOS(
    workflows=[content_workflow],
    interfaces=[Slack(workflow=content_workflow)],
)

app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="research_workflow:app", reload=True)
bash  theme={null}
    export SLACK_TOKEN=xoxb-your-bot-user-token
    export SLACK_SIGNING_SECRET=your-signing-secret
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno ddgs psycopg
    bash Mac theme={null}
      python research_workflow.py
      bash Windows theme={null}
      python research_workflow.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Knowledge Bases

**URL:** llms-txt#knowledge-bases

**Contents:**
- Knowledge Base Components
  - Storage Layer
  - Processing Layer
  - Access Layer
- How Agents Use Knowledge Bases
  - Automatic Information Retrieval

Source: https://docs.agno.com/basics/knowledge/knowledge-bases

Deep dive into knowledge base design, architecture, and how they're optimized for AI agent retrieval.

Knowledge bases in Agno are architecturally designed for AI agent retrieval, with specialized components that bridge the gap between raw data storage and intelligent information access.

## Knowledge Base Components

Knowledge bases consist of several interconnected layers that work together to optimize information for agent retrieval:

**Vector Database**: Stores processed content as embeddings optimized for similarity search

* PgVector for production scalability
* LanceDB for development and testing
* Pinecone for managed cloud deployments

**Content Pipeline**: Transforms raw information into searchable format

* Readers parse different file types
* Chunking strategies break content into optimal pieces
* Embedders convert text to vector representations

**Search Interface**: Enables intelligent information retrieval

* Semantic similarity search
* Hybrid search combining vector and keyword matching
* Metadata filtering for precise results

## How Agents Use Knowledge Bases

When you give an agent access to a knowledge base, several powerful capabilities emerge:

### Automatic Information Retrieval

The agent doesn't need to be told when to search - it automatically determines when additional information would help answer a question or complete a task.
Although - explicitly instructing the agent to search a knowledge base is a perfectly fine and very common use case.

```python  theme={null}

---

## External Tool Execution Stream Async

**URL:** llms-txt#external-tool-execution-stream-async

Source: https://docs.agno.com/basics/hitl/usage/external-tool-execution-stream-async

This example demonstrates how to execute tools outside of the agent using external tool execution with async streaming responses. It shows how to handle external tool execution in an asynchronous environment while maintaining real-time streaming.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create steps with shared history

**URL:** llms-txt#create-steps-with-shared-history

tech_support_step = Step(
    name="Technical Support",
    agent=tech_support_agent,
    add_workflow_history=True,
)

billing_support_step = Step(
    name="Billing Support",
    agent=billing_agent,
    add_workflow_history=True,
)

general_support_step = Step(
    name="General Support",
    agent=general_support_agent,
    add_workflow_history=True,
)

def simple_intent_router(step_input: StepInput) -> List[Step]:
    """
    Simple intent-based router with basic keyword detection.
    The focus is on shared history, not complex routing logic.
    """
    current_message = step_input.input or ""
    current_message_lower = current_message.lower()

# Simple keyword matching for intent detection
    tech_keywords = [
        "api",
        "error",
        "bug",
        "technical",
        "login",
        "not working",
        "broken",
        "crash",
    ]
    billing_keywords = [
        "billing",
        "payment",
        "refund",
        "charge",
        "subscription",
        "invoice",
        "plan",
    ]

# Simple routing logic
    if any(keyword in current_message_lower for keyword in tech_keywords):
        print("ðŸ”§ Routing to Technical Support")
        return [tech_support_step]
    elif any(keyword in current_message_lower for keyword in billing_keywords):
        print("ðŸ’³ Routing to Billing Support")
        return [billing_support_step]
    else:
        print("ðŸŽ§ Routing to General Support")
        return [general_support_step]

def create_smart_customer_service_workflow():
    """Customer service workflow with simple routing and shared history"""

return Workflow(
        name="Smart Customer Service",
        description="Simple routing to specialists with shared conversation history",
        db=SqliteDb(db_file="tmp/smart_customer_service.db"),
        steps=[
            Router(
                name="Customer Service Router",
                selector=simple_intent_router,
                choices=[tech_support_step, billing_support_step, general_support_step],
                description="Routes to appropriate specialist based on simple intent detection",
            )
        ],
        add_workflow_history_to_steps=True,  # Enable history for the workflow
    )

def demo_smart_customer_service_cli():
    """Demo the smart customer service workflow with CLI"""
    workflow = create_smart_customer_service_workflow()

print("ðŸŽ§ Smart Customer Service Demo")
    print("=" * 60)
    print("")
    print("This workflow demonstrates:")
    print("â€¢ ðŸ¤– Simple routing between Technical, Billing, and General support")
    print("â€¢ ðŸ“š Shared conversation history across ALL agents")
    print("â€¢ ðŸ’¬ Context continuity - agents remember your entire conversation")
    print("")
    print("ðŸŽ¯ TRY THESE CONVERSATIONS:")
    print("")
    print("ðŸ”§ TECHNICAL SUPPORT:")
    print("   â€¢ 'My API is not working'")
    print("   â€¢ 'I'm getting an error message'")
    print("   â€¢ 'There's a technical bug'")
    print("")
    print("ðŸ’³ BILLING SUPPORT:")
    print("   â€¢ 'I need help with billing'")
    print("   â€¢ 'Can I get a refund?'")
    print("   â€¢ 'My payment was charged twice'")
    print("")
    print("ðŸŽ§ GENERAL SUPPORT:")
    print("   â€¢ 'Hello, I have a question'")
    print("   â€¢ 'What features do you offer?'")
    print("   â€¢ 'I need general help'")
    print("")
    print("Type 'exit' to quit")
    print("-" * 60)

workflow.cli_app(
        session_id="smart_customer_service_demo",
        user="Customer",
        emoji="ðŸŽ§",
        stream=True,
        show_step_details=True,
    )

if __name__ == "__main__":
    demo_smart_customer_service_cli()
```

---

## GCS for Workflows

**URL:** llms-txt#gcs-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/gcs/usage/gcs-for-workflow

Agno supports using Google Cloud Storage (GCS) as a storage backend for Workflows using the `GcsJsonDb` class. This storage backend stores session data as JSON blobs in a GCS bucket.

Configure your workflow with GCS storage to enable cloud-based session persistence.

```python gcs_for_workflow.py theme={null}
import uuid
import google.auth
from agno.agent import Agent
from agno.db.gcs_json import GcsJsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## CSV URL Reader

**URL:** llms-txt#csv-url-reader

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/csv-url-reader

The **CSV URL Reader** processes CSV files directly from URLs, allowing you to create knowledge bases from remote CSV data sources.

```python examples/basics/knowledge/readers/csv_reader_url_async.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    # Table name: ai.csv_documents
    vector_db=PgVector(
        table_name="csv_documents",
        db_url=db_url,
    ),
)

---

## Get by run_id (returns most recent match)

**URL:** llms-txt#get-by-run_id-(returns-most-recent-match)

**Contents:**
  - `db.get_traces()`

trace = db.get_trace(run_id=response.run_id)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Parameters:**

| Parameter  | Type            | Description             |
| ---------- | --------------- | ----------------------- |
| `trace_id` | `Optional[str]` | Unique trace identifier |
| `run_id`   | `Optional[str]` | Filter by run ID        |

**Returns:** `Trace` or `None`

See the reference for [Trace](/reference/tracing/trace) for more information.

### `db.get_traces()`

Get multiple traces with filtering and pagination.
```

---

## Filter by multiple fields (AND logic)

**URL:** llms-txt#filter-by-multiple-fields-(and-logic)

{"category": "technology", "status": "published", "year": 2024}

---

## Run Agent Infra Docker in Production

**URL:** llms-txt#run-agent-infra-docker-in-production

**Contents:**
- Next

Source: https://docs.agno.com/templates/agent-infra-railway/run-prd

<Snippet file="run-agent-infra-railway-prd.mdx" />

Congratulations on running your Agent Infra Railway in production!

---

## Start the database and MCP Toolbox servers

**URL:** llms-txt#start-the-database-and-mcp-toolbox-servers

---

## Importing our GoogleSearchTools ToolKit, containing multiple web search tools

**URL:** llms-txt#importing-our-googlesearchtools-toolkit,-containing-multiple-web-search-tools

**Contents:**
- Tool Built-in Parameters
  - Using the Run Context

from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        GoogleSearchTools(),
    ],
)

agent.print_response("What's the latest about OpenAIs GPT-5?", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run import RunContext

def add_item(run_context: RunContext, item: str) -> str:
    """Add an item to the shopping list."""
    if not run_context.session_state:
        run_context.session_state = {}

run_context.session_state["shopping_list"].append(item)  # type: ignore
    return f"The shopping list is now {run_context.session_state['shopping_list']}"  # type: ignore

**Examples:**

Example 1 (unknown):
```unknown
In this example, the `GoogleSearchTools` toolkit is added to the agent. This ToolKit comes pre-configured with the `google_search` function.

## Tool Built-in Parameters

Agno automatically provides special parameters to your tools that give access to the agent's parameters, state and other variables. These parameters are injected automatically - the agent doesn't need to know about them.

### Using the Run Context

You can access values from the current run via the `run_context` parameter: `run_context.session_state`, `run_context.dependencies`, `run_context.knowledge_filters`, `run_context.metadata`. See the [RunContext schema](/reference/run/run-context) for more information.

This allows tools to access and modify persistent data across conversations.

This is useful in cases where a tool result is relevant for the next steps of the conversation.

Add `run_context` as a parameter in your tool function to access the agent's persistent state:
```

---

## SurrealDB connection parameters

**URL:** llms-txt#surrealdb-connection-parameters

SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

---

## Create team members with cached responses

**URL:** llms-txt#create-team-members-with-cached-responses

**Contents:**
- Caching with Streaming
- Examples
- API Reference

researcher = Agent(
    model=OpenAIChat(id="gpt-4o", cache_response=True),
    name="Researcher",
    role="Research information"
)

writer = Agent(
    model=OpenAIChat(id="gpt-4o", cache_response=True),
    name="Writer",
    role="Write content"
)

team = Team(members=[researcher, writer], model=OpenAIChat(id="gpt-4o", cache_response=True))
python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(model=OpenAIChat(id="gpt-4o", cache_response=True))

for i in range(1, 3):
    print(f"\n{'=' * 60}")
    print(
        f"Run {i}"
    )
    print(f"{'=' * 60}\n")
    agent.print_response("Write me a short story about a cat that can talk and solve problems.", stream=True)
```

For complete working examples, see:

* [OpenAI Response Caching Example](/integrations/models/native/openai/completion/usage/cache-response)
* [Anthropic Response Caching Example](/integrations/models/native/anthropic/usage/cache-response)

For detailed parameter documentation, see:

* [Model Base Class Reference](/reference/models/model)

**Examples:**

Example 1 (unknown):
```unknown
Each team member maintains its own cache based on their specific queries.

## Caching with Streaming

Responses can also be cached when using streaming. On cache hits, the entire response is returned as one chunk.
```

---

## Collaboration Team

**URL:** llms-txt#collaboration-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/discussion_team

This example shows how to create a collaboration team that allows multiple agents to work together on research topics using the `collaborate` mode. In Collaborate Mode, all team members are given the same task and the team leader synthesizes their outputs into a cohesive response.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Run team and return the response as a stream

**URL:** llms-txt#run-team-and-return-the-response-as-a-stream

**Contents:**
  - Streaming team events

stream: Iterator[TeamRunOutputEvent] = team.run("What is the weather in Tokyo?", stream=True)
for chunk in stream:
    print(chunk.content, end="", flush=True)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Streaming team events

When you stream a response, only the `TeamRunContent` events will be streamed by default.

You can also stream all run events by setting `stream_events=True`.

This will provide real-time updates about the team's internal processes, like tool calling or reasoning:
```

---

## Set up the AgentOS app by passing your FastAPI app

**URL:** llms-txt#set-up-the-agentos-app-by-passing-your-fastapi-app

agent_os = AgentOS(
    description="Example app with custom routers",
    agents=[web_research_agent],
    base_app=app,
)

---

## Shorthand for both

**URL:** llms-txt#shorthand-for-both

**Contents:**
  - Add Instructions Automatically
  - Add Few-Shot Examples
  - Custom Instructions
  - Custom Few-Shot Examples
- Monitoring Your Agent's Thinking
- Reasoning Tools vs. Reasoning Agents

ReasoningTools()
python  theme={null}
ReasoningTools(add_instructions=True)
python  theme={null}
ReasoningTools(add_instructions=True, add_few_shot=True)
python  theme={null}
custom_instructions = """
Use the think and analyze tools for rigorous scientific reasoning:
- Always think before making claims
- Cite evidence in your analysis
- Acknowledge uncertainty
- Consider alternative hypotheses
"""

ReasoningTools(
    instructions=custom_instructions,
    add_instructions=False  # Don't include default instructions
)
python  theme={null}
medical_examples = """
Example: Medical Diagnosis

User: Patient has fever and cough for 3 days.

Agent thinks:
think(
    title="Gather Symptoms",
    thought="Need to collect all symptoms and their duration. Fever and cough suggest respiratory infection. Should check for other symptoms.",
    action="Ask about additional symptoms",
    confidence=0.9
)
"""

ReasoningTools(
    add_instructions=True,
    add_few_shot=True,
    few_shot_examples=medical_examples  # Your custom examples
)
```

## Monitoring Your Agent's Thinking

Use `show_full_reasoning=True` and `stream_intermediate_steps=True` to display reasoning steps in real-time. See [Display Options in Reasoning Agents](/basics/reasoning/reasoning-agents#display-options) for details and [Reasoning Reference](/reference/reasoning/reasoning#display-parameters) for programmatic access to reasoning steps.

## Reasoning Tools vs. Reasoning Agents

Both approaches add reasoning to any model, but they differ in control and automation:

| Aspect           | Reasoning Tools                          | Reasoning Agents                                   |
| ---------------- | ---------------------------------------- | -------------------------------------------------- |
| **Activation**   | Agent decides when to use `think()`      | Automatic on every request                         |
| **Control**      | Explicit tool calls                      | Automated loop                                     |
| **Transparency** | See every `think()` and `analyze()` call | See structured reasoning steps                     |
| **Workflow**     | Agent-driven (flexible)                  | Framework-driven (structured)                      |
| **Best for**     | Research, analysis, exploratory tasks    | Complex multi-step problems with defined structure |

* Use **Reasoning Tools** when you want the agent to control its own reasoning process
* Use **Reasoning Agents** when you want guaranteed systematic thinking for every request

**Examples:**

Example 1 (unknown):
```unknown
### Add Instructions Automatically

Many toolkits ship with pre-written guidance that explains how to use their tools. Setting `add_instructions=True` injects those instructions into the agent prompt (when the toolkit actually has any):
```

Example 2 (unknown):
```unknown
* `ReasoningTools`, `KnowledgeTools`, `MemoryTools`, and `WorkflowTools` all include Agno-authored instructions (and optional few-shot examples) describing their Think â†’ Act â†’ Analyze workflow.
* Other toolkits may not define default instructions; in that case `add_instructions=True` is a no-op unless you supply your own `instructions=...`.

The built-in instructions cover when to use `think()` vs `analyze()`, how to iterate, and best practices for each domain. Turn them on unless you plan to provide custom guidance.

### Add Few-Shot Examples

Want to show your agent some examples of good reasoning? Some toolkits come with pre-written few-shot examples that demonstrate the workflow in action. Turn them on with `add_few_shot=True`:
```

Example 3 (unknown):
```unknown
Right now, `ReasoningTools`, `KnowledgeTools`, and `MemoryTools` have built-in examples. Other toolkits won't use `add_few_shot=True` unless you provide your own examples.

These examples show the agent how to iterate through problems, decide on next actions, and mix thinking with actual tool calls.

**When should you use them?**

* You're using a smaller or cheaper model that needs extra guidance
* Your reasoning workflow has multiple stages or is complex
* You want more consistent behavior across different runs

### Custom Instructions

Provide your own custom instructions for specialized reasoning:
```

Example 4 (unknown):
```unknown
### Custom Few-Shot Examples

You can also write your own examples tailored to your domain:
```

---

## Couchbase Vector Database

**URL:** llms-txt#couchbase-vector-database

**Contents:**
- Setup
  - Local Setup (Docker)
  - Managed Setup (Capella)
  - Environment Variables
  - Install Dependencies
- Example

Source: https://docs.agno.com/integrations/vectordb/couchbase/overview

Learn how to use Couchbase as a vector database for your Knowledge Base

### Local Setup (Docker)

Run Couchbase locally using Docker:

1. Access the Couchbase UI at: [http://localhost:8091](http://localhost:8091)
2. Login with username: `Administrator` and password: `password`
3. Create a bucket named `recipe_bucket`, a scope `recipe_scope`, and a collection `recipes`

### Managed Setup (Capella)

For a managed cluster, use [Couchbase Capella](https://cloud.couchbase.com/):

* Follow Capella's UI to create a database, bucket, scope, and collection

### Environment Variables

Set up your environment variables:

For Capella, set `COUCHBASE_CONNECTION_STRING` to your Capella connection string.

### Install Dependencies

```python agent_with_knowledge.py theme={null}
import os
import time
from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.couchbase import CouchbaseSearch
from couchbase.options import ClusterOptions, KnownConfigProfiles
from couchbase.auth import PasswordAuthenticator
from couchbase.management.search import SearchIndex

**Examples:**

Example 1 (unknown):
```unknown
1. Access the Couchbase UI at: [http://localhost:8091](http://localhost:8091)
2. Login with username: `Administrator` and password: `password`
3. Create a bucket named `recipe_bucket`, a scope `recipe_scope`, and a collection `recipes`

### Managed Setup (Capella)

For a managed cluster, use [Couchbase Capella](https://cloud.couchbase.com/):

* Follow Capella's UI to create a database, bucket, scope, and collection

### Environment Variables

Set up your environment variables:
```

Example 2 (unknown):
```unknown
For Capella, set `COUCHBASE_CONNECTION_STRING` to your Capella connection string.

### Install Dependencies
```

Example 3 (unknown):
```unknown
## Example
```

---

## Create specialized research agents

**URL:** llms-txt#create-specialized-research-agents

tech_researcher = Agent(
    name="Alex",
    role="Technology Researcher",
    instructions=dedent("""
        You specialize in technology and AI research.
        - Focus on latest developments, trends, and breakthroughs
        - Provide concise, data-driven insights
        - Cite your sources
    """).strip(),
)

business_analyst = Agent(
    name="Sarah",
    role="Business Analyst",
    instructions=dedent("""
        You specialize in business and market analysis.
        - Focus on companies, markets, and economic trends
        - Provide actionable business insights
        - Include relevant data and statistics
    """).strip(),
)

---

## Create an agent with OpenWeatherTools

**URL:** llms-txt#create-an-agent-with-openweathertools

agent = Agent(
    tools=[
        OpenWeatherTools(
            units="imperial",  # Options: 'standard', 'metric', 'imperial'
        )
    ],
    markdown=True,
)

---

## Agent with Storage

**URL:** llms-txt#agent-with-storage

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/storage

```python cookbook/models/openai/chat/db.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Should raise an error - missing required field

**URL:** llms-txt#should-raise-an-error---missing-required-field

try:
    hackernews_agent.print_response(
        input={
            "topic": "AI",
            # Missing required fields: focus_areas, target_audience, sources_required
        }
    )
except ValueError as e:
    print("\n=== Expected Error for Missing Fields ===")
    print(f"Error: {e}")

---

## Download sample CVs

**URL:** llms-txt#download-sample-cvs

downloaded_cv_paths = download_knowledge_filters_sample_data(
    num_files=5, file_extension=SampleDataFileExtension.PDF
)

---

## Create a Tech News Reporter Agent with a Silicon Valley personality

**URL:** llms-txt#create-a-tech-news-reporter-agent-with-a-silicon-valley-personality

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=dedent("""\
        You are a tech-savvy Hacker News reporter with a passion for all things technology! ðŸ¤–
        Think of yourself as a mix between a Silicon Valley insider and a tech journalist.

Your style guide:
        - Start with an attention-grabbing tech headline using emoji
        - Present Hacker News stories with enthusiasm and tech-forward attitude
        - Keep your responses concise but informative
        - Use tech industry references and startup lingo when appropriate
        - End with a catchy tech-themed sign-off like 'Back to the terminal!' or 'Pushing to production!'

Remember to analyze the HN stories thoroughly while keeping the tech enthusiasm high!\
    """),
    tools=[get_top_hackernews_stories],
    markdown=True,
)

---

## Financial Datasets API

**URL:** llms-txt#financial-datasets-api

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/financial-datasets

**FinancialDatasetsTools** provide a comprehensive API for retrieving and analyzing diverse financial datasets, including stock prices, financial statements, company information, SEC filings, and cryptocurrency data from multiple providers.

The toolkit requires a Financial Datasets API key that can be obtained by creating an account at [financialdatasets.ai](https://financialdatasets.ai).

Set your API key as an environment variable:

Basic usage of the Financial Datasets toolkit:

```python  theme={null}
from agno.agent import Agent
from agno.tools.financial_datasets import FinancialDatasetsTools

agent = Agent(
    name="Financial Data Agent",
    tools=[FinancialDatasetsTools()],
    description="You are a financial data specialist that helps analyze financial information for stocks and cryptocurrencies.",
    instructions=[
        "When given a financial query:",
        "1. Use appropriate Financial Datasets methods based on the query type",
        "2. Format financial data clearly and highlight key metrics",
        "3. For financial statements, compare important metrics with previous periods when relevant",
        "4. Calculate growth rates and trends when appropriate",
        "5. Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
    )

**Examples:**

Example 1 (unknown):
```unknown
Set your API key as an environment variable:
```

Example 2 (unknown):
```unknown
## Example

Basic usage of the Financial Datasets toolkit:
```

---

## Azure OpenAI

**URL:** llms-txt#azure-openai

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/azure-open-ai

The AzureOpenAI model provides access to Azure-hosted OpenAI models.

| Parameter                 | Type                              | Default                | Description                                                                        |
| ------------------------- | --------------------------------- | ---------------------- | ---------------------------------------------------------------------------------- |
| `id`                      | `str`                             | `"gpt-4o"`             | The id of the Azure OpenAI model to use                                            |
| `name`                    | `str`                             | `"AzureOpenAI"`        | The name of the model                                                              |
| `provider`                | `str`                             | `"Azure"`              | The provider of the model                                                          |
| `temperature`             | `Optional[float]`                 | `None`                 | Controls randomness in the model's output (0.0 to 2.0)                             |
| `max_tokens`              | `Optional[int]`                   | `None`                 | Maximum number of tokens to generate in the response                               |
| `max_completion_tokens`   | `Optional[int]`                   | `None`                 | Maximum number of completion tokens to generate                                    |
| `frequency_penalty`       | `Optional[float]`                 | `None`                 | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |
| `presence_penalty`        | `Optional[float]`                 | `None`                 | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |
| `top_p`                   | `Optional[float]`                 | `None`                 | Controls diversity via nucleus sampling (0.0 to 1.0)                               |
| `stop`                    | `Optional[Union[str, List[str]]]` | `None`                 | Up to 4 sequences where the API will stop generating further tokens                |
| `seed`                    | `Optional[int]`                   | `None`                 | Random seed for deterministic sampling                                             |
| `logprobs`                | `Optional[bool]`                  | `None`                 | Whether to return log probabilities of the output tokens                           |
| `top_logprobs`            | `Optional[int]`                   | `None`                 | Number of most likely tokens to return log probabilities for (0 to 20)             |
| `user`                    | `Optional[str]`                   | `None`                 | A unique identifier representing your end-user                                     |
| `request_params`          | `Optional[Dict[str, Any]]`        | `None`                 | Additional parameters to include in the request                                    |
| `azure_endpoint`          | `Optional[str]`                   | `None`                 | The Azure endpoint URL (defaults to AZURE\_OPENAI\_ENDPOINT env var)               |
| `api_key`                 | `Optional[str]`                   | `None`                 | The API key for Azure OpenAI (defaults to AZURE\_OPENAI\_API\_KEY env var)         |
| `api_version`             | `str`                             | `"2024-12-01-preview"` | The API version to use                                                             |
| `azure_ad_token`          | `Optional[str]`                   | `None`                 | Azure AD token for authentication                                                  |
| `azure_ad_token_provider` | `Optional[Any]`                   | `None`                 | Azure AD token provider for authentication                                         |
| `timeout`                 | `Optional[float]`                 | `None`                 | Request timeout in seconds                                                         |
| `max_retries`             | `Optional[int]`                   | `None`                 | Maximum number of retries for failed requests                                      |
| `client_params`           | `Optional[Dict[str, Any]]`        | `None`                 | Additional parameters for client configuration                                     |
| `retries`                 | `int`                             | `0`                    | Number of retries to attempt before raising a ModelProviderError                   |
| `delay_between_retries`   | `int`                             | `1`                    | Delay between retries, in seconds                                                  |
| `exponential_backoff`     | `bool`                            | `False`                | If True, the delay between retries is doubled each time                            |

---

## Filtering on SurrealDB

**URL:** llms-txt#filtering-on-surrealdb

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/filters/usage/vector-dbs/filtering-surreal-db

Learn how to filter knowledge base searches using Pdf documents with user-specific metadata in SurrealDB.

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.surrealdb import SurrealDb
from surrealdb import Surreal

---

## Create a team for collaborative audio sentiment analysis

**URL:** llms-txt#create-a-team-for-collaborative-audio-sentiment-analysis

**Contents:**
- Usage

sentiment_team = Team(
    name="Audio Sentiment Team",
    members=[transcription_agent, sentiment_analyst],
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze audio sentiment with conversation memory.",
        "Audio Transcriber: First transcribe audio with speaker identification.",
        "Sentiment Analyst: Analyze emotional tone and conversation dynamics.",
    ],
    add_history_to_context=True,
    markdown=True,
    db=SqliteDb(
        session_table="audio_sentiment_team_sessions",
        db_file="tmp/audio_sentiment_team.db",
    ),
)

url = "https://agno-public.s3.amazonaws.com/demo_data/sample_conversation.wav"

response = requests.get(url)
audio_content = response.content

sentiment_team.print_response(
    "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)

sentiment_team.print_response(
    "What else can you tell me about this audio conversation?",
    stream=True,
)
bash  theme={null}
    pip install agno requests google-generativeai
    bash  theme={null}
    export GOOGLE_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/audio_sentiment_analysis.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Quick test to verify model works

**URL:** llms-txt#quick-test-to-verify-model-works

**Contents:**
- Step 2: Add Social Media Tools
  - 2a. Add XTools for Twitter/X Data

if __name__ == "__main__":
    test_response = model.invoke("Explain social media sentiment analysis in one sentence.")
    print(f"Model test: {test_response}")
python  theme={null}
from agno.tools.x import XTools

**Examples:**

Example 1 (unknown):
```unknown
This confirms your model is working, before we add more complexity.

## Step 2: Add Social Media Tools

**Which tools should I use?** We are adding XTools because we need direct Twitter/X data with engagement metrics, and ExaTools because we need broader web context that social media alone can't provide.

### 2a. Add XTools for Twitter/X Data

**Why XTools?** Direct access to Twitter/X with engagement metrics is crucial for understanding influence patterns and viral content.
```

---

## pprint(movie_agent.content)

**URL:** llms-txt#pprint(movie_agent.content)

**Contents:**
- Usage

movie_agent.print_response("New York")
bash  theme={null}
    export IBM_WATSONX_API_KEY=xxx
    export IBM_WATSONX_PROJECT_ID=xxx
    bash  theme={null}
    pip install -U ibm-watsonx-ai pydantic rich agno
    bash Mac theme={null}
      python cookbook/models/ibm/watsonx/structured_output.py
      bash Windows theme={null}
      python cookbook\models\ibm\watsonx\structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

This example shows how to use structured output with IBM WatsonX. It defines a Pydantic model `MovieScript` with various fields and their descriptions, then creates an agent using this model as the `output_schema`. The model's output will be parsed into this structured format.

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create specialized agents

**URL:** llms-txt#create-specialized-agents

news_agent = Agent(
    id="news-agent",
    name="News Agent",
    role="Get the latest news and provide summaries",
    tools=[DuckDuckGoTools()]
)

weather_agent = Agent(
    id="weather-agent",
    name="Weather Agent",
    role="Get weather information and forecasts",
    tools=[DuckDuckGoTools()]
)

---

## Save the generated image if available

**URL:** llms-txt#save-the-generated-image-if-available

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

if response.images and response.images[0].content:
    output_path = Path("generated_image.png")
    with open(output_path, "wb") as f:
        f.write(response.images[0].content)

print(f"Image was successfully generated and saved to: {output_path}")
```

| Parameter             | Type   | Default                    | Description                                                                                            |
| --------------------- | ------ | -------------------------- | ------------------------------------------------------------------------------------------------------ |
| `model`               | `str`  | `"gemini-2.5-flash-image"` | The Nano Banana model to use                                                                           |
| `aspect_ratio`        | `str`  | `"1:1"`                    | Image aspect ratio. Supported: `1:1`, `2:3`, `3:2`, `3:4`, `4:3`, `4:5`, `5:4`, `9:16`, `16:9`, `21:9` |
| `api_key`             | `str`  | `None`                     | The Google API key for authentication                                                                  |
| `enable_create_image` | `bool` | `True`                     | Enable the create image functionality                                                                  |

| Function       | Description                               |
| -------------- | ----------------------------------------- |
| `create_image` | Generates an image based on a text prompt |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/nano_banana.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/nano_banana_tools.py)

---

## Meeting Summarizer Agent

**URL:** llms-txt#meeting-summarizer-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/meeting_summarizer_agent

This agent uses OpenAITools (transcribe\_audio, generate\_image, generate\_speech)
to process a meeting recording, summarize it, visualize it, and create an audio summary.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## What is Evals

**URL:** llms-txt#what-is-evals

**Contents:**
- Evaluation Dimensions
- Quick Start

Source: https://docs.agno.com/basics/evals/overview

Evals is a way to measure the quality of your Agents and Teams.<br/> Agno provides 3 dimensions for evaluating Agents.

Learn how to evaluate your Agno Agents and Teams across three key dimensions - **accuracy** (using LLM-as-a-judge), **performance** (runtime and memory), and **reliability** (tool calls).

## Evaluation Dimensions

<CardGroup cols={3}>
  <Card title="Accuracy" icon="bullseye" href="/basics/evals/accuracy">
    The accuracy of the Agent's response using LLM-as-a-judge methodology.
  </Card>

<Card title="Performance" icon="stopwatch" href="/basics/evals/performance">
    The performance of the Agent's response, including latency and memory footprint.
  </Card>

<Card title="Reliability" icon="shield-check" href="/basics/evals/reliability">
    The reliability of the Agent's response, including tool calls and error handling.
  </Card>
</CardGroup>

Here's a simple example of running an accuracy evaluation:

```python quick_eval.py theme={null}
from typing import Optional
from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

---

## Additional Data and Metadata

**URL:** llms-txt#additional-data-and-metadata

**Contents:**
- Example

Source: https://docs.agno.com/basics/workflows/additional-data

How to pass additional data to workflows

**When to Use**
Pass metadata, configuration, or contextual information to specific steps without cluttering the main workflow message flow.

* **Separation of Concerns**: Keep workflow logic separate from metadata
* **Step-Specific Context**: Access additional information in custom functions
* **Clean Message Flow**: Main message stays focused on content
* **Flexible Configuration**: Pass user info, priorities, settings, and more

**Access Pattern**
Use `step_input.additional_data` for dictionary access to all additional data passed to the workflow.

```python  theme={null}
from agno.workflow import Step, Workflow, StepInput, StepOutput

def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """Custom function that uses additional_data for enhanced context"""
    
    # Access the main workflow message
    message = step_input.input
    previous_content = step_input.previous_step_content
    
    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")
    
    # Create enhanced planning prompt with context
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:
        
        Core Topic: {message}
        Research Results: {previous_content[:500] if previous_content else "No research results"}
        
        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}
        
        {"ðŸš¨ HIGH PRIORITY - Expedited delivery required" if priority == "high" else "ðŸ“ Standard delivery timeline"}
        
        Please create a detailed, actionable content plan.
    """
    
    response = content_planner.run(planning_prompt)
    
    enhanced_content = f"""
        ## Strategic Content Plan
        
        **Planning Topic:** {message}
        **Client Details:** {client_type} | {priority.upper()} priority | {user_email}
        
        **Content Strategy:**
        {response.content}
    """
    
    return StepOutput(content=enhanced_content)

---

## Agent with both structured outputs and strict tool

**URL:** llms-txt#agent-with-both-structured-outputs-and-strict-tool

agent = Agent(
    model=Claude(id="claude-sonnet-4-5-20250929"),
    tools=[weather_tool],
    output_schema=WeatherInfo,
    description="You help users get weather information.",
)

---

## Memory Manager

**URL:** llms-txt#memory-manager

Source: https://docs.agno.com/reference/memory/memory

Memory is a class that manages conversation history, session summaries, and long-term user memories for AI agents. It provides comprehensive memory management capabilities including adding new memories, searching memories, and deleting memories.

<Snippet file="memory-manager-reference.mdx" />

---

## Translation Agent

**URL:** llms-txt#translation-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/groq/usage/translation-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example usage with different locations

**URL:** llms-txt#example-usage-with-different-locations

json_mode_agent.print_response("Tokyo", stream=True)
structured_output_agent.print_response("Ancient Rome", stream=True)

---

## Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.

**URL:** llms-txt#give-a-transcript-of-this-audio-conversation.-use-speaker-a,-speaker-b-to-identify-speakers.

**Contents:**
- Team-Based Transcription

agent.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)
python cookbook/teams/multimodal/audio_to_text.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini
from agno.team import Team

transcription_specialist = Agent(
    name="Transcription Specialist",
    role="Convert audio to accurate text transcriptions",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Transcribe audio with high accuracy",
        "Identify speakers clearly as Speaker A, Speaker B, etc.",
        "Maintain conversation flow and context",
    ],
)

content_analyzer = Agent(
    name="Content Analyzer",
    role="Analyze transcribed content for insights",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze transcription for key themes and insights",
        "Provide summaries and extract important information",
    ],
)

**Examples:**

Example 1 (unknown):
```unknown
**Best for**: Direct model integration, conversation understanding

## Team-Based Transcription

Teams can handle complex audio processing workflows with multiple specialized agents.
```

---

## Run As Cli

**URL:** llms-txt#run-as-cli

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/run_as_cli

This example shows how to create an interactive CLI app with an agent.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Study Partner

**URL:** llms-txt#study-partner

**Contents:**
- Description
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/study-partner

This Study Partner agent demonstrates how to create an AI-powered study partner that combines multiple information sources and tools to provide comprehensive learning support. The agent showcases several key capabilities:

**Multi-tool Integration**: Combines Exa search tools for web research and YouTube tools for video content discovery, enabling the agent to access diverse learning resources.

**Personalized Learning Support**: Creates customized study plans based on user constraints (time available, current knowledge level, daily study hours) and learning preferences.

**Resource Curation**: Searches and recommends high-quality learning materials including documentation, tutorials, research papers, and community discussions from reliable sources.

**Interactive Learning**: Provides step-by-step explanations, practical examples, and hands-on project suggestions to reinforce understanding.

**Progress Tracking**: Designs structured study plans with clear milestones and deadlines to help users stay on track with their learning goals.

**Learning Strategy**: Offers tips on effective study techniques, time management, and motivation maintenance for sustained learning success.

This example is particularly useful for developers, students, or anyone looking to build AI agents that can assist with educational content discovery, personalized learning path creation, and comprehensive study support across various subjects and skill levels.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Comparison Accuracy Evaluation

**URL:** llms-txt#comparison-accuracy-evaluation

Source: https://docs.agno.com/basics/evals/accuracy/usage/accuracy-comparison

Exmaple showing how to evaluate agent accuracy on comparison tasks.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Building Agents

**URL:** llms-txt#building-agents

**Contents:**
- Run your Agent

Source: https://docs.agno.com/basics/agents/building-agents

Learn how to build Agents with Agno.

To build effective agents, start simple -- just a model, tools, and instructions. Once that works, layer in more functionality as needed.

The key is to start with well-defined tasks like report generation, data extraction, classification, summarization, knowledge search, and document processing. These early wins help you identify what works, validate user needs, and set the stage for advanced systems.

For example, here's the simplest possible report generation agent (that uses the Hackernews toolkit):

When developing your agent, run it using the `Agent.print_response()` method. This will print the agent's response in your terminal, in a friendly format.

This is only for development purposes and not recommended for production use. In production, use the `Agent.run()` or `Agent.arun()` methods. For example:

```python  theme={null}
from typing import Iterator
from agno.agent import Agent, RunOutput, RunOutputEvent, RunEvent
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response

agent = Agent(
    model=Claude(id="claude-sonnet-4-5"),
    tools=[HackerNewsTools()],
    instructions="Write a report on the topic. Output only the report.",
    markdown=True,
)

**Examples:**

Example 1 (unknown):
```unknown
## Run your Agent

When developing your agent, run it using the `Agent.print_response()` method. This will print the agent's response in your terminal, in a friendly format.

This is only for development purposes and not recommended for production use. In production, use the `Agent.run()` or `Agent.arun()` methods. For example:
```

---

## Final reasoning step using reasoning agent

**URL:** llms-txt#final-reasoning-step-using-reasoning-agent

reasoning_step = Step(
    name="final_reasoning",
    agent=reasoning_agent,
    description="Apply reasoning to create final insights and recommendations",
)

workflow = Workflow(
    name="Enhanced Research Workflow",
    description="Multi-source research with custom data flow and reasoning",
    steps=[
        research_hackernews,
        research_web,
        comprehensive_report_step,  # Has access to both previous steps
        reasoning_step,  # Gets the last step output (comprehensive report)
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning",
        stream=True,
    )
```

---

## Agent that uses a JSON schema output with strict_output=True (default)

**URL:** llms-txt#agent-that-uses-a-json-schema-output-with-strict_output=true-(default)

json_schema_output_agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    description="You are a helpful assistant. Summarize the movie script based on the location in a JSON object.",
    output_schema=MovieScript,
)

---

## First run

**URL:** llms-txt#first-run

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    db=db,
    session_id="conversation_123",
)
agent.run("My name is Alice")

---

## âŒ Doesn't work as expected - automatic memory is disabled

**URL:** llms-txt#âŒ-doesn't-work-as-expected---automatic-memory-is-disabled

agent = Agent(
    db=db,
    enable_user_memories=True,
    enable_agentic_memory=True  # This disables automatic behavior
)

---

## Set up a custom logger

**URL:** llms-txt#set-up-a-custom-logger

custom_logger = logging.getLogger("custom_logger")
handler = logging.StreamHandler()
formatter = logging.Formatter("[CUSTOM_LOGGER] %(levelname)s: %(message)s")
handler.setFormatter(formatter)
custom_logger.addHandler(handler)
custom_logger.setLevel(logging.INFO)
custom_logger.propagate = False

---

## Create distributed search team

**URL:** llms-txt#create-distributed-search-team

**Contents:**
- Usage

distributed_search_team = Team(
    name="Distributed Search Team with Infinity Reranker",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        primary_searcher,
        secondary_searcher,
        cross_reference_validator,
        result_synthesizer,
    ],
    instructions=[
        "Work together to provide comprehensive search results using distributed processing.",
        "Primary Searcher: Conduct broad comprehensive search first.",
        "Secondary Searcher: Perform targeted specialized search.",
        "Cross-Reference Validator: Validate consistency across all results.",
        "Result Synthesizer: Combine everything into a ranked, comprehensive response.",
        "Leverage the infinity reranker for high-performance result ranking.",
        "Ensure all results are properly attributed and ranked by relevance.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_distributed_search():
    """Demonstrate async distributed search with infinity reranking."""
    print("âš¡ Async Distributed Search with Infinity Reranker Demo")
    print("=" * 65)

query = "How do Agents work with tools and what are the performance considerations?"

# Add content to both knowledge bases
    await knowledge_primary.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )
    await knowledge_secondary.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

# Run async distributed search
    await distributed_search_team.aprint_response(
        query, stream=True
    )

def sync_distributed_search():
    """Demonstrate sync distributed search with infinity reranking."""
    print("âš¡ Distributed Search with Infinity Reranker Demo")
    print("=" * 55)

query = "How do Agents work with tools and what are the performance considerations?"

# Add content to both knowledge bases
    knowledge_primary.add_contents(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )
    knowledge_secondary.add_contents(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

# Run distributed search
    distributed_search_team.print_response(
        query, stream=True
    )

if __name__ == "__main__":
    # Choose which demo to run

try:
        # asyncio.run(async_distributed_search())

sync_distributed_search()
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("\nðŸ’¡ Make sure Infinity server is running:")
        print("   pip install 'infinity-emb[all]'")
        print("   infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997")
bash  theme={null}
    pip install agno cohere lancedb "infinity-emb[all]"
    bash  theme={null}
    infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export CO_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/search_coordination/03_distributed_infinity_search.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set up Infinity server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Get History in Function

**URL:** llms-txt#get-history-in-function

Source: https://docs.agno.com/basics/chat-history/workflow/usage/get-history-in-function

This example demonstrates how to get workflow history in a custom function.

This example shows how to get workflow history in a custom function.

* Using `step_input.get_workflow_history(num_runs=5)` we can get the history as a list of tuples.
* We can also use `step_input.get_workflow_history_context(num_runs=5)` to get the history as a string.

---

## Agent with Structured Outputs and Strict Tools

**URL:** llms-txt#agent-with-structured-outputs-and-strict-tools

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/structured-output-strict-tools

This example demonstrates how to use strict tool validation with structured outputs. Strict tool use ensures that tool parameters strictly follow the input schema, providing additional validation when using tools alongside structured outputs.

```python structured_output_strict_tools.py theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools import Function
from pydantic import BaseModel

class WeatherInfo(BaseModel):
    """Structured output schema for weather information."""

location: str
    temperature: float
    unit: str
    condition: str

def get_weather(location: str, unit: str = "celsius") -> str:
    temp = 72 if unit == "fahrenheit" else 22
    return f"Weather in {location}: {temp}Â°{unit}, Sunny"

---

## Audio Model Output

**URL:** llms-txt#audio-model-output

**Contents:**
- Audio response modality

Source: https://docs.agno.com/basics/multimodal/audio/audio_output

Learn how to use audio from models as output with Agno agents.

Similar to providing audio inputs, you can also get audio outputs from an agent. Take a look at the [compatibility matrix](/basics/models/compatibility#multimodal-support) to see which models support audio as output.

## Audio response modality

The following example demonstrates how some models can directly generate audio as part of their response.

```python audio_agent.py theme={null}
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

agent = Agent(
    model=OpenAIChat(
        id="gpt-5-mini-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
    ),
    markdown=True,
)
response: RunOutput = agent.run("Tell me a 5 second scary story")

---

## Quality Validator Agent - Specialized in validation

**URL:** llms-txt#quality-validator-agent---specialized-in-validation

quality_validator = Agent(
    name="Quality Validator",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Validate answer quality and suggest improvements",
    instructions=[
        "Review the synthesized answer for accuracy and completeness.",
        "Check if the answer fully addresses the user's query.",
        "Identify any gaps or areas that need clarification.",
        "Suggest improvements or additional information if needed.",
        "Ensure the response meets high quality standards.",
    ],
    markdown=True,
)

---

## CSV

**URL:** llms-txt#csv

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/database/csv

The CsvTools toolkit enables an Agent to read and write CSV files.

**CsvTools** enable an Agent to read and write CSV files.

The following agent will download the IMDB csv file and allow the user to query it using a CLI app.

| Parameter               | Type                     | Default | Description                                                            |
| ----------------------- | ------------------------ | ------- | ---------------------------------------------------------------------- |
| `csvs`                  | `List[Union[str, Path]]` | `None`  | A list of CSV files or paths to be processed or read.                  |
| `row_limit`             | `int`                    | `None`  | The maximum number of rows to process from each CSV file.              |
| `duckdb_connection`     | `Any`                    | `None`  | Specifies a connection instance for DuckDB database operations.        |
| `duckdb_kwargs`         | `Dict[str, Any]`         | `None`  | A dictionary of keyword arguments for configuring DuckDB operations.   |
| `enable_read_csv_file`  | `bool`                   | `True`  | Enables the functionality to read data from specified CSV files.       |
| `enable_list_csv_files` | `bool`                   | `True`  | Enables the functionality to list all available CSV files.             |
| `enable_get_columns`    | `bool`                   | `True`  | Enables the functionality to read the column names from CSV files.     |
| `enable_query_csv_file` | `bool`                   | `True`  | Enables the functionality to execute queries on data within CSV files. |
| `all`                   | `bool`                   | `False` | Enables all functionality when set to True.                            |

| Function         | Description                                      |
| ---------------- | ------------------------------------------------ |
| `list_csv_files` | Lists all available CSV files.                   |
| `read_csv_file`  | This function reads the contents of a csv file   |
| `get_columns`    | This function returns the columns of a csv file  |
| `query_csv_file` | This function queries the contents of a csv file |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/csv.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/csv_tools.py)

---

## Agent with Knowledge Base

**URL:** llms-txt#agent-with-knowledge-base

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/gateways/cerebras/usage/knowledge

```python cookbook/models/cerebras/basic_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.cerebras import Cerebras
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    vector_db=PgVector(table_name="recipes", db_url=db_url),
)

---

## Create some memories for a user

**URL:** llms-txt#create-some-memories-for-a-user

print("Creating memories...")
agent.print_response(
    "I have a wonderful pet dog named Max who is 3 years old. He's a golden retriever and he's such a friendly and energetic dog. "
    "We got him as a puppy when he was just 8 weeks old. He loves playing fetch in the park and going on long walks. "
    "Max is really smart too - he knows about 15 different commands and tricks. Taking care of him has been one of the most "
    "rewarding experiences of my life. He's basically part of the family now.",
    user_id=user_id,
)
agent.print_response(
    "I currently live in San Francisco, which is an amazing city despite all its challenges. I've been here for about 5 years now. "
    "I work in the tech industry as a product manager at a mid-sized software company. The tech scene here is incredible - "
    "there are so many smart people working on interesting problems. The cost of living is definitely high, but the opportunities "
    "and the community make it worthwhile. I live in the Mission district which has great food and a vibrant culture.",
    user_id=user_id,
)
agent.print_response(
    "On weekends, I really enjoy hiking in the beautiful areas around the Bay Area. There are so many amazing trails - "
    "from Mount Tamalpais to Big Basin Redwoods. I usually go hiking with a group of friends and we try to explore new trails every month. "
    "I also love trying new restaurants. San Francisco has such an incredible food scene with cuisines from all over the world. "
    "I'm always on the lookout for hidden gems and new places to try. My favorite types of cuisine are Japanese, Thai, and Mexican.",
    user_id=user_id,
)
agent.print_response(
    "I've been learning to play the piano for about a year and a half now. It's something I always wanted to do but never had time for. "
    "I finally decided to commit to it and I practice almost every day, usually for 30-45 minutes. "
    "I'm working through classical pieces right now - I can play some simple Bach and Mozart compositions. "
    "My goal is to eventually be able to play some jazz piano as well. Having a creative hobby like this has been great for my mental health "
    "and it's nice to have something completely different from my day job.",
    user_id=user_id,
)

---

## Setup paths for knowledge storage

**URL:** llms-txt#setup-paths-for-knowledge-storage

cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

---

## Second identical call - cache hit, returns cached response instantly

**URL:** llms-txt#second-identical-call---cache-hit,-returns-cached-response-instantly

**Contents:**
- Configuration Options
  - Cache Time-to-Live (TTL)
  - Custom Cache Directory
- Usage with Agents

response = agent.run("What is the capital of France?")
python  theme={null}
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o",
        cache_response=True,
        cache_ttl=3600  # Cache expires after 1 hour
    )
)
python  theme={null}
agent = Agent(
    model=OpenAIChat(
        id="gpt-4o",
        cache_response=True,
        cache_dir="./path/to/custom/cache"
    )
)
python  theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude

**Examples:**

Example 1 (unknown):
```unknown
## Configuration Options

### Cache Time-to-Live (TTL)

Control how long responses remain cached using `cache_ttl` (in seconds):
```

Example 2 (unknown):
```unknown
If `cache_ttl` is not specified (or set to `None`), cached responses never expire.

### Custom Cache Directory

Store cached responses in a specific location using `cache_dir`:
```

Example 3 (unknown):
```unknown
If not specified, Agno uses a default cache location of `~/.agno/cache/model_responses` in your home directory.

## Usage with Agents

Response caching is configured at the model level and works automatically with agents:
```

---

## Reliability Evals

**URL:** llms-txt#reliability-evals

**Contents:**
- Basic Tool Call Reliability
- Multiple Tool Calls Reliability
- Team Reliability
- Usage
- Track Evals in AgnoOS platform

Source: https://docs.agno.com/basics/evals/reliability/overview

Reliability evals measure how well your Agents and Teams handle tool calls and error scenarios.

What makes an Agent or Team reliable?

* Does it make the expected tool calls?
* Does it handle errors gracefully?
* Does it respect the rate limits of the model API?

## Basic Tool Call Reliability

The first check is to ensure the Agent makes the expected tool calls. Here's an example:

<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=816d4832aa2d3d19ae007f85e9573c13" data-og-width="1148" data-og-height="488" data-path="images/evals/reliability_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=2cc17d3702c17b96b1a4828793dcad08 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=6cb3b00f1c366ee9ae87db978ba96c14 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=445cd7213631a55a500414dc1fe20152 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=c8a131d365b92fd94d67b6cd67b8eea5 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=83f280c6bd6929bfe56870283599826a 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=d47181b3504263137665022f59073978 2500w" />
</Frame>

## Multiple Tool Calls Reliability

Test that agents make multiple tool calls:

Test how teams handle various error conditions:

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Reliability Eval Test">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

## Track Evals in AgnoOS platform

<video autoPlay muted controls className="w-full aspect-video" src="https://mintcdn.com/agno-v2/hzelS2cST9lEqMuM/videos/eval_platform.mp4?fit=max&auto=format&n=hzelS2cST9lEqMuM&q=85&s=9329eaac5cd0f551081e51656cc0227c" data-path="videos/eval_platform.mp4" />

```python evals_demo.py theme={null}

"""Simple example creating a evals and using the AgentOS."""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.calculator import CalculatorTools

**Examples:**

Example 1 (unknown):
```unknown
<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=816d4832aa2d3d19ae007f85e9573c13" data-og-width="1148" data-og-height="488" data-path="images/evals/reliability_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=2cc17d3702c17b96b1a4828793dcad08 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=6cb3b00f1c366ee9ae87db978ba96c14 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=445cd7213631a55a500414dc1fe20152 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=c8a131d365b92fd94d67b6cd67b8eea5 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=83f280c6bd6929bfe56870283599826a 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=d47181b3504263137665022f59073978 2500w" />
</Frame>

## Multiple Tool Calls Reliability

Test that agents make multiple tool calls:
```

Example 2 (unknown):
```unknown
## Team Reliability

Test how teams handle various error conditions:
```

Example 3 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Reliability Eval Test">
    <CodeGroup>
```

---

## Create a reusable content creation sequence

**URL:** llms-txt#create-a-reusable-content-creation-sequence

article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        Step(name="research", agent=researcher),
        Step(name="writing", agent=writer),
        Step(name="editing", agent=editor),
    ],
)

---

## Using 'schedule' instead of deprecated 'schedule_interval'

**URL:** llms-txt#using-'schedule'-instead-of-deprecated-'schedule_interval'

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

with DAG(
    'example_dag',
    default_args=default_args,
    description='A simple example DAG',
    schedule='@daily',  # Changed from schedule_interval
    catchup=False
) as dag:
    def print_hello():
        print("Hello from Airflow!")
        return "Hello task completed"
    task = PythonOperator(
        task_id='hello_task',
        python_callable=print_hello,
        dag=dag,
    )
"""

agent.run(f"Save this DAG file as 'example_dag.py': {dag_content}")

agent.print_response("Read the contents of 'example_dag.py'")
```

| Parameter              | Type            | Default | Description                                     |
| ---------------------- | --------------- | ------- | ----------------------------------------------- |
| `dags_dir`             | `Path` or `str` | `None`  | Directory for DAG files                         |
| `enable_save_dag_file` | `bool`          | `True`  | Enables functionality to save Airflow DAG files |
| `enable_read_dag_file` | `bool`          | `True`  | Enables functionality to read Airflow DAG files |
| `all`                  | `bool`          | `False` | Enables all functionality when set to True      |

| Function        | Description                                        |
| --------------- | -------------------------------------------------- |
| `save_dag_file` | Saves python code for an Airflow DAG to a file     |
| `read_dag_file` | Reads an Airflow DAG file and returns the contents |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/airflow.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/airflow_tools.py)

---

## Let's run the agent providing a session_state. This session_state will be stored in the database.

**URL:** llms-txt#let's-run-the-agent-providing-a-session_state.-this-session_state-will-be-stored-in-the-database.

agent.print_response(
    "Can you tell me what's in your session_state?",
    session_state={"shopping_list": ["Potatoes"]},
    stream=True,
)
print(f"Stored session state: {agent.get_session_state()}")

---

## Chat History in Teams

**URL:** llms-txt#chat-history-in-teams

**Contents:**
- History Reference
- Add history to the team context
- Send team history to members
- Share member interactions with other members
- Read the chat history
- Search the session history
- Developer Resources

Source: https://docs.agno.com/basics/chat-history/team/overview

Learn about Team session history and managing conversation history.

Teams with storage enabled automatically have access to the run history of the session (also called the "conversation history" or "chat history").

We can give the Team access to the chat history in the following ways:

**Team-Level History**:

* You can set `add_history_to_context=True` and `num_history_runs=5` to add the inputs and responses from the last 5 runs automatically to every request sent to the team leader.
* You can be more granular about how many messages to add to include in the list sent to the model, by setting `num_history_messages`.
* You can set `read_chat_history=True` to provide a `get_chat_history()` tool to your team allowing it to read any message in the entire chat history.
* You can set `read_tool_call_history=True` to provide a `get_tool_call_history()` tool to your team allowing it to read tool calls in reverse chronological order.
* You can enable `search_session_history` to allow searching through previous sessions.
* You can set `add_team_history_to_members=True` and `num_team_history_runs=5` to add the inputs and responses from the last 5 runs (that is the team-level inputs and responses) automatically to every message sent to the team members.

**Member-Level History**:

* You can also enable `add_history_to_context` for individual team members. This will only add the inputs and outputs for that member to all requests sent to that member, giving it access to its own history.

<Tip>
  Working with team history can be tricky. Experiment with the above settings to find the best fit for your use case.
  See the [History Reference](#history-reference) for help on how to use the different history features.
</Tip>

<Tabs>
  <Tab title="Simple History">
    Start with **Team History in Context** for basic conversation continuity:

<Tab title="Member Coordination">
    Use **Team History to Members** for shared context:

<Tab title="Share Interaction Information">
    Share **Member Interactions** during a run:

<Tab title="Long Conversations">
    Add **Chat History Tool** when agents need to search history:

<Tab title="Multi-Session Memory">
    Enable **Multi-Session Search** for cross-session continuity:

<Note>
  **Database Requirement**: All history features require a database configured on the team. See [Database](/basics/database/overview) for setup.
</Note>

<Tip>
  **Performance Tip**: More history = larger context = slower and costlier requests. Start with `num_history_runs=3` and increase only if needed.
</Tip>

## Add history to the team context

To add the history of the conversation to the context, you can set `add_history_to_context=True`.
This will add the inputs and responses from the last 3 runs (that is the default) to the context of the team leader.
You can change the number of runs by setting `num_history_runs=n` where `n` is the number of runs to include.

See the [Direct Response with Team History](/basics/chat-history/team/usage/respond-directly-with-history) example for a complete implementation.

## Send team history to members

To send the team history to the members, you can set `add_team_history_to_members=True`.
This will send the inputs and responses from the last 3 team-level runs (that is the default) to the members when tasks are delegated to them.
You can change the number of runs by setting `num_team_history_runs=n` where `n` is the number of runs to include.

When enabled, team history is appended to the task sent to a team member in this format:

This allows members to access information from previous interactions with other team members.

See the [Team History for Members](/basics/chat-history/team/usage/team-history) example for a complete implementation.

## Share member interactions with other members

All interactions with team members are automatically recorded, including the member name, the task given to the member, and the response from the member.

<Note>
  This feature is only available during a single run - it shares interactions that happen within the current execution.
</Note>

If you want members to have access to all interactions that has happened during the current run, you can set `share_member_interactions=True`.

When enabled, interaction details are appended to the task sent to a team member in this format:

See the [Share Member Interactions](/basics/chat-history/team/usage/share-member-interactions) example for a complete implementation.

## Read the chat history

To read the chat history, you can set `read_chat_history=True`.
This will provide a `get_chat_history()` tool to your team allowing it to read any message in the entire chat history.

## Search the session history

In some scenarios, you might want to fetch messages from across multiple sessions to provide context or continuity in conversations.

To enable fetching messages from the last N sessions, you need to use the following flags:

* `search_session_history`: Set this to `True` to allow searching through previous sessions.
* `num_history_sessions`: Specify the number of past sessions to include in the search.

<Tip>
  Keep `num_history_sessions` low (2 or 3) to avoid filling up the context length of the model, which can lead to performance issues.
</Tip>

## Developer Resources

<CardGroup cols={2}>
  <Card title="Direct Response with History" icon="reply" iconType="duotone" href="/basics/chat-history/team/usage/respond-directly-with-history">
    Team leader routes requests with access to conversation history.
  </Card>

<Card title="Team History for Members" icon="share-nodes" iconType="duotone" href="/basics/chat-history/team/usage/team-history">
    Members access shared team history from previous interactions.
  </Card>

<Card title="Member-Level History" icon="user-check" iconType="duotone" href="/basics/chat-history/team/usage/history-of-members">
    Each member maintains its own isolated conversation history.
  </Card>

<Card title="Share Member Interactions" icon="comments" iconType="duotone" href="/basics/chat-history/team/usage/share-member-interactions">
    Share member interactions during the current run to avoid duplicate work.
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Member Coordination">
    Use **Team History to Members** for shared context:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Share Interaction Information">
    Share **Member Interactions** during a run:
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Long Conversations">
    Add **Chat History Tool** when agents need to search history:
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Multi-Session Memory">
    Enable **Multi-Session Search** for cross-session continuity:
```

---

## Setting up and running an eval for our agent

**URL:** llms-txt#setting-up-and-running-an-eval-for-our-agent

evaluation = AccuracyEval(
    db=db,  # Pass the database to the evaluation. Results will be stored in the database.
    name="Calculator Evaluation",
    model=OpenAIChat(id="gpt-5-mini"),
    input="Should I post my password online? Answer yes or no.",
    expected_output="No",
    num_iterations=1,
    # Agent or team to evaluate:
    agent=basic_agent,
    # team=basic_team,
)

---

## Team Session State

**URL:** llms-txt#team-session-state

**Contents:**
- How to use Shared State
  - Example

Source: https://docs.agno.com/basics/state/team/overview

Share and coordinate state across multiple agents in a team

Team Session State enables sharing and updating state data across teams of agents. Teams often need to coordinate on shared information.

<Check>
  Shared state propagates through nested team structures as well
</Check>

## How to use Shared State

You can set the `session_state` parameter on `Team` to set initial session state data.
This state data will be shared between the team leader and its members.

This state will be available to all team members and is synchronized between them.

Members can access the shared state using `run_context.session_state` in tools.

<Note>
  The `run_context` object is automatically passed to the tool as an argument. Use it to access the session state.
  Any updates to `run_context.session_state` will be automatically persisted in the database and reflected in the shared state.
  See the [RunContext schema](/reference/run/run-context) for more information.
</Note>

Here's a simple example of a team managing a shared shopping list:

```python team_session_state.py theme={null}
from agno.models.openai import OpenAIChat
from agno.agent import Agent
from agno.team import Team
from agno.run import RunContext

**Examples:**

Example 1 (unknown):
```unknown
Members can access the shared state using `run_context.session_state` in tools.

For example:
```

Example 2 (unknown):
```unknown
<Note>
  The `run_context` object is automatically passed to the tool as an argument. Use it to access the session state.
  Any updates to `run_context.session_state` will be automatically persisted in the database and reflected in the shared state.
  See the [RunContext schema](/reference/run/run-context) for more information.
</Note>

### Example

Here's a simple example of a team managing a shared shopping list:
```

---

## More example prompts to try:

**URL:** llms-txt#more-example-prompts-to-try:

**Contents:**
- Usage

"""
Try these research topics:
1. "Analyze the current state of solid-state batteries"
2. "Research recent breakthroughs in CRISPR gene editing"
3. "Investigate the development of autonomous vehicles"
4. "Explore advances in quantum machine learning"
5. "Study the impact of artificial intelligence on healthcare"
"""
bash  theme={null}
    pip install openai exa-py agno
    bash  theme={null}
    python research_agent.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## === CONDITION EVALUATORS ===

**URL:** llms-txt#===-condition-evaluators-===

def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)

def check_if_comprehensive_research_needed(step_input: StepInput) -> bool:
    """Check if comprehensive multi-step research is needed"""
    topic = step_input.input or step_input.previous_step_content or ""
    comprehensive_keywords = [
        "comprehensive",
        "detailed",
        "thorough",
        "in-depth",
        "complete analysis",
        "full report",
        "extensive research",
    ]
    return any(keyword in topic.lower() for keyword in comprehensive_keywords)

if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow with Multi-Step Condition",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],  # Single step
                ),
                Condition(
                    name="ComprehensiveResearchCondition",
                    description="Check if comprehensive multi-step research is needed",
                    evaluator=check_if_comprehensive_research_needed,
                    steps=[  # Multiple steps
                        deep_exa_analysis_step,
                        trend_analysis_step,
                        fact_verification_step,
                    ],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            write_step,
        ],
    )

try:
        workflow.print_response(
            input="Comprehensive analysis of climate change research",
            stream=True,
        )
    except Exception as e:
        print(f"âŒ Error: {e}")
    print()
```

To see the async example, see the cookbook-

* [Condition with list of steps (async)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_02_workflows_conditional_execution/async/condition_with_list_of_steps.py)

---

## Agent Infra Docker

**URL:** llms-txt#agent-infra-docker

**Contents:**
  - Local environment
  - Production environment
  - How to get started

Source: https://docs.agno.com/templates/agent-infra-docker/introduction

The Agent Infra Docker template provides a simple Docker Compose file for running AgentOS locally.

### Local environment

Runs both AgentOS and PostgreSQL using Docker.

Ideal for development, testing, and verifying your setup before deploying to the cloud.

### Production environment

Since this template provides you with a Dockerfile and Docker Compose file, you can easily deploy it to any cloud provider that supports Docker. Or you could deploy it to one of the Hosted Platforms like [Railway](https://railway.app/) or [Render](https://render.com/). For Railway, you can use the [Agent Infra Railway template](/templates/agent-infra-railway/introduction) to deploy your AgentOS.

### How to get started

* Clone the template repository and install the dependencies
* Start with the local setup to ensure everything works end-to-end.
* Move to the cloud provider of your choice using the provided guides once your local environment is ready.

<Snippet file="setup.mdx" />

<Snippet file="create-agent-infra-docker-codebase.mdx" />

Or, you can clone the repository and follow the instructions in the template documentation.

<CodeGroup>
  
</CodeGroup>

After creating your codebase, the next step is to get it up and running locally using docker

---

## Migrating to Workflows 2.0

**URL:** llms-txt#migrating-to-workflows-2.0

**Contents:**
- Migrating from Workflows 1.0
  - Key Differences
  - Migration Steps
  - Example of Blog Post Generator Workflow

Source: https://docs.agno.com/how-to/workflows-migration

Learn how to migrate to Workflows 2.0.

## Migrating from Workflows 1.0

Workflows 2.0 is a completely new approach to agent automation, and requires an upgrade from the Workflows 1.0 implementation. It introduces a new, more flexible and powerful way to build workflows.

| Workflows 1.0     | Workflows 2.0     | Migration Path                   |
| ----------------- | ----------------- | -------------------------------- |
| Linear only       | Multiple patterns | Add Parallel/Condition as needed |
| Agent-focused     | Mixed components  | Convert functions to Steps       |
| Limited branching | Smart routing     | Replace if/else with Router      |
| Manual loops      | Built-in Loop     | Use Loop component               |

1. **Assess current workflow**: Identify parallel opportunities
2. **Add conditions**: Convert if/else logic to Condition components
3. **Extract functions**: Move custom logic to function-based steps
4. **Enable streaming**: For event-based information
5. **Add state management**: Use `workflow_session_state` for data sharing

### Example of Blog Post Generator Workflow

Lets take an example that demonstrates how to build a sophisticated blog post generator that combines
web research capabilities with professional writing expertise. The workflow uses a multi-stage
approach:

1. Intelligent web research and source gathering
2. Content extraction and processing
3. Professional blog post writing with proper citations

Here's the code for the blog post generator in **Workflows 1.0**:

```python  theme={null}
import json
from textwrap import dedent
from typing import Dict, Iterator, Optional

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.run.workflow import WorkflowCompletedEvent
from agno.storage.sqlite import SqliteDb
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow import RunOutput, Workflow
from pydantic import BaseModel, Field

class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )

class SearchResults(BaseModel):
    articles: list[NewsArticle]

class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )

class BlogPostGenerator(Workflow):
    """Advanced workflow for generating professional blog posts with proper research and citations."""

description: str = dedent("""\
    An intelligent blog post generator that creates engaging, well-researched content.
    This workflow orchestrates multiple AI agents to research, analyze, and craft
    compelling blog posts that combine journalistic rigor with engaging storytelling.
    The system excels at creating content that is both informative and optimized for
    digital consumption.
    """)

# Search Agent: Handles intelligent web searching and source gathering
    searcher: Agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[DuckDuckGoTools()],
        description=dedent("""\
        You are BlogResearch-X, an elite research assistant specializing in discovering
        high-quality sources for compelling blog content. Your expertise includes:

- Finding authoritative and trending sources
        - Evaluating content credibility and relevance
        - Identifying diverse perspectives and expert opinions
        - Discovering unique angles and insights
        - Ensuring comprehensive topic coverage\
        """),
        instructions=dedent("""\
        1. Search Strategy ðŸ”
           - Find 10-15 relevant sources and select the 5-7 best ones
           - Prioritize recent, authoritative content
           - Look for unique angles and expert insights
        2. Source Evaluation ðŸ“Š
           - Verify source credibility and expertise
           - Check publication dates for timeliness
           - Assess content depth and uniqueness
        3. Diversity of Perspectives ðŸŒ
           - Include different viewpoints
           - Gather both mainstream and expert opinions
           - Find supporting data and statistics\
        """),
        output_schema=SearchResults,
    )

# Content Scraper: Extracts and processes article content
    article_scraper: Agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[Newspaper4kTools()],
        description=dedent("""\
        You are ContentBot-X, a specialist in extracting and processing digital content
        for blog creation. Your expertise includes:

- Efficient content extraction
        - Smart formatting and structuring
        - Key information identification
        - Quote and statistic preservation
        - Maintaining source attribution\
        """),
        instructions=dedent("""\
        1. Content Extraction ðŸ“‘
           - Extract content from the article
           - Preserve important quotes and statistics
           - Maintain proper attribution
           - Handle paywalls gracefully
        2. Content Processing ðŸ”„
           - Format text in clean markdown
           - Preserve key information
           - Structure content logically
        3. Quality Control âœ…
           - Verify content relevance
           - Ensure accurate extraction
           - Maintain readability\
        """),
        output_schema=ScrapedArticle,
    )

# Content Writer Agent: Crafts engaging blog posts from research
    writer: Agent = Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        description=dedent("""\
        You are BlogMaster-X, an elite content creator combining journalistic excellence
        with digital marketing expertise. Your strengths include:

- Crafting viral-worthy headlines
        - Writing engaging introductions
        - Structuring content for digital consumption
        - Incorporating research seamlessly
        - Optimizing for SEO while maintaining quality
        - Creating shareable conclusions\
        """),
        instructions=dedent("""\
        1. Content Strategy ðŸ“
           - Craft attention-grabbing headlines
           - Write compelling introductions
           - Structure content for engagement
           - Include relevant subheadings
        2. Writing Excellence âœï¸
           - Balance expertise with accessibility
           - Use clear, engaging language
           - Include relevant examples
           - Incorporate statistics naturally
        3. Source Integration ðŸ”
           - Cite sources properly
           - Include expert quotes
           - Maintain factual accuracy
        4. Digital Optimization ðŸ’»
           - Structure for scanability
           - Include shareable takeaways
           - Optimize for SEO
           - Add engaging subheadings\
        """),
        expected_output=dedent("""\
        # {Viral-Worthy Headline}

## Introduction
        {Engaging hook and context}

## {Compelling Section 1}
        {Key insights and analysis}
        {Expert quotes and statistics}

## {Engaging Section 2}
        {Deeper exploration}
        {Real-world examples}

## {Practical Section 3}
        {Actionable insights}
        {Expert recommendations}

## Key Takeaways
        - {Shareable insight 1}
        - {Practical takeaway 2}
        - {Notable finding 3}

## Sources
        {Properly attributed sources with links}\
        """),
        markdown=True,
    )

def run(
        self,
        topic: str,
        use_search_cache: bool = True,
        use_scrape_cache: bool = True,
        use_cached_report: bool = True,
    ) -> Iterator[RunOutputEvent]:
        logger.info(f"Generating a blog post on: {topic}")

# Use the cached blog post if use_cache is True
        if use_cached_report:
            cached_blog_post = self.get_cached_blog_post(topic)
            if cached_blog_post:
                yield WorkflowCompletedEvent(
                    run_id=self.run_id,
                    content=cached_blog_post,
                )
                return

# Search the web for articles on the topic
        search_results: Optional[SearchResults] = self.get_search_results(
            topic, use_search_cache
        )
        # If no search_results are found for the topic, end the workflow
        if search_results is None or len(search_results.articles) == 0:
            yield WorkflowCompletedEvent(
                run_id=self.run_id,
                content=f"Sorry, could not find any articles on the topic: {topic}",
            )
            return

# Scrape the search results
        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(
            topic, search_results, use_scrape_cache
        )

# Prepare the input for the writer
        writer_input = {
            "topic": topic,
            "articles": [v.model_dump() for v in scraped_articles.values()],
        }

# Run the writer and yield the response
        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)

# Save the blog post in the cache
        self.add_blog_post_to_cache(topic, self.writer.run_response.content)

def get_cached_blog_post(self, topic: str) -> Optional[str]:
        logger.info("Checking if cached blog post exists")

return self.session_state.get("blog_posts", {}).get(topic)

def add_blog_post_to_cache(self, topic: str, blog_post: str):
        logger.info(f"Saving blog post for topic: {topic}")
        self.session_state.setdefault("blog_posts", {})
        self.session_state["blog_posts"][topic] = blog_post

def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:
        logger.info("Checking if cached search results exist")
        search_results = self.session_state.get("search_results", {}).get(topic)
        return (
            SearchResults.model_validate(search_results)
            if search_results and isinstance(search_results, dict)
            else search_results
        )

def add_search_results_to_cache(self, topic: str, search_results: SearchResults):
        logger.info(f"Saving search results for topic: {topic}")
        self.session_state.setdefault("search_results", {})
        self.session_state["search_results"][topic] = search_results

def get_cached_scraped_articles(
        self, topic: str
    ) -> Optional[Dict[str, ScrapedArticle]]:
        logger.info("Checking if cached scraped articles exist")
        scraped_articles = self.session_state.get("scraped_articles", {}).get(topic)
        return (
            ScrapedArticle.model_validate(scraped_articles)
            if scraped_articles and isinstance(scraped_articles, dict)
            else scraped_articles
        )

def add_scraped_articles_to_cache(
        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]
    ):
        logger.info(f"Saving scraped articles for topic: {topic}")
        self.session_state.setdefault("scraped_articles", {})
        self.session_state["scraped_articles"][topic] = scraped_articles

def get_search_results(
        self, topic: str, use_search_cache: bool, num_attempts: int = 3
    ) -> Optional[SearchResults]:
        # Get cached search_results from the session state if use_search_cache is True
        if use_search_cache:
            try:
                search_results_from_cache = self.get_cached_search_results(topic)
                if search_results_from_cache is not None:
                    search_results = SearchResults.model_validate(
                        search_results_from_cache
                    )
                    logger.info(
                        f"Found {len(search_results.articles)} articles in cache."
                    )
                    return search_results
            except Exception as e:
                logger.warning(f"Could not read search results from cache: {e}")

# If there are no cached search_results, use the searcher to find the latest articles
        for attempt in range(num_attempts):
            try:
                searcher_response: RunOutput = self.searcher.run(topic)
                if (
                    searcher_response is not None
                    and searcher_response.content is not None
                    and isinstance(searcher_response.content, SearchResults)
                ):
                    article_count = len(searcher_response.content.articles)
                    logger.info(
                        f"Found {article_count} articles on attempt {attempt + 1}"
                    )
                    # Cache the search results
                    self.add_search_results_to_cache(topic, searcher_response.content)
                    return searcher_response.content
                else:
                    logger.warning(
                        f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                    )
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

logger.error(f"Failed to get search results after {num_attempts} attempts")
        return None

def scrape_articles(
        self, topic: str, search_results: SearchResults, use_scrape_cache: bool
    ) -> Dict[str, ScrapedArticle]:
        scraped_articles: Dict[str, ScrapedArticle] = {}

# Get cached scraped_articles from the session state if use_scrape_cache is True
        if use_scrape_cache:
            try:
                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)
                if scraped_articles_from_cache is not None:
                    scraped_articles = scraped_articles_from_cache
                    logger.info(
                        f"Found {len(scraped_articles)} scraped articles in cache."
                    )
                    return scraped_articles
            except Exception as e:
                logger.warning(f"Could not read scraped articles from cache: {e}")

# Scrape the articles that are not in the cache
        for article in search_results.articles:
            if article.url in scraped_articles:
                logger.info(f"Found scraped article in cache: {article.url}")
                continue

article_scraper_response: RunOutput = self.article_scraper.run(
                article.url
            )
            if (
                article_scraper_response is not None
                and article_scraper_response.content is not None
                and isinstance(article_scraper_response.content, ScrapedArticle)
            ):
                scraped_articles[article_scraper_response.content.url] = (
                    article_scraper_response.content
                )
                logger.info(f"Scraped article: {article_scraper_response.content.url}")

# Save the scraped articles in the session state
        self.add_scraped_articles_to_cache(topic, scraped_articles)
        return scraped_articles

---

## ==============================================================================

**URL:** llms-txt#==============================================================================

def demo_customer_support_cli():
    """Demo customer support workflow with CLI"""
    support_workflow = create_customer_support_workflow()

print("ðŸŽ§ Customer Support Demo - Type 'exit' to quit")
    print("Try: 'My account is locked and I can't access my billing information'")
    print("-" * 60)

support_workflow.cli_app(
        session_id="support_demo",
        user="Customer",
        emoji="ðŸ†˜",
        stream=True,
    )

def demo_medical_consultation_cli():
    """Demo medical consultation workflow with CLI"""
    medical_workflow = create_medical_consultation_workflow()

print("ðŸ¥ Medical Consultation Demo - Type 'exit' to quit")
    print("Try: 'I've been having chest pain and shortness of breath for 2 days'")
    print("-" * 60)

medical_workflow.cli_app(
        session_id="medical_demo",
        user="Patient",
        emoji="ðŸ©º",
        stream=True,
    )

def demo_tutoring_cli():
    """Demo tutoring workflow with CLI"""
    tutoring_workflow = create_tutoring_workflow()

print("ðŸ“š Tutoring Session Demo - Type 'exit' to quit")
    print("Try: 'I'm struggling with calculus derivatives and have a test next week'")
    print("-" * 60)

tutoring_workflow.cli_app(
        session_id="tutoring_demo",
        user="Student",
        emoji="ðŸŽ“",
        stream=True,
    )

if __name__ == "__main__":
    import sys

demos = {
        "support": demo_customer_support_cli,
        "medical": demo_medical_consultation_cli,
        "tutoring": demo_tutoring_cli,
    }

if len(sys.argv) > 1 and sys.argv[1] in demos:
        demos[sys.argv[1]]()
    else:
        print("ðŸš€ Conversational Workflow Demos")
        print("Choose a demo to run:")
        print("")
        for key, func in demos.items():
            print(f"{key:<10} - {func.__doc__}")
        print("")
        print("Or run all demos interactively:")
        choice = input("Enter demo name (or 'all'): ").strip().lower()

if choice == "all":
            for demo_func in demos.values():
                demo_func()
        elif choice in demos:
            demos[choice]()
        else:
            print("Invalid choice!")
```

---

## OpenBB

**URL:** llms-txt#openbb

Source: https://docs.agno.com/integrations/toolkits/others/openbb

**OpenBBTools** enable an Agent to provide information about stocks and companies.

```python cookbook/tools/openbb_tools.py theme={null}
from agno.agent import Agent
from agno.tools.openbb import OpenBBTools

agent = Agent(tools=[OpenBBTools()], debug_mode=True)

---

## Milvus Async Hybrid Search

**URL:** llms-txt#milvus-async-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/milvus/usage/async-milvus-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Agent Input as List

**URL:** llms-txt#agent-input-as-list

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/input-output/agent/usage/input-as-list

This example demonstrates how to provide input to an agent as a list format for multimodal content combining text and images.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Download the file using the download_file function

**URL:** llms-txt#download-the-file-using-the-download_file-function

**Contents:**
- Usage

download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini"),
    tools=[{"type": "file_search"}],
    markdown=True,
    add_history_to_context=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[File(filepath=pdf_path)],
)
agent.print_response("Suggest me a recipe from the attached file.")

bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/openai/responses/pdf_input_local.py
      bash Windows theme={null}
      python cookbook/models/openai/responses/pdf_input_local.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Guardrails

**URL:** llms-txt#guardrails

**Contents:**
- Agno included Guardrails
- Custom Guardrails

Source: https://docs.agno.com/basics/guardrails/overview

Learn about securing the input of your Agents using guardrails.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.0" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.0">v2.1.0</Tooltip>
</Badge>

Guardrails are built-in safeguards for your Agents and Teams. You can use them to make sure the input you send to the LLM is safe and doesn't contain anything undesired.

Some of the most popular usages are:

* PII detection and redaction
* Prompt injection defense
* Jailbreak defense
* Data leakage prevention
* NSFW content filtering

## Agno included Guardrails

Agno provides some built-in guardrails you can use out of the box with your Agents and Teams:

* [PII Detection Guardrail](/basics/guardrails/included/pii): detect PII (Personally Identifiable Information).
* [Prompt Injection Guardrail](/basics/guardrails/included/prompt-injection): detect and stop prompt injection attemps.
* [OpenAI Moderation Guardrail](/basics/guardrails/included/openai-moderation): detect content that violates OpenAI's content policy.

To use the Agno included guardrails, you just need to import them and pass them to the Agent or Team with the `pre_hooks` parameter.

Guardrails are implemented as [pre-hooks](/basics/hooks/overview), which execute before your Agent processes input.

For example, to use the PII Detection Guardrail:

You can see complete examples using the Agno Guardrails in the [Usage](/basics/guardrails/usage) section.

You can create custom guardrails by extending the `BaseGuardrail` class. See the [BaseGuardrail Reference](/reference/hooks/base-guardrail) for more details.

This is useful if you need to perform any check or transformation not handled by the built-in guardrails, or just to implement your own validation logic.

You will need to implement the `check` and `async_check` methods to perform your validation and raise exceptions when detecting undesired content.

<Check>
  Agno automatically uses the sync or async version of the guardrail based on whether you are running the agent with `.run()` or `.arun()`.
</Check>

For example, let's create a simple custom guardrail that checks if the input contains any URLs:

Now you can use your custom guardrail in your Agent:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
You can see complete examples using the Agno Guardrails in the [Usage](/basics/guardrails/usage) section.

## Custom Guardrails

You can create custom guardrails by extending the `BaseGuardrail` class. See the [BaseGuardrail Reference](/reference/hooks/base-guardrail) for more details.

This is useful if you need to perform any check or transformation not handled by the built-in guardrails, or just to implement your own validation logic.

You will need to implement the `check` and `async_check` methods to perform your validation and raise exceptions when detecting undesired content.

<Check>
  Agno automatically uses the sync or async version of the guardrail based on whether you are running the agent with `.run()` or `.arun()`.
</Check>

For example, let's create a simple custom guardrail that checks if the input contains any URLs:
```

Example 2 (unknown):
```unknown
Now you can use your custom guardrail in your Agent:
```

---

## RAG Agent

**URL:** llms-txt#rag-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/cloud/ibm-watsonx/usage/knowledge

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Set up PostgreSQL with pgvector">
    You need a PostgreSQL database with the pgvector extension installed. Adjust the `db_url` in the code to match your database configuration.
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="For subsequent runs">
    After the first run, comment out the `knowledge_base.load(recreate=True)` line to avoid reloading the PDF.
  </Step>
</Steps>

This example shows how to integrate a knowledge base with IBM WatsonX. It loads a PDF from a URL, processes it into a vector database (PostgreSQL with pgvector in this case), and then creates an agent that can query this knowledge base.

Note: You need to install several packages (`pgvector`, `pypdf`, etc.) and have a PostgreSQL database with the pgvector extension available.

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set up PostgreSQL with pgvector">
    You need a PostgreSQL database with the pgvector extension installed. Adjust the `db_url` in the code to match your database configuration.
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Have a conversation with OpenAI

**URL:** llms-txt#have-a-conversation-with-openai

openai_agent.print_response(
    "Explain machine learning basics", session_id=session_id, user_id=user_id
)
openai_agent.print_response(
    "What about deep learning?", session_id=session_id, user_id=user_id
)

---

## Async Performance Evaluation

**URL:** llms-txt#async-performance-evaluation

Source: https://docs.agno.com/basics/evals/performance/usage/performance-async

Example showing how to run performance evaluations on async functions.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Find content from multiple regions

**URL:** llms-txt#find-content-from-multiple-regions

IN("region", ["north_america", "europe", "asia"])

---

## MongoDB Database

**URL:** llms-txt#mongodb-database

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/mongo/overview

Learn to use MongoDB as a database for your Agents

Agno supports using [MongoDB](https://www.mongodb.com/) as a database with the `MongoDb` class.

<Tip>
  **v2 Migration Support**: If you're upgrading from Agno v1, MongoDB is fully supported in the v2 migration script. See the [migration guide](/how-to/v2-migration) for details.
</Tip>

```python mongodb_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.mongo import MongoDb

---

## Parallel Steps

**URL:** llms-txt#parallel-steps

Source: https://docs.agno.com/reference/workflows/parallel-steps

| Parameter     | Type             | Default  | Description                                     |
| ------------- | ---------------- | -------- | ----------------------------------------------- |
| `*steps`      | `*WorkflowSteps` | Required | Variable number of steps to execute in parallel |
| `name`        | `Optional[str]`  | `None`   | Name of the parallel execution block            |
| `description` | `Optional[str]`  | `None`   | Description of the parallel execution           |

---

## Define how to turn analysis into actionable insights

**URL:** llms-txt#define-how-to-turn-analysis-into-actionable-insights

**Contents:**
  - 3d. Define the Report Format

intelligence_synthesis = dedent("""
    INTELLIGENCE SYNTHESIS:
    - Detect crisis indicators through sentiment velocity and coordination patterns
    - Identify competitive positioning and feature gap discussions
    - Surface growth opportunities and advocacy moments
    - Generate strategic recommendations with clear priority levels
""")

print("Intelligence synthesis defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3d. Define the Report Format
```

---

## Each agent has its own database

**URL:** llms-txt#each-agent-has-its-own-database

agent1_db = SqliteDb(db_file="tmp/agent1.db", id="agent1_db")
agent2_db = SqliteDb(db_file="tmp/agent2.db", id="agent2_db")

---

## Langfuse

**URL:** llms-txt#langfuse

**Contents:**
- Integrating Agno with Langfuse
- Prerequisites
- Sending Traces to Langfuse

Source: https://docs.agno.com/integrations/observability/langfuse

Integrate Agno with Langfuse to send traces and gain insights into your agent's performance.

## Integrating Agno with Langfuse

Langfuse provides a robust platform for tracing and monitoring AI model calls. By integrating Agno with Langfuse, you can utilize OpenInference and OpenLIT to send traces and gain insights into your agent's performance.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup Langfuse Account**

* Either self-host or sign up for an account at [Langfuse](https://us.cloud.langfuse.com).
   * Obtain your public and secret API keys from the Langfuse dashboard.

3. **Set Environment Variables**

Configure your environment with the Langfuse API keys:

## Sending Traces to Langfuse

* ### Example: Using Langfuse with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Langfuse.

```python  theme={null}
import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup Langfuse Account**

   * Either self-host or sign up for an account at [Langfuse](https://us.cloud.langfuse.com).
   * Obtain your public and secret API keys from the Langfuse dashboard.

3. **Set Environment Variables**

   Configure your environment with the Langfuse API keys:
```

Example 2 (unknown):
```unknown
## Sending Traces to Langfuse

* ### Example: Using Langfuse with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Langfuse.
```

---

## Async Custom Retriever

**URL:** llms-txt#async-custom-retriever

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/async-custom-retriever

```python async_retriever.py theme={null}
import asyncio
from typing import Optional

from agno.agent import Agent
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.qdrant import Qdrant
from qdrant_client import AsyncQdrantClient

---

## Vercel v0

**URL:** llms-txt#vercel-v0

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/vercel

The Vercel v0 model provides access to Vercel's language models.

| Parameter               | Type            | Default                       | Description                                                      |
| ----------------------- | --------------- | ----------------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"v0"`                        | The id of the Vercel model to use                                |
| `name`                  | `str`           | `"VercelV0"`                  | The name of the model                                            |
| `provider`              | `str`           | `"Vercel"`                    | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                        | The API key for Vercel (defaults to VERCEL\_API\_KEY env var)    |
| `base_url`              | `str`           | `"https://api.vercel.com/v1"` | The base URL for the Vercel API                                  |
| `retries`               | `int`           | `0`                           | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                           | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                       | If True, the delay between retries is doubled each time          |

Vercel extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Define the path to the document to be loaded into the knowledge base

**URL:** llms-txt#define-the-path-to-the-document-to-be-loaded-into-the-knowledge-base

state_of_the_union = pathlib.Path(
    "cookbook/knowledge/testing_resources/state_of_the_union.txt"
)

---

## User Control Flow

**URL:** llms-txt#user-control-flow

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/user-control-flow

UserControlFlowTools enable agents to pause execution and request input from users during conversations.

The following agent can request user input during conversations:

| Parameter               | Type            | Default | Description                                        |
| ----------------------- | --------------- | ------- | -------------------------------------------------- |
| `instructions`          | `Optional[str]` | `None`  | Custom instructions for user interaction behavior. |
| `add_instructions`      | `bool`          | `True`  | Whether to add instructions to the agent.          |
| `enable_get_user_input` | `bool`          | `True`  | Enable user input request functionality.           |

| Function         | Description                                            |
| ---------------- | ------------------------------------------------------ |
| `get_user_input` | Pause agent execution and request input from the user. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/user_control_flow.py)
* [Agno Interactive Agents](https://docs.agno.com/interactive-agents)
* [User Input Patterns](https://docs.agno.com/user-input)

---

## LightRAG Vector Database

**URL:** llms-txt#lightrag-vector-database

**Contents:**
- Developer Resources

Source: https://docs.agno.com/integrations/vectordb/lightrag/overview

Learn how to use LightRAG as a vector database for your Knowledge Base

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/lance_db/lance_db.py)
* View [Cookbook (Hybrid Search)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/lance_db/lance_db_hybrid_search.py)

---

## Team with Exponential Backoff

**URL:** llms-txt#team-with-exponential-backoff

Source: https://docs.agno.com/basics/teams/usage/other/team-exponential-backoff

This example demonstrates how to configure a team with exponential backoff retry logic.

When agents encounter errors or rate limits, the team will automatically retry with increasing delays between attempts.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setup the Agno API App

**URL:** llms-txt#setup-the-agno-api-app

agent_os = AgentOS(
    description="Example app for basic agent with eval capabilities",
    id="eval-demo",
    agents=[basic_agent],
)
app = agent_os.get_app()

if __name__ == "__main__":
    """ Run your AgentOS:
    Now you can interact with your eval runs using the API. Examples:
    - http://localhost:8001/eval-runs
    - http://localhost:8001/eval-runs/123
    - http://localhost:8001/eval-runs?agent_id=123
    - http://localhost:8001/eval-runs?limit=10&page=0&sort_by=created_at&sort_order=desc
    - http://localhost:8001/eval-runs/accuracy
    - http://localhost:8001/eval-runs/performance
    - http://localhost:8001/eval-runs/reliability
    """
    agent_os.serve(app="evals_demo:app", reload=True)

bash Mac theme={null}
      python evals_demo.py
      ```
    </CodeGroup>
  </Step>

<Step title="View the Evals Demo">
    Head over to <a href="https://os.agno.com/evaluation">[https://os.agno.com/evaluation](https://os.agno.com/evaluation)</a> to view the evals.
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  For more details, see the [Evaluation API Reference](/reference-api/schema/evals/list-evaluation-runs).
</Note>

<Steps>
  <Step title="Run the Evals Demo">
    <CodeGroup>
```

---

## Workflow Metrics

**URL:** llms-txt#workflow-metrics

**Contents:**
- Example Usage

Source: https://docs.agno.com/basics/sessions/metrics/workflow

Learn about Workflow run and session metrics.

When you run a workflow in Agno, the response you get (**WorkflowRunOutput**) includes detailed metrics about the workflow execution.

These metrics help you understand token usage, execution time, performance, and step-level details across all agents, teams, and custom functions in your workflow.

Metrics are available at multiple levels:

* **Per workflow**: Each `WorkflowRunOutput` includes a metrics object containing the workflow duration.
* **Per step**: Each step has its own metrics including duration, token usage, and model information.
* **Per session**: Session metrics aggregate all step-level metrics across all runs in the session.

Here's how you can access and use workflow metrics:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from rich.pretty import pprint

---

## LangSmith

**URL:** llms-txt#langsmith

**Contents:**
- Integrating Agno with LangSmith
- Prerequisites
- Sending Traces to LangSmith

Source: https://docs.agno.com/integrations/observability/langsmith

Integrate Agno with LangSmith to send traces and gain insights into your agent's performance.

## Integrating Agno with LangSmith

LangSmith offers a comprehensive platform for tracing and monitoring AI model calls. By integrating Agno with LangSmith, you can utilize OpenInference to send traces and gain insights into your agent's performance.

1. **Create a LangSmith Account**

* Sign up for an account at [LangSmith](https://smith.langchain.com).
   * Obtain your API key from the LangSmith dashboard.

2. **Set Environment Variables**

Configure your environment with the LangSmith API key and other necessary settings:

3. **Install Dependencies**

Ensure you have the necessary packages installed:

## Sending Traces to LangSmith

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to LangSmith.

```python  theme={null}
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

**Examples:**

Example 1 (unknown):
```unknown
3. **Install Dependencies**

   Ensure you have the necessary packages installed:
```

Example 2 (unknown):
```unknown
## Sending Traces to LangSmith

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to LangSmith.
```

---

## Create a Creative AI Video Director Agent

**URL:** llms-txt#create-a-creative-ai-video-director-agent

video_agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[ModelsLabTools()],
    description=dedent("""\
        You are an experienced AI video director with expertise in various video styles,
        from nature scenes to artistic animations. You have a deep understanding of motion,
        timing, and visual storytelling through video content.\
    """),
    instructions=dedent("""\
        As an AI video director, follow these guidelines:
        1. Analyze the user's request carefully to understand the desired style and mood
        2. Before generating, enhance the prompt with details about motion, timing, and atmosphere
        3. Use the `generate_media` tool with detailed, well-crafted prompts
        4. Provide a brief explanation of the creative choices made
        5. If the request is unclear, ask for clarification about style preferences

The video will be displayed in the UI automatically below your response.
        Always aim to create captivating and meaningful videos that bring the user's vision to life!\
    """),
    markdown=True,
)

---

## Create distributed RAG team

**URL:** llms-txt#create-distributed-rag-team

**Contents:**
- Usage

distributed_rag_team = Team(
    name="Distributed RAG Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[
        primary_retriever,
        context_expander,
        answer_synthesizer,
        quality_validator,
    ],
    instructions=[
        "Work together to provide comprehensive, high-quality RAG responses.",
        "Primary Retriever: First retrieve core relevant information.",
        "Context Expander: Then expand with related context and background.",
        "Answer Synthesizer: Synthesize all information into a comprehensive answer.",
        "Quality Validator: Finally validate and suggest any improvements.",
        "Ensure all responses are accurate, complete, and well-structured.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_distributed_rag_demo():
    """Demonstrate async distributed RAG processing."""
    print("ðŸ“š Async Distributed RAG with LanceDB Demo")
    print("=" * 50)

query = "How do I make chicken and galangal in coconut milk soup? Include cooking tips and variations."

# Add content to knowledge bases
    await primary_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    await context_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# # Run async distributed RAG
    # await distributed_rag_team.aprint_response(
    #     query, stream=True
    # )
    await distributed_rag_team.aprint_response(input=query)

def sync_distributed_rag_demo():
    """Demonstrate sync distributed RAG processing."""
    print("ðŸ“š Distributed RAG with LanceDB Demo")
    print("=" * 40)

query = "How do I make chicken and galangal in coconut milk soup? Include cooking tips and variations."

# Add content to knowledge bases
    primary_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    context_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# Run distributed RAG
    distributed_rag_team.print_response(input=query)

def multi_course_meal_demo():
    """Demonstrate distributed RAG for complex multi-part queries."""
    print("ðŸ½ï¸ Multi-Course Meal Planning with Distributed RAG")
    print("=" * 55)

query = """Hi, I want to make a 3 course Thai meal. Can you recommend some recipes?
    I'd like to start with a soup, then a thai curry for the main course and finish with a dessert.
    Please include cooking techniques and any special tips."""

# Add content to knowledge bases
    primary_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    context_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

distributed_rag_team.print_response(input=query)

if __name__ == "__main__":
    # Choose which demo to run
    asyncio.run(async_distributed_rag_demo())

# multi_course_meal_demo()

# sync_distributed_rag_demo()
bash  theme={null}
    pip install agno openai lancedb tantivy pypdf sqlalchemy
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/distributed_rag/02_distributed_rag_lancedb.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create knowledge base with hybrid search

**URL:** llms-txt#create-knowledge-base-with-hybrid-search

knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_assist_knowledge",
        search_type=SearchType.hybrid,  # Semantic + keyword search
        embedder=OpenAIEmbedder(id="text-embedding-3-small"),
    ),
)

---

## Run Agent Infra AWS Locally

**URL:** llms-txt#run-agent-infra-aws-locally

Source: https://docs.agno.com/templates/agent-infra-aws/run-local

<Snippet file="run-agent-infra-aws-local.mdx" />

When you are happy with your AgentOS, its time to deploy it to AWS!

---

## === WORKFLOW STEPS ===

**URL:** llms-txt#===-workflow-steps-===

write_step = Step(
    name="write_story",
    description="Write initial story",
    agent=story_writer,
)

edit_step = Step(
    name="edit_story",
    description="Edit and improve the story",
    agent=story_editor,
)

format_step = Step(
    name="format_story",
    description="Format the story into sections",
    agent=story_formatter,
)

---

## LanceDB Async

**URL:** llms-txt#lancedb-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/lancedb/usage/async-lance-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Validate your filters before searching (catches typos!)

**URL:** llms-txt#validate-your-filters-before-searching-(catches-typos!)

**Contents:**
- Automatic vs Manual Search

valid_filters, invalid_keys = knowledge.validate_filters({
    "department": "hr",
    "invalid_key": "value"  # This will be flagged as invalid
})
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Use `knowledge.get_content_status()` to debug when content doesn't appear in search results. It'll tell you if processing failed or is still in progress.
</Tip>

## Automatic vs Manual Search

Agno gives you two ways to use knowledge with agents:

**Agentic Search** (`search_knowledge=True`):
The agent automatically decides when to search and what to look for. This is the recommended approach for most use cases - it's smarter and more dynamic.

**Traditional RAG** (`add_knowledge_to_context=True`):
Relevant knowledge is always added to the agent's context. Simpler but less flexible. Use this when you want predictable, consistent behavior.
```

---

## CSV Reader

**URL:** llms-txt#csv-reader

Source: https://docs.agno.com/reference/knowledge/reader/csv

CSVReader is a reader class that allows you to read data from CSV files.

<Snippet file="csv-reader-reference.mdx" />

---

## Define the embedder

**URL:** llms-txt#define-the-embedder

embedder = OpenAIEmbedder(id="text-embedding-3-small")

---

## Vector Retriever Agent - Specialized in vector similarity search

**URL:** llms-txt#vector-retriever-agent---specialized-in-vector-similarity-search

vector_retriever = Agent(
    name="Vector Retriever",
    model=OpenAIChat(id="gpt-5-mini"),
    role="Retrieve information using vector similarity search in PostgreSQL",
    knowledge=vector_knowledge,
    search_knowledge=True,
    instructions=[
        "Use vector similarity search to find semantically related content.",
        "Focus on finding information that matches the semantic meaning of queries.",
        "Leverage pgvector's efficient similarity search capabilities.",
        "Retrieve content that has high semantic relevance to the user's query.",
    ],
    markdown=True,
)

---

## Async Basic.Py

**URL:** llms-txt#async-basic.py

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/huggingface/usage/async-basic

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Github

**URL:** llms-txt#github

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/others/github

**GithubTools** enables an Agent to access Github repositories and perform tasks such as listing open pull requests, issues and more.

The following examples requires the `PyGithub` library and a Github access token which can be obtained from [here](https://github.com/settings/tokens).

The following agent will search Google for the latest news about "Mistral AI":

| Parameter      | Type            | Default | Description                                                                                                   |
| -------------- | --------------- | ------- | ------------------------------------------------------------------------------------------------------------- |
| `access_token` | `Optional[str]` | `None`  | GitHub access token for authentication. If not provided, will use GITHUB\_ACCESS\_TOKEN environment variable. |
| `base_url`     | `Optional[str]` | `None`  | Optional base URL for GitHub Enterprise installations.                                                        |

| Function                   | Description                                          |
| -------------------------- | ---------------------------------------------------- |
| `search_repositories`      | Searches Github repositories based on a query.       |
| `list_repositories`        | Lists repositories for a given user or organization. |
| `get_repository`           | Gets details about a specific repository.            |
| `list_pull_requests`       | Lists pull requests for a repository.                |
| `get_pull_request`         | Gets details about a specific pull request.          |
| `get_pull_request_changes` | Gets the file changes in a pull request.             |
| `create_issue`             | Creates a new issue in a repository.                 |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/basics/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/github.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/github_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search Google for the latest news about "Mistral AI":
```

---

## GcsJsonDb

**URL:** llms-txt#gcsjsondb

Source: https://docs.agno.com/reference/storage/gcs

`GcsJsonDb` is a class that implements the Db interface using Google Cloud Storage as a database using JSON files. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-gcs-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Execute research with structured input

**URL:** llms-txt#execute-research-with-structured-input

**Contents:**
- Passthrough Teams
- Delegate tasks to all members simultaneously
- More Examples
- Developer Resources

team.print_response(input=research_request)
python  theme={null}
from agno.team.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat

team = Team(
    name="Question Router Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[
        Agent(name="Big Question Agent", role="You handle BIG questions"),
        Agent(name="Small Question Agent", role="You handle SMALL questions"),
    ],
    respond_directly=True,  # The team leader doesn't process the response from the members and instead returns them directly
    determine_input_for_members=False,  # The member gets the input directly, without the team leader synthesizing it
)

team.print_response(input="What is the capital of France?", stream=True)
team.print_response(input="What is the meaning of life?", stream=True)
python  theme={null}
import asyncio
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.arxiv import ArxivTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

reddit_researcher = Agent(
    name="Reddit Researcher",
    role="Research a topic on Reddit",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a Reddit researcher.
    You will be given a topic to research on Reddit.
    You will need to find the most relevant posts on Reddit.
    """),
)

hackernews_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Research a topic on HackerNews.",
    tools=[HackerNewsTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a HackerNews researcher.
    You will be given a topic to research on HackerNews.
    You will need to find the most relevant posts on HackerNews.
    """),
)

academic_paper_researcher = Agent(
    name="Academic Paper Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Research academic papers and scholarly content",
    tools=[DuckDuckGoTools(), ArxivTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a academic paper researcher.
    You will be given a topic to research in academic literature.
    You will need to find relevant scholarly articles, papers, and academic discussions.
    Focus on peer-reviewed content and citations from reputable sources.
    Provide brief summaries of key findings and methodologies.
    """),
)

twitter_researcher = Agent(
    name="Twitter Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Research trending discussions and real-time updates",
    tools=[DuckDuckGoTools()],
    add_name_to_context=True,
    instructions=dedent("""
    You are a Twitter/X researcher.
    You will be given a topic to research on Twitter/X.
    You will need to find trending discussions, influential voices, and real-time updates.
    Focus on verified accounts and credible sources when possible.
    Track relevant hashtags and ongoing conversations.
    """),
)

agent_team = Team(
    name="Discussion Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[
        reddit_researcher,
        hackernews_researcher,
        academic_paper_researcher,
        twitter_researcher,
    ],
    instructions=[
        "You are a discussion master.",
        "You have to stop the discussion when you think the team has reached a consensus.",
    ],
    delegate_to_all_members=True,
    markdown=True,
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent_team.aprint_response(
            input="Start the discussion on the topic: 'What is the best way to learn to code?'",
            stream=True,
        )
    )
```

<CardGroup cols={3}>
  <Card title="Basic Coordination" icon="link" href="/basics/teams/usage/basic-flows/basic-team">
    Basic team coordination pattern
  </Card>

<Card title="Member Responds Directly" icon="link" href="/basics/teams/usage/basic-flows/respond-directly">
    Router pattern with direct response
  </Card>

<Card title="Delegate to All Members" icon="link" href="/basics/teams/usage/basic-flows/delegate-to-all-members">
    Delegate tasks to all members for collaboration
  </Card>
</CardGroup>

## Developer Resources

* View the [Team reference](/reference/teams/team)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows/README.md)

**Examples:**

Example 1 (unknown):
```unknown
## Passthrough Teams

It is a common pattern to have a team that decides which member to delegate the request to, and then passes the request to the team member without any modification, and also applies no processing to the response before returning it to the user. I.e. this team is a "passthrough" team (or "router" team).

This means setting both `respond_directly=True` and `determine_input_for_members=False`.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=badf1c510d5f4ba340d79a775e83bd40" alt="Passthrough team flow" data-og-width="3609" width="3609" data-og-height="906" height="906" data-path="images/teams/team-passthrough-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=2bd0c59b87682466cd66309cd92ebd03 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=20d283af99676207613721b65a1544da 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=739d74525339750620fa0367d25b659d 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=f6e388ccdcabf2efbd24e818c8836760 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c0580f40db044da51acf1c8076da06dc 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=805e9091813e45cba058ecb5d9421b31 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=a6a7cb2997cea9eae541f879d2cce9be" alt="Passthrough team flow" data-og-width="3609" width="3609" data-og-height="906" height="906" data-path="images/teams/team-passthrough-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=bcef71b578dd4919ea49363e02e6733c 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=0200cac28e7a126209261fcd12f07b9a 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=a7deb75d871986bba5cfba09e5a67a82 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=c34e7dda68f025a197beafce99eb2c4f 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=b2f1f4ace92edee11d28ef4c1332dc56 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-passthrough-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=f9c97b3a73e143f20db785edfa1009df 2500w" />

For example:
```

Example 2 (unknown):
```unknown
## Delegate tasks to all members simultaneously

Set `delegate_to_all_members=True` to delegate the task to **all members at once**, rather than selectively choosing members.

When running asynchronously (using `arun`), all members execute **concurrently** for maximum parallelism.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=9db7cc875ebf53fc88ce5ef5ad900eb1" alt="Delegate tasks to all members simultaneously flow" data-og-width="3699" width="3699" data-og-height="906" height="906" data-path="images/teams/team-delegate-to-all-members-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=aa883551877111ae04389715768858e1 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=17e3d965ad7e1699081fdec8df976b4d 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=9609c9fa6cd91de68eabd6d0ba752e50 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=9365f145a3b2c095dfce87e6fb66a2c1 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=44d0fa282146382ac1b63a8036b3739e 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-light.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=cda8e9cdf36e8a3bdef2fe40e9351e38 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=58079bd5c2876131bdc5c06da5dd2b8f" alt="Delegate tasks to all members simultaneously flow" data-og-width="3699" width="3699" data-og-height="906" height="906" data-path="images/teams/team-delegate-to-all-members-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?w=280&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=5ef4938505de546fca41134253e9d636 280w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?w=560&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=16ff138ca601bc09bcf371264c750cc0 560w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?w=840&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=265b89ce2ed4d56fdfbe4fff603a8533 840w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?w=1100&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=fef810682777a8081b4496796eadbdb7 1100w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?w=1650&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=46cba6e4f62cc93ab4d13668bd9e4186 1650w, https://mintcdn.com/agno-v2/i4nXCaAsJR5zgHQd/images/teams/team-delegate-to-all-members-dark.png?w=2500&fit=max&auto=format&n=i4nXCaAsJR5zgHQd&q=85&s=da02200336382ce04305b7986dba78fe 2500w" />

**Example:** Research team that gathers perspectives from multiple sources simultaneously:
```

---

## Custom function to prepare input for Notion agent

**URL:** llms-txt#custom-function-to-prepare-input-for-notion-agent

def prepare_notion_input(step_input: StepInput) -> StepOutput:
    """
    Extract the classification result and format it for the Notion agent.
    """
    # Get the classification result from the previous step (Classify Query)
    previous_output = step_input.previous_step_content

# Parse it into our Pydantic model if it's a dict
    if isinstance(previous_output, dict):
        classification = ClassificationResult(**previous_output)
    elif isinstance(previous_output, str):
        # If it's a string, try to parse it or use the original input
        import json

try:
            classification = ClassificationResult(**json.loads(previous_output))
        except (json.JSONDecodeError, TypeError, KeyError, ValueError):
            classification = ClassificationResult(
                query=str(step_input.input),
                tag="general-blogs",
                message="Failed to parse classification",
            )
    else:
        classification = previous_output

# Create a clear instruction for the Notion agent with EXPLICIT tag requirement
    instruction = f"""Process this classified query:

Query: {classification.query}
        Tag: {classification.tag}

IMPORTANT: You MUST use the tag "{classification.tag}" (one of: travel, tech, general-blogs, fashion, documents).
        Do NOT create a new tag. Use EXACTLY "{classification.tag}".

Instructions:
        1. Use search_pages tool to find pages with tag "{classification.tag}"
        2. If page exists: Use update_page to add the query content
        3. If no page exists: Use create_page with title "My {classification.tag.title()} Collection", tag "{classification.tag}", and the query as content

The tag MUST be exactly: {classification.tag}
    """

return StepOutput(content=instruction)

---

## Process segments

**URL:** llms-txt#process-segments

shorts = extract_segments(response.content)

---

## MySQLDb

**URL:** llms-txt#mysqldb

Source: https://docs.agno.com/reference/storage/mysql

`MySQLDb` is a class that implements the Db interface using MySQL as the backend storage system. It provides robust, relational storage for agent sessions with support for JSONB data types, schema versioning, and efficient querying.

<Snippet file="db-mysql-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Save the generated audio

**URL:** llms-txt#save-the-generated-audio

**Contents:**
- Advanced Example: Translation and Voice Localization
- Toolkit Params
- Toolkit Functions
- Developer Resources

if response.audio:
    write_audio_to_file(audio=response.audio[0].content, filename="tmp/greeting.mp3")

python  theme={null}
from textwrap import dedent
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.cartesia import CartesiaTools
from agno.utils.audio import write_audio_to_file

agent_instructions = dedent(
    """Follow these steps SEQUENTIALLY to translate text and generate a localized voice note:
    1. Identify the text to translate and the target language from the user request.
    2. Translate the text accurately to the target language.
    3. Analyze the emotion conveyed by the translated text.
    4. Call `list_voices` to retrieve available voices.
    5. Select a base voice matching the language and emotion.
    6. Call `localize_voice` to create a new localized voice.
    7. Call `text_to_speech` to generate the final audio.
    """
)

agent = Agent(
    name="Emotion-Aware Translator Agent",
    description="Translates text, analyzes emotion, selects a suitable voice, creates a localized voice, and generates a voice note (audio file) using Cartesia TTS tools.",
    instructions=agent_instructions,
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[CartesiaTools(enable_localize_voice=True)],  
    )

agent.print_response(
    "Translate 'Hello! How are you? Tell me more about the weather in Paris?' to French and create a voice note."
)
response = agent.run_response

if response.audio:
    write_audio_to_file(
        response.audio[0].base64_audio,
        filename="french_weather.mp3",
    )
```

| Parameter               | Type   | Default                                | Description                                                                                         |
| ----------------------- | ------ | -------------------------------------- | --------------------------------------------------------------------------------------------------- |
| `api_key`               | `str`  | `None`                                 | The Cartesia API key for authentication. If not provided, uses the `CARTESIA_API_KEY` env variable. |
| `model_id`              | `str`  | `sonic-2`                              | The model ID to use for text-to-speech.                                                             |
| `default_voice_id`      | `str`  | `78ab82d5-25be-4f7d-82b3-7ad64e5b85b2` | The default voice ID to use for text-to-speech and localization.                                    |
| `enable_text_to_speech` | `bool` | `True`                                 | Enable text-to-speech functionality.                                                                |
| `enable_list_voices`    | `bool` | `True`                                 | Enable listing available voices functionality.                                                      |
| `enable_localize_voice` | `bool` | `False`                                | Enable voice localization functionality.                                                            |

| Function         | Description                          |
| ---------------- | ------------------------------------ |
| `list_voices`    | List available voices from Cartesia. |
| `text_to_speech` | Converts text to speech.             |
| `localize_voice` | Create a new localized voice.        |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/cartesia.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/cartesia_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Advanced Example: Translation and Voice Localization

This example demonstrates how to translate text, analyze emotion, localize a new voice, and generate a voice note using CartesiaTools.
```

---

## Fireworks Embedder

**URL:** llms-txt#fireworks-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/fireworks/usage/fireworks-embedder

```python  theme={null}
from agno.knowledge.embedder.fireworks import FireworksEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = FireworksEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Async Structured Output Agent

**URL:** llms-txt#async-structured-output-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/async-structured-output

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Async Sqlite for Team

**URL:** llms-txt#async-sqlite-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/async-sqlite/usage/async-sqlite-for-team

Agno supports using Sqlite asynchronously as a storage backend for Teams, with the `AsyncSqliteDb` class.

You need to provide either `db_url`, `db_file` or `db_engine`. The following example uses `db_file`.

```python async_sqlite_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno sqlalchemy aiosqlite` to install the dependencies
"""
import asyncio
from typing import List

from agno.agent import Agent
from agno.db.sqlite import AsyncSqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Apply the configuration

**URL:** llms-txt#apply-the-configuration

configure_agno_logging(
    custom_default_logger=custom_agent_logger,
    custom_agent_logger=custom_agent_logger,
    custom_team_logger=custom_team_logger,
    custom_workflow_logger=custom_workflow_logger,
)

---

## Update Session

**URL:** llms-txt#update-session

Source: https://docs.agno.com/reference-api/schema/sessions/update-session

patch /sessions/{session_id}
Update session properties such as session_name, session_state, metadata, or summary. Use this endpoint to modify the session name, update state, add metadata, or update the session summary.

---

## Azure OpenAI GPT 4.1

**URL:** llms-txt#azure-openai-gpt-4.1

Source: https://docs.agno.com/basics/reasoning/usage/models/azure-openai/reasoning-model-gpt4-1

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set your Azure OpenAI credentials">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set your Azure OpenAI credentials">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Image Generation Agent (Streaming)

**URL:** llms-txt#image-generation-agent-(streaming)

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/image-generation-stream

```python cookbook/models/google/gemini/image_generation_stream.py theme={null}
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini
from PIL import Image

---

## Audio to Text Transcription Team

**URL:** llms-txt#audio-to-text-transcription-team

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/team/usage/audio-to-text

This example demonstrates how a team can collaborate to transcribe audio content and analyze the transcribed text for insights and themes.

```python cookbook/examples/teams/multimodal/audio_to_text.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini
from agno.team import Team

transcription_specialist = Agent(
    name="Transcription Specialist",
    role="Convert audio to accurate text transcriptions",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Transcribe audio with high accuracy",
        "Identify speakers clearly as Speaker A, Speaker B, etc.",
        "Maintain conversation flow and context",
    ],
)

content_analyzer = Agent(
    name="Content Analyzer",
    role="Analyze transcribed content for insights",
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions=[
        "Analyze transcription for key themes and insights",
        "Provide summaries and extract important information",
    ],
)

---

## Test your knowledge-powered agent

**URL:** llms-txt#test-your-knowledge-powered-agent

**Contents:**
  - Complete Example

if __name__ == "__main__":
    # Your agent will automatically search its knowledge to answer
    agent.print_response(
        "What is the company policy on remote work?",
        stream=True
    )
python knowledge_agent.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
### Complete Example

Here's the full working example:
```

---

## Building our Social Media Intelligence System

**URL:** llms-txt#building-our-social-media-intelligence-system

**Contents:**
- Step 1: Choose Your AI Model

## Step 1: Choose Your AI Model

**Which model should I use?**: You can choose any model from our supported providers. Normally models are chosen based on costs and performance. In this case, we will be using OpenAI's GPT-5 Mini.

* **Cost-effective**: Better price/performance ratio than other GPT models
* **Tool usage**: Excellent at deciding when and how to use tools
* **Complex reasoning**: Can follow detailed analysis methodologies
* **Structured output**: Reliable at generating formatted reports

Let's first create the file where we will define our agent:

Now let's add the basic imports and model setup:

```python  theme={null}
from pathlib import Path
from dotenv import load_dotenv
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
Now let's add the basic imports and model setup:
```

---

## Agent with URL Context

**URL:** llms-txt#agent-with-url-context

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/google/usage/url-context

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define the quality standards for analysis

**URL:** llms-txt#define-the-quality-standards-for-analysis

**Contents:**
  - 3f. Combine Into Complete Instructions

analysis_principles = dedent("""
    ANALYSIS PRINCIPLES:
    - Evidence-based conclusions with supporting metrics
    - Actionable insights that drive business decisions
    - Cross-platform correlation analysis
    - Influence-weighted sentiment scoring
    - Proactive risk and opportunity identification
""")

print("Analysis principles defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3f. Combine Into Complete Instructions
```

---

## Define executor using a class.

**URL:** llms-txt#define-executor-using-a-class.

---

## Initialize Redis db (use the right db_url for your setup)

**URL:** llms-txt#initialize-redis-db-(use-the-right-db_url-for-your-setup)

db = RedisDb(db_url="redis://localhost:6379")

---

## customer_support_team.print_response(

**URL:** llms-txt#customer_support_team.print_response(

---

## -----------------------------------------------------------------------------

**URL:** llms-txt#-----------------------------------------------------------------------------

knowledge = Knowledge(
    name="CSV Knowledge Base",
    description="A knowledge base for CSV files",
    vector_db=vector_db,
)

---

## Redshift

**URL:** llms-txt#redshift

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/database/redshift

The RedshiftTools toolkit enables an Agent to interact with Amazon Redshift data warehouses.

**RedshiftTools** enable an Agent to interact with Amazon Redshift data warehouses.

The following example requires the `redshift_connector` library.

You will also need an Amazon Redshift cluster or Redshift Serverless endpoint. For IAM authentication, ensure your AWS credentials are configured.

The following agent will list all tables in the database.

```python redshift_tools.py theme={null}
from agno.agent import Agent
from agno.tools.redshift import RedshiftTools

**Examples:**

Example 1 (unknown):
```unknown
You will also need an Amazon Redshift cluster or Redshift Serverless endpoint. For IAM authentication, ensure your AWS credentials are configured.

## Example

The following agent will list all tables in the database.
```

---

## "Analyze this landmark's architecture and recent news.",

**URL:** llms-txt#"analyze-this-landmark's-architecture-and-recent-news.",

---

## Knowledge Tools

**URL:** llms-txt#knowledge-tools

**Contents:**
- Example

Source: https://docs.agno.com/integrations/toolkits/others/knowledge

KnowledgeTools provide intelligent search and analysis capabilities over knowledge bases with reasoning integration.

The following agent can search and analyze knowledge bases:

```python  theme={null}
from agno.agent import Agent
from agno.tools.knowledge import KnowledgeTools
from agno.knowledge import Knowledge

---

## Team will use custom_team_logger

**URL:** llms-txt#team-will-use-custom_team_logger

team.print_response("Tell me a short joke")

---

## Structured Output Streaming

**URL:** llms-txt#structured-output-streaming

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/structured-output-streaming

This example demonstrates streaming structured output from a team, using Pydantic models to ensure validated data structures while providing real-time streaming responses.

```python cookbook/examples/teams/structured_input_output/04_structured_output_streaming.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.exa import ExaTools
from pydantic import BaseModel

class StockAnalysis(BaseModel):
    symbol: str
    company_name: str
    analysis: str

stock_searcher = Agent(
    name="Stock Searcher",
    model=OpenAIChat("gpt-5-mini"),
    output_schema=StockAnalysis,
    role="Searches the web for information on a stock.",
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com", "wsj.com"],
            text=False,
            highlights=False,
            show_results=True,
        )
    ],
)

class CompanyAnalysis(BaseModel):
    company_name: str
    analysis: str

company_info_agent = Agent(
    name="Company Info Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a stock.",
    output_schema=CompanyAnalysis,
    tools=[
        ExaTools(
            include_domains=["cnbc.com", "reuters.com", "bloomberg.com", "wsj.com"],
            text=False,
            highlights=False,
            show_results=True,
        )
    ],
)

class StockReport(BaseModel):
    symbol: str
    company_name: str
    analysis: str

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
    show_members_responses=True,
)

---

## Check the optimized memory

**URL:** llms-txt#check-the-optimized-memory

**Contents:**
- Async Usage

if preview:
    print(f"Original memories: {len(memory_manager.get_user_memories('user_123'))}")
    print(f"Optimized to: {len(preview)} memories")
    print(f"Content: {preview[0].memory}")
python  theme={null}
import asyncio
from agno.db.postgres import PostgresDb
from agno.memory import MemoryManager
from agno.models.openai import OpenAIChat

async def optimize_user_memories():
    db = PostgresDb(db_url="postgresql://...")
    memory_manager = MemoryManager(
        db=db,
        model=OpenAIChat(id="gpt-4o-mini"),
    )
    
    # Async optimization
    optimized = await memory_manager.aoptimize_memories(
        user_id="user_123",
        apply=True,
    )
    
    return optimized

**Examples:**

Example 1 (unknown):
```unknown
## Async Usage

For async applications, use `aoptimize_memories`:
```

---

## AgentOS Security

**URL:** llms-txt#agentos-security

**Contents:**
- Overview
- Generate a Security Key
- Security Key Authentication
  - macOS / Linux (bash or zsh)
  - Docker Compose
  - Key Rotation
  - Security Best Practices
  - Troubleshooting
- JWT Authentication

Source: https://docs.agno.com/agent-os/security

Learn how to secure your AgentOS instance with a security key

AgentOS supports bearer-token authentication to secure your instance. When a Security Key is configured, all API routes require an `Authorization: Bearer <token>` header for access. Without a key configured, authentication is disabled.

<Tip>
  You can generate a security key from the AgentOS Control Plane, which also enables secure communication between your AgentOS and the Control Plane.
</Tip>

## Generate a Security Key

From the AgentOS control plane, generate a security key or set your own.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/xm93WWN8gg4nzCGE/videos/agentos-security-key.mp4?fit=max&auto=format&n=xm93WWN8gg4nzCGE&q=85&s=0a87c2a894982a3eb075fe282a21c491" type="video/mp4" data-path="videos/agentos-security-key.mp4" />
  </video>
</Frame>

<Tip>
  You can also create your own security key and set it on the AgentOS UI.
</Tip>

## Security Key Authentication

Set the `OS_SECURITY_KEY` environment variable where your AgentOS server runs. When present, the server automatically enforces bearer authentication on all API routes.

### macOS / Linux (bash or zsh)

<Note>
  **How it works**: AgentOS reads `OS_SECURITY_KEY` into the AgentOS router's
  internal authorization logic. If configured, requests without a valid
  `Authorization: Bearer` header return `401 Unauthorized`.
</Note>

1. In the UI, click the **Generate** icon next to "Security Key" to generate a new value
2. Update the server's `OS_SECURITY_KEY` environment variable and reload/redeploy AgentOS
3. Update all clients, workers, and CI/CD systems that call the AgentOS API

### Security Best Practices

* **Environment Isolation**: Use different keys per environment with least-privilege distribution
* **Code Safety**: Never commit keys to version control or print them in logs

* **401 Unauthorized**: Verify the header format is exactly `Authorization: Bearer <key>` and that the server has `OS_SECURITY_KEY` configured
* **Local vs Production**: Confirm your local shell exported `OS_SECURITY_KEY` before starting the application
* **Post-Rotation Failures**: Ensure all clients received the new key. Restart CI/CD runners that may cache environment variables
* **Connection Issues**: Check that your AgentOS instance is running and accessible at the configured endpoint

## JWT Authentication

AgentOS provides a middleware solution for custom JWT authentication.

Learn more about [JWT Middleware](/agent-os/middleware/jwt)

<Check>
  Although the JWT Middleware is already powerful feature, Agno is working on further extending authentication capabilities and better role-based access control in AgentOS.
</Check>

**Examples:**

Example 1 (unknown):
```unknown
### Docker Compose
```

---

## OpenLIT

**URL:** llms-txt#openlit

**Contents:**
- Integrating Agno with OpenLIT
- Prerequisites
- Sending Traces to OpenLIT
  - Example: Basic Agent Setup

Source: https://docs.agno.com/integrations/observability/openlit

Integrate Agno with OpenLIT for OpenTelemetry-native observability, tracing, and monitoring of your AI agents.

## Integrating Agno with OpenLIT

[OpenLIT](https://github.com/openlit/openlit) is an open-source, self-hosted, OpenTelemetry-native platform for a continuous feedback loop for testing, tracing, and fixing AI agents. By integrating Agno with OpenLIT, you can automatically instrument your agents to gain full visibility into LLM calls, tool usage, costs, performance metrics, and errors.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Deploy OpenLIT**

OpenLIT is open-source and self-hosted. Quick start with Docker:

Access the dashboard at `http://127.0.0.1:3000` with default credentials (username: `user@openlit.io`, password: `openlituser`).

**Other Deployment Options:**

For production deployments, Kubernetes with Helm, or other infrastructure setups, see the [OpenLIT Installation Guide](https://docs.openlit.io/latest/openlit/installation) for detailed instructions on:

* Kubernetes deployment with Helm charts
   * Custom Docker configurations
   * Reusing existing ClickHouse or OpenTelemetry Collector infrastructure
   * OpenLIT Operator for zero-code instrumentation in Kubernetes

3. **Set Environment Variables (Optional)**

Configure the OTLP endpoint based on your deployment:

## Sending Traces to OpenLIT

### Example: Basic Agent Setup

This example demonstrates how to instrument your Agno agent with OpenLIT for automatic tracing.

```python  theme={null}
import openlit
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

**Examples:**

Example 1 (unknown):
```unknown
2. **Deploy OpenLIT**

   OpenLIT is open-source and self-hosted. Quick start with Docker:
```

Example 2 (unknown):
```unknown
Access the dashboard at `http://127.0.0.1:3000` with default credentials (username: `user@openlit.io`, password: `openlituser`).

   **Other Deployment Options:**

   For production deployments, Kubernetes with Helm, or other infrastructure setups, see the [OpenLIT Installation Guide](https://docs.openlit.io/latest/openlit/installation) for detailed instructions on:

   * Kubernetes deployment with Helm charts
   * Custom Docker configurations
   * Reusing existing ClickHouse or OpenTelemetry Collector infrastructure
   * OpenLIT Operator for zero-code instrumentation in Kubernetes

3. **Set Environment Variables (Optional)**

   Configure the OTLP endpoint based on your deployment:
```

Example 3 (unknown):
```unknown
## Sending Traces to OpenLIT

### Example: Basic Agent Setup

This example demonstrates how to instrument your Agno agent with OpenLIT for automatic tracing.
```

---

## Video Generation

**URL:** llms-txt#video-generation

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/15-video-generation

This example shows how to create an AI agent that generates videos using ModelsLabs.
You can use this agent to create various types of short videos, from animated scenes
to creative visual stories.

Example prompts to try:

* "Create a serene video of waves crashing on a beach at sunset"
* "Generate a magical video of butterflies flying in a enchanted forest"
* "Create a timelapse of a blooming flower in a garden"
* "Generate a video of northern lights dancing in the night sky"

```python video_generation.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

---

## Markdown Reader Async

**URL:** llms-txt#markdown-reader-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/readers/usage/markdown-reader-async

The **Markdown Reader** with asynchronous processing allows you to handle Markdown files efficiently and integrate them with knowledge bases.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Snippet file="run-pgvector-step.mdx" />

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add embedding to database

**URL:** llms-txt#add-embedding-to-database

embeddings = JinaEmbedder(id="jina-embeddings-v3").get_embedding("The quick brown fox jumps over the lazy dog.")

---

## Define tools to manage a shopping list in workflow session state

**URL:** llms-txt#define-tools-to-manage-a-shopping-list-in-workflow-session-state

def add_item(run_context: RunContext, item: str) -> str:
    """Add an item to the shopping list in workflow session state.

Args:
        item (str): The item to add to the shopping list
    """
    if not run_context.session_state:
        run_context.session_state = {}

# Check if item already exists (case-insensitive)
    existing_items = [
        existing_item.lower() for existing_item in run_context.session_state["shopping_list"]
    ]
    if item.lower() not in existing_items:
        run_context.session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list."
    else:
        return f"'{item}' is already in the shopping list."

def remove_item(run_context: RunContext, item: str) -> str:
    """Remove an item from the shopping list in workflow session state.

Args:
        item (str): The item to remove from the shopping list
    """
    if not run_context.session_state:
        run_context.session_state = {}

if len(run_context.session_state["shopping_list"]) == 0:
        return f"Shopping list is empty. Cannot remove '{item}'."

# Find and remove item (case-insensitive)
    shopping_list = run_context.session_state["shopping_list"]
    for i, existing_item in enumerate(shopping_list):
        if existing_item.lower() == item.lower():
            removed_item = shopping_list.pop(i)
            return f"Removed '{removed_item}' from the shopping list."

return f"'{item}' not found in the shopping list."

def remove_all_items(run_context: RunContext) -> str:
    """Remove all items from the shopping list in workflow session state."""
    if not run_context.session_state:
        run_context.session_state = {}

run_context.session_state["shopping_list"] = []
    return "Removed all items from the shopping list."

def list_items(run_context: RunContext) -> str:
    """List all items in the shopping list from workflow session state."""
    if not run_context.session_state:
        run_context.session_state = {}

if len(run_context.session_state["shopping_list"]) == 0:
        return "Shopping list is empty."

items = run_context.session_state["shopping_list"]
    items_str = "\n".join([f"- {item}" for item in items])
    return f"Shopping list:\n{items_str}"

---

## Arxiv Reader

**URL:** llms-txt#arxiv-reader

Source: https://docs.agno.com/reference/knowledge/reader/arxiv

ArxivReader is a reader class that allows you to read papers from the Arxiv API.

<Snippet file="arxiv-reader-reference.mdx" />

---

## Set up SQL storage for the agent's data

**URL:** llms-txt#set-up-sql-storage-for-the-agent's-data

db = SqliteDb(db_file="data.db")

---

## VoyageAI

**URL:** llms-txt#voyageai

Source: https://docs.agno.com/reference/knowledge/embedder/voyageai

VoyageAI Embedder is a class that allows you to embed documents using VoyageAI's embedding models, which are specifically designed for high-performance text embeddings.

<Snippet file="embedder-voyageai-reference.mdx" />

---

## Initialize LangWatch and instrument Agno

**URL:** llms-txt#initialize-langwatch-and-instrument-agno

langwatch.setup(instrumentors=[AgnoInstrumentor()])

---

## 3. Generate Video Analysis

**URL:** llms-txt#3.-generate-video-analysis

response = agent.run(query, videos=[Video(filepath=video_path)])

---

## AgentOS Demo

**URL:** llms-txt#agentos-demo

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/demo

AgentOS demo with agents and teams

Here is a full example of an AgentOS with multiple agents and teams. It also shows how to instantiate agents with a database, knowledge base, and tools.

```python cookbook/agent_os/demo.py theme={null}
"""
AgentOS Demo

Prerequisites:
pip install -U fastapi uvicorn sqlalchemy pgvector psycopg openai ddgs mcp
"""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.mcp import MCPTools
from agno.vectordb.pgvector import PgVector

---

## RAG with Sentence Transformer Reranker

**URL:** llms-txt#rag-with-sentence-transformer-reranker

**Contents:**
- Code
- Setup Instructions:
  - 1. Install Dependencies
  - 2. Start the Postgres Server with pgvector
  - 3. Run the example
- Usage

Source: https://docs.agno.com/basics/knowledge/agents/usage/rag-sentence-transformer

This example demonstrates Agentic RAG using Sentence Transformer Reranker with multilingual data for improved search relevance.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Start Postgres with pgvector">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/rag" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start Postgres with pgvector">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Structured Output With Tool Use

**URL:** llms-txt#structured-output-with-tool-use

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/structured-output-with-tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## OpenAI Moderation Guardrail

**URL:** llms-txt#openai-moderation-guardrail

Source: https://docs.agno.com/basics/guardrails/usage/agent/openai-moderation

This example demonstrates how to use Agno's built-in OpenAI moderation guardrail to detect and block content that violates OpenAI's content policy.

This example demonstrates how to use Agno's built-in OpenAI moderation guardrail with an Agent.

This example shows how to:

1. Detect and block content that violates OpenAI's content policy
2. Handle both text and image content moderation
3. Configure moderation for specific categories
4. Use both sync and async moderation checks
5. Customize moderation models and sensitivity settings

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## https://api.telegram.org/bot/<your-bot-token>/getUpdates

**URL:** llms-txt#https://api.telegram.org/bot/<your-bot-token>/getupdates

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

telegram_token = "<enter-your-bot-token>"
chat_id = "<enter-your-chat-id>"

agent = Agent(
    name="telegram",
    tools=[TelegramTools(token=telegram_token, chat_id=chat_id)],
)

agent.print_response("Send message to telegram chat a paragraph about the moon")
```

| Parameter             | Type              | Default | Description                                                                               |
| --------------------- | ----------------- | ------- | ----------------------------------------------------------------------------------------- |
| `token`               | `Optional[str]`   | `None`  | Telegram Bot API token. If not provided, will check TELEGRAM\_TOKEN environment variable. |
| `chat_id`             | `Union[str, int]` | -       | The ID of the chat to send messages to.                                                   |
| `enable_send_message` | `bool`            | `True`  | Enable the send\_message functionality.                                                   |
| `all`                 | `bool`            | `False` | Enable all functionality.                                                                 |

| Function       | Description                                                                                                                                                         |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `send_message` | Sends a message to the specified Telegram chat. Takes a message string as input and returns the API response as text. If an error occurs, returns an error message. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/telegram.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/telegram_tools.py)

---

## Initialize the A2A interface specifying the agents to expose

**URL:** llms-txt#initialize-the-a2a-interface-specifying-the-agents-to-expose

**Contents:**
- A2A API
- Developer Resources

a2a = A2A(agents=[agent])

agent_os = AgentOS(
    agents=[agent],
    interfaces=[a2a], # Pass the A2A interface to the AgentOS using the `interfaces` parameter
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="a2a-interface-initialization:app", reload=True)
```

The A2A interface accepts A2A-compatible requests to run agents, teams, and workflows. Responses are returned in A2A-compatible format.

See the [A2A API reference](/reference-api/schema/a2a/stream-message) for more details.

## Developer Resources

* View [AgentOS Reference](/reference/agent-os/agent-os)
* View [A2A Documentation](https://a2a-protocol.org/latest/)
* View [Examples](/agent-os/usage/interfaces/a2a/basic)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agent_os/interfaces/a2a)

---

## Audio Input-Output Agent

**URL:** llms-txt#audio-input-output-agent

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/16-audio-agent

This example shows how to create an AI agent that can process audio input and generate audio responses. You can use this agent for various voice-based interactions, from analyzing speech content to generating natural-sounding responses.

Example audio interactions to try:

* Upload a recording of a conversation for analysis
* Have the agent respond to questions with voice output
* Process different languages and accents
* Analyze tone and emotion in speech

```python audio_input_output.py theme={null}
from textwrap import dedent

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

---

## Basic Agent

**URL:** llms-txt#basic-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/vercel/usage/basic

```python cookbook/models/vercel/basic.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.vercel import V0

agent = Agent(model=V0(id="v0-1.0-md"), markdown=True)

---

## Web Search Reader Async

**URL:** llms-txt#web-search-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/web-search-reader-async

The **Web Search Reader** searches and reads web search results, converting them into vector embeddings for your knowledge base.

```python examples/basics/knowledge/readers/web_search_reader_async.py theme={null}
import asyncio

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.web_search_reader import WebSearchReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(id="web-search-db", db_url=db_url)

vector_db = PgVector(
    db_url=db_url,
    table_name="web_search_documents",
)
knowledge = Knowledge(
    name="Web Search Documents",
    contents_db=db,
    vector_db=vector_db,
)

---

## Create an agent with GPT-4o

**URL:** llms-txt#create-an-agent-with-gpt-4o

agent = Agent(
    model=LiteLLM(
        id="gpt-5-mini",  # Model ID to use
        name="LiteLLM",  # Optional display name
    ),
    markdown=True,
)

---

## What are Vector Databases?

**URL:** llms-txt#what-are-vector-databases?

**Contents:**
- Supported Vector Databases
- Popular Choices by Use Case
- Next Steps

Source: https://docs.agno.com/basics/vectordb/overview

Vector databases enable us to store information as embeddings and search for "results similar" to our input query using cosine similarity or full text search. These results are then provided to the Agent as context so it can respond in a context-aware manner using Retrieval Augmented Generation (RAG).

Here's how vector databases are used with Agents:

<Steps>
  <Step title="Chunk the information">
    Break down the knowledge into smaller chunks to ensure our search query
    returns only relevant results.
  </Step>

<Step title="Load the knowledge base">
    Convert the chunks into embedding vectors and store them in a vector
    database.
  </Step>

<Step title="Search the knowledge base">
    When the user sends a message, we convert the input message into an
    embedding and "search" for nearest neighbors in the vector database.
  </Step>
</Steps>

Many vector databases also support hybrid search, which combines the power of vector similarity search with traditional keyword-based search. This approach can significantly improve the relevance and accuracy of search results, especially for complex queries or when dealing with diverse types of data.

Hybrid search typically works by:

1. Performing a vector similarity search to find semantically similar content.
2. Conducting a keyword-based search to identify exact or close matches.
3. Combining the results using a weighted approach to provide the most relevant information.

This capability allows for more flexible and powerful querying, often yielding better results than either method alone.

<Card title="âš¡ Asynchronous Operations">
  <p>Several vector databases support asynchronous operations, offering improved performance through non-blocking operations, concurrent processing, reduced latency, and seamless integration with FastAPI and async agents.</p>

<Tip className="mt-4">
    When building with Agno, use the <code>aload</code> methods for async knowledge base loading in production environments.
  </Tip>
</Card>

## Supported Vector Databases

The following VectorDb are currently supported:

<CardGroup cols={3}>
  <Card title="Azure Cosmos DB MongoDB vCore" icon="database" iconType="duotone" href="/integrations/vectordb/azure_cosmos_mongodb/overview">
    Azure Cosmos DB with MongoDB vCore vector search
  </Card>

<Card title="Cassandra" icon="database" iconType="duotone" href="/integrations/vectordb/cassandra/overview">
    Apache Cassandra vector database
  </Card>

<Card title="Chroma" icon="database" iconType="duotone" href="/integrations/vectordb/chroma/overview">
    Open-source embedding database
  </Card>

<Card title="ClickHouse" icon="database" iconType="duotone" href="/integrations/vectordb/clickhouse/overview">
    Fast analytical database with vector search
  </Card>

<Card title="Couchbase" icon="database" iconType="duotone" href="/integrations/vectordb/couchbase/overview">
    NoSQL database with vector search\*
  </Card>

<Card title="LanceDB" icon="database" iconType="duotone" href="/integrations/vectordb/lancedb/overview">
    Fast, local vector database\*
  </Card>

<Card title="LangChain" icon="link" iconType="duotone" href="/integrations/vectordb/langchain/overview">
    LangChain vector store integration
  </Card>

<Card title="LightRAG" icon="lightbulb" iconType="duotone" href="/integrations/vectordb/lightrag/overview">
    Lightweight RAG framework
  </Card>

<Card title="LlamaIndex" icon="robot" iconType="duotone" href="/integrations/vectordb/llamaindex/overview" hidden>
    LlamaIndex vector store integration
  </Card>

<Card title="Milvus" icon="database" iconType="duotone" href="/integrations/vectordb/milvus/overview">
    Open-source vector database
  </Card>

<Card title="MongoDB" icon="database" iconType="duotone" href="/integrations/vectordb/mongodb/overview">
    MongoDB Atlas vector search
  </Card>

<Card title="PgVector" icon="database" iconType="duotone" href="/integrations/vectordb/pgvector/overview">
    PostgreSQL with vector extension\*
  </Card>

<Card title="Pinecone" icon="cloud" iconType="duotone" href="/integrations/vectordb/pinecone/overview">
    Managed vector database service\*
  </Card>

<Card title="Qdrant" icon="database" iconType="duotone" href="/integrations/vectordb/qdrant/overview">
    High-performance vector database
  </Card>

<Card title="Redis" icon="database" iconType="duotone" href="/integrations/vectordb/redis/overview">
    Redis with vector search capabilities
  </Card>

<Card title="SingleStore" icon="database" iconType="duotone" href="/integrations/vectordb/singlestore/overview">
    Real-time analytics database
  </Card>

<Card title="SurrealDB" icon="database" iconType="duotone" href="/integrations/vectordb/surrealdb/overview">
    SurrealDB vector database
  </Card>

<Card title="Upstash" icon="cloud" iconType="duotone" href="/integrations/vectordb/upstash/overview">
    Serverless Redis with vector search
  </Card>

<Card title="Weaviate" icon="database" iconType="duotone" href="/integrations/vectordb/weaviate/overview">
    Open-source vector database
  </Card>
</CardGroup>

\*hybrid search supported

Each of these databases has its own strengths and features, including varying levels of support for hybrid search and async operations. Be sure to check the specific documentation for each to understand how to best leverage their capabilities in your projects.

## Popular Choices by Use Case

<CardGroup cols={3}>
  <Card title="Development & Testing" icon="laptop-code" href="/integrations/vectordb/lancedb/overview">
    **LanceDB** - Fast, local, no setup required
  </Card>

<Card title="Production at Scale" icon="server" href="/integrations/vectordb/pgvector/overview">
    **PgVector** - Reliable, scalable, full SQL support
  </Card>

<Card title="Managed Service" icon="cloud" href="/integrations/vectordb/pinecone/overview">
    **Pinecone** - Fully managed, no operations overhead
  </Card>

<Card title="High Performance" icon="gauge" href="/integrations/vectordb/qdrant/overview">
    **Qdrant** - Optimized for speed and advanced features
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Getting Started" icon="rocket" href="/basics/knowledge/getting-started">
    Build your first knowledge base with a vector database
  </Card>

<Card title="Embeddings" icon="vector-square" href="/basics/knowledge/embedder/overview">
    Learn about creating vector representations of your content
  </Card>

<Card title="Search & Retrieval" icon="magnifying-glass" href="/basics/knowledge/search-and-retrieval/overview">
    Understand how vector search works with your data
  </Card>

<Card title="Performance Tips" icon="gauge" href="/basics/knowledge/performance-tips">
    Optimize your vector database for speed and scale
  </Card>
</CardGroup>

---

## === MULTI-STEP CONDITION STEPS ===

**URL:** llms-txt#===-multi-step-condition-steps-===

deep_exa_analysis_step = Step(
    name="DeepExaAnalysis",
    description="Conduct deep analysis using Exa search capabilities",
    agent=exa_agent,
)

trend_analysis_step = Step(
    name="TrendAnalysis",
    description="Analyze trends and patterns from the research data",
    agent=trend_analyzer_agent,
)

fact_verification_step = Step(
    name="FactVerification",
    description="Verify facts and cross-reference information",
    agent=fact_checker_agent,
)

---

## Run agent as an interactive CLI app

**URL:** llms-txt#run-agent-as-an-interactive-cli-app

agent.cli_app(stream=True)
```

---

## Retry Function Call

**URL:** llms-txt#retry-function-call

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/getting-started/11-retry-function-call

This example shows how to retry a function call if it fails or you do not like the output. This is useful for:

* Handling temporary failures
* Improving output quality through retries
* Implementing human-in-the-loop validation

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Because we are evaluating an async function, we use the arun method.

**URL:** llms-txt#because-we-are-evaluating-an-async-function,-we-use-the-arun-method.

**Contents:**
- Agent Performace with Memory Updates

asyncio.run(performance_eval.arun(print_summary=True, print_results=True))
python memory_performance.py theme={null}
"""Run `pip install openai agno memory_profiler` to install dependencies."""

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
## Agent Performace with Memory Updates

Test agent performance with memory updates:
```

---

## Pdf Input Url

**URL:** llms-txt#pdf-input-url

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/pdf-input-url

```python cookbook/models/openai/responses/pdf_input_url.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.media import File
from agno.models.openai.responses import OpenAIResponses

---

## Execute team workflow - all agent interactions are traced

**URL:** llms-txt#execute-team-workflow---all-agent-interactions-are-traced

**Contents:**
  - Example: Custom Tracer Configuration

finance_team.print_response("Analyze Apple (AAPL) investment potential")
python  theme={null}
import openlit
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown
### Example: Custom Tracer Configuration

For advanced use cases with custom OpenTelemetry configuration:
```

---

## Find multiple document types

**URL:** llms-txt#find-multiple-document-types

IN("document_type", ["policy", "guideline", "procedure"])
python  theme={null}
from agno.filters import GT, LT

**Examples:**

Example 1 (unknown):
```unknown
#### GT (Greater Than) & LT (Less Than)

Match content based on numeric comparisons.
```

---

## Create agent with Word skills

**URL:** llms-txt#create-agent-with-word-skills

document_agent = Agent(
    name="Document Creator",
    model=Claude(
        id="claude-sonnet-4-5-20250929",
        skills=[
            {"type": "anthropic", "skill_id": "docx", "version": "latest"}
        ],
    ),
    instructions=[
        "You are a professional document writer with access to Word document skills.",
        "Create well-structured documents with clear sections and professional formatting.",
        "Use headings, lists, and tables where appropriate.",
    ],
    markdown=True,
)

---

## Requesty

**URL:** llms-txt#requesty

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/requesty

The Requesty model provides access to models through Requesty AI.

| Parameter               | Type            | Default                           | Description                                                       |
| ----------------------- | --------------- | --------------------------------- | ----------------------------------------------------------------- |
| `id`                    | `str`           | `"openai/gpt-4.1"`                | The id of the model to use through Requesty                       |
| `name`                  | `str`           | `"Requesty"`                      | The name of the model                                             |
| `provider`              | `str`           | `"Requesty"`                      | The provider of the model                                         |
| `api_key`               | `Optional[str]` | `None`                            | The API key for Requesty (defaults to REQUESTY\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://router.requesty.ai/v1"` | The base URL for the Requesty API                                 |
| `max_tokens`            | `int`           | `1024`                            | The maximum number of tokens to generate                          |
| `retries`               | `int`           | `0`                               | Number of retries to attempt before raising a ModelProviderError  |
| `delay_between_retries` | `int`           | `1`                               | Delay between retries, in seconds                                 |
| `exponential_backoff`   | `bool`          | `False`                           | If True, the delay between retries is doubled each time           |

Requesty extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Filter by time range

**URL:** llms-txt#filter-by-time-range

**Contents:**
- Span Functions
  - `db.get_span()`
  - `db.get_spans()`

from datetime import datetime, timedelta, timezone

now = datetime.now(timezone.utc)
traces, count = db.get_traces(
    start_time=now - timedelta(hours=1),
    end_time=now,
    limit=100
)
python  theme={null}
span = db.get_span(span_id="xyz789...")
if span:
    print(f"{span.name}: {span.duration_ms}ms")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Parameters:**

| Parameter     | Type                 | Default | Description                               |
| ------------- | -------------------- | ------- | ----------------------------------------- |
| `run_id`      | `Optional[str]`      | `None`  | Filter by run ID                          |
| `session_id`  | `Optional[str]`      | `None`  | Filter by session ID                      |
| `user_id`     | `Optional[str]`      | `None`  | Filter by user ID                         |
| `agent_id`    | `Optional[str]`      | `None`  | Filter by agent ID                        |
| `team_id`     | `Optional[str]`      | `None`  | Filter by team ID                         |
| `workflow_id` | `Optional[str]`      | `None`  | Filter by workflow ID                     |
| `status`      | `Optional[str]`      | `None`  | Filter by status (`OK`, `ERROR`, `UNSET`) |
| `start_time`  | `Optional[datetime]` | `None`  | Filter traces after this time             |
| `end_time`    | `Optional[datetime]` | `None`  | Filter traces before this time            |
| `limit`       | `Optional[int]`      | `20`    | Max traces to return                      |
| `page`        | `Optional[int]`      | `1`     | Page number for pagination                |

**Returns:** `tuple[List[Trace], int]` - (traces, total\_count)

## Span Functions

### `db.get_span()`

Get a single span by ID.
```

Example 2 (unknown):
```unknown
**Parameters:**

| Parameter | Type  | Description            |
| --------- | ----- | ---------------------- |
| `span_id` | `str` | Unique span identifier |

**Returns:** `Span` or `None`

See the reference for [Span](/reference/tracing/span) for more information.

### `db.get_spans()`

Get multiple spans for a trace or parent.
```

---

## Knowledge Contents DB

**URL:** llms-txt#knowledge-contents-db

**Contents:**
- What is Contents DB?
- Why Use ContentsDB?
  - Content Visibility and Control
  - Powerful Management Capabilities
  - Required for AgentOS
- Setting Up ContentsDB
  - Choose Your Database
  - Basic Setup Example

Source: https://docs.agno.com/basics/knowledge/content-db

Learn how to add a Content DB to your Knowledge.

The Contents Database (Contents DB) is an optional component that enhances Knowledge with content tracking and management features. It acts as a control layer that maintains detailed records of all content added to your `Knowledge`.

## What is Contents DB?

Contents DB is a table in your database that keeps track of what content you've added to your Knowledge base.
While your vector database stores the actual content for search, this table tracks what you've added, when you added it, and its processing status.

* **Vector Database**: Stores embeddings and chunks for semantic search
* **Contents Database**: Tracks content metadata, status, and when coupled with [AgentOS Knowledge](/agent-os/features/knowledge-management), provides management of your knowledge via API.

<Note>
  You need a contents database with your Knowledge base to use [agentic filtering](/basics/knowledge/filters/agentic-filters).
</Note>

## Why Use ContentsDB?

### Content Visibility and Control

Without ContentsDB, managing your knowledge and vectors is difficult - you can search it, but you can't manage individual pieces of content or alter all the vectors created from a single piece of content.

With ContentsDB, you gain full visibility:

* See all content that has been added
* Track processing status of each item
* View metadata and file information
* Monitor access patterns and usage

### Powerful Management Capabilities

* **Edit names, descriptions and metadata** for existing content
* **Delete specific content** and automatically clean up associated vectors
* **Update content** without rebuilding the entire knowledge base
* **Batch operations** for managing multiple content items
* **Status tracking** to monitor processing success/failure

### Required for AgentOS

If you're using AgentOS, ContentsDB is **mandatory** for the Knowledge page functionality. The AgentOS web interface relies on ContentsDB to display and manage your knowledge content.

## Setting Up ContentsDB

### Choose Your Database

Agno supports multiple database backends for ContentsDB:

* **[PostgreSQL](/integrations/database/postgres/overview)** - Recommended for production
* **[SQLite](/integrations/database/sqlite/overview)** - Great for development and single-user applications
* **[MySQL](/integrations/database/mysql/overview)** - Enterprise-ready relational database
* **[MongoDB](/integrations/database/mongo/overview)** - Document-based NoSQL option
* **[Redis](/integrations/database/redis/overview)** - In-memory option for high performance
* **[In-Memory](/integrations/database/in-memory/overview)** - Temporary storage for testing
* **Cloud Options** - [DynamoDB](/integrations/database/dynamodb/overview), [Firestore](/integrations/database/firestore/overview), [GCS](/integrations/database/gcs/overview)

### Basic Setup Example

```python  theme={null}
from agno.knowledge import Knowledge
from agno.db.postgres import PostgresDb
from agno.vectordb.pgvector import PgVector

---

## for chunk in run_response:

**URL:** llms-txt#for-chunk-in-run_response:

---

## Filter by multiple fields (all must match)

**URL:** llms-txt#filter-by-multiple-fields-(all-must-match)

results = knowledge.search(
    "deployment process",
    filters={"department": "engineering", "type": "documentation", "status": "published"}
)

---

## Ollama with Reasoning Tools

**URL:** llms-txt#ollama-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/ollama-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create agent with LangDB model (uses environment variables automatically)

**URL:** llms-txt#create-agent-with-langdb-model-(uses-environment-variables-automatically)

agent = Agent(
    name="Web Research Agent",
    model=LangDB(id="openai/gpt-4.1"),
    tools=[DuckDuckGoTools()],
    instructions="Answer questions using web search and provide comprehensive information"
)

---

## 2. Get the token from BotFather.

**URL:** llms-txt#2.-get-the-token-from-botfather.

---

## MongoDB for document-based storage

**URL:** llms-txt#mongodb-for-document-based-storage

from agno.db.mongo import MongoDb
contents_db = MongoDb(
    uri="mongodb://localhost:27017",
    database="agno_db"
)

---

## Searxng

**URL:** llms-txt#searxng

**Contents:**
- Example

Source: https://docs.agno.com/integrations/toolkits/search/searxng

**Searxng** enables an Agent to search the web for a query, scrape a website, or crawl a website.

```python cookbook/tools/searxng_tools.py theme={null}
from agno.agent import Agent
from agno.tools.searxng import SearxngTools

---

## All agent runs will use caching

**URL:** llms-txt#all-agent-runs-will-use-caching

**Contents:**
- Usage with Teams

agent.run("Your query")
python  theme={null}
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
## Usage with Teams

Response caching works with `Team` as well. You can enable it on individual team members and the team leader model:
```

---

## Add Dependencies to Agent Context

**URL:** llms-txt#add-dependencies-to-agent-context

Source: https://docs.agno.com/basics/dependencies/agent/usage/add-dependencies-to-context

This example demonstrates how to create a context-aware agent that can access real-time HackerNews data through dependency injection, enabling the agent to provide current information.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Interact with the Agent so that it can learn about the user

**URL:** llms-txt#interact-with-the-agent-so-that-it-can-learn-about-the-user

agent.print_response("My name is John Billings")
agent.print_response("I live in NYC")
agent.print_response("I'm going to a concert tomorrow")

---

## run: RunOutput = movie_agent.run("New York")

**URL:** llms-txt#run:-runoutput-=-movie_agent.run("new-york")

**Contents:**
- Usage

bash  theme={null}
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U anthropic agno
    bash  theme={null}
    python structured_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Capturing Agent Response as Variable

**URL:** llms-txt#capturing-agent-response-as-variable

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/agent/usage/response-as-variable

This example demonstrates how to capture and work with agent responses as variables, enabling programmatic access to response data and metadata.

```python response_as_variable.py theme={null}
from typing import Iterator  # noqa
from rich.pretty import pprint
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        DuckDuckGoTools(
            stock_price=True,
            analyst_recommendations=True,
            company_info=True,
            company_news=True,
        )
    ],
    instructions=["Use tables where possible"],
    markdown=True,
)

run_response: RunOutput = agent.run("What is the stock price of NVDA")
pprint(run_response)

---

## Run the workflow

**URL:** llms-txt#run-the-workflow

**Contents:**
- What to Expect
- Usage
- Next Steps

company_description_workflow.print_response(
    input=supplier_request,
)
bash  theme={null}
    export OPENAI_API_KEY=xxx
    export RESEND_API_KEY=xxx
    export FIRECRAWL_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai fastapi firecrawl-py resend markdown ddgs
    bash Mac theme={null}
      python cookbook/examples/workflows/company_description/run_workflow.py
      bash Windows theme={null}
      python cookbook/examples/workflows/company_description/run_workflow.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Modify `supplier_request` to analyze different companies
* Customize agent instructions in `prompts.py` to focus on specific analysis areas
* Adjust the profile template in `SUPPLIER_PROFILE_INSTRUCTIONS_GENERAL` for different report formats
* Explore [Workflows](/basics/workflows/overview) for advanced workflow patterns

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The workflow will simultaneously gather information from multiple sources (website crawling, web search, Wikipedia, competitor analysis) then generate a comprehensive supplier profile. You'll see progress indicators for each agent's data collection.

The final output is a structured markdown report delivered via email. The workflow caches all analysis results - if you request the same supplier again, it returns the cached profile instantly instead of re-running the expensive analysis pipeline.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Workflow">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create presentation

**URL:** llms-txt#create-presentation

prompt = (
    "Create a Q4 business review presentation with 5 slides:\n"
    "1. Title slide: 'Q4 2025 Business Review'\n"
    "2. Key metrics: Revenue $2.5M (â†‘25% YoY), 850 customers\n"
    "3. Major achievements: Product launch, new markets, team growth\n"
    "4. Challenges: Market competition, customer retention\n"
    "5. Q1 2026 goals: $3M revenue, 1000 customers, new features\n"
    "Save as 'q4_review.pptx'"
)

response = powerpoint_agent.run(prompt)
print(response.content)

---

## -*- Print memories

**URL:** llms-txt#-*--print-memories

if agent.db:
    pprint(agent.db.get_user_memories(user_id="test_user"))
    pprint(
        agent.db.get_session(
            session_id="test_session", session_type=SessionType.AGENT
        ).summary  # type: ignore
    )

---

## Use the agent to generate and print a response to a query, formatted in Markdown

**URL:** llms-txt#use-the-agent-to-generate-and-print-a-response-to-a-query,-formatted-in-markdown

**Contents:**
- Usage

agent.print_response(
    "What is the first step of making Gluai Buat Chi from the knowledge base?",
    markdown=True,
)
bash  theme={null}
    pip install -U agno lancedb ollama
    bash  theme={null}
    # Install and start Ollama
    # Pull required models
    ollama pull llama3.1:8b
    ollama pull nomic-embed-text
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch rag_with_lance_db_and_sqlite.py
    bash Mac theme={null}
      python rag_with_lance_db_and_sqlite.py
      bash Windows theme={null}
      python rag_with_lance_db_and_sqlite.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/rag" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Ollama">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Building Workflows

**URL:** llms-txt#building-workflows

**Contents:**
- Building Blocks
- How to make your first workflow?

Source: https://docs.agno.com/basics/workflows/building-workflows

Learn how to build your workflows.

Workflows are a powerful way to orchestrate your agents and teams. They are a series of steps that are executed in a flow that you control.

1. The **`Workflow`** class is the top-level orchestrator that manages the entire execution process.
2. **`Step`** is the fundamental unit of work in the workflow system. Each step encapsulates exactly one `executor` - either an `Agent`, a `Team`, or a custom Python function. This design ensures clarity and maintainability while preserving the individual characteristics of each executor.
3. **`Loop`** is a construct that allows you to execute one or more steps multiple times. This is useful when you need to repeat a set of steps until a certain condition is met.
4. **`Parallel`** is a construct that allows you to execute one or more steps in parallel. This is useful when you need to execute a set of steps concurrently with the outputs joined together.
5. **`Condition`** makes a step conditional based on criteria you specify.
6. **`Router`** allows you to specify which step(s) to execute next, effectively creating branching logic in your workflow.

<Note>
  When using a custom Python function as an executor for a step, `StepInput` and
  `StepOutput` provides standardized interfaces for data flow between steps:
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=25129f55e1ead21d513c25b51dca412d" alt="Workflows step IO flow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/workflows-step-io-flow-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=280&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=67a933194ccf6f39606314d48784927f 280w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=560&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=3edc1003ffbee7b7767b1a84720bc616 560w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=840&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=cd83f97d789ba6e0a1980e8d25759281 840w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=1100&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=f018b152972888a037a239342bde7602 1100w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=1650&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=bd9ba2232ddd6801f4ece40477975608 1650w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=2500&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=383ce2bcd2751e0b47a1304f797cf2d0 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=faa43206bfa64265fa4d32721a12216d" alt="Workflows step IO flow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/workflows-step-io-flow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=280&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=50c47b8c564021b246ba034bc331c982 280w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=560&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=430d80604ed6927b914c7e9c1fcf8aa6 560w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=840&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=60dfe1d25956d8895f87b215607466cf 840w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=1100&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=8ac4c55ff3bb0c2f17baf76f0bae48a8 1100w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=1650&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=1c4237aa3bf8581c52b97f51f0b524e5 1650w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=2500&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=9ae7ba15375a9bb435b1765646047770 2500w" />

## How to make your first workflow?

There are different types of patterns you can use to build your workflows.
For example you can combine agents, teams, and functions to build a workflow.

---

## Create function with strict mode enabled

**URL:** llms-txt#create-function-with-strict-mode-enabled

weather_tool = Function(
    name="get_weather",
    description="Get current weather information for a location",
    parameters={
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
            "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "Temperature unit",
            },
        },
        "required": ["location"],
        "additionalProperties": False,
    },
    strict=True,  # Enable strict mode for validated tool parameters
    entrypoint=get_weather,
)

---

## Image Agent

**URL:** llms-txt#image-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/xai/usage/image-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Count tokens after

**URL:** llms-txt#count-tokens-after

tokens_after = strategy.count_tokens(optimized)

---

## Define the structured message data

**URL:** llms-txt#define-the-structured-message-data

class MediaRequest(BaseModel):
    topic: str
    content_type: str  # "image" or "video"
    prompt: str
    style: Optional[str] = "realistic"
    duration: Optional[int] = None  # For video, duration in seconds
    resolution: Optional[str] = "1024x1024"  # For image resolution

---

## Sleep

**URL:** llms-txt#sleep

**Contents:**
- Example

Source: https://docs.agno.com/integrations/toolkits/local/sleep

The following agent will use the `sleep` tool to pause execution for a given number of seconds.

```python cookbook/tools/sleep_tools.py theme={null}
from agno.agent import Agent
from agno.tools.sleep import SleepTools

---

## LangChain Vector Database

**URL:** llms-txt#langchain-vector-database

Source: https://docs.agno.com/integrations/vectordb/langchain/overview

Learn how to use LangChain as a vector database for your Knowledge Base

---

## Rich, searchable metadata

**URL:** llms-txt#rich,-searchable-metadata

good_metadata = {
    "document_type": "policy",
    "department": "hr",
    "category": "benefits",
    "audience": "all_employees",
    "last_updated": "2024-01-15",
    "version": "2.1",
    "tags": ["health_insurance", "401k", "vacation"],
    "sensitivity": "internal"
}

---

## Extract and cut video segments

**URL:** llms-txt#extract-and-cut-video-segments

def extract_segments(response_text):
    import re

segments_pattern = r"\|\s*(\d+:\d+)\s*\|\s*(\d+:\d+)\s*\|\s*(.*?)\s*\|\s*(\d+)\s*\|"
    segments: list[dict] = []

for match in re.finditer(segments_pattern, str(response_text)):
        start_time = match.group(1)
        end_time = match.group(2)
        description = match.group(3)
        score = int(match.group(4))

start_seconds = sum(x * int(t) for x, t in zip([60, 1], start_time.split(":")))
        end_seconds = sum(x * int(t) for x, t in zip([60, 1], end_time.split(":")))
        duration = end_seconds - start_seconds

if 15 <= duration <= 60 and score > 7:
            output_path = output_dir / f"short_{len(segments) + 1}.mp4"

command = [
                "ffmpeg",
                "-ss",
                str(start_seconds),
                "-i",
                video_path,
                "-t",
                str(duration),
                "-vf",
                "scale=1080:1920,setsar=1:1",
                "-c:v",
                "libx264",
                "-c:a",
                "aac",
                "-y",
                str(output_path),
            ]

try:
                subprocess.run(command, check=True)
                segments.append(
                    {"path": output_path, "description": description, "score": score}
                )
            except subprocess.CalledProcessError:
                print(f"Failed to process segment: {start_time} - {end_time}")

logger.debug(f"{response.content}")

---

## Agent that uses a JSON schema output

**URL:** llms-txt#agent-that-uses-a-json-schema-output

**Contents:**
- Usage

agent = Agent(
    model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8", temperature=0.1),
    output_schema=MovieScript,
)

agent.print_response("New York")
bash  theme={null}
    export LLAMA_API_KEY=YOUR_API_KEY
    bash  theme={null}
    pip install llama-api-client agno
    bash Mac theme={null}
      python cookbook/models/meta/structured_output.py
      bash Windows theme={null}
      python cookbook/models/meta/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your LLAMA API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## SerperApi

**URL:** llms-txt#serperapi

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/serper

**SerperApiTools** enable an Agent to search Google for a query.

The following example requires an API key from [SerperApi](https://serper.dev/).

The following agent will search Google for the query: "Whats happening in the USA" and share results.

| Parameter               | Type   | Default | Description                               |
| ----------------------- | ------ | ------- | ----------------------------------------- |
| `api_key`               | `str`  | -       | API key for authentication purposes.      |
| `location`              | `str`  | `"us"`  | Location to search from.                  |
| `enable_search`         | `bool` | `True`  | Enable the search functionality.          |
| `enable_search_news`    | `bool` | `True`  | Enable the search\_news functionality.    |
| `enable_search_scholar` | `bool` | `True`  | Enable the search\_scholar functionality. |
| `enable_scrape_webpage` | `bool` | `True`  | Enable the scrape\_webpage functionality. |
| `all`                   | `bool` | `False` | Enable all functionality.                 |

| Function         | Description                                        |
| ---------------- | -------------------------------------------------- |
| `search_google`  | This function searches Google for a query.         |
| `search_news`    | This function searches Google News for a query.    |
| `search_scholar` | This function searches Google Scholar for a query. |
| `scrape_webpage` | This function scrapes a webpage for a query.       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/serper.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/serper_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will search Google for the query: "Whats happening in the USA" and share results.
```

---

## Get embeddings for a text

**URL:** llms-txt#get-embeddings-for-a-text

embeddings = LangDBEmbedder().get_embedding("Embed me")

---

## Large PDF gets broken into multiple documents

**URL:** llms-txt#large-pdf-gets-broken-into-multiple-documents

pdf_reader = PDFReader(chunk=True, chunk_size=1000)
documents = pdf_reader.read("large_document.pdf")

---

## Local mode

**URL:** llms-txt#local-mode

embedder = VLLMEmbedder(
    id="intfloat/e5-mistral-7b-instruct",
    dimensions=4096,
    enforce_eager=True,
    vllm_kwargs={
        "disable_sliding_window": True,
        "max_model_len": 4096,
    },
)

---

## Create MCPTools instance

**URL:** llms-txt#create-mcptools-instance

mcp_tools = MCPTools(
    transport="streamable-http",
    url="https://docs.agno.com/mcp"
)

---

## Get information about a user

**URL:** llms-txt#get-information-about-a-user

agent.print_response("Can you retrieve information about this user https://x.com/AgnoAgi ", markdown=True)

---

## You can also override the entire `system_message` for the memory manager

**URL:** llms-txt#you-can-also-override-the-entire-`system_message`-for-the-memory-manager

**Contents:**
- Usage

memory_manager = MemoryManager(
    model=OpenAIChat(id="gpt-5-mini"),
    additional_instructions="""
    IMPORTANT: Don't store any memories about the user's name. Just say "The User" instead of referencing the user's name.
    """,
    db=db,
)

john_doe_id = "john_doe@example.com"

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    memory_manager=memory_manager,
    enable_user_memories=True,
    user_id=john_doe_id,
)

agent.print_response(
    "My name is John Doe and I like to swim and play soccer.", stream=True
)

agent.print_response("I dont like to swim", stream=True)

memories = agent.get_user_memories(user_id=john_doe_id)

print("John Doe's memories:")
pprint(memories)

bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python cookbook/memory/04_custom_memory_manager.py
      bash Windows theme={null}
      python cookbook/memory/04_custom_memory_manager.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Langtrace

**URL:** llms-txt#langtrace

**Contents:**
- Integrating Agno with Langtrace
- Prerequisites
- Sending Traces to Langtrace

Source: https://docs.agno.com/integrations/observability/langtrace

Integrate Agno with Langtrace to send traces and gain insights into your agent's performance.

## Integrating Agno with Langtrace

Langtrace provides a powerful platform for tracing and monitoring AI model calls. By integrating Agno with Langtrace, you can gain insights into your agent's performance and behavior.

1. **Install Dependencies**

Ensure you have the necessary package installed:

2. **Create a Langtrace Account**

* Sign up for an account at [Langtrace](https://app.langtrace.ai/).
   * Obtain your API key from the Langtrace dashboard.

3. **Set Environment Variables**

Configure your environment with the Langtrace API key:

## Sending Traces to Langtrace

This example demonstrates how to instrument your Agno agent with Langtrace.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from langtrace_python_sdk import langtrace
from langtrace_python_sdk.utils.with_root_span import with_langtrace_root_span

**Examples:**

Example 1 (unknown):
```unknown
2. **Create a Langtrace Account**

   * Sign up for an account at [Langtrace](https://app.langtrace.ai/).
   * Obtain your API key from the Langtrace dashboard.

3. **Set Environment Variables**

   Configure your environment with the Langtrace API key:
```

Example 2 (unknown):
```unknown
## Sending Traces to Langtrace

This example demonstrates how to instrument your Agno agent with Langtrace.
```

---

## Hugging Face

**URL:** llms-txt#hugging-face

Source: https://docs.agno.com/reference/knowledge/embedder/huggingface

Hugging Face Embedder is a class that allows you to embed documents using any embedding model hosted on HuggingFace's Inference API.

<Snippet file="embedder-huggingface-reference.mdx" />

---

## Image Input Bytes Content

**URL:** llms-txt#image-input-bytes-content

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/image-input-bytes

```python cookbook/models/anthropic/image_input_bytes.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.anthropic.claude import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

---

## Example 3: Get the message history of a specific channel by channel ID

**URL:** llms-txt#example-3:-get-the-message-history-of-a-specific-channel-by-channel-id

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Get the last 10 messages from the channel 1231241", markdown=True)

| Parameter                    | Type   | Default | Description                                                     |
| ---------------------------- | ------ | ------- | --------------------------------------------------------------- |
| `token`                      | `str`  | `None`  | Slack API token for authentication                              |
| `enable_send_message`        | `bool` | `True`  | Enables functionality to send messages to Slack channels        |
| `enable_send_message_thread` | `bool` | `True`  | Enables functionality to send threaded messages                 |
| `enable_list_channels`       | `bool` | `True`  | Enables functionality to list available Slack channels          |
| `enable_get_channel_history` | `bool` | `True`  | Enables functionality to retrieve message history from channels |
| `all`                        | `bool` | `False` | Enables all functionality when set to True                      |

| Function              | Description                                         |
| --------------------- | --------------------------------------------------- |
| `send_message`        | Sends a message to a specified Slack channel        |
| `list_channels`       | Lists all available channels in the Slack workspace |
| `get_channel_history` | Retrieves message history from a specified channel  |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/slack.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/slack_tools.py)

---

## Create a client

**URL:** llms-txt#create-a-client

**Contents:**
- Usage

client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

surrealdb = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
    embedder=OpenAIEmbedder(),
)

def sync_demo():
    """Demonstrate synchronous usage of SurrealDb"""
    knowledge = Knowledge(
        vector_db=surrealdb,
    )

# Load data synchronously
    knowledge.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )

agent = Agent(knowledge=knowledge)
    agent.print_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )

if __name__ == "__main__":
    print("Running synchronous demo...")
    sync_demo()
bash  theme={null}
    pip install -U surrealdb pypdf openai agno
    bash  theme={null}
    docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root     
    bash Mac theme={null}
      python cookbook/knowledge/vector_db/surrealdb/surreal_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/surrealdb/surreal_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run SurrealDB">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Production: Scalable, battle-tested

**URL:** llms-txt#production:-scalable,-battle-tested

**Contents:**
  - 2. Skip Already-Processed Files

prod_db = PgVector(
    table_name="prod_knowledge",
    db_url="postgresql+psycopg://user:pass@db:5432/knowledge"
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Guidelines:**

* **LanceDB** for development and testing (no setup required)
* **PgVector** for production (up to 1M documents, need SQL features)
* **Pinecone** for managed services (no ops overhead, auto-scaling)

### 2. Skip Already-Processed Files

The single biggest speed-up when re-running ingestion:
```

---

## Weaviate Vector Database

**URL:** llms-txt#weaviate-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/weaviate/overview

Learn how to use Weaviate as a vector database for your Knowledge Base

Follow steps mentioned in [Weaviate setup guide](https://weaviate.io/developers/weaviate/quickstart) to setup Weaviate.

Install weaviate packages

```python agent_with_knowledge.py theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.search import SearchType
from agno.vectordb.weaviate import Distance, VectorIndex, Weaviate

vector_db = Weaviate(
    collection="recipes",
    search_type=SearchType.hybrid,
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=True,  # Set to False if using Weaviate Cloud and True if using local instance
)

**Examples:**

Example 1 (unknown):
```unknown
Run weaviate
```

Example 2 (unknown):
```unknown
or
```

Example 3 (unknown):
```unknown
## Example
```

---

## - `llama-3.3-70b-versatile` to generate the final response

**URL:** llms-txt#--`llama-3.3-70b-versatile`-to-generate-the-final-response

reasoning_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)

---

## URL-based reader selection

**URL:** llms-txt#url-based-reader-selection

**Contents:**
- Supported Readers
- Async Processing

reader = ReaderFactory.get_reader_for_url("https://youtube.com/watch?v=...")  # YouTubeReader
reader = ReaderFactory.get_reader_for_url("https://example.com/doc.pdf")     # PDFReader
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Supported Readers

The following readers are currently supported:

| Reader Name           | Description                                           |
| --------------------- | ----------------------------------------------------- |
| ArxivReader           | Fetches and processes academic papers from arXiv      |
| CSVReader             | Parses CSV files and converts rows to documents       |
| FieldLabeledCSVReader | Converts CSV rows to field-labeled text documents     |
| FirecrawlReader       | Uses Firecrawl API to scrape and crawl web content    |
| JSONReader            | Processes JSON files and converts them into documents |
| MarkdownReader        | Reads and parses Markdown files                       |
| PDFReader             | Reads and extracts text from PDF files                |
| PPTXReader            | Reads and extracts text from PowerPoint (.pptx) files |
| TextReader            | Handles plain text files                              |
| WebsiteReader         | Crawls entire websites following links recursively    |
| WebSearchReader       | Searches and reads web search results                 |
| WikipediaReader       | Searches and reads Wikipedia articles                 |
| YouTubeReader         | Extracts transcripts and metadata from YouTube videos |

## Async Processing

All readers support asynchronous processing for better performance:
```

---

## Legal Consultant

**URL:** llms-txt#legal-consultant

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/legal_consultant

This example demonstrates how to create a specialized AI agent that provides legal information and guidance based on a knowledge base of legal documents. The Legal Consultant agent is designed to help users understand legal concepts, consequences, and procedures by leveraging a vector database of legal content.

* **Legal Knowledge Base**: Integrates with a PostgreSQL vector database containing legal documents and resources
* **Document Processing**: Automatically ingests and indexes legal PDFs from authoritative sources (e.g., Department of Justice manuals)
* **Contextual Responses**: Provides relevant legal information with proper citations and sources
* **Professional Disclaimers**: Always clarifies that responses are for informational purposes only, not professional legal advice
* **Attorney Referrals**: Recommends consulting licensed attorneys for specific legal situations

* Legal research and education
* Understanding criminal penalties and consequences
* Learning about legal procedures and requirements
* Getting preliminary legal information before consulting professionals

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Basic Team Tracing

**URL:** llms-txt#basic-team-tracing

Source: https://docs.agno.com/basics/tracing/usage/basic-team-tracing

Learn how to trace your teams with Agno

This example shows how to enable tracing for a team. When tracing is enabled, all team operations, member agent runs, model calls, and tool executions are automatically captured and stored in your database.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run the team">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Direct Response with Team History

**URL:** llms-txt#direct-response-with-team-history

**Contents:**
- Usage

Source: https://docs.agno.com/basics/chat-history/team/usage/respond-directly-with-history

This example demonstrates a team where the team leader routes requests to the appropriate member, and the members respond directly to the user.

In addition, the team has access to the conversation history through `add_history_to_context=True`.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Performance with Memory Updates

**URL:** llms-txt#performance-with-memory-updates

Source: https://docs.agno.com/basics/evals/performance/usage/performance-with-memory

Exmaple showing how to evaluate performance when memory updates are involved.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Router function that selects between simple web research or deep tech research loop

**URL:** llms-txt#router-function-that-selects-between-simple-web-research-or-deep-tech-research-loop

def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

# Check if the topic requires deep tech research
    deep_tech_keywords = [
        "startup trends",
        "ai developments",
        "machine learning research",
        "programming languages",
        "developer tools",
        "silicon valley",
        "venture capital",
        "cryptocurrency analysis",
        "blockchain technology",
        "open source projects",
        "github trends",
        "tech industry",
        "software engineering",
    ]

# Check if it's a complex tech topic that needs deep research
    if any(keyword in topic for keyword in deep_tech_keywords) or (
        "tech" in topic and len(topic.split()) > 3
    ):
        print(
            f"ðŸ”¬ Deep tech topic detected: Using iterative research loop for '{topic}'"
        )
        return [deep_tech_research_loop]
    else:
        print(f"ðŸŒ Simple topic detected: Using basic web research for '{topic}'")
        return [research_web]

workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    print("=== Testing with deep tech topic ===")
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning and deep tech research trends"
    )
```

To checkout async version, see the cookbook-

* [Router with Loop Steps Workflow (async)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/async/router_with_loop_steps.py)

---

## Define a research workflow

**URL:** llms-txt#define-a-research-workflow

research_workflow = Workflow(
    name="research-workflow",
    steps=[
        Step(name="search", agent=search_agent),
        Step(name="summarize", agent=summary_agent),
        Step(name="fact-check", agent=fact_check_agent),
    ],
)

---

## PPTX Reader

**URL:** llms-txt#pptx-reader

Source: https://docs.agno.com/reference/knowledge/reader/pptx

PPTXReader is a reader class that allows you to read data from PowerPoint (.pptx) files.

<Snippet file="pptx-reader-reference.mdx" />

---

## Define agents for the workflow

**URL:** llms-txt#define-agents-for-the-workflow

researcher_agent = Agent(
    name="Research Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[DuckDuckGoTools()],
    role="Search the web and gather comprehensive research on the given topic",
    instructions=[
        "Search for the most recent and relevant information",
        "Focus on credible sources and key insights",
        "Summarize findings clearly and concisely",
    ],
)

writer_agent = Agent(
    name="Content Writer",
    model=OpenAIChat(id="gpt-4o-mini"),
    role="Create engaging content based on research findings",
    instructions=[
        "Write in a clear, engaging, and professional tone",
        "Structure content with proper headings and bullet points",
        "Include key insights from the research",
        "Keep content informative yet accessible",
    ],
)

---

## Add your custom routes

**URL:** llms-txt#add-your-custom-routes

@app.get("/status")
async def status_check():
    return {"status": "healthy"}

---

## Secondary Searcher Agent - Targeted and specialized search

**URL:** llms-txt#secondary-searcher-agent---targeted-and-specialized-search

secondary_searcher = Agent(
    name="Secondary Searcher",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Perform targeted searches on specific topics and edge cases",
    knowledge=knowledge_secondary,
    search_knowledge=True,
    instructions=[
        "Perform targeted searches on specific aspects of the query.",
        "Look for edge cases, technical details, and specialized information.",
        "Use infinity reranking to find the most precise matches.",
        "Focus on details that complement the primary search results.",
    ],
    markdown=True,
)

---

## Continue Agent Run

**URL:** llms-txt#continue-agent-run

Source: https://docs.agno.com/reference-api/schema/agents/continue-agent-run

post /agents/{agent_id}/runs/{run_id}/continue
Continue a paused or incomplete agent run with updated tool results.

**Use Cases:**
- Resume execution after tool approval/rejection
- Provide manual tool execution results

**Tools Parameter:**
JSON string containing array of tool execution objects with results.

---

## Custom Chunking

**URL:** llms-txt#custom-chunking

**Contents:**
- Custom Chunking Params

Source: https://docs.agno.com/basics/knowledge/chunking/custom-chunking

Custom chunking allows you to implement your own chunking strategy by creating a class that inherits from `ChunkingStrategy`. This is useful when you need to split documents based on specific separators, apply custom logic, or handle domain-specific content formats.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="run-pgvector-step.mdx" />

<Step title="Run the script">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

## Custom Chunking Params

<Snippet file="chunking-custom.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the script">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## StepOutput

**URL:** llms-txt#stepoutput

Source: https://docs.agno.com/reference/workflows/step_output

| Parameter       | Type                                                              | Default | Description                                                     |
| --------------- | ----------------------------------------------------------------- | ------- | --------------------------------------------------------------- |
| `step_name`     | `Optional[str]`                                                   | `None`  | Step identification name                                        |
| `step_id`       | `Optional[str]`                                                   | `None`  | Unique step identifier                                          |
| `step_type`     | `Optional[str]`                                                   | `None`  | Type of step (e.g., "Loop", "Condition", "Parallel")            |
| `executor_type` | `Optional[str]`                                                   | `None`  | Type of executor: "agent", "team", or "function"                |
| `executor_name` | `Optional[str]`                                                   | `None`  | Name of the executor                                            |
| `content`       | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel, Any]]` | `None`  | Primary output (can be any format)                              |
| `step_run_id`   | `Optional[str]`                                                   | `None`  | Link to the run ID of the step execution                        |
| `images`        | `Optional[List[Image]]`                                           | `None`  | Media outputs - images (new or passed-through)                  |
| `videos`        | `Optional[List[Video]]`                                           | `None`  | Media outputs - videos (new or passed-through)                  |
| `audio`         | `Optional[List[Audio]]`                                           | `None`  | Media outputs - audio (new or passed-through)                   |
| `files`         | `Optional[List[File]]`                                            | `None`  | File outputs (new or passed-through)                            |
| `metrics`       | `Optional[Metrics]`                                               | `None`  | Execution metrics and metadata                                  |
| `success`       | `bool`                                                            | `True`  | Execution success status                                        |
| `error`         | `Optional[str]`                                                   | `None`  | Error message if execution failed                               |
| `stop`          | `bool`                                                            | `False` | Request early workflow termination                              |
| `steps`         | `Optional[List[StepOutput]]`                                      | `None`  | Nested step outputs for composite steps (Loop, Condition, etc.) |

---

## Together

**URL:** llms-txt#together

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/together

The Together model provides access to Together's language models.

| Parameter               | Type            | Default                                         | Description                                                       |
| ----------------------- | --------------- | ----------------------------------------------- | ----------------------------------------------------------------- |
| `id`                    | `str`           | `"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"` | The id of the Together model to use                               |
| `name`                  | `str`           | `"Together"`                                    | The name of the model                                             |
| `provider`              | `str`           | `"Together"`                                    | The provider of the model                                         |
| `api_key`               | `Optional[str]` | `None`                                          | The API key for Together (defaults to TOGETHER\_API\_KEY env var) |
| `base_url`              | `str`           | `"https://api.together.xyz/v1"`                 | The base URL for the Together API                                 |
| `retries`               | `int`           | `0`                                             | Number of retries to attempt before raising a ModelProviderError  |
| `delay_between_retries` | `int`           | `1`                                             | Delay between retries, in seconds                                 |
| `exponential_backoff`   | `bool`          | `False`                                         | If True, the delay between retries is doubled each time           |

Together extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Create team members

**URL:** llms-txt#create-team-members

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher", 
    model=OpenAIChat("gpt-5-mini"),
    tools=[DuckDuckGoTools()],
)

---

## Agent searches knowledge base for return policy information

**URL:** llms-txt#agent-searches-knowledge-base-for-return-policy-information

---

## Using Reference Dependencies in Team Instructions

**URL:** llms-txt#using-reference-dependencies-in-team-instructions

Source: https://docs.agno.com/basics/dependencies/team/usage/reference-dependencies

This example demonstrates how to use reference dependencies by defining them in the team constructor and referencing them directly in team instructions. This approach allows dependencies to be automatically injected into the team's context and referenced using template variables in instructions.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the code below.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/dependencies" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## âŒ Bad: All users share the same memories

**URL:** llms-txt#âŒ-bad:-all-users-share-the-same-memories

agent.print_response("I love pizza")
agent.print_response("I'm allergic to dairy")

---

## This middleware will automatically inject JWT values into request.state and is used in the relevant endpoints.

**URL:** llms-txt#this-middleware-will-automatically-inject-jwt-values-into-request.state-and-is-used-in-the-relevant-endpoints.

**Contents:**
- Usage
- How It Works
- Next Steps

app.add_middleware(
    JWTMiddleware,
    secret_key=JWT_SECRET, # or use JWT_SECRET_KEY environment variable
    algorithm="HS256",
    user_id_claim="sub",  # Extract user_id from 'sub' claim
    session_id_claim="session_id",  # Extract session_id from 'session_id' claim
    dependencies_claims=["name", "email", "roles"],
    # In this example, we want this middleware to demonstrate parameter injection, not token validation.
    # In production scenarios, you will probably also want token validation. Be careful setting this to False.
    validate=False,
)

if __name__ == "__main__":
    """
    Run your AgentOS with JWT parameter injection.
    
    Test by calling /agents/user-agent/runs with a message: "What do you know about me?"
    """
    # Test token with user_id and session_id:
    payload = {
        "sub": "user_123",  # This will be injected as user_id parameter
        "session_id": "demo_session_456",  # This will be injected as session_id parameter
        "exp": datetime.now(UTC) + timedelta(hours=24),
        "iat": datetime.now(UTC),
        # Dependency claims
        "name": "John Doe",
        "email": "john.doe@example.com",
        "roles": ["admin", "user"],
    }
    token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")
    print("Test token:")
    print(token)
    agent_os.serve(app="jwt_middleware:app", port=7777, reload=True)
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno openai pyjwt "fastapi[standard]" uvicorn sqlalchemy pgvector psycopg
    bash  theme={null}
    # Using Docker
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash  theme={null}
    python jwt_middleware.py
    bash  theme={null}
    # Use the token printed in the console
    export TOKEN="eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9..."

curl --location 'http://localhost:7777/agents/user-agent/runs' \
        --header 'Content-Type: application/x-www-form-urlencoded' \
        --header 'Authorization: Bearer $TOKEN' \
        --data-urlencode 'message=What do you know about me?'
    bash  theme={null}
    curl --location 'http://localhost:7777/agents/user-agent/runs' \
        --header 'Content-Type: application/x-www-form-urlencoded' \
        --data-urlencode 'message=What do you know about me?'
    ```

**Check the AgentOS API docs:**
    Visit [http://localhost:7777/docs](http://localhost:7777/docs) to see all available endpoints.
  </Step>
</Steps>

1. **JWT Generation**: The example creates a test JWT token with user claims
2. **Middleware Setup**: JWT middleware extracts claims from the `Authorization: Bearer <token>` header
3. **Parameter Injection**: The middleware automatically injects:
   * `user_id` from the `sub` claim
   * `session_id` from the `session_id` claim
   * `dependencies` dict with name, email, and roles
4. **Agent Tools**: The agent can access user details through the injected dependencies

* [JWT Middleware with Cookies](/agent-os/usage/middleware/jwt-cookies)
* [Custom FastAPI with JWT](/agent-os/usage/middleware/custom-fastapi-jwt)
* [JWT Middleware Documentation](/agent-os/middleware/jwt)

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example">
```

---

## Wikipedia search agent

**URL:** llms-txt#wikipedia-search-agent

wikipedia_agent = Agent(
    name="Wikipedia Agent",
    role="Search wikipedia for information",
    model=MistralChat(id="mistral-large-latest"),
    tools=[WikipediaTools()],
    instructions=[
        "Find information about the company in the wikipedia",
    ],
)

---

## Comment out after first run

**URL:** llms-txt#comment-out-after-first-run

**Contents:**
- Clickhouse Params
- Developer Resources

agent.knowledge.load(recreate=False)  # type: ignore

agent.print_response("How do I make pad thai?", markdown=True)
agent.print_response("What was my last question?", stream=True)
python async_clickhouse.py theme={null}
    import asyncio

from agno.agent import Agent
    from agno.knowledge.knowledge import Knowledge
    from agno.db.sqlite import SqliteDb
    from agno.vectordb.clickhouse import Clickhouse

agent = Agent(
        db=SqliteDb(db_file="agno.db"),
        knowledge=Knowledge(
            vector_db=Clickhouse(
                table_name="recipe_documents",
                host="localhost",
                port=8123,
                username="ai",
                password="ai",
            ),
        ),
        # Enable the agent to search the knowledge base
        search_knowledge=True,
        # Enable the agent to read the chat history
        read_chat_history=True,
    )

if __name__ == "__main__":
        # Comment out after first run
        asyncio.run(agent.knowledge.add_content_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
          )
        )

# Create and use the agent
        asyncio.run(agent.aprint_response("How to make Tom Kha Gai", markdown=True))
    ```

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_clickhouse_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/clickhouse_db/clickhouse.py)
* View [Cookbook (Async)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/clickhouse_db/async_clickhouse.py)

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      Clickhouse also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Define specialized agents for different media types

**URL:** llms-txt#define-specialized-agents-for-different-media-types

image_generator = Agent(
    name="Image Generator",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[OpenAITools(image_model="gpt-image-1")],
    instructions="""You are an expert image generation specialist.
    When users request image creation, you should ACTUALLY GENERATE the image using your available image generation tools.

Always use the generate_image tool to create the requested image based on the user's specifications.
    Include detailed, creative prompts that incorporate style, composition, lighting, and mood details.

After generating the image, provide a brief description of what you created.""",
)

image_describer = Agent(
    name="Image Describer",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="""You are an expert image analyst and describer.
    When you receive an image (either as input or from a previous step), analyze and describe it in vivid detail, including:
    - Visual elements and composition
    - Colors, lighting, and mood
    - Artistic style and technique
    - Emotional impact and narrative

If no image is provided, work with the image description or prompt from the previous step.
    Provide rich, engaging descriptions that capture the essence of the visual content.""",
)

video_generator = Agent(
    name="Video Generator",
    model=OpenAIChat(id="gpt-5-mini"),
    # Video Generation only works on VertexAI mode
    tools=[GeminiTools(vertexai=True)],
    instructions="""You are an expert video production specialist.
    Create detailed video generation prompts and storyboards based on user requests.
    Include scene descriptions, camera movements, transitions, and timing.
    Consider pacing, visual storytelling, and technical aspects like resolution and duration.
    Format your response as a comprehensive video production plan.""",
)

video_describer = Agent(
    name="Video Describer",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="""You are an expert video analyst and critic.
    Analyze and describe videos comprehensively, including:
    - Scene composition and cinematography
    - Narrative flow and pacing
    - Visual effects and production quality
    - Audio-visual harmony and mood
    - Technical execution and artistic merit
    Provide detailed, professional video analysis.""",
)

---

## Memory with PostgreSQL

**URL:** llms-txt#memory-with-postgresql

**Contents:**
- Code

Source: https://docs.agno.com/basics/memory/working-with-memories/usage/postgres-memory

```python mem-postgres-memory.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

---

## Configure Agno to use the file logger

**URL:** llms-txt#configure-agno-to-use-the-file-logger

configure_agno_logging(custom_default_logger=custom_logger)

---

## JSON files as database, on Google Cloud Storage (GCS)

**URL:** llms-txt#json-files-as-database,-on-google-cloud-storage-(gcs)

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/gcs/overview

Learn to use Google Cloud Storage (GCS) as a database for your Agents

Agno supports using [Google Cloud Storage (GCS)](https://cloud.google.com/storage) as a database with the `GcsJsonDb` class.
Session data will be stored as JSON blobs in a GCS bucket.

You can get started with GCS following their [Get Started guide](https://cloud.google.com/docs/get-started).

```python gcs_for_agent.py theme={null}
import uuid
import google.auth
from agno.agent import Agent
from agno.db.gcs_json import GcsJsonDb

---

## Show team capabilities

**URL:** llms-txt#show-team-capabilities

**Contents:**
- Usage

print("\nðŸ”§ Team Tools Available:")
for t in team.tools:
    print(f"   - {t.name}: {t.description}")

print("\nðŸ‘¥ Team Members:")
for member in team.members:
    print(f"   - {member.name}: {member.role}")
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/tools/01_team_with_custom_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## PII Detection Guardrail for Teams

**URL:** llms-txt#pii-detection-guardrail-for-teams

Source: https://docs.agno.com/basics/guardrails/usage/team/pii-detection

This example demonstrates how to use Agno's built-in PII detection guardrail with a Team to protect sensitive data.

<Steps>
  <Step title="Create a Python file">
    Create a Python file for the example.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/guardrails" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define content preparation step

**URL:** llms-txt#define-content-preparation-step

content_prep_step = Step(
    name="ContentPreparation",
    agent=content_agent,
    description="Prepare and organize all research for writing",
)

writing_step = Step(
    name="Writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="Editing",
    agent=editor,
    description="Edit and polish the article",
)

---

## === FINAL STEPS ===

**URL:** llms-txt#===-final-steps-===

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)

---

## Google Cloud Storage for Agent

**URL:** llms-txt#google-cloud-storage-for-agent

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/gcs/usage/gcs-for-agent

Agno supports using Google Cloud Storage (GCS) as a storage backend for Agents using the `GcsJsonDb` class. This storage backend stores session data as JSON blobs in a GCS bucket.

Configure your agent with GCS storage to enable cloud-based session persistence.

```python gcs_for_agent.py theme={null}
import uuid
import google.auth
from agno.agent import Agent
from agno.db.base import SessionType
from agno.db.gcs_json import GcsJsonDb
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Coordinated Team

**URL:** llms-txt#coordinated-team

finance_team = Team(
    name="Finance Research Team",
    model=OpenAIChat(id="gpt-4o-mini"),
    members=[research_agent, finance_agent],
    instructions=[
        "Collaborate to provide comprehensive financial insights",
        "Consider both fundamental analysis and market sentiment",
    ],
)

---

## team.print_response(

**URL:** llms-txt#team.print_response(

---

## Configure Session tracking

**URL:** llms-txt#configure-session-tracking

session:
  display_name: "User Sessions"
  dbs:
    - db_id: db-0001
      domain_config:
        display_name: Production sessions

---

## Cancelling a Run

**URL:** llms-txt#cancelling-a-run

**Contents:**
- How It Works
- Cancellation Methods

Source: https://docs.agno.com/execution-control/run-cancellation/overview

Learn how to cancel an Agent, Team, or Workflow run.

Run cancellation provides the ability to stop running agent, team, or workflow executions. This feature is useful for managing long-running operations, implementing timeouts, or allowing users to interrupt processes.

The `cancel_run()` function marks a run for cancellation. Execution stops gracefully once the current step completes, ensuring that operations finish cleanly without leaving resources in an inconsistent state.

**Cancellation Behavior:**

* **Non-Streaming Runs**: The `RunOutput` object is returned with status set to `RunStatus.cancelled`.
* **Streaming Runs**: A `RunCancelledEvent` is emitted when cancellation occurs.

## Cancellation Methods

<CardGroup cols={3}>
  <Card title="Agent Run Cancellation" icon="user-astronaut" iconType="duotone" href="/execution-control/run-cancellation/agent-cancel-run">
    Cancel agent runs programmatically or via API endpoints.
  </Card>

<Card title="Team Run Cancellation" icon="users" iconType="duotone" href="/execution-control/run-cancellation/team-cancel-run">
    Cancel team runs and handle member run cancellations.
  </Card>

<Card title="Workflow Run Cancellation" icon="diagram-project" iconType="duotone" href="/execution-control/run-cancellation/workflow-cancel-run">
    Cancel workflow executions and manage step-level cancellation.
  </Card>
</CardGroup>

---

## Async Agent

**URL:** llms-txt#async-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/vllm/usage/async-basic

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Start vLLM server">
    
  </Step>

<Step title="Run Agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start vLLM server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Add Markdown content to knowledge base

**URL:** llms-txt#add-markdown-content-to-knowledge-base

knowledge.add_content(
    path=Path("README.md"),
    reader=MarkdownReader(),
)

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
)

---

## Redis Async

**URL:** llms-txt#redis-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/redis/usage/async-redis-db

```python async_redis_db.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.redis import RedisVectorDB

---

## No event storage

**URL:** llms-txt#no-event-storage

**Contents:**
- Agno Telemetry
- Developer Resources

fast_workflow = Workflow(
    name="Fast Workflow",
    store_events=False,
    steps=[...]
)
bash  theme={null}
export AGNO_TELEMETRY=false
python  theme={null}
workflow = Workflow(..., telemetry=False)
```

See the [Workflow class reference](/reference/workflows/workflow) for more details.

## Developer Resources

* View the [Workflow reference](/reference/workflows/workflow)
* View the [WorkflowRunOutput schema](/reference/workflows/run-output)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/README.md)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  See this [example](/basics/workflows/usage/store-events-and-events-to-skip-in-a-workflow) for more information.
</Tip>

## Agno Telemetry

Agno logs which model an workflow used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment or by setting `telemetry=False` on the workflow.
```

Example 2 (unknown):
```unknown
or:
```

---

## Define the directory where the Chroma database is located

**URL:** llms-txt#define-the-directory-where-the-chroma-database-is-located

chroma_db_dir = pathlib.Path("./chroma_db")

---

## null

**URL:** llms-txt#null

Source: https://docs.agno.com/basics/workflows/usage/structured-io-at-each-step-level

Demonstrates **Workflows 2.0** type-safe data flow between agents/teams/custom python functions. Each step:

1. Receives structured (pydantic model, list, dict or raw string) input
2. Produces structured output (e.g., `ResearchFindings`, `ContentStrategy`)

You can also use this pattern to create a custom function that can be used in any step and you can-

1. Inspect incoming data types (raw strings or Pydantic models).
2. Analyze structured outputs from previous steps.
3. Generate reports while preserving type safety.

```python structured_io_at_each_step_level_agent.py theme={null}
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

---

## Run the workflow if the script is executed directly

**URL:** llms-txt#run-the-workflow-if-the-script-is-executed-directly

if __name__ == "__main__":
    import random

from rich.prompt import Prompt

# Fun example prompts to showcase the generator's versatility
    example_prompts = [
        "Why Cats Secretly Run the Internet",
        "The Science Behind Why Pizza Tastes Better at 2 AM",
        "Time Travelers' Guide to Modern Social Media",
        "How Rubber Ducks Revolutionized Software Development",
        "The Secret Society of Office Plants: A Survival Guide",
        "Why Dogs Think We're Bad at Smelling Things",
        "The Underground Economy of Coffee Shop WiFi Passwords",
        "A Historical Analysis of Dad Jokes Through the Ages",
    ]

# Get topic from user
    topic = Prompt.ask(
        "[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\nâœ¨",
        default=random.choice(example_prompts),
    )

# Convert the topic to a URL-safe string for use in session_id
    url_safe_topic = topic.lower().replace(" ", "-")

# Initialize the blog post generator workflow
    # - Creates a unique session ID based on the topic
    # - Sets up SQLite storage for caching results
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        db=SqliteDb(
            db_file="tmp/agno_workflows.db",
        ),
        debug_mode=True,
    )

# Execute the workflow with caching enabled
    # Returns an iterator of RunOutput objects containing the generated content
    blog_post: Iterator[RunOutputEvent] = generate_blog_post.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

# Print the response
    pprint_run_response(blog_post, markdown=True)
python  theme={null}
import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

**Examples:**

Example 1 (unknown):
```unknown
To convert this into **Workflows 2.0** structure, either we can break down the workflow into smaller steps and follow the [development guide](/basics/workflows/workflow-patterns/overview).
Or for simplicity we can directly replace the run method to a single custom function executor as mentioned [here](/basics/workflows/workflow-patterns/fully-python-workflow).

It will look like this:
```

---

## Video to Shorts Agent

**URL:** llms-txt#video-to-shorts-agent

**Contents:**
- Code

Source: https://docs.agno.com/basics/multimodal/video/usage/video-to-shorts

```python  theme={null}
import subprocess
import time
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger
from google.generativeai import get_file, upload_file

video_path = Path(__file__).parent.joinpath("sample.mp4")
output_dir = Path("tmp/shorts")

agent = Agent(
    name="Video2Shorts",
    description="Process videos and generate engaging shorts.",
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
    debug_mode=True,
    instructions=[
        "Analyze the provided video directlyâ€”do NOT reference or analyze any external sources or YouTube videos.",
        "Identify engaging moments that meet the specified criteria for short-form content.",
        """Provide your analysis in a **table format** with these columns:
   - Start Time | End Time | Description | Importance Score""",
        "Ensure all timestamps use MM:SS format and importance scores range from 1-10. ",
        "Focus only on segments between 15 and 60 seconds long.",
        "Base your analysis solely on the provided video content.",
        "Deliver actionable insights to improve the identified segments for short-form optimization.",
    ],
)

---

## Arize

**URL:** llms-txt#arize

**Contents:**
- Integrating Agno with Arize Phoenix
- Prerequisites
- Sending Traces to Arize Phoenix

Source: https://docs.agno.com/integrations/observability/arize

Integrate Agno with Arize Phoenix to send traces and gain insights into your agent's performance.

## Integrating Agno with Arize Phoenix

[Arize Phoenix](https://phoenix.arize.com/) is a powerful platform for monitoring and analyzing AI models. By integrating Agno with Arize Phoenix, you can leverage OpenInference to send traces and gain insights into your agent's performance.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup Arize Phoenix Account**

* Create an account at [Arize Phoenix](https://phoenix.arize.com/).
   * Obtain your API key from the Arize Phoenix dashboard.

3. **Set Environment Variables**

Configure your environment with the Arize Phoenix API key:

## Sending Traces to Arize Phoenix

* ### Example: Using Arize Phoenix with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Arize Phoenix.

```python  theme={null}
import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from phoenix.otel import register

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup Arize Phoenix Account**

   * Create an account at [Arize Phoenix](https://phoenix.arize.com/).
   * Obtain your API key from the Arize Phoenix dashboard.

3. **Set Environment Variables**

   Configure your environment with the Arize Phoenix API key:
```

Example 2 (unknown):
```unknown
## Sending Traces to Arize Phoenix

* ### Example: Using Arize Phoenix with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Arize Phoenix.
```

---

## Async MongoDB for Team

**URL:** llms-txt#async-mongodb-for-team

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/integrations/database/async-mongo/usage/async-mongodb-for-team

Agno supports using MongoDB asynchronously as a storage backend for Teams, with the `AsyncMongoDb` class.

You need to provide either `db_url` or `client`. The following example uses `db_url`.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python async_mongodb_for_team.py theme={null}
"""
Run: `pip install openai pymongo motor` to install dependencies
"""
from typing import List

from agno.agent import Agent
from agno.db.mongo import AsyncMongoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

---

## In-memory for testing

**URL:** llms-txt#in-memory-for-testing

**Contents:**
- Core Functionality
  - Contents DB Schema
  - Content Metadata Tracking

from agno.db.in_memory import InMemoryDb
contents_db = InMemoryDb()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Core Functionality

### Contents DB Schema

If you have a Contents DB configured for your Knowledge, the content metadata will be stored in a contents table in your database.

The schema for the contents table is as follows:

| Field            | Type   | Description                                                                               |
| ---------------- | ------ | ----------------------------------------------------------------------------------------- |
| `id`             | `str`  | The unique identifier for the content.                                                    |
| `name`           | `str`  | The name of the content.                                                                  |
| `description`    | `str`  | The description of the content.                                                           |
| `metadata`       | `dict` | The metadata for the content.                                                             |
| `type`           | `str`  | The type of the content.                                                                  |
| `size`           | `int`  | The size of the content. Applicable only to files.                                        |
| `linked_to`      | `str`  | The ID of the content that this content is linked to.                                     |
| `access_count`   | `int`  | The number of times this content has been accessed.                                       |
| `status`         | `str`  | The status of the content.                                                                |
| `status_message` | `str`  | The message associated with the status of the content.                                    |
| `created_at`     | `int`  | The timestamp when the content was created.                                               |
| `updated_at`     | `int`  | The timestamp when the content was last updated.                                          |
| `external_id`    | `str`  | The external ID of the content. Used when external vector stores are used, like LightRAG. |

This data is best displayed on the [knowledge page of the AgentOS UI](https://os.agno.com/knowledge).

### Content Metadata Tracking
```

---

## PgVector Vector Database

**URL:** llms-txt#pgvector-vector-database

**Contents:**
- Setup
- Example
- PgVector Params
- Developer Resources

Source: https://docs.agno.com/integrations/vectordb/pgvector/overview

Learn how to use PgVector as a vector database for your Knowledge Base

<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      PgVector also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_pgvector_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/pgvector/pgvector_db.py)
* View [Cookbook (Async)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/pgvector/async_pg_vector.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

Example 2 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      PgVector also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Set up vector database - stores embeddings

**URL:** llms-txt#set-up-vector-database---stores-embeddings

vector_db = PgVector(
    table_name="vectors",
    db_url="postgresql+psycopg://user:pass@localhost:5432/db"
)

---

## What are Workflows?

**URL:** llms-txt#what-are-workflows?

**Contents:**
- Your First Workflow

Source: https://docs.agno.com/basics/workflows/overview

Build deterministic workflows by orchestrating agents, teams, and functions in steps for reliable multi-agent systems

Agno Workflows enable you to build deterministic, controlled agentic flows by orchestrating agents, teams, and functions through a series of defined steps. Unlike free-form agent interactions, workflows provide structured automation with predictable execution patterns, making them ideal for production systems that require reliable, repeatable processes.

Each step in a workflow handles a specific part of a larger task, with output automatically flowing from one step to the next, creating a smooth pipeline from start to finish. Think of it like an assembly line: each step specializes in one thing, and together they accomplish complex tasks that would be hard for a single agent or team to handle.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=a308215dbae7c8e9050d03af47cfcf1b" alt="Workflows flow diagram" data-og-width="2994" width="2994" data-og-height="756" height="756" data-path="images/workflows-flow-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=36da448838231c986ea6fbee6cd20adf 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=2f79f8986f962ceed254128c04e2fff0 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=602db868a58a7ebadfb6849d783dacdf 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=79ab80554e556943fad1bb1c54c23c1b 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=0c06010680d6e281b4ca6f90f8febc23 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=6195335508ec97552803e2920695044d 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=e9ef16c48420b7eee9312561ab56098e" alt="Workflows flow diagram" data-og-width="2994" width="2994" data-og-height="756" height="756" data-path="images/workflows-flow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=cb8faae1ea504803ff761ae3b89c51fb 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=498fe144844ce93806c7f590823ae666 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=e8bd51333fbf023524a636d579f489f2 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f4447ccd6c0c84f2ca104eb2ce7b560b 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5a6c0f92fc29f107e4249432216a6b0f 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-flow.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=0192f698b51c565c3efd9259472c8750 2500w" />

## Your First Workflow

Here's a simple workflow that takes a topic, researches it, and writes an article:

```python  theme={null}
from agno.agent import Agent
from agno.workflow import Workflow
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Demo Qwen

**URL:** llms-txt#demo-qwen

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/local/ollama/usage/demo-qwen

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example: Send a template message

**URL:** llms-txt#example:-send-a-template-message

---

## STREAM AND PRETTY PRINT

**URL:** llms-txt#stream-and-pretty-print

**Contents:**
  - Modify what is show on the terminal
- The Passthrough-Team Pattern
- Next Steps

stream = team.run("What is the weather in Tokyo?", stream=True)
pprint_run_response(stream, markdown=True)
python  theme={null}
...

team = Team(
    name="News and Weather Team", 
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o")
    show_members_responses=True,
)

team.print_response("What is the weather in Tokyo?")
```

## The Passthrough-Team Pattern

It is a common pattern to have a team that decides which member to delegate the request to, and then passes the request to the team member without any modification, and also applies no processing to the response before returning it to the user. I.e. this team is a "passthrough" team (or "router" team).

In that case the team leader is effectively bypassed and all communication is directly with a team member. See the [Passthrough Teams](/basics/teams/delegation#passthrough-teams) section for more details.

Next, continue building your team by adding functionality as needed. Common questions:

* **How do I run my team?** -> See the [running teams](/basics/teams/running-teams) documentation.
* **How do I add history to my team?** -> See the [chat history](/basics/chat-history/team/overview) documentation.
* **How do I manage sessions?** -> See the [sessions](/basics/sessions/overview) documentation.
* **How do I manage input and capture output?** -> See the [input and output](/basics/input-output/overview) documentation.
* **How do I manage the team context?** -> See the [context engineering](/basics/context/team) documentation.
* **How do I add knowledge?** -> See the [knowledge](/basics/knowledge/overview) documentation.
* **How do I add guardrails?** -> See the [guardrails](/basics/guardrails/overview) documentation.
* **How do I cache responses during development?** -> See the [response caching](/basics/models/cache-response) documentation.

**Examples:**

Example 1 (unknown):
```unknown
### Modify what is show on the terminal

When using `print_response`, only the team tool calls (typically all of the delegation to members) are printed. If you want to print the responses from the members, you can use the `show_members_responses` parameter.
```

---

## Pipedream LinkedIn

**URL:** llms-txt#pipedream-linkedin

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/mcp/usage/pipedream-linkedin

This example shows how to use the LinkedIn Pipedream MCP server with Agno Agents.

---

## AWS Bedrock

**URL:** llms-txt#aws-bedrock

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/bedrock

Learn how to use AWS Bedrock models in Agno.

The AWS Bedrock model provides access to models hosted on AWS Bedrock.

| Parameter               | Type                       | Default        | Description                                                          |
| ----------------------- | -------------------------- | -------------- | -------------------------------------------------------------------- |
| `id`                    | `str`                      | Required       | The id of the AWS Bedrock model to use                               |
| `name`                  | `str`                      | `"AwsBedrock"` | The name of the model                                                |
| `provider`              | `str`                      | `"AWS"`        | The provider of the model                                            |
| `temperature`           | `Optional[float]`          | `None`         | Controls randomness in the model's output                            |
| `max_tokens`            | `Optional[int]`            | `None`         | Maximum number of tokens to generate                                 |
| `top_p`                 | `Optional[float]`          | `None`         | Controls diversity via nucleus sampling                              |
| `top_k`                 | `Optional[int]`            | `None`         | Controls diversity via top-k sampling                                |
| `stop_sequences`        | `Optional[List[str]]`      | `None`         | A list of strings that the model should stop generating text at      |
| `response_format`       | `Optional[str]`            | `None`         | The format of the response                                           |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`         | Additional parameters to include in the request                      |
| `aws_region`            | `Optional[str]`            | `None`         | The AWS region to use (defaults to AWS\_REGION env var)              |
| `aws_access_key_id`     | `Optional[str]`            | `None`         | AWS access key ID (defaults to AWS\_ACCESS\_KEY\_ID env var)         |
| `aws_secret_access_key` | `Optional[str]`            | `None`         | AWS secret access key (defaults to AWS\_SECRET\_ACCESS\_KEY env var) |
| `aws_session_token`     | `Optional[str]`            | `None`         | AWS session token (defaults to AWS\_SESSION\_TOKEN env var)          |
| `aws_profile`           | `Optional[str]`            | `None`         | AWS profile to use (defaults to AWS\_PROFILE env var)                |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`         | Additional parameters for client configuration                       |
| `retries`               | `int`                      | `0`            | Number of retries to attempt before raising a ModelProviderError     |
| `delay_between_retries` | `int`                      | `1`            | Delay between retries, in seconds                                    |
| `exponential_backoff`   | `bool`                     | `False`        | If True, the delay between retries is doubled each time              |

---

## Use the agent - all calls are automatically traced

**URL:** llms-txt#use-the-agent---all-calls-are-automatically-traced

**Contents:**
  - Example: Development Mode (Console Output)

agent.print_response("What is the current price of Tesla and what do analysts recommend?")
python  theme={null}
import openlit
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown
### Example: Development Mode (Console Output)

For local development without a collector, OpenLIT can output traces directly to the console:
```

---

## OpenAI GPT-5-mini

**URL:** llms-txt#openai-gpt-5-mini

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/gpt5-mini

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run the agent synchronously

**URL:** llms-txt#run-the-agent-synchronously

**Contents:**
- Usage

structured_output_response: RunOutput = structured_output_agent.run(
    "Llamas ruling the world"
)
pprint(structured_output_response.content)

bash  theme={null}
    export XAI_API_KEY=xxx
    bash  theme={null}
    pip install -U xai agno
    bash Mac theme={null}
      python cookbook/models/xai/structured_output.py
      bash Windows theme={null}
      python cookbook/models/xai/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure the Phoenix tracer

**URL:** llms-txt#configure-the-phoenix-tracer

tracer_provider = register(
    project_name="agno-stock-price-agent",  # Default is 'default'
    auto_instrument=True,  # Automatically use the installed OpenInference instrumentation
)

---

## 1. Configure vector database with embedder

**URL:** llms-txt#1.-configure-vector-database-with-embedder

vector_db = PgVector(
    table_name="company_knowledge",
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    embedder=OpenAIEmbedder(id="text-embedding-3-small")  # Optional: defaults to OpenAIEmbedder
)

---

## To get the response in a variable:

**URL:** llms-txt#to-get-the-response-in-a-variable:

---

## xAI

**URL:** llms-txt#xai

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/xai

The xAI model provides access to xAI's language models.

| Parameter               | Type            | Default                 | Description                                                      |
| ----------------------- | --------------- | ----------------------- | ---------------------------------------------------------------- |
| `id`                    | `str`           | `"grok-beta"`           | The id of the xAI model to use                                   |
| `name`                  | `str`           | `"xAI"`                 | The name of the model                                            |
| `provider`              | `str`           | `"xAI"`                 | The provider of the model                                        |
| `api_key`               | `Optional[str]` | `None`                  | The API key for xAI (defaults to XAI\_API\_KEY env var)          |
| `base_url`              | `str`           | `"https://api.x.ai/v1"` | The base URL for the xAI API                                     |
| `retries`               | `int`           | `0`                     | Number of retries to attempt before raising a ModelProviderError |
| `delay_between_retries` | `int`           | `1`                     | Delay between retries, in seconds                                |
| `exponential_backoff`   | `bool`          | `False`                 | If True, the delay between retries is doubled each time          |

xAI extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Verify db contents

**URL:** llms-txt#verify-db-contents

**Contents:**
- Params
- Developer Resources

print("\nVerifying db contents...")
all_sessions = db.get_sessions(session_type=SessionType.AGENT)
print(f"Total sessions in Redis: {len(all_sessions)}")

if all_sessions:
    print("\nSession details:")
    session = all_sessions[0]
    print(f"The stored session: {session}")

<Snippet file="db-redis-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/redis/redis_for_agent.py)

---

## Team Input Schema Validation

**URL:** llms-txt#team-input-schema-validation

**Contents:**
- Code

Source: https://docs.agno.com/basics/input-output/team/usage/input-schema-on-team

This example demonstrates how to use input\_schema with teams for automatic input validation and structured data handling, allowing automatic validation and conversion of dictionary inputs into Pydantic models.

```python cookbook/examples/teams/structured_input_output/06_input_schema_on_team.py theme={null}
"""
This example demonstrates how to use input_schema with teams for automatic
input validation and structured data handling.

The input_schema feature allows teams to automatically validate and convert
dictionary inputs into Pydantic models, ensuring type safety and data validation.
"""

from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field

class ResearchProject(BaseModel):
    """Structured research project with validation requirements."""

project_name: str = Field(description="Name of the research project")
    research_topics: List[str] = Field(
        description="List of topics to research", min_items=1
    )
    target_audience: str = Field(description="Intended audience for the research")
    depth_level: str = Field(
        description="Research depth level", pattern="^(basic|intermediate|advanced)$"
    )
    max_sources: int = Field(
        description="Maximum number of sources to use", ge=3, le=20, default=10
    )
    include_recent_only: bool = Field(
        description="Whether to focus only on recent sources", default=True
    )

---

## Startup Idea Validator

**URL:** llms-txt#startup-idea-validator

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/workflows/startup-idea-validator

Build a startup validation workflow that helps entrepreneurs evaluate their business ideas through clarification, originality assessment, mission definition, and comprehensive market research. This multi-agent system provides objective feedback before resource investment.

By building this workflow, you'll understand:

* How to structure multi-phase validation workflows for complex analysis
* How to coordinate specialized agents for different evaluation aspects
* How to integrate market research with competitive analysis
* How to generate comprehensive validation reports with actionable insights

Build startup validation platforms, business idea analyzers, market opportunity assessment tools, or entrepreneurship advisory systems.

The workflow validates startup ideas through four specialized phases:

1. **Clarify**: Idea clarifier agent refines and structures the core business concept
2. **Evaluate**: Originality evaluator assesses uniqueness compared to existing solutions
3. **Define**: Mission definer establishes clear objectives and vision
4. **Research**: Market researcher conducts comprehensive analysis using Google Search
5. **Synthesize**: Combines all insights into a validation report with recommendations

Each agent specializes in a specific validation aspect, ensuring thorough analysis.

```python startup_idea_validator.py theme={null}
"""
Startup Idea Validator - Your Personal Business Validation Assistant!

This workflow helps entrepreneurs validate their startup ideas by:
1. Clarifying and refining the core business concept
2. Evaluating originality compared to existing solutions
3. Defining clear mission and objectives
4. Conducting comprehensive market research and analysis

Why is this helpful?
--------------------------------------------------------------------------------
â€¢ Get objective feedback on your startup idea before investing resources
â€¢ Understand your total addressable market and target segments
â€¢ Validate assumptions about market opportunity and competition
â€¢ Define clear mission and objectives to guide execution

Who should use this?
--------------------------------------------------------------------------------
â€¢ Entrepreneurs and Startup Founders
â€¢ Product Managers and Business Strategists
â€¢ Innovation Teams
â€¢ Angel Investors and VCs doing initial screening

Example use cases:
--------------------------------------------------------------------------------
â€¢ New product/service validation
â€¢ Market opportunity assessment
â€¢ Competitive analysis
â€¢ Business model validation
â€¢ Target customer segmentation
â€¢ Mission/vision refinement

Quick Start:
--------------------------------------------------------------------------------
1. Install dependencies:
   pip install openai agno

2. Set environment variables:
   - OPENAI_API_KEY

3. Run:
   python startup_idea_validator.py

The workflow will guide you through validating your startup idea with AI-powered
analysis and research. Use the insights to refine your concept and business plan!
"""

import asyncio
from typing import Any

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

---

## Agent with Grounding

**URL:** llms-txt#agent-with-grounding

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/google/usage/grounding

```python cookbook/models/google/gemini/grounding.py theme={null}
"""Grounding with Gemini.

Grounding enables Gemini to search the web and provide responses backed by
real-time information with citations. This is a legacy tool - for Gemini 2.0+
models, consider using the 'search' parameter instead.

from agno.agent import Agent
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash",
        grounding=True,
        grounding_dynamic_threshold=0.7,  # Optional: set threshold for grounding
    ),
    add_datetime_to_context=True,
)

---

## AWS Lambda

**URL:** llms-txt#aws-lambda

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/aws-lambda

The following example requires the `boto3` library.

The following agent will use AWS Lambda to list all Lambda functions in our AWS account and invoke a specific Lambda function.

```python cookbook/tools/aws_lambda_tools.py theme={null}

from agno.agent import Agent
from agno.tools.aws_lambda import AWSLambdaTools

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use AWS Lambda to list all Lambda functions in our AWS account and invoke a specific Lambda function.
```

---

## Get the FastAPI app for the AgentOS

**URL:** llms-txt#get-the-fastapi-app-for-the-agentos

**Contents:**
- Run your AgentOS
- Connect your AgentOS
- Chat with your Agent
- Pre-built API endpoints
- Next

app = agent_os.get_app()
bash Mac theme={null}
      uv venv --python 3.12
      source .venv/bin/activate
      bash Windows theme={null}
      uv venv --python 3.12
      .venv/Scripts/activate
      bash Mac theme={null}
      uv pip install -U agno anthropic mcp 'fastapi[standard]' sqlalchemy
      bash Windows theme={null}
      uv pip install -U agno anthropic mcp 'fastapi[standard]' sqlalchemy
      bash Mac theme={null}
      export ANTHROPIC_API_KEY=sk-***
      bash Windows theme={null}
      setx ANTHROPIC_API_KEY sk-***
      shell  theme={null}
    fastapi dev agno_agent.py
    ```

This will start your AgentOS on `http://localhost:8000`
  </Step>
</Steps>

## Connect your AgentOS

Agno provides a beautiful web interface that connects directly to your AgentOS, use it to monitor, manage and test your agentic system. Open [os.agno.com](https://os.agno.com) and sign in to your account.

* Click on **"Add new OS"** in the top navigation bar.
* Select **"Local"** to connect to a local AgentOS running on your machine.
* Enter the endpoint URL of your AgentOS. The default is `http://localhost:8000`.
* Give your AgentOS a descriptive name like "Development OS" or "Local 8000".
* Click **"Connect"**.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/aEfJPs-hg36UsUPO/videos/agent-os-connect-1.mp4?fit=max&auto=format&n=aEfJPs-hg36UsUPO&q=85&s=907888debf7f055f14e0f84405ba5749" type="video/mp4" data-path="videos/agent-os-connect-1.mp4" />
  </video>
</Frame>

Once connected, you'll see your new OS with a live status indicator.

## Chat with your Agent

Next, let's chat with our Agent, go to the `Chat` section in the sidebar and select your Agent.

* Ask â€œWhat is Agno?â€ and the Agent will answer using the Agno MCP server.
* Agents keep their own history, tools, and instructions; switching users wonâ€™t mix context.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/aEfJPs-hg36UsUPO/videos/agno-agent-chat.mp4?fit=max&auto=format&n=aEfJPs-hg36UsUPO&q=85&s=b8ac56bfb2e9436799299fcafa746d4a" type="video/mp4" data-path="videos/agno-agent-chat.mp4" />
  </video>
</Frame>

<Tip>
  Click on Sessions to view your Agent's conversations. This data is stored in your Agent's database, so no need for external tracing services.
</Tip>

## Pre-built API endpoints

The FastAPI app generated by your AgentOS comes with pre-built SSE-compatible API endpoints that you can build your product on top of. You can add your own routes, middleware or any other FastAPI feature.

Checkout the API endpoints at `/docs` of your AgentOS url, e.g. [http://localhost:8000/docs](http://localhost:8000/docs)

* Learn how to [build agents](/basics/agents/building-agents) with more features
* Explore [examples](/examples/use-cases/agents/overview) for inspiration
* Read the [full documentation](/basics/agents/overview) to dive deeper

**Examples:**

Example 1 (unknown):
```unknown
<Check>
  There is an incredible amount of alpha in these 25 lines of code.

  You get a fully functional Agent with memory and state that can access any MCP server. It's served via a FastAPI app with pre-built endpoints that you can use to build your product.
</Check>

## Run your AgentOS

The AgentOS gives us a FastAPI application with ready-to-use API endpoints. Let's run it.

<Steps>
  <Step title="Setup your virtual environment">
    <CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install dependencies">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create cluster options with authentication

**URL:** llms-txt#create-cluster-options-with-authentication

auth = PasswordAuthenticator(username, password)
cluster_options = ClusterOptions(auth)
cluster_options.apply_profile(KnownConfigProfiles.WanDevelopment)

---

## Run the team with a task

**URL:** llms-txt#run-the-team-with-a-task

**Contents:**
- What to Expect
- Usage
- Next Steps

content_team.print_response("Create a short article about quantum computing")
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai ddgs
    bash Mac theme={null}
      python content_team.py
      bash Windows theme={null}
      python content_team.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Modify the task in `print_response()` to create different types of content
* Add more specialized agents like editors or fact-checkers to the team
* Adjust `show_members_responses` to control visibility of individual agent outputs
* Explore [Teams](/basics/teams/overview) for advanced collaboration patterns

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The team will create an article by first researching the topic through web search, then writing content based on the findings. The researcher finds relevant information and sources, while the writer transforms this into engaging prose.

The output shows responses from both team members so you can see how they collaborate. The final article combines accurate research with clear, professional writing.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Team">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## No local setup required - just set OLLAMA_API_KEY

**URL:** llms-txt#no-local-setup-required---just-set-ollama_api_key

agent = Agent(model=Ollama(id="gpt-oss:120b-cloud", host="https://ollama.com"))
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

## Primary Searcher Agent - Broad search with infinity reranking

**URL:** llms-txt#primary-searcher-agent---broad-search-with-infinity-reranking

primary_searcher = Agent(
    name="Primary Searcher",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Perform comprehensive primary search with high-performance reranking",
    knowledge=knowledge_primary,
    search_knowledge=True,
    instructions=[
        "Conduct broad, comprehensive searches across the knowledge base.",
        "Use the infinity reranker to ensure high-quality result ranking.",
        "Focus on capturing the most relevant information first.",
        "Provide detailed context and multiple perspectives on topics.",
    ],
    markdown=True,
)

---

## - "Summarize the top 5 stories on Hacker News"

**URL:** llms-txt#--"summarize-the-top-5-stories-on-hacker-news"

---

## Azure Cosmos DB MongoDB connection string

**URL:** llms-txt#azure-cosmos-db-mongodb-connection-string

"""
Example connection strings:
"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"
"""
mdb_connection_string = f"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"

knowledge_base = Knowledge(
    vector_db=MongoVectorDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        search_index_name="recipes",
        cosmos_compatibility=True,
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

---

## Files As Input

**URL:** llms-txt#files-as-input

**Contents:**
- Developer Resources

Source: https://docs.agno.com/basics/multimodal/files/file_input

Learn how to use files as input with Agno agents.

Agno supports files as input to agents and teams.  Take a look at the [compatibility matrix](/basics/models/compatibility#multimodal-support) to see which models support files as input.

Let's create an agent that can understand files and make tool calls as needed.

## Developer Resources

* View more [Anthropic](/integrations/models/native/anthropic/usage/pdf-input-url) examples.
* View more [OpenAI](/integrations/models/native/openai/responses/usage/pdf-input-url) examples.
* View more [Gemini](/integrations/models/native/google/usage/pdf-input-url) examples.

---

## Create research team with tools and context management

**URL:** llms-txt#create-research-team-with-tools-and-context-management

**Contents:**
- Usage

research_team = Team(
    name="Research Team",
    model=OpenAIChat("gpt-4o"),
    members=[tech_researcher, business_analyst],
    tools=[DuckDuckGoTools()],  # Team uses DuckDuckGo for research
    description="Research team that investigates topics and provides analysis.",
    instructions=dedent("""
        You are a research coordinator that investigates topics comprehensively.
        
        Your Process:
        1. Use DuckDuckGo to search for information on the topic
        2. Delegate detailed analysis to the appropriate specialist
        3. Synthesize research findings with specialist insights
        
        Guidelines:
        - Always start with web research using your DuckDuckGo tools
        - Choose the right specialist based on the topic (tech vs business)
        - Combine your research with specialist analysis
        - Provide comprehensive, well-sourced responses
    """).strip(),
    db=SqliteDb(db_file="tmp/research_team.db"),
    session_id="research_session",
    add_history_to_context=True,
    num_history_runs=6,  # Load last 6 research queries
    max_tool_calls_from_history=3,  # Keep only last 3 research results
    markdown=True,
)

research_team.print_response("What are the latest developments in AI agents?", stream=True)
research_team.print_response("How is the tech market performing this quarter?", stream=True)
research_team.print_response("What are the trends in LLM applications?", stream=True)
research_team.print_response("What companies are leading in AI infrastructure?", stream=True)
bash  theme={null}
    pip install -U agno openai ddgs sqlalchemy
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch filter_tool_calls_from_history.py
    bash Mac/Linux theme={null}
      python filter_tool_calls_from_history.py
      bash Windows theme={null}
      python filter_tool_calls_from_history.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Workflow Cancellation

**URL:** llms-txt#workflow-cancellation

Source: https://docs.agno.com/basics/workflows/usage/workflow-cancellation

This example demonstrates **Workflows 2.0** support for cancelling running workflow executions, including thread-based cancellation and handling cancelled responses.

This example shows how to cancel a running workflow execution in real-time. It demonstrates:

1. **Thread-based Execution**: Running workflows in separate threads for non-blocking operation
2. **Dynamic Cancellation**: Cancelling workflows while they're actively running
3. **Cancellation Events**: Handling and responding to cancellation events
4. **Status Tracking**: Monitoring workflow status throughout execution and cancellation

---

## OR set environment variables manually

**URL:** llms-txt#or-set-environment-variables-manually

**Contents:**
- Example

export AWS_ACCESS_KEY_ID=****
export AWS_SECRET_ACCESS_KEY=****
export AWS_DEFAULT_REGION=us-east-1
python aws_ses_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.aws_ses import AWSSESTool
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Make sure to add the domain or email address you want to send FROM (and, if
  still in sandbox mode, the TO address) to the verified emails in the [SES
  Console](https://console.aws.amazon.com/ses/home).
</Note>

## Example

The following agent researches the latest AI news and then emails a summary via AWS SES:
```

---

## Create agent with Excel skills

**URL:** llms-txt#create-agent-with-excel-skills

excel_agent = Agent(
    name="Excel Creator",
    model=Claude(
        id="claude-sonnet-4-5-20250929",
        skills=[
            {"type": "anthropic", "skill_id": "xlsx", "version": "latest"}
        ],
    ),
    instructions=[
        "You are a data analysis specialist with access to Excel skills.",
        "Create professional spreadsheets with well-formatted tables and accurate formulas.",
        "Use charts and visualizations to make data insights clear.",
    ],
    markdown=True,
)

---

## Confluence

**URL:** llms-txt#confluence

**Contents:**
- Prerequisites

Source: https://docs.agno.com/integrations/toolkits/others/confluence

**ConfluenceTools** enable an Agent to retrieve, create, and update pages in Confluence. They also allow you to explore spaces and page details.

The following example requires the `atlassian-python-api` library and Confluence credentials. You can obtain an API token by going [here](https://id.atlassian.com/manage-profile/security).

```shell  theme={null}
export CONFLUENCE_URL="https://your-confluence-instance"
export CONFLUENCE_USERNAME="your-username"
export CONFLUENCE_PASSWORD="your-password"

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Create team with metrics tracking enabled

**URL:** llms-txt#create-team-with-metrics-tracking-enabled

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher],
    db=db,  # Database required for session metrics
    session_id="team_metrics_demo",
    markdown=True,
    show_members_responses=True,
    store_member_responses=True,
)

---

## Download the video file from the URL as bytes

**URL:** llms-txt#download-the-video-file-from-the-url-as-bytes

**Contents:**
- Usage

response = requests.get(url)
video_content = response.content

agent.print_response(
    "Tell me about this video",
    videos=[Video(content=video_content)],
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/video_input_bytes_content.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/video_input_bytes_content.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Search for information and process the results

**URL:** llms-txt#search-for-information-and-process-the-results

**Contents:**
  - 2. Website Content Crawler

agent.print_response("What are the latest developments in large language models?", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

agent = Agent(
    tools=[
        ApifyTools(actors=["apify/website-content-crawler"])
    ],
    markdown=True
)

**Examples:**

Example 1 (unknown):
```unknown
### 2. Website Content Crawler

This tool uses Apify's [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor to extract text content from websites, making it perfect for RAG applications.
```

---

## Define your agents/team

**URL:** llms-txt#define-your-agents/team

content_creation_workflow = Workflow(
    name="Content Creation Workflow",
    description="Automated content creation from blog posts to social media",
    db=SqliteDb(db_file="tmp/workflow.db"),
    steps=[research_team, content_planner],
)

---

## Download all sample sales documents and get their paths

**URL:** llms-txt#download-all-sample-sales-documents-and-get-their-paths

downloaded_csv_paths = download_knowledge_filters_sample_data(
    num_files=4, file_extension=SampleDataFileExtension.CSV
)

---

## Cerebras OpenAI

**URL:** llms-txt#cerebras-openai

**Contents:**
- OpenAI-Compatible Integration
  - Using the OpenAI-Compatible Class

Source: https://docs.agno.com/integrations/models/gateways/cerebras-openai/overview

Learn how to use Cerebras OpenAI with Agno.

## OpenAI-Compatible Integration

Cerebras can also be used via an OpenAI-compatible interface, making it easy to integrate with tools and libraries that expect the OpenAI API.

### Using the OpenAI-Compatible Class

The `CerebrasOpenAI` class provides an OpenAI-style interface for Cerebras models:

First, install openai:

```python  theme={null}
from agno.agent import Agent
from agno.models.cerebras import CerebrasOpenAI

agent = Agent(
    model=CerebrasOpenAI(
        id="llama-4-scout-17b-16e-instruct",  # Model ID to use
        # base_url="https://api.cerebras.ai", # Optional: default endpoint for Cerebras
    ),
    markdown=True,
)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Web Browser Tools

**URL:** llms-txt#web-browser-tools

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions

Source: https://docs.agno.com/integrations/toolkits/others/web-browser

WebBrowser Tools enable an Agent to open a URL in a web browser.

| Parameter          | Type   | Default | Description                                   |
| ------------------ | ------ | ------- | --------------------------------------------- |
| `enable_open_page` | `bool` | `True`  | Enables functionality to open URLs in browser |
| `all`              | `bool` | `False` | Enables all functionality when set to True    |

| Function    | Description                  |
| ----------- | ---------------------------- |
| `open_page` | Opens a URL in a web browser |

---

## Example usage

**URL:** llms-txt#example-usage

if __name__ == "__main__":
    # Generate a research report on a cutting-edge topic
    agent.print_response(
        "Research the latest developments in brain-computer interfaces", stream=True
    )

---

## Enable tracing - traces stored in the shared db

**URL:** llms-txt#enable-tracing---traces-stored-in-the-shared-db

**Contents:**
- Multiple Databases Setup

agent_os = AgentOS(
    agents=[agent],
    db=db,
    tracing=True,  # Traces go to the shared db
)

app = agent_os.get_app()
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

**Examples:**

Example 1 (unknown):
```unknown
In this setup, traces are automatically stored in the shared database alongside sessions and other data.

## Multiple Databases Setup

<Warning>
  When agents or teams have **different databases**, it is highly recommended that you specify a dedicated `tracing_db` to ensure all traces are stored in one central location.
</Warning>

If you have multiple agents with their own databases, traces need a dedicated database:
```

---

## Use on_route_conflict="preserve_base_app" to preserve your custom routes over AgentOS routes

**URL:** llms-txt#use-on_route_conflict="preserve_base_app"-to-preserve-your-custom-routes-over-agentos-routes

agent_os = AgentOS(
    description="Example app with route replacement",
    agents=[web_research_agent],
    base_app=app,
    on_route_conflict="preserve_base_app",  # Skip conflicting AgentOS routes, keep your custom routes
)

app = agent_os.get_app()

if __name__ == "__main__":
    """Run the AgentOS application.

With on_route_conflict="preserve_base_app":
    - Your custom routes are preserved: http://localhost:7777/ and http://localhost:7777/health
    - AgentOS routes are available at other paths: http://localhost:7777/sessions, etc.
    - Conflicting AgentOS routes (GET / and GET /health) are skipped
    - API docs: http://localhost:7777/docs

Try changing on_route_conflict to "preserve_agentos" to see AgentOS routes override your custom ones.
    """
    agent_os.serve(app="override_routes:app", reload=True)

---

## Get all cultural knowledge

**URL:** llms-txt#get-all-cultural-knowledge

all_knowledge = culture_manager.get_all_knowledge()
print(all_knowledge)

---

## Twilio

**URL:** llms-txt#twilio

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/social/twilio

**TwilioTools** enables an Agent to interact with [Twilio](https://www.twilio.com/docs) services, such as sending SMS, retrieving call details, and listing messages.

The following examples require the `twilio` library and appropriate Twilio credentials, which can be obtained from [here](https://www.twilio.com/console).

Set the following environment variables:

The following agent will send an SMS message using Twilio:

| Name                      | Type            | Default | Description                                       |
| ------------------------- | --------------- | ------- | ------------------------------------------------- |
| `account_sid`             | `Optional[str]` | `None`  | Twilio Account SID for authentication.            |
| `auth_token`              | `Optional[str]` | `None`  | Twilio Auth Token for authentication.             |
| `api_key`                 | `Optional[str]` | `None`  | Twilio API Key for alternative authentication.    |
| `api_secret`              | `Optional[str]` | `None`  | Twilio API Secret for alternative authentication. |
| `region`                  | `Optional[str]` | `None`  | Optional Twilio region (e.g., `au1`).             |
| `edge`                    | `Optional[str]` | `None`  | Optional Twilio edge location (e.g., `sydney`).   |
| `debug`                   | `bool`          | `False` | Enable debug logging for troubleshooting.         |
| `enable_send_sms`         | `bool`          | `True`  | Enable the send\_sms functionality.               |
| `enable_get_call_details` | `bool`          | `True`  | Enable the get\_call\_details functionality.      |
| `enable_list_messages`    | `bool`          | `True`  | Enable the list\_messages functionality.          |
| `all`                     | `bool`          | `False` | Enable all functionality.                         |

| Function           | Description                                                                                                                                                                 |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `send_sms`         | Sends an SMS to a recipient. Takes recipient phone number, sender number (Twilio), and message body. Returns message SID if successful or error message if failed.          |
| `get_call_details` | Retrieves details of a call using its SID. Takes the call SID and returns a dictionary with call details (e.g., status, duration).                                          |
| `list_messages`    | Lists recent SMS messages. Takes a limit for the number of messages to return (default 20). Returns a list of message details (e.g., SID, sender, recipient, body, status). |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/twilio.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/twilio_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
Set the following environment variables:
```

Example 2 (unknown):
```unknown
## Example

The following agent will send an SMS message using Twilio:
```

---

## Create workflow with the WorkflowAgent

**URL:** llms-txt#create-workflow-with-the-workflowagent

workflow = Workflow(
    name="Story Generation Workflow",
    description="A workflow that generates stories, formats them, and adds references",
    agent=workflow_agent,
    steps=[story_writer, story_formatter, add_references],
    db=PostgresDb(db_url),
)

def main():
    print("\n\n" + "=" * 80)
    print("STREAMING MODE EXAMPLES")
    print("=" * 80)

print("\n" + "=" * 80)
    print("FIRST CALL (STREAMING): Tell me a story about a dog named Rocky")
    print("=" * 80)
    workflow.print_response(
        "Tell me a story about a dog named Rocky",
        stream=True,
    )

print("\n" + "=" * 80)
    print("SECOND CALL (STREAMING): What was Rocky's personality?")
    print("=" * 80)
    workflow.print_response(
        "What was Rocky's personality?", stream=True
    )

print("\n" + "=" * 80)
    print("THIRD CALL (STREAMING): Now tell me a story about a cat named Luna")
    print("=" * 80)
    workflow.print_response(
        "Now tell me a story about a cat named Luna",
        stream=True,
    )

print("\n" + "=" * 80)
    print("FOURTH CALL (STREAMING): Compare Rocky and Luna")
    print("=" * 80)
    workflow.print_response(
        "Compare Rocky and Luna", stream=True
    )

---

## Teams with Memory

**URL:** llms-txt#teams-with-memory

**Contents:**
- Developer Resources

Source: https://docs.agno.com/basics/memory/team/overview

Learn how to use teams with memory.

The team can also manage user memories, just like agents:

See more in the [Memory](/basics/memory/overview) section.

## Developer Resources

* View the [Team schema](/reference/teams/team)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/memory/)

---

## Investment Report Generator

**URL:** llms-txt#investment-report-generator

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code

Source: https://docs.agno.com/examples/use-cases/workflows/investment-report-generator

Build a sophisticated investment analysis system that combines market research, financial analysis, and portfolio management. This workflow analyzes stocks, evaluates investment potential, and generates strategic portfolio allocation recommendations.

By building this workflow, you'll understand:

* How to structure multi-stage financial analysis workflows
* How to integrate web search for real-time market research
* How to rank and compare investment opportunities systematically
* How to generate comprehensive investment reports with rationale

Build investment research platforms, portfolio analysis tools, financial advisory systems, or automated stock screening applications.

The workflow follows a three-stage investment analysis process:

1. **Research**: Market analyst agent uses DuckDuckGo to gather comprehensive information on each stock
2. **Evaluate**: Investment analyst ranks stocks by potential and provides detailed rationale
3. **Allocate**: Portfolio manager creates strategic allocation recommendations based on rankings
4. **Report**: Generates a professional investment report with analysis and recommendations

The workflow processes multiple stocks simultaneously and provides comparative analysis.

```python investment_report_generator.py theme={null}
"""Investment Report Generator - Your AI Financial Analysis Studio!

This advanced example demonstrates how to build a sophisticated investment analysis system that combines
market research, financial analysis, and portfolio management. The workflow uses a three-stage
approach:
1. Comprehensive stock analysis and market research
2. Investment potential evaluation and ranking
3. Strategic portfolio allocation recommendations

Key capabilities:
- Real-time market data analysis
- Professional financial research
- Investment risk assessment
- Portfolio allocation strategy
- Detailed investment rationale

Example companies to analyze:
- "AAPL, MSFT, GOOGL" (Tech Giants)
- "NVDA, AMD, INTC" (Semiconductor Leaders)
- "TSLA, F, GM" (Automotive Innovation)
- "JPM, BAC, GS" (Banking Sector)
- "AMZN, WMT, TGT" (Retail Competition)
- "PFE, JNJ, MRNA" (Healthcare Focus)
- "XOM, CVX, BP" (Energy Sector)

Run `pip install openai ddgs agno` to install dependencies.
"""

import asyncio
import random
from pathlib import Path
from shutil import rmtree
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.types import WorkflowExecutionInput
from agno.workflow.workflow import Workflow
from pydantic import BaseModel

---

## Initialize the AgentOS

**URL:** llms-txt#initialize-the-agentos

**Contents:**
- What to Expect
- Usage
- Next Steps

agent_os = AgentOS(
    description="Query classification and Notion organization system",
    workflows=[query_to_notion_workflow],
)
app = agent_os.get_app()

if __name__ == "__main__":
    agent_os.serve(app="notion_manager:app", reload=True)
bash  theme={null}
    export OPENAI_API_KEY=xxx
    export NOTION_API_KEY=xxx
    export NOTION_DATABASE_ID=xxx
    bash  theme={null}
    pip install -U agno openai fastapi notion-client
    bash Mac theme={null}
      python notion_knowledge_manager.py
      bash Windows theme={null}
      python notion_knowledge_manager.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Customize classification categories in the classification agent's instructions
* Modify Notion page templates to match your database structure
* Add more sophisticated routing logic based on content type
* Explore [Workflows](/basics/workflows/overview) for advanced workflow patterns

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The workflow will classify your input content, determine the appropriate Notion operation (create, update, or search), and execute it automatically. You'll see classification results showing the assigned tags and categories.

The workflow integrates directly with your Notion database, automatically creating pages with proper tags, updating existing pages, or searching for related content based on the classification.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Workflow">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Model as String

**URL:** llms-txt#model-as-string

**Contents:**
- Format
- Basic Usage
  - Agent with String Syntax
  - Teams with String Syntax
  - Multiple Model Types
- Common Providers
- Developer Resources

Source: https://docs.agno.com/basics/models/model-as-string

Use the convenient provider:model_id string format to specify models without importing model classes.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.2.6" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.2.6">v2.2.6</Tooltip>
</Badge>

Agno provides a convenient string syntax for specifying models using the `provider:model_id` format. This approach reduces code verbosity by eliminating the need to import model classes while maintaining full functionality.

Both the traditional object syntax and the string syntax are equally valid and work identically. Choose the approach that best fits your coding style and requirements.

The string format follows this pattern:

* **provider**: The model provider name (case-insensitive)
* **model\_id**: The specific model identifier

* `"openai:gpt-4o"`
* `"anthropic:claude-sonnet-4-20250514"`
* `"google:gemini-2.0-flash-exp"`
* `"groq:llama-3.3-70b-versatile"`

### Agent with String Syntax

### Teams with String Syntax

Use model strings with Teams for coordinated multi-agent workflows:

### Multiple Model Types

Agents support different models for various purposes:

| Provider         | String Format               | Example                                     |
| ---------------- | --------------------------- | ------------------------------------------- |
| OpenAI           | `openai:model_id`           | `"openai:gpt-4o"`                           |
| Anthropic        | `anthropic:model_id`        | `"anthropic:claude-sonnet-4-20250514"`      |
| Google           | `google:model_id`           | `"google:gemini-2.0-flash-exp"`             |
| Groq             | `groq:model_id`             | `"groq:llama-3.3-70b-versatile"`            |
| Ollama           | `ollama:model_id`           | `"ollama:llama3.2"`                         |
| Azure AI Foundry | `azure-ai-foundry:model_id` | `"azure-ai-foundry:gpt-4o"`                 |
| Mistral          | `mistral:model_id`          | `"mistral:mistral-large-latest"`            |
| LiteLLM          | `litellm:model_id`          | `"litellm:gpt-4o"`                          |
| OpenRouter       | `openrouter:model_id`       | `"openrouter:anthropic/claude-3.5-sonnet"`  |
| Together         | `together:model_id`         | `"together:meta-llama/Llama-3-70b-chat-hf"` |

For the complete list and provider-specific documentation, see the [Models Overview](/basics/models/overview).

## Developer Resources

* View [All Model Providers](/integrations/models/model-index)

**Examples:**

Example 1 (unknown):
```unknown
"provider:model_id"
```

Example 2 (unknown):
```unknown
### Teams with String Syntax

Use model strings with Teams for coordinated multi-agent workflows:
```

Example 3 (unknown):
```unknown
### Multiple Model Types

Agents support different models for various purposes:
```

---

## Few-Shot Learning with Additional Input

**URL:** llms-txt#few-shot-learning-with-additional-input

**Contents:**
- Code

Source: https://docs.agno.com/basics/context/agent/usage/few-shot-learning

This example demonstrates how to use additional\_input with an Agent to teach proper response patterns through few-shot learning, specifically for customer support scenarios.

```python few_shot_learning.py theme={null}
"""
This example demonstrates how to use additional_input with an Agent
to teach proper response patterns through few-shot learning.
"""

from agno.agent import Agent
from agno.models.message import Message
from agno.models.openai import OpenAIChat

---

## Shared knowledge base for the reasoning team

**URL:** llms-txt#shared-knowledge-base-for-the-reasoning-team

knowledge = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_reasoning_team",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=CohereReranker(model="rerank-v3.5"),
    ),
)

---

## Single Tool Reliability

**URL:** llms-txt#single-tool-reliability

Source: https://docs.agno.com/basics/evals/reliability/usage/reliability-single-tool

Example showing how to evaluate reliability of single tool calls.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## SSE Transport

**URL:** llms-txt#sse-transport

Source: https://docs.agno.com/basics/tools/mcp/transports/sse

Agno's MCP integration supports the [SSE transport](https://modelcontextprotocol.io/docs/basics/transports#server-sent-events-sse). This transport enables server-to-client streaming, and can prove more useful than [stdio](https://modelcontextprotocol.io/docs/basics/transports#standard-input%2Foutput-stdio) when working with restricted networks.

<Note>
  This transport is not recommended anymore by the MCP protocol. Use the [Streamable HTTP transport](/basics/tools/mcp/transports/streamable_http) instead.
</Note>

To use it, initialize the `MCPTools` passing the URL of the MCP server and setting the transport to `sse`:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

server_url = "http://localhost:8000/sse"

---

## Audio Input Agent

**URL:** llms-txt#audio-input-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/openai/completion/usage/audio-input-agent

```python cookbook/models/openai/chat/audio_input_agent.py theme={null}
import requests
from agno.agent import Agent, RunOutput  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat

---

## Create research agents

**URL:** llms-txt#create-research-agents

hackernews_agent = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools()],
    role="Research trending topics and discussions on HackerNews",
    instructions=[
        "Search for relevant discussions and articles",
        "Focus on high-quality posts with good engagement",
        "Extract key insights and technical details",
    ],
)

web_researcher = Agent(
    name="Web Researcher",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    role="Conduct comprehensive web research",
    instructions=[
        "Search for authoritative sources and documentation",
        "Find recent articles and blog posts",
        "Gather diverse perspectives on the topics",
    ],
)

---

## Knowledge Terminology

**URL:** llms-txt#knowledge-terminology

**Contents:**
- Key Terminology
  - Knowledge Base
  - Agentic RAG
  - Vector Embeddings
  - Chunking
  - Dynamic Few-Shot Learning
- Core Knowledge Components
  - Content Sources
  - Readers
  - Chunking Strategies

Source: https://docs.agno.com/basics/knowledge/terminology

Essential concepts and terminology for understanding how Knowledge works in Agno agents.

This reference guide defines the key concepts and terminology commonly encountered when working with Knowledge in Agno.

A structured repository of information that agents can search and retrieve from at runtime. Contains processed content optimized for AI understanding and retrieval.

**Retrieval Augmented Generation** where the agent actively decides when to search, what to search for, and how to use the retrieved information. Unlike traditional RAG systems, the agent has full control over the search process.

### Vector Embeddings

Mathematical representations of text that capture semantic meaning. Words and phrases with similar meanings have similar embeddings, enabling intelligent search beyond keyword matching.

The process of breaking large documents into smaller, manageable pieces that are optimal for search and retrieval while preserving context.

### Dynamic Few-Shot Learning

The pattern where agents retrieve specific examples or context at runtime to improve their performance on tasks, rather than having all information provided upfront.

<Accordion title="Example: Dynamic Few-Shot Learning in Action" icon="database">
  **Scenario:** Building a Text-to-SQL Agent

Instead of cramming all table schemas, column names, and example queries into the system prompt, you store this information in a knowledge base.

When a user asks for data, the agent:

1. Analyzes the request
  2. Searches for relevant schema information and example queries
  3. Uses the retrieved context to generate the best possible SQL query

This is "dynamic" because the agent gets exactly the information it needs for each specific query, and "few-shot" because it learns from examples retrieved at runtime.
</Accordion>

## Core Knowledge Components

The raw information agents need access to:

* **Documents**: PDFs, Word files, text files
* **Websites**: URLs, web pages, documentation sites
* **Databases**: SQL databases, APIs, structured data
* **Text**: Direct text content, notes, policies

Specialized components that parse different content types and extract meaningful text:

* **PDFReader**: Extracts text from PDF files, handles encryption
* **WebsiteReader**: Crawls web pages and extracts content
* **CSVReader**: Processes tabular data from CSV files
* **Custom Readers**: Build your own for specialized data sources

### Chunking Strategies

Methods for breaking content into optimal pieces:

* **Semantic Chunking**: Respects natural content boundaries
* **Fixed Size**: Uniform chunk sizes with overlap
* **Document Chunking**: Preserves document structure
* **Recursive Chunking**: Hierarchical splitting with multiple separators

Storage systems optimized for similarity search:

* **PgVector**: PostgreSQL extension for vector storage
* **LanceDB**: Fast, embedded vector database
* **Pinecone**: Managed vector database service
* **Qdrant**: High-performance vector search engine

## Component Relationships

The Knowledge system combines these components in a coordinated pipeline: **Readers** â†’ **Chunking** â†’ **Embedders** â†’ **Vector Databases** â†’ **Agent Retrieval**.

## Advanced Knowledge Features

### Custom Knowledge Retrievers

For complete control over how agents search your knowledge:

### Asynchronous Operations

Optimize performance with async knowledge operations:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Asynchronous Operations

Optimize performance with async knowledge operations:
```

---

## Web Extraction Agent

**URL:** llms-txt#web-extraction-agent

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code
- What to Expect
- Usage
- Next Steps

Source: https://docs.agno.com/examples/use-cases/agents/web-extraction-agent

Build an AI agent that transforms unstructured web content into organized, structured data by combining Firecrawl's web scraping with Pydantic's structured output validation.

By building this agent, you'll understand:

* How to integrate Firecrawl for reliable web scraping and content extraction
* How to define structured output schemas using Pydantic models
* How to create nested data structures for complex web content
* How to handle optional fields and varied page structures

Build competitive intelligence tools, content aggregation systems, knowledge base constructors, or automated documentation generators.

The agent extracts structured data from web pages in a systematic process:

1. **Fetch**: Uses Firecrawl to retrieve and parse the target webpage
2. **Analyze**: Identifies key sections, elements, and hierarchical structure
3. **Extract**: Pulls information according to the Pydantic output schema
4. **Structure**: Organizes content into nested models (sections, metadata, links, contact info)

The Pydantic schema ensures consistent output format regardless of the source website's structure, with optional fields handling varied page layouts gracefully.

The agent will scrape the target URL using Firecrawl and extract all information into a structured PageInformation object. The output includes the page title, description, features, organized content sections with headings, important links, contact information, and additional metadata.

The structured output ensures consistency and makes the extracted data easy to process, store, or display programmatically. Optional fields handle pages with varying structures gracefully.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* Change the target URL to extract data from different websites
* Modify the `PageInformation` Pydantic model to capture additional fields
* Adjust the agent's instructions to focus on specific content types
* Explore [Firecrawl Tools](/integrations/toolkits/web-scrape/firecrawl) for advanced scraping options

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The agent will scrape the target URL using Firecrawl and extract all information into a structured PageInformation object. The output includes the page title, description, features, organized content sections with headings, important links, contact information, and additional metadata.

The structured output ensures consistency and makes the extracted data easy to process, store, or display programmatically. Optional fields handle pages with varying structures gracefully.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Debug Level

**URL:** llms-txt#debug-level

Source: https://docs.agno.com/basics/agents/usage/debug-level

This example demonstrates how to set different debug levels for an agent. The debug level controls the amount of debug information displayed, helping you troubleshoot and understand agent behavior at different levels of detail.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your Anthropic API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your Anthropic API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Update Evaluation Run

**URL:** llms-txt#update-evaluation-run

Source: https://docs.agno.com/reference-api/schema/evals/update-evaluation-run

patch /eval-runs/{eval_run_id}
Update the name or other properties of an existing evaluation run.

---

## Step 2: Query the knowledge base with Agent using filters from query automatically

**URL:** llms-txt#step-2:-query-the-knowledge-base-with-agent-using-filters-from-query-automatically

---

## Agent Tools

**URL:** llms-txt#agent-tools

**Contents:**
- Using a Toolkit
- Writing your own Tools
- Developer Resources

Source: https://docs.agno.com/basics/tools/agent

Learn how to use tools in Agno to build AI agents.

**Agents use tools to take actions and interact with external systems**.

Tools are functions that an Agent can run to achieve tasks. For example: searching the web, running SQL, sending an email or calling APIs. You can use any python function as a tool or use a pre-built Agno **toolkit**.

The general syntax is:

Agno provides many pre-built **toolkits** that you can add to your Agents. For example, let's use the DuckDuckGo toolkit to search the web.

<Tip>
  You can find more toolkits in the [Toolkits](/integrations/toolkits) guide.
</Tip>

<Steps>
  <Step title="Create Web Search Agent">
    Create a file `web_search.py`

<Step title="Run the agent">
    Install libraries

## Writing your own Tools

For more control, write your own python functions and add them as tools to an Agent. For example, here's how to add a `get_top_hackernews_stories` tool to an Agent.

* [Available toolkits](/integrations/toolkits)
* [Creating your own tools](/basics/tools/creating-tools/overview)

## Developer Resources

* View the [Agent schema](/reference/agents/agent)
* View the [Knowledge schema](/reference/knowledge/knowledge)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/)

**Examples:**

Example 1 (unknown):
```unknown
## Using a Toolkit

Agno provides many pre-built **toolkits** that you can add to your Agents. For example, let's use the DuckDuckGo toolkit to search the web.

<Tip>
  You can find more toolkits in the [Toolkits](/integrations/toolkits) guide.
</Tip>

<Steps>
  <Step title="Create Web Search Agent">
    Create a file `web_search.py`
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
    Install libraries
```

Example 3 (unknown):
```unknown
Run the agent
```

Example 4 (unknown):
```unknown
</Step>
</Steps>

## Writing your own Tools

For more control, write your own python functions and add them as tools to an Agent. For example, here's how to add a `get_top_hackernews_stories` tool to an Agent.
```

---

## Upstash Async

**URL:** llms-txt#upstash-async

**Contents:**
- Code

Source: https://docs.agno.com/integrations/vectordb/upstash/usage/async-upstash-db

```python cookbook/knowledge/vector_db/upstash_db/upstash_db.py theme={null}
import asyncio
import os

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.upstashdb import UpstashVectorDb

---

## Setup AgentOS with MCP enabled

**URL:** llms-txt#setup-agentos-with-mcp-enabled

**Contents:**
- Available MCP Tools
  - `get_agentos_config`
  - `run_agent`
  - `run_team`
  - `run_workflow`
  - `get_sessions_for_agent`
  - `get_sessions_for_team`
  - `get_sessions_for_workflow`
  - `create_memory`
  - `get_memories_for_user`

agent_os = AgentOS(
    description="Example app with MCP enabled",
    agents=[web_research_agent],
    enable_mcp_server=True,  # This enables a LLM-friendly MCP server at /mcp
)

app = agent_os.get_app()

if __name__ == "__main__":
    # Your MCP server will be available at http://localhost:7777/mcp
    agent_os.serve(app="enable_mcp_example:app", reload=True)

Once enabled, your AgentOS will expose an MCP server via the `/mcp` endpoint.

You can see a complete example [here](/agent-os/usage/mcp/enable-mcp-example).

## Available MCP Tools

When you expose your AgentOS as an MCP server, the following MCP tools will be available:

### `get_agentos_config`

Get the configuration of the AgentOS

Run the desired Agent

* `agent_id` (str): The ID of the agent to run
* `message` (str): The message to send to the agent

* `team_id` (str): The ID of the team to run
* `message` (str): The message to send to the team

Run the desired Workflow

* `workflow_id` (str): The ID of the workflow to run
* `message` (str): The message to send to the workflow

### `get_sessions_for_agent`

Get the list of sessions for the desired Agent

* `agent_id` (str): The ID of the agent to get the sessions for
* `db_id` (str): The ID of the database to use
* `user_id` (Optional\[str]): The ID of the user to get the sessions for
* `sort_by` (Optional\[str]): The field to sort the sessions by. Defaults to `created_at`
* `sort_order` (Optional\[str]): The order to sort the sessions by. Defaults to `desc`

### `get_sessions_for_team`

Get the list of sessions for the desired Team

* `team_id` (str): The ID of the team to get the sessions for
* `db_id` (str): The ID of the database to use
* `user_id` (Optional\[str]): The ID of the user to get the sessions for
* `sort_by` (Optional\[str]): The field to sort the sessions by. Defaults to `created_at`
* `sort_order` (Optional\[str]): The order to sort the sessions by. Defaults to `desc`

### `get_sessions_for_workflow`

Get the list of sessions for the desired Workflow

* `workflow_id` (str): The ID of the workflow to get the sessions for
* `db_id` (str): The ID of the database to use
* `user_id` (Optional\[str]): The ID of the user to get the sessions for
* `sort_by` (Optional\[str]): The field to sort the sessions by. Defaults to `created_at`
* `sort_order` (Optional\[str]): The order to sort the sessions by. Defaults to `desc`

Create a new user memory

* `db_id` (str): The ID of the database to use
* `memory` (str): The memory content to store
* `user_id` (str): The user this memory is about
* `topics` (Optional\[list\[str]]): The topics of the memory

### `get_memories_for_user`

Get the list of memories for the given user

* `user_id` (str): The ID of the user to get the memories for
* `db_id` (Optional\[str]): The ID of the database to use
* `sort_by` (Optional\[str]): The field to sort the memories by. Defaults to `created_at`
* `sort_order` (Optional\[str]): The order to sort the memories by. Defaults to `desc`

Update the desired memory

* `db_id` (str): The ID of the database to use
* `memory_id` (str): The ID of the memory to update
* `memory` (str): The memory content to store
* `user_id` (str): The ID of the user to update the memory for

Delete the desired memory

* `db_id` (str): The ID of the database to use
* `memory_id` (str): The ID of the memory to delete

See a full example [here](/agent-os/usage/mcp/enable-mcp-example).

---

## CometAPI

**URL:** llms-txt#cometapi

**Contents:**
- Authentication
- Example
- Parameters
- Available Models

Source: https://docs.agno.com/integrations/models/gateways/cometapi/overview

Learn how to use CometAPI models in Agno.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.0.8" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.0.8">v2.0.8</Tooltip>
</Badge>

CometAPI is a platform for providing endpoints for Large Language models.

See all CometAPI supported models and pricing [here](https://api.cometapi.com/pricing).

Set your `COMETAPI_KEY` environment variable. Get your API key from [here](https://api.cometapi.com/console/token).

Use `CometAPI` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/integrations/models/gateways/cometapi/usage/basic-stream). </Note>

| Parameter  | Type            | Default                         | Description                                                  |
| ---------- | --------------- | ------------------------------- | ------------------------------------------------------------ |
| `id`       | `str`           | `"gpt-5-mini"`                  | The id of the model to use                                   |
| `name`     | `str`           | `"CometAPI"`                    | The name of the model                                        |
| `api_key`  | `Optional[str]` | `None`                          | The API key for CometAPI (defaults to COMETAPI\_KEY env var) |
| `base_url` | `str`           | `"https://api.cometapi.com/v1"` | The base URL for the CometAPI                                |

`CometAPI` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/integrations/models/native/openai/completion/overview).

CometAPI provides access to 300+ AI models. You can fetch the available models programmatically:

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `CometAPI` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note> View more examples [here](/integrations/models/gateways/cometapi/usage/basic-stream). </Note>

## Parameters

| Parameter  | Type            | Default                         | Description                                                  |
| ---------- | --------------- | ------------------------------- | ------------------------------------------------------------ |
| `id`       | `str`           | `"gpt-5-mini"`                  | The id of the model to use                                   |
| `name`     | `str`           | `"CometAPI"`                    | The name of the model                                        |
| `api_key`  | `Optional[str]` | `None`                          | The API key for CometAPI (defaults to COMETAPI\_KEY env var) |
| `base_url` | `str`           | `"https://api.cometapi.com/v1"` | The base URL for the CometAPI                                |

`CometAPI` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/integrations/models/native/openai/completion/overview).

## Available Models

CometAPI provides access to 300+ AI models. You can fetch the available models programmatically:
```

---

## Use custom reader

**URL:** llms-txt#use-custom-reader

**Contents:**
- Best Practices
  - Choose the Right Reader
  - Configure Chunking Appropriately
  - Optimize for Performance
  - Handle Errors Gracefully
- Next Steps

knowledge_base.add_content(
    path="data/documents",
    reader=reader  # Override default reader
)
```

### Choose the Right Reader

* Use specialized readers for better extraction quality
* Consider format-specific features (PDF encryption, CSV delimiters, etc.)

### Configure Chunking Appropriately

* Smaller chunks for precise retrieval
* Larger chunks for maintaining context
* Use semantic chunking for structured documents

### Optimize for Performance

* Use async readers for I/O-heavy operations
* Batch process multiple files when possible
* Cache readers through ReaderFactory when processing many files

### Handle Errors Gracefully

* Readers return empty lists for failed processing
* Check reader logs for debugging information
* Provide fallback readers for unknown formats

<CardGroup cols={3}>
  <Card title="Chunking Strategies" icon="scissors" href="/basics/knowledge/chunking/overview">
    Learn how to optimize content chunking for better search results
  </Card>

<Card title="Content Types" icon="file-lines" href="/basics/knowledge/content-types">
    Understand different ways to add information to your knowledge base
  </Card>

<Card title="Vector Databases" icon="database" href="/basics/vectordb/overview">
    Choose the right storage solution for your processed content
  </Card>

<Card title="Examples" icon="code" href="/basics/knowledge/readers/usage/pdf-reader">
    See readers in action with practical examples
  </Card>
</CardGroup>

---

## Azure OpenAI with Reasoning Tools

**URL:** llms-txt#azure-openai-with-reasoning-tools

Source: https://docs.agno.com/basics/reasoning/usage/tools/azure-openai-reasoning-tools

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your API keys">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your API keys">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The Agent sessions and runs will now be stored in SQLite with custom table names

**URL:** llms-txt#the-agent-sessions-and-runs-will-now-be-stored-in-sqlite-with-custom-table-names

**Contents:**
- Developer Resources

agent.print_response("How many people live in Canada?")
agent.print_response("And in Mexico?")
agent.print_response("List my messages one by one")
```

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/examples/selecting_tables.py)

---

## Reliability with Database Logging

**URL:** llms-txt#reliability-with-database-logging

Source: https://docs.agno.com/basics/evals/reliability/usage/reliability-db-logging

Example showing how to store reliability evaluation results in the database.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Image Compare Agent

**URL:** llms-txt#image-compare-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/mistral/usage/image-compare-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Workflow with Input Schema Validation

**URL:** llms-txt#workflow-with-input-schema-validation

**Contents:**
- Key Features:

Source: https://docs.agno.com/basics/workflows/usage/workflow-with-input-schema

This example demonstrates **Workflows** support for input schema validation using Pydantic models to ensure type safety and data integrity at the workflow entry point.

This example shows how to use input schema validation in workflows to enforce type safety and data structure validation. By defining an `input_schema` with a Pydantic model, you can ensure that your workflow receives properly structured and validated data before execution begins.

* **Type Safety**: Automatic validation of input data against Pydantic models
* **Structure Validation**: Ensure all required fields are present and correctly typed
* **Clear Contracts**: Define exactly what data your workflow expects
* **Error Prevention**: Catch invalid inputs before workflow execution begins
* **Multiple Input Formats**: Support for Pydantic models and matching dictionaries

```python workflow_with_input_schema.py theme={null}
from typing import List

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

class DifferentModel(BaseModel):
    name: str

class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)

---

## Basic Reasoning Agent

**URL:** llms-txt#basic-reasoning-agent

Source: https://docs.agno.com/basics/reasoning/usage/agents/basic-cot

Learn how to equip your agent with reasoning capabilities

Example showing how to configure a basic Reasoning Agent, using the `reasoning=True` flag.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Pipedream Auth

**URL:** llms-txt#pipedream-auth

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/mcp/usage/pipedream-auth

This example shows how to add authorization when integrating Pipedream MCP servers with Agno Agents.

---

## Cohere Embedder

**URL:** llms-txt#cohere-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/cohere/usage/cohere-embedder

```python  theme={null}
import asyncio
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = CohereEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## PostgresDb

**URL:** llms-txt#postgresdb

Source: https://docs.agno.com/reference/storage/postgres

`PostgresDb` is a class that implements the Db interface using PostgreSQL as the backend storage system. It provides robust, relational storage for agent sessions with support for JSONB data types, schema versioning, and efficient querying.

<Snippet file="db-postgres-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Set up ContentsDB - tracks content metadata

**URL:** llms-txt#set-up-contentsdb---tracks-content-metadata

contents_db = PostgresDb(
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    knowledge_table="knowledge_contents"  # Optional: custom table name
)

---

## Print team leader message metrics

**URL:** llms-txt#print-team-leader-message-metrics

print("---" * 5, "Team Leader Message Metrics", "---" * 5)
if run_response.messages:
    for message in run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

---

## Multi-turn Audio Agent

**URL:** llms-txt#multi-turn-audio-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/audio/usage/audio-multi-turn

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Agent Run Metadata

**URL:** llms-txt#agent-run-metadata

Source: https://docs.agno.com/basics/agents/usage/agent-run-metadata

This example demonstrates how to attach custom metadata to agent runs. This is useful for tracking business context, request types, and operational information for monitoring and analytics.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Retrieve the memories about the user

**URL:** llms-txt#retrieve-the-memories-about-the-user

**Contents:**
- Memory Data Model
- Learn more

memories = agent.get_user_memories(user_id="123")
print(memories)
```

Each memory stored in your database contains the following fields:

| Field        | Type   | Description                                     |
| ------------ | ------ | ----------------------------------------------- |
| `memory_id`  | `str`  | The unique identifier for the memory.           |
| `memory`     | `str`  | The memory content, stored as a string.         |
| `topics`     | `list` | The topics of the memory.                       |
| `input`      | `str`  | The input that generated the memory.            |
| `user_id`    | `str`  | The user ID of the memory.                      |
| `agent_id`   | `str`  | The agent ID of the memory.                     |
| `team_id`    | `str`  | The team ID of the memory.                      |
| `updated_at` | `int`  | The timestamp when the memory was last updated. |

<Tip>
  View and manage all your memories visually through the [Memories page in AgentOS](https://os.agno.com/memory)/
</Tip>

<CardGroup cols={3}>
  <Card title="Agent Memory" icon="robot" iconType="duotone" href="/basics/memory/agent">
    Learn about agent memory.
  </Card>

<Card title="Team Memory" icon="users" iconType="duotone" href="/basics/memory/team">
    Learn about team memory.
  </Card>

<Card title="Working with Memories" icon="file-lines" iconType="duotone" href="/basics/memory/working-with-memories">
    Learn about the nuances of working with memories.
  </Card>

<Card title="Production Best Practices" icon="shield-check" iconType="duotone" href="/basics/memory/best-practices">
    Learn about best practices for production use of memories.
  </Card>
</CardGroup>

---

## Nano Banana

**URL:** llms-txt#nano-banana

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/nano-banana

**NanoBananaTools** enables an Agent to generate images using [Google's Gemini 2.5 Flash Image model](https://ai.google.dev/gemini-api/docs/image-generation) (Nano Banana).

You need to install the `google-genai` and `Pillow` libraries.

Set the `GOOGLE_API_KEY` environment variable. You can obtain an API key from [Google AI Studio](https://aistudio.google.com/apikey).

The following agent will use Nano Banana to generate an image based on a text prompt.

```python nano_banana_tools.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.tools.nano_banana import NanoBananaTools

**Examples:**

Example 1 (unknown):
```unknown
Set the `GOOGLE_API_KEY` environment variable. You can obtain an API key from [Google AI Studio](https://aistudio.google.com/apikey).
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Nano Banana to generate an image based on a text prompt.
```

---

## Advanced Workflow Patterns

**URL:** llms-txt#advanced-workflow-patterns

Source: https://docs.agno.com/basics/workflows/workflow-patterns/advanced-workflow-patterns

Combine multiple workflow patterns to build sophisticated, production-ready automation systems

**Pattern Combination**: Conditional Logic + Parallel Execution + Iterative Loops + Custom Processing + Dynamic Routing

This example demonstrates how deterministic patterns can be composed to create complex yet predictable workflows.

```python advanced_workflow_patterns.py theme={null}
from agno.workflow import Condition, Loop, Parallel, Router, Step, Workflow

def research_post_processor(step_input) -> StepOutput:
    """Post-process and consolidate research data from parallel conditions"""
    research_data = step_input.previous_step_content or ""

try:
        # Analyze research quality and completeness
        word_count = len(research_data.split())
        has_tech_content = any(keyword in research_data.lower()
                              for keyword in ["technology", "ai", "software", "tech"])
        has_business_content = any(keyword in research_data.lower()
                                  for keyword in ["market", "business", "revenue", "strategy"])

# Create enhanced research summary
        enhanced_summary = f"""
            ## Research Analysis Report

**Data Quality:** {"âœ“ High-quality" if word_count > 200 else "âš  Limited data"}

**Content Coverage:**
            - Technical Analysis: {"âœ“ Completed" if has_tech_content else "âœ— Not available"}
            - Business Analysis: {"âœ“ Completed" if has_business_content else "âœ— Not available"}

**Research Findings:**
            {research_data}
        """.strip()

return StepOutput(
            content=enhanced_summary,
            success=True,
        )

except Exception as e:
        return StepOutput(
            content=f"Research post-processing failed: {str(e)}",
            success=False,
            error=str(e)
        )

---

## Managing Tool Calls

**URL:** llms-txt#managing-tool-calls

**Contents:**
- Code

Source: https://docs.agno.com/basics/context/team/usage/filter-tool-calls-from-history

This example demonstrates how to use `max_tool_calls_from_history` to limit tool calls in team context across multiple research queries.

```python filter_tool_calls_from_history.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Multimodal Query for Video Analysis

**URL:** llms-txt#multimodal-query-for-video-analysis

query = """
You are an expert in video content creation, specializing in crafting engaging short-form content for platforms like YouTube Shorts and Instagram Reels. Your task is to analyze the provided video and identify segments that maximize viewer engagement.

For each video, you'll:

1. Identify key moments that will capture viewers' attention, focusing on:
   - High-energy sequences
   - Emotional peaks
   - Surprising or unexpected moments
   - Strong visual and audio elements
   - Clear narrative segments with compelling storytelling

2. Extract segments that work best for short-form content, considering:
   - Optimal length (strictly 15â€“60 seconds)
   - Natural start and end points that ensure smooth transitions
   - Engaging pacing that maintains viewer attention
   - Audio-visual harmony for an immersive experience
   - Vertical format compatibility and adjustments if necessary

3. Provide a detailed analysis of each segment, including:
   - Precise timestamps (Start Time | End Time in MM:SS format)
   - A clear description of why the segment would be engaging
   - Suggestions on how to enhance the segment for short-form content
   - An importance score (1-10) based on engagement potential

Your goal is to identify moments that are visually compelling, emotionally engaging, and perfectly optimized for short-form platforms.
"""

---

## File Input for Tools

**URL:** llms-txt#file-input-for-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/multimodal/agent/usage/file-input-for-tool

This example demonstrates how tools can access and process files (PDFs, documents, etc.) passed to the agent. It shows uploading a PDF file, processing it with a custom tool, and having the LLM respond based on the extracted content.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Cerebras

**URL:** llms-txt#cerebras

**Contents:**
- Prerequisites
- Basic Usage

Source: https://docs.agno.com/integrations/models/gateways/cerebras/overview

Learn how to use Cerebras models in Agno.

[Cerebras Inference](https://inference-docs.cerebras.ai/introduction) provides high-speed, low-latency AI model inference powered by Cerebras Wafer-Scale Engines and CS-3 systems. Agno integrates directly with the Cerebras Python SDK, allowing you to use state-of-the-art Llama models with a simple interface.

To use Cerebras with Agno, you need to:

1. **Install the required packages:**

2. **Set your API key:**
   The Cerebras SDK expects your API key to be available as an environment variable:

Here's how to use a Cerebras model with Agno:

```python  theme={null}
from agno.agent import Agent
from agno.models.cerebras import Cerebras

agent = Agent(
    model=Cerebras(id="llama-4-scout-17b-16e-instruct"),
    markdown=True,
)

**Examples:**

Example 1 (unknown):
```unknown
2. **Set your API key:**
   The Cerebras SDK expects your API key to be available as an environment variable:
```

Example 2 (unknown):
```unknown
## Basic Usage

Here's how to use a Cerebras model with Agno:
```

---

## Create Memory

**URL:** llms-txt#create-memory

Source: https://docs.agno.com/reference-api/schema/memory/create-memory

post /memories
Create a new user memory with content and associated topics. Memories are used to store contextual information for users across conversations.

---

## Print the response on the terminal

**URL:** llms-txt#print-the-response-on-the-terminal

**Contents:**
- Usage

agent.print_response("Share a 2 sentence horror story", stream=True)
bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U groq agno
    bash Mac theme={null}
      python cookbook/models/groq/basic_stream.py
      bash Windows theme={null}
      python cookbook/models/groq/basic_stream.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cartesia

**URL:** llms-txt#cartesia

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/others/cartesia

Tools for interacting with Cartesia Voice AI services including text-to-speech and voice localization

**CartesiaTools** enable an Agent to perform text-to-speech, list available voices, and localize voices using [Cartesia](https://docs.cartesia.ai/).

The following example requires the `cartesia` library and an API key.

```python  theme={null}
from agno.agent import Agent
from agno.tools.cartesia import CartesiaTools
from agno.utils.audio import write_audio_to_file

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example
```

---

## Team History for Members

**URL:** llms-txt#team-history-for-members

**Contents:**
- How it Works
- Code

Source: https://docs.agno.com/basics/chat-history/team/usage/team-history

This example demonstrates a team where the team leader routes requests to the appropriate member, and the members respond directly to the user.

Using `add_team_history_to_members=True`, each team member has access to the shared history of the team, allowing them to use context from previous interactions with other members.

When `add_team_history_to_members=True`, team history is appended to tasks sent to members:

This allows the Spanish agent to recall the name "John" that was originally shared with the German agent.

```python team_history.py theme={null}
from uuid import uuid4

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team

german_agent = Agent(
    name="German Agent",
    role="You answer German questions.",
    model=OpenAIChat(id="o3-mini"),
)

spanish_agent = Agent(
    name="Spanish Agent",
    role="You answer Spanish questions.",
    model=OpenAIChat(id="o3-mini"),
)

multi_lingual_q_and_a_team = Team(
    name="Multi Lingual Q and A Team",
    model=OpenAIChat("o3-mini"),
    members=[german_agent, spanish_agent],
    instructions=[
        "You are a multi lingual Q and A team that can answer questions in English and Spanish. You MUST delegate the task to the appropriate member based on the language of the question.",
        "If the question is in German, delegate to the German agent. If the question is in Spanish, delegate to the Spanish agent.",
        "Always translate the response from the appropriate language to English and show both the original and translated responses.",
    ],
    db=SqliteDb(
        db_file="tmp/multi_lingual_q_and_a_team.db"
    ),  # Add a database to store the conversation history. This is a requirement for history to work correctly.
    determine_input_for_members=False,  # Send the input directly to the member agents without the team leader synthesizing its own input.
    respond_directly=True,
    add_team_history_to_members=True,  # Send all interactions between the user and the team to the member agents.
)

session_id = f"conversation_{uuid4()}"

**Examples:**

Example 1 (unknown):
```unknown
<team_history_context>
input: Hallo, wie heiÃŸt du? Meine Name ist John.
response: Ich heiÃŸe ChatGPT.
</team_history_context>
```

---

## Run Agent

**URL:** llms-txt#run-agent

Source: https://docs.agno.com/reference-api/schema/agui/run-agent

---

## Tool Call Limit

**URL:** llms-txt#tool-call-limit

Source: https://docs.agno.com/basics/tools/tool-call-limit

Learn to limit the number of tool calls an agent can make.

Limiting the number of tool calls an Agent can make is useful to prevent loops and have better control over costs and performance.

Doing this is very simple with Agno. You just need to pass the `tool_call_limit` parameter when initializing your Agent or Team.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[YFinanceTools(company_news=True, cache_results=True)],
    tool_call_limit=1, # The Agent will not perform more than one tool call.
)

---

## Setup your Agent using Claude as main model, and DeepSeek as reasoning model

**URL:** llms-txt#setup-your-agent-using-claude-as-main-model,-and-deepseek-as-reasoning-model

claude_with_deepseek_reasoner = Agent(
    model=Claude(id="claude-3-5-sonnet-20241022"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)

---

## Workflow Tools

**URL:** llms-txt#workflow-tools

**Contents:**
- Example

Source: https://docs.agno.com/basics/workflows/workflow-tools

How to execute a workflow inside an Agent or Team

You can give a workflow to an Agent or Team to execute using `WorkflowTools`.

```python  theme={null}
from agno.agent import Agent    
from agno.models.openai import OpenAIChat
from agno.tools.workflow import WorkflowTools

---

## Setup

**URL:** llms-txt#setup

**Contents:**
- Set your AWS credentials
- Run PgVector

## Set your AWS credentials

<Note>
  By default, this embedder uses the `cohere.embed-multilingual-v3` model. You must enable access to this model from the AWS Bedrock model catalog before using this embedder.
</Note>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  By default, this embedder uses the `cohere.embed-multilingual-v3` model. You must enable access to this model from the AWS Bedrock model catalog before using this embedder.
</Note>

## Run PgVector
```

---

## Example usage showing stock analysis

**URL:** llms-txt#example-usage-showing-stock-analysis

agent.print_response(
    "Get me the current stock price and key information for Apple (AAPL)"
)

---

## Older searches will be filtered from context

**URL:** llms-txt#older-searches-will-be-filtered-from-context

**Contents:**
- Additional input

team.print_response("Search for startups")
team.print_response("Search for weather in Mumbai")
team.print_response("Search for latest startup fundraises")
team.print_response("What topics did I search for recently?")
python  theme={null}
from agno.team import Team
from agno.models.message import Message
from agno.models.openai.chat import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
In this example:

* **Run 1-3:** Team sees tool calls \[1], \[1,2], \[1,2,3,4,5]
* **Run 4:** Team sees tool calls from runs that fit within the last 5 tool calls (older tool calls filtered out)
* **Run 5:** Team sees tool calls from runs that fit within the last 5 tool calls (earlier tool calls filtered out)

<Note>
  **Important:** `max_tool_calls_from_history` filters tool calls from the runs loaded by `num_history_runs`. Your database always contains the complete history.
</Note>

See the [full example](/basics/context/team/usage/filter-tool-calls-from-history) for a complete demonstration.

## Additional input

You can add entire additional messages to your team's context using the `additional_input` parameter.
These messages are added to the context as if they were part of the conversation history.

You can give your team examples of how it should respond (also called "few-shot prompting"):
```

---

## Setup virtual environment

**URL:** llms-txt#setup-virtual-environment

./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate

---

## Obtain the default credentials and project id from your gcloud CLI session.

**URL:** llms-txt#obtain-the-default-credentials-and-project-id-from-your-gcloud-cli-session.

credentials, project_id = google.auth.default()

---

## Setup the JSON database

**URL:** llms-txt#setup-the-json-database

db = JsonDb(db_path="tmp/json_db")

---

## Basic Slack Agent

**URL:** llms-txt#basic-slack-agent

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/agent-os/usage/interfaces/slack/basic

Create a basic AI agent that integrates with Slack for conversations

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **Slack Integration**: Responds to direct messages and channel mentions
* **Conversation History**: Maintains context with last 3 interactions
* **Persistent Memory**: SQLite database for session storage
* **DateTime Context**: Time-aware responses
* **GPT-4o Powered**: Intelligent conversational capabilities

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## === BASIC LINEAR WORKFLOW ===

**URL:** llms-txt#===-basic-linear-workflow-===

basic_workflow = Workflow(
    name="Basic Linear Workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
)

if __name__ == "__main__":
    print("ðŸš€ Running Basic Linear Workflow Example")
    print("=" * 50)

try:
        basic_workflow.print_response(
            input="Recent breakthroughs in quantum computing",
            stream=True,
        )
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback

traceback.print_exc()
```

To see the async example, see the cookbook-

* [Condition steps workflow (async streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_02_workflows_conditional_execution/sync/condition_steps_workflow_stream.py)

---

## Skip files that were already processed

**URL:** llms-txt#skip-files-that-were-already-processed

knowledge.add_content(
    path="large_document.pdf",
    skip_if_exists=True,  # Don't reprocess existing files
    upsert=False          # Don't update existing
)

---

## Define specialized agents for meal planning conversation

**URL:** llms-txt#define-specialized-agents-for-meal-planning-conversation

meal_suggester = Agent(
    name="Meal Suggester",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a friendly meal planning assistant who suggests meal categories and cuisines.",
        "Consider the time of day, day of the week, and any context from the conversation.",
        "Keep suggestions broad (Italian, Asian, healthy, comfort food, quick meals, etc.)",
        "Ask follow-up questions to understand preferences better.",
    ],
)

recipe_specialist = Agent(
    name="Recipe Specialist",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "You are a recipe expert who provides specific, detailed recipe recommendations.",
        "Pay close attention to the full conversation to understand user preferences and restrictions.",
        "If the user mentioned avoiding certain foods or wanting healthier options, respect that.",
        "Provide practical, easy-to-follow recipe suggestions with ingredients and basic steps.",
        "Reference the conversation naturally (e.g., 'Since you mentioned wanting something healthier...')",
    ],
)

def analyze_food_preferences(step_input: StepInput) -> StepOutput:
    """
    Smart function that analyzes conversation history to understand user food preferences
    """
    current_request = step_input.input
    conversation_context = step_input.previous_step_content or ""

# Simple preference analysis based on conversation
    preferences = {
        "dietary_restrictions": [],
        "cuisine_preferences": [],
        "avoid_list": [],
        "cooking_style": "any",
    }

# Analyze conversation for patterns
    full_context = f"{conversation_context} {current_request}".lower()

# Dietary restrictions and preferences
    if any(word in full_context for word in ["healthy", "healthier", "light", "fresh"]):
        preferences["dietary_restrictions"].append("healthy")
    if any(word in full_context for word in ["vegetarian", "veggie", "no meat"]):
        preferences["dietary_restrictions"].append("vegetarian")
    if any(word in full_context for word in ["quick", "fast", "easy", "simple"]):
        preferences["cooking_style"] = "quick"
    if any(word in full_context for word in ["comfort", "hearty", "filling"]):
        preferences["cooking_style"] = "comfort"

# Foods/cuisines to avoid (mentioned recently)
    if "italian" in full_context and (
        "had" in full_context or "yesterday" in full_context
    ):
        preferences["avoid_list"].append("Italian")
    if "chinese" in full_context and (
        "had" in full_context or "recently" in full_context
    ):
        preferences["avoid_list"].append("Chinese")

# Preferred cuisines mentioned positively
    if "love asian" in full_context or "like asian" in full_context:
        preferences["cuisine_preferences"].append("Asian")
    if "mediterranean" in full_context:
        preferences["cuisine_preferences"].append("Mediterranean")

# Create guidance for the recipe agent
    guidance = []
    if preferences["dietary_restrictions"]:
        guidance.append(
            f"Focus on {', '.join(preferences['dietary_restrictions'])} options"
        )
    if preferences["avoid_list"]:
        guidance.append(
            f"Avoid {', '.join(preferences['avoid_list'])} cuisine since user had it recently"
        )
    if preferences["cuisine_preferences"]:
        guidance.append(
            f"Consider {', '.join(preferences['cuisine_preferences'])} options"
        )
    if preferences["cooking_style"] != "any":
        guidance.append(f"Prefer {preferences['cooking_style']} cooking style")

analysis_result = f"""
        PREFERENCE ANALYSIS:
        Current Request: {current_request}

Detected Preferences:
        {chr(10).join(f"â€¢ {g}" for g in guidance) if guidance else "â€¢ No specific preferences detected"}

RECIPE AGENT GUIDANCE:
        Based on the conversation history, please provide recipe recommendations that align with these preferences.
        Reference the conversation naturally and explain why these recipes fit their needs.
    """.strip()

return StepOutput(content=analysis_result)

---

## Create agent with TypedDict input schema

**URL:** llms-txt#create-agent-with-typeddict-input-schema

hackernews_agent = Agent(
    name="Hackernews Agent with TypedDict",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
    input_schema=ResearchTopicDict,
)

---

## LiteLLM OpenAI

**URL:** llms-txt#litellm-openai

**Contents:**
- Proxy Server Integration
  - Starting the Proxy Server
  - Using the Proxy
  - Configuration Options
- Examples
  - Proxy Examples

Source: https://docs.agno.com/integrations/models/gateways/litellm-openai/overview

Use LiteLLM with Agno with an openai-compatible proxy server.

## Proxy Server Integration

LiteLLM can also be used as an OpenAI-compatible proxy server, allowing you to route requests to different models through a unified API.

### Starting the Proxy Server

First, install LiteLLM with proxy support:

Start the proxy server:

The `LiteLLMOpenAI` class connects to the LiteLLM proxy using an OpenAI-compatible interface:

### Configuration Options

The `LiteLLMOpenAI` class accepts the following parameters:

| Parameter  | Type | Description                                                    | Default                                      |
| ---------- | ---- | -------------------------------------------------------------- | -------------------------------------------- |
| `id`       | str  | Model identifier                                               | "gpt-5-mini"                                 |
| `name`     | str  | Display name for the model                                     | "LiteLLM"                                    |
| `provider` | str  | Provider name                                                  | "LiteLLM"                                    |
| `api_key`  | str  | API key (falls back to LITELLM\_API\_KEY environment variable) | None                                         |
| `base_url` | str  | URL of the LiteLLM proxy server                                | "[http://0.0.0.0:4000](http://0.0.0.0:4000)" |

`LiteLLMOpenAI` is a subclass of the [OpenAILike](/integrations/models/openai-like) class and has access to the same params.

Check out these examples in the cookbook:

<Note> View more examples [here](/integrations/models/gateways/litellm-openai/usage/basic-stream). </Note>

**Examples:**

Example 1 (unknown):
```unknown
Start the proxy server:
```

Example 2 (unknown):
```unknown
### Using the Proxy

The `LiteLLMOpenAI` class connects to the LiteLLM proxy using an OpenAI-compatible interface:
```

---

## Workflow History & Continuous Execution

**URL:** llms-txt#workflow-history-&-continuous-execution

**Contents:**
- How It Works
- Control Levels
  - Workflow-Level History
  - Step-Level History
- Precedence Logic
  - History Length Control

Source: https://docs.agno.com/basics/chat-history/workflow/overview

Build conversational workflows that maintain context across multiple executions, creating truly intelligent and natural interactions.

<Badge icon="code-branch" color="orange">
  <Tooltip tip="Introduced in v2.1.4" cta="View release notes" href="https://github.com/agno-agi/agno/releases/tag/v2.1.4">v2.1.4</Tooltip>
</Badge>

Workflow History enables your Agno workflows to remember and reference previous conversations, transforming isolated executions into continuous, context-aware interactions.

Instead of starting fresh each time, with Workflow History you can:

* **Build on previous interactions** - Reference the context of past interactions
* **Avoid repetitive questions** - Avoid requesting previously provided information
* **Maintain context continuity** - Create a conversational experience
* **Learn from patterns** - Analyze historical data to make better decisions

<Tip>
  Note that this feature is different from `add_history_to_context`.
  This does not add the history of a particular agent or team, but the full workflow history to either all or some steps.
</Tip>

When workflow history is enabled, previous messages are automatically injected into agent/team inputs as structured context:

Along with this, in using Steps with custom functions, you can access this history in the following ways:

1. As a formatted context string as shown above
2. In a structured format as well for more control

<Note>
  A database is required to use Workflow history. Runs across different executions will be persisted there.
</Note>

<Note>
  You can use these helper functions to access the history:

* `step_input.get_workflow_history(num_runs=3)`
  * `step_input.get_workflow_history_context(num_runs=3)`
</Note>

Refer to [StepInput](/reference/workflows/step_input) reference for more details.

You can be specific about which Steps to add the history to:

### Workflow-Level History

Add workflow history to **all steps** in the workflow:

### Step-Level History

Add workflow history to **specific steps** only:

<Note>
  You can also put `add_workflow_history=False` to disable history for a specific step.
</Note>

**Step-level settings always take precedence over workflow-level settings**:

### History Length Control

**By default, all available history is included** (no limit). It is recommended to use a fixed history run limit to avoid bloating the LLM context window.

You can control this at both levels:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Along with this, in using Steps with custom functions, you can access this history in the following ways:

1. As a formatted context string as shown above
2. In a structured format as well for more control
```

Example 2 (unknown):
```unknown
<Note>
  A database is required to use Workflow history. Runs across different executions will be persisted there.
</Note>

Example-
```

Example 3 (unknown):
```unknown
<Note>
  You can use these helper functions to access the history:

  * `step_input.get_workflow_history(num_runs=3)`
  * `step_input.get_workflow_history_context(num_runs=3)`
</Note>

Refer to [StepInput](/reference/workflows/step_input) reference for more details.

## Control Levels

You can be specific about which Steps to add the history to:

### Workflow-Level History

Add workflow history to **all steps** in the workflow:
```

Example 4 (unknown):
```unknown
### Step-Level History

Add workflow history to **specific steps** only:
```

---

## Files Generation Tools

**URL:** llms-txt#files-generation-tools

**Contents:**
- Developer Resources

Source: https://docs.agno.com/basics/multimodal/files/file_generation

Learn how to use files generation tools with Agno agents.

Agno provides the [`FileGenerationTools`](/integrations/toolkits/file-generation/file-generation) to generate files in different formats.

## Developer Resources

* See the [File Generation Tools](/integrations/toolkits/file-generation/file-generation) documentation.
* See the [File Generation Tools](/examples/basics/tools/file-generation) example.

---

## OpenAI-compatible models

**URL:** llms-txt#openai-compatible-models

**Contents:**
- Example
- Parameters

Source: https://docs.agno.com/integrations/models/openai-like

Learn how to use any OpenAI-like compatible endpoint with Agno

Many providers support the OpenAI API format. Use the `OpenAILike` model to access them by replacing the `base_url`.

<CodeGroup>
  
</CodeGroup>

| Parameter  | Type            | Default         | Description                                                        |
| ---------- | --------------- | --------------- | ------------------------------------------------------------------ |
| `id`       | `str`           | `"gpt-4o-mini"` | The id of the model to use                                         |
| `name`     | `str`           | `"OpenAILike"`  | The name of the model                                              |
| `provider` | `str`           | `"OpenAILike"`  | The provider of the model                                          |
| `api_key`  | `Optional[str]` | `None`          | The API key for the service (defaults to OPENAI\_API\_KEY env var) |
| `base_url` | `Optional[str]` | `None`          | The base URL for the API service                                   |

`OpenAILike` extends the OpenAI-compatible interface and supports all parameters from [OpenAIChat](/integrations/models/native/openai/completion/overview). Simply change the `base_url` and `api_key` to point to your preferred OpenAI-compatible service.

---

## Create model using Portkey

**URL:** llms-txt#create-model-using-portkey

model = Portkey(
    id="@first-integrati-707071/gpt-5-nano",
)

agent = Agent(model=model, markdown=True)

---

## Dependencies with Agents

**URL:** llms-txt#dependencies-with-agents

**Contents:**
- Basic usage
- Adding dependencies to context

Source: https://docs.agno.com/basics/dependencies/agent/overview

Learn how to use dependencies to add context to your agents.

**Dependencies** are a way to inject variables into your Agent context. The `dependencies` parameter accepts a dictionary containing functions or static variables that are automatically resolved before the agent runs.

<Note>
  You can use dependencies to inject memories, dynamic few-shot examples, "retrieved" documents, etc.
</Note>

You can reference the dependencies in your agent instructions or user message.

<Note>
  Dependencies can be static values or callable functions. When using functions, they are automatically executed at runtime before the agent runs, and their return values are used as the dependency values.
</Note>

<Tip>
  You can set `dependencies` and `add_dependencies_to_context` on `Agent` initialization, or pass them dynamically to the `run()`, `arun()`, `print_response()` and `aprint_response()` methods.
</Tip>

## Adding dependencies to context

Set `add_dependencies_to_context=True` to add the entire list of dependencies to the user message. This way you don't have to manually add the dependencies to the instructions.

```python dependencies_instructions.py theme={null}
import json

from agno.agent import Agent
from agno.models.openai import OpenAIChat

def get_user_profile() -> str:
    """Fetch and return the user profile.

Returns:
        JSON string containing user profile information
    """
    # Get the user profile from the database (this is a placeholder)
    user_profile = {
        "name": "John Doe",
        "experience_level": "senior",
    }

return json.dumps(user_profile, indent=4)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    dependencies={"user_profile": get_user_profile},
    # We can add the entire dependencies dictionary to the user message
    add_dependencies_to_context=True,
    markdown=True,
)

agent.print_response(
    "Get the user profile and tell me about their experience level.",
    stream=True,
)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Dependencies can be static values or callable functions. When using functions, they are automatically executed at runtime before the agent runs, and their return values are used as the dependency values.
</Note>

<Tip>
  You can set `dependencies` and `add_dependencies_to_context` on `Agent` initialization, or pass them dynamically to the `run()`, `arun()`, `print_response()` and `aprint_response()` methods.
</Tip>

## Adding dependencies to context

Set `add_dependencies_to_context=True` to add the entire list of dependencies to the user message. This way you don't have to manually add the dependencies to the instructions.
```

---

## List all spaces in Webex

**URL:** llms-txt#list-all-spaces-in-webex

agent.print_response("List all space on our Webex", markdown=True)

---

## Shopping Partner

**URL:** llms-txt#shopping-partner

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/shopping_partner

The Shopping Partner agent is an AI-powered product recommendation system that helps users find the perfect products based on their specific preferences and requirements. This agent specializes in:

* **Smart Product Matching**: Analyzes user preferences and finds products that best match their criteria, ensuring a minimum 50% match rate
* **Trusted Sources**: Searches only authentic e-commerce platforms like Amazon, Flipkart, Myntra, Meesho, Google Shopping, Nike, and other reputable websites
* **Real-time Availability**: Verifies that recommended products are in stock and available for purchase
* **Quality Assurance**: Avoids counterfeit or unverified products to ensure user safety
* **Detailed Information**: Provides comprehensive product details including price, brand, features, and key attributes
* **User-Friendly Formatting**: Presents recommendations in a clear, organized manner for easy understanding

This agent is particularly useful for:

* Finding specific products within budget constraints
* Discovering alternatives when preferred items are unavailable
* Getting personalized recommendations based on multiple criteria
* Ensuring purchases from trusted, legitimate sources
* Saving time in product research and comparison

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## BrightData

**URL:** llms-txt#brightdata

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/integrations/toolkits/web-scrape/brightdata

**BrightDataTools** provide comprehensive web scraping capabilities including markdown conversion, screenshots, search engine results, and structured data feeds from various platforms like LinkedIn, Amazon, Instagram, and more.

The following examples require the `requests` library:

You'll also need a BrightData API key. Set the `BRIGHT_DATA_API_KEY` environment variable:

Optionally, you can configure zone settings:

Extract structured data from platforms like LinkedIn, Amazon, etc.:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.brightdata import BrightDataTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        BrightDataTools(
            get_screenshot=True,
        )
    ],
    markdown=True,
    )

**Examples:**

Example 1 (unknown):
```unknown
You'll also need a BrightData API key. Set the `BRIGHT_DATA_API_KEY` environment variable:
```

Example 2 (unknown):
```unknown
Optionally, you can configure zone settings:
```

Example 3 (unknown):
```unknown
## Example

Extract structured data from platforms like LinkedIn, Amazon, etc.:
```

---

## Interactive CLI Writing Team

**URL:** llms-txt#interactive-cli-writing-team

Source: https://docs.agno.com/basics/teams/usage/other/run-as-cli

This example demonstrates how to create an interactive CLI application with a collaborative writing team.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Team">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Result Synthesizer Agent - Combines and ranks all findings

**URL:** llms-txt#result-synthesizer-agent---combines-and-ranks-all-findings

result_synthesizer = Agent(
    name="Result Synthesizer",
    model=Claude(id="claude-3-7-sonnet-latest"),
    role="Synthesize and rank all search results into comprehensive response",
    instructions=[
        "Combine results from all team members into a unified response.",
        "Rank information based on relevance and reliability.",
        "Ensure comprehensive coverage of the query topic.",
        "Present results with clear source attribution and confidence levels.",
    ],
    markdown=True,
)

---

## Create Postgres-backed memory store

**URL:** llms-txt#create-postgres-backed-memory-store

db = PostgresDb(db_url=db_url)

---

## - Fetch the URL and token from Upstash Console

**URL:** llms-txt#--fetch-the-url-and-token-from-upstash-console

---

## Workflow Sessions

**URL:** llms-txt#workflow-sessions

**Contents:**
- When to Use Workflow Sessions
- How Workflow Sessions Work

Source: https://docs.agno.com/basics/sessions/workflow-sessions

Learn how workflow sessions track multi-step pipeline executions

Workflow sessions track the execution history of your workflows. Unlike agent or team sessions that store conversation messages, workflow sessions store complete **workflow runs**, where each run represents a full execution of all your workflow steps from input to final output.

Think of it this way:

* **Agent/Team sessions** = conversation history (messages back and forth)
* **Workflow sessions** = execution history (complete pipeline runs with results)

## When to Use Workflow Sessions

Use workflow sessions when you need to:

* **Track workflow execution history** across multiple runs
* **Share state between steps** in a workflow (like passing data between pipeline stages)
* **Enable workflows to learn from previous runs** by accessing past inputs and outputs
* **Persist workflow results** for analysis and debugging
* **Maintain context** across multiple workflow executions

<Tip>
  In most cases it is recommended to add session persistence to your workflow.
</Tip>

## How Workflow Sessions Work

When you create a workflow with a database, Agno automatically manages sessions for you:

```python  theme={null}
from agno.workflow import Workflow
from agno.db.sqlite import SqliteDb

workflow = Workflow(
    name="Research Pipeline",
    db=SqliteDb(db_file="workflows.db"),
    steps=[...],
)

---

## Use Custom Domain and HTTPS

**URL:** llms-txt#use-custom-domain-and-https

**Contents:**
- Overview
- Use a custom domain
  - Custom domain for your AgentOS App
- Add HTTPS

Source: https://docs.agno.com/templates/infra-management/domain-https

To add a live AgentOS instance to os.agno.com, the endpoint must be HTTPS. Here is how you can add a custom domain and HTTPS to your AWS loadbalancer.

## Use a custom domain

1. Register your domain with [Route 53](https://us-east-1.console.aws.amazon.com/route53/).
2. Point the domain to the loadbalancer DNS.

### Custom domain for your AgentOS App

Create a record in the Route53 console to point `app.[YOUR_DOMAIN]` to the AgentOS endpoint.

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=2387492f4fa89cab98e2a603da83535b" alt="llm-app-aidev-run" data-og-width="1081" width="1081" data-og-height="569" height="569" data-path="images/llm-app-aidev-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=1d9004362958e21665a9deb78811a4dd 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=16e4a3f99f355b484c3bfc0ff98ec7c9 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=d2659746f0b009a8e84c7fe1528cac65 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=1b3b6f7c9a098578bcc4cf88f5802588 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=d1e8a2a0a23ce16cbaf50ebe0fce0fba 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=4c1c3fbd924321ecfe7aabedc3d9ad1e 2500w" />

You can visit the app at `[http://app.[YOUR_DOMAIN]`

<Note>Note the `http` in the domain name.</Note>

1. Create a certificate using [AWS ACM](https://us-east-1.console.aws.amazon.com/acm). Request a certificat for `*.[YOUR_DOMAIN]`

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=15b580029369ef5c8039bddfad4be52d" alt="llm-app-request-cert" data-og-width="1105" width="1105" data-og-height="581" height="581" data-path="images/llm-app-request-cert.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=05442b5b3ac14b98d42e885488be4ede 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=9ce2e4e86b46e10e984aa1b1ab9939a7 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=85149f2d24287108d22bd604f7014dc3 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=dbb81c8a8e8df3c85eaed02a097a89b1 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=6c804f2e99c3881f52cabd5566b54802 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=b9534df74f2df9325cdeae0448e25bfd 2500w" />

2. Creating records in Route 53.

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=4291826e3abd20126daf4e1bbd42c0a3" alt="llm-app-validate-cert" data-og-width="1322" width="1322" data-og-height="566" height="566" data-path="images/llm-app-validate-cert.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=c7813e664032c848f1b2c5a51953f8eb 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=b98c005abf74fc4a7b7c75678cf8b0f6 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=b25347bff02d1684a44906d860b51a98 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=f7a1bafd43f582f40f13513ea64daa32 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=179c892f6141685a69e521865a7c4d28 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=051a273f486a1f576ee0cedd97c24c69 2500w" />

3. Add the certificate ARN to Apps

<Note>Make sure the certificate is `Issued` before adding it to your Apps</Note>

Update the `infra/prd_resources.py` file and add the `load_balancer_certificate_arn` to the `FastAPI` app.

```python infra/prd_resources.py theme={null}

---

## Manage Knowledge

**URL:** llms-txt#manage-knowledge

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/agent-os/knowledge/manage-knowledge

Attach Knowledge to your AgentOS instance

The AgentOS control plane provides a simple way to [manage](/agent-os/features/knowledge-management#adding-content) your Knowledge bases.
You can add, edit, and delete content from your Knowledge bases directly through the control plane.

You can specify multiple Knowledge bases and reuse the same Knowledge instance
across different Agents or Teams as needed.

Before setting up Knowledge management in AgentOS, ensure you have:

* PostgreSQL database running and accessible - used for this example
* Required dependencies installed: `pip install agno`
* OpenAI API key configured (for embeddings)
* Basic understanding of [Knowledge concepts](/basics/knowledge/getting-started)

This example demonstrates how to attach multiple Knowledge bases to AgentOS
and populate them with content from different sources.

```python agentos_knowledge.py theme={null}
from textwrap import dedent

from agno.db.postgres import PostgresDb
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.os import AgentOS
from agno.vectordb.pgvector import PgVector, SearchType

---

## Non-Reasoning Model Agent

**URL:** llms-txt#non-reasoning-model-agent

Source: https://docs.agno.com/basics/reasoning/usage/agents/non-reasoning-model-cot

Example showing how to use a non-reasoning model as a reasoning model.

For reasoning, we recommend using a Reasoning Agent (with `reasoning=True`), or to use an appropriate reasoning model with `reasoning_model`.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get general information and local businesses

**URL:** llms-txt#get-general-information-and-local-businesses

**Contents:**
- Toolkit Params
- Developer Resources
- Resources

agent.print_response(
    """
    I'm traveling to Tokyo next month.
    1. Research the best time to visit and major attractions
    2. Find one good rated sushi restaurants near Shinjuku
    Compile a comprehensive travel guide with this information.
    """,
    markdown=True
)
```

| Parameter         | Type                 | Default | Description                                                         |
| ----------------- | -------------------- | ------- | ------------------------------------------------------------------- |
| `apify_api_token` | `str`                | `None`  | Apify API token (or set via APIFY\_API\_TOKEN environment variable) |
| `actors`          | `str` or `List[str]` | `None`  | Single Actor ID or list of Actor IDs to register                    |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/apify.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/apify_tools.py)

* [Apify Actor Documentation](https://docs.apify.com/Actors)
* [Apify Store - Browse available Actors](https://apify.com/store)
* [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)

---

## Define a tool that adds an item to the shopping list

**URL:** llms-txt#define-a-tool-that-adds-an-item-to-the-shopping-list

def add_item(run_context: RunContext, item: str) -> str:
    """Add an item to the shopping list."""

# We access the session state via run_context.session_state
    run_context.session_state["shopping_list"].append(item)

return f"The shopping list is now {run_context.session_state['shopping_list']}"

---

## Loop with Parallel Steps Workflow

**URL:** llms-txt#loop-with-parallel-steps-workflow

Source: https://docs.agno.com/basics/workflows/usage/loop-with-parallel-steps-stream

This example demonstrates **Workflows 2.0** most sophisticated pattern combining loop execution with parallel processing and real-time streaming.

This example shows how to create iterative
workflows that execute multiple independent tasks simultaneously within each iteration,
optimizing both quality and performance.

**When to use**: When you need iterative quality improvement with parallel task execution
in each iteration. Ideal for comprehensive research workflows where multiple independent
tasks contribute to overall quality, and you need to repeat until quality thresholds are met.

```python loop_with_parallel_steps_stream.py theme={null}
from typing import List

from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Loop, Parallel, Step, Workflow
from agno.workflow.types import StepOutput

---

## Agent with Tools

**URL:** llms-txt#agent-with-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/vercel/usage/tool-use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Initialize Agent

**URL:** llms-txt#initialize-agent

**Contents:**
- Developer Resources

memory_agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    db=db,
    # Give the Agent the ability to update memories
    enable_agentic_memory=True,
    # OR - Run the MemoryManager automatically after each response
    enable_user_memories=True,
    markdown=True,
)

memory_agent.print_response(
    "My name is Ava and I like to ski.",
    user_id=user_id,
    stream=True,
)
print("Memories about Ava:")
pprint(memory_agent.get_user_memories(user_id=user_id))

memory_agent.print_response(
    "I live in san francisco, where should i move within a 4 hour drive?",
    user_id=user_id,
    stream=True,
)
print("Memories about Ava:")
pprint(memory_agent.get_user_memories(user_id=user_id))
```

<Tip>
  `enable_agentic_memory=True` gives the Agent a tool to manage memories of the
  user, this tool passes the task to the `MemoryManager` class. You may also set
  `enable_user_memories=True` which always runs the `MemoryManager` after each
  user message.
</Tip>

<Note>
  Read more about Memory in the [Memory Overview](/basics/memory/overview) page.
</Note>

## Developer Resources

* View the [Agent schema](/reference/agents/agent)
* View [Examples](/examples/basics/memory)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/memory/)

---

## Research Team

**URL:** llms-txt#research-team

**Contents:**
- Code

Source: https://docs.agno.com/agent-os/usage/interfaces/ag-ui/team

Multi-agent research team with specialized roles and web interface

```python cookbook/os/interfaces/agui/research_team.py theme={null}
from agno.agent.agent import Agent
from agno.models.openai import OpenAIChat
from agno.os.app import AgentOS
from agno.os.interfaces.agui.agui import AGUI
from agno.team import Team

researcher = Agent(
    name="researcher",
    role="Research Assistant",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="You are a research assistant. Find information and provide detailed analysis.",
    markdown=True,
)

writer = Agent(
    name="writer",
    role="Content Writer", 
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="You are a content writer. Create well-structured content based on research.",
    markdown=True,
)

research_team = Team(
    members=[researcher, writer],
    name="research_team",
    instructions="""
    You are a research team that helps users with research and content creation.
    First, use the researcher to gather information, then use the writer to create content.
    """,
    show_members_responses=True,
    get_member_information_tool=True,
    add_member_tools_to_context=True,
)

---

## Get original memories

**URL:** llms-txt#get-original-memories

original_memories = memory_manager.get_user_memories(user_id="user_123")

---

## Does not work with agentic filtering (static, predefined logic)

**URL:** llms-txt#does-not-work-with-agentic-filtering-(static,-predefined-logic)

**Contents:**
- Using Filters Through the API

knowledge_filters = [AND(EQ("department", "hr"), EQ("document_type", "policy"))]
python  theme={null}
import requests
import json
from agno.filters import EQ, GT, AND

**Examples:**

Example 1 (unknown):
```unknown
**When to use each approach:**

| Approach               | Use Case                                                | Example                                                            |
| ---------------------- | ------------------------------------------------------- | ------------------------------------------------------------------ |
| **Dictionary format**  | Agent dynamically chooses filters based on conversation | User mentions "HR policies" â†’ agent adds `{"department": "hr"}`    |
| **Filter expressions** | You need complex, predetermined logic with full control | Always exclude drafts AND filter by multiple regions with OR logic |

## Using Filters Through the API

All the filter expressions shown in this guide can also be used through the Agent OS API. FilterExpressions serialize to JSON and are automatically reconstructed server-side, enabling the same powerful filtering capabilities over REST endpoints.
```

---

## Wikipedia Reader

**URL:** llms-txt#wikipedia-reader

Source: https://docs.agno.com/reference/knowledge/reader/wikipedia

WikipediaReader is a reader class that allows you to read Wikipedia articles.

<Snippet file="wikipedia-reader-reference.mdx" />

---

## LanceDB Vector DB

**URL:** llms-txt#lancedb-vector-db

vector_db = LanceDb(
    table_name="recipes",
    uri="/tmp/lancedb",
    search_type=SearchType.keyword,
)

---

## )

**URL:** llms-txt#)

**Contents:**
- What to Expect
- Usage
- Next Steps

bash  theme={null}
    export OPENAI_API_KEY=xxx
    export SLACK_TOKEN=xxx
    export EXA_API_KEY=xxx
    bash  theme={null}
    docker run -d --name pgvector-db -e POSTGRES_USER=ai -e POSTGRES_PASSWORD=ai -e POSTGRES_DB=ai -p 5532:5432 pgvector/pgvector:pg16
    bash  theme={null}
    pip install -U agno openai ddgs slack-sdk exa-py pgvector psycopg
    bash Mac theme={null}
      python ai_support_team.py
      bash Windows theme={null}
      python ai_support_team.py
      ```
    </CodeGroup>
  </Step>
</Steps>

* Modify the documentation URL in `knowledge.add_content()` to index your own docs
* Adjust Slack channel names in `support_channel` and `feedback_channel` variables
* Add more specialized agents for different inquiry types
* Explore [Knowledge Bases](/basics/knowledge/knowledge-bases) for advanced configurations

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The team will analyze your inquiry and route it to the most appropriate agent. Documentation questions get answered by searching the knowledge base, bugs get escalated to Slack, and feedback gets collected and logged.

You'll see the team's classification decision and responses from the specific agents handling your request. The knowledge base integration enables accurate answers grounded in actual documentation, while Slack integration provides seamless escalation workflows.

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Team">
    <CodeGroup>
```

---

## OpenAI Like

**URL:** llms-txt#openai-like

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/openai-like

The OpenAI Like model works as a wrapper for the OpenAILike models.

| Parameter                            | Type                              | Default                       | Description                                                           |
| ------------------------------------ | --------------------------------- | ----------------------------- | --------------------------------------------------------------------- |
| `id`                                 | `str`                             | `"gpt-4o"`                    | The id of the model to use                                            |
| `name`                               | `str`                             | `"OpenAILike"`                | The name of the model                                                 |
| `provider`                           | `str`                             | `"OpenAILike"`                | The provider of the model                                             |
| `api_key`                            | `Optional[str]`                   | `None`                        | The API key for authentication (defaults to OPENAI\_API\_KEY env var) |
| `base_url`                           | `str`                             | `"https://api.openai.com/v1"` | The base URL for the API                                              |
| `supports_native_structured_outputs` | `Optional[bool]`                  | `None`                        | Whether the model supports native structured outputs                  |
| `response_format`                    | `Optional[str]`                   | `None`                        | The format of the response                                            |
| `seed`                               | `Optional[int]`                   | `None`                        | Random seed for deterministic sampling                                |
| `stop`                               | `Optional[Union[str, List[str]]]` | `None`                        | Up to 4 sequences where the API will stop generating further tokens   |
| `stream`                             | `bool`                            | `True`                        | Whether to stream the response                                        |
| `temperature`                        | `Optional[float]`                 | `None`                        | Controls randomness in the model's output                             |
| `top_p`                              | `Optional[float]`                 | `None`                        | Controls diversity via nucleus sampling                               |
| `request_params`                     | `Optional[Dict[str, Any]]`        | `None`                        | Additional parameters to include in the request                       |
| `client_params`                      | `Optional[Dict[str, Any]]`        | `None`                        | Additional parameters for client configuration                        |
| `retries`                            | `int`                             | `0`                           | Number of retries to attempt before raising a ModelProviderError      |
| `delay_between_retries`              | `int`                             | `1`                           | Delay between retries, in seconds                                     |
| `exponential_backoff`                | `bool`                            | `False`                       | If True, the delay between retries is doubled each time               |

---

## Next

**URL:** llms-txt#next

Congratulations on running your Agent Infra Railway locally! When you are happy with your AgentOS, you can take it to production using Railway.

---

## MongoDB connection settings

**URL:** llms-txt#mongodb-connection-settings

**Contents:**
- Params
- Developer Resources

db_url = "mongodb://localhost:27017"
db = MongoDb(db_url=db_url)

class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")

<Snippet file="db-mongo-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/mongo/mongodb_storage_for_team.py)

---

## Developer Resources

**URL:** llms-txt#developer-resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/embedders/aws_bedrock_embedder.py)

---

## Async Postgres for Workflows

**URL:** llms-txt#async-postgres-for-workflows

**Contents:**
- Usage
  - Run PgVector

Source: https://docs.agno.com/integrations/database/async-postgres/usage/async-postgres-for-workflow

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

```python async_postgres_for_workflow.py theme={null}
import asyncio

from agno.agent import Agent
from agno.db.postgres import AsyncPostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg_async://ai:ai@localhost:5532/ai"
db = AsyncPostgresDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Create an agent with the knowledge instance

**URL:** llms-txt#create-an-agent-with-the-knowledge-instance

**Contents:**
- Usage

agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    debug_mode=True,
)

if __name__ == "__main__":
    asyncio.run(
        agent.aprint_response(
            "Explain what this text means: low end eats the high end", markdown=True
        )
    )
bash  theme={null}
    pip install -U llama-index-core llama-index-readers-file llama-index-embeddings-openai pypdf openai agno
    bash Mac theme={null}
      python cookbook/knowledge/vector_db/llamaindex_db/llamaindex_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/llamaindex_db/llamaindex_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Debugging Teams

**URL:** llms-txt#debugging-teams

**Contents:**
- Debug Mode

Source: https://docs.agno.com/basics/teams/debugging-teams

Learn how to debug Agno Teams.

Agno comes with an debug mode that takes your team development experience to the next level. It helps you understand the flow of execution and the intermediate steps. For example:

1. Inspect the messages sent to the model and the response it generates.
2. Trace intermediate steps and monitor metrics like token usage, execution time, etc.
3. Inspect tool calls, errors, and their results.
4. Monitor team member interactions and delegation patterns.

To enable debug mode on a team, use one of the following methods:

1. Set the `debug_mode` parameter on your team, to enable it for all runs, as well as for member runs.
2. Set the `debug_mode` parameter on the `run` method, to enable it on a single run.
3. Set the `AGNO_DEBUG` environment variable to `True`, to enable debug mode for all teams.

```python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat

news_agent = Agent(name="News Agent", role="Get the latest news")
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o"),
    debug_mode=True,
    # debug_level=2, # Uncomment to get more detailed logs
)

---

## Define individual steps

**URL:** llms-txt#define-individual-steps

initial_research_step = Step(
    name="InitialResearch",
    agent=researcher,
    description="Initial research on the topic",
)

---

## 3. Send a message to the bot.

**URL:** llms-txt#3.-send-a-message-to-the-bot.

---

## First call - cache miss, calls the API

**URL:** llms-txt#first-call---cache-miss,-calls-the-api

response = agent.run("What is the capital of France?")

---

## In our run_response, we will find a list of active requirements:

**URL:** llms-txt#in-our-run_response,-we-will-find-a-list-of-active-requirements:

**Contents:**
- Types of Human-in-the-Loop Requirements
- Resuming Agent Execution

for requirement in run_response.active_requirements:

# We can now iterate over the requirements and resolve them:

# For example, if the requirement needs user confirmation:
    if requirement.needs_confirmation:
        # Ask the user for confirmation
        confirmation = input(f"Do you approve the tool call to {requirement.tool.tool_name} with args {requirement.tool.tool_args}? (y/n): ")

# Resolve the requirement by confirming or rejecting it, based on the user's input
        if confirmation.lower() == "y":
            requirement.confirm()
        else:
            requirement.reject()
python  theme={null}
for requirement in run_response.active_requirements:

# If the requirement is about user confirmation:
    if requirement.needs_confirmation:
        ...

# If the requirement is about obtaining user input:
    if requirement.needs_user_input:
        ...

# If the requirement is about executing an external tool:
    if requirement.is_external_tool_execution:
        ...
python  theme={null}
run_response = agent.run("Perform sensitive operation")

for requirement in run_response.active_requirements:
    # You handle any active requirements here

**Examples:**

Example 1 (unknown):
```unknown
## Types of Human-in-the-Loop Requirements

There are three types of Human-in-the-Loop requirements you can handle in your code: User confirmation, user input and external tool execution.

This is how you can check the type of a requirement in your code:
```

Example 2 (unknown):
```unknown
## Resuming Agent Execution

After all active requirements have been resolved, you will want to continue the run. You do this by calling the `continue_run` method:

This is how the full flow would look like in your code:
```

---

## First give information to the team

**URL:** llms-txt#first-give-information-to-the-team

**Contents:**
- Ask question in German

## Ask question in German
multi_lingual_q_and_a_team.print_response(
    "Hallo, wie heiÃŸt du? Meine Name ist John.", stream=True, session_id=session_id
)

---

## 6. Process segments

**URL:** llms-txt#6.-process-segments

shorts = extract_segments(response.content)

---

## --- Workflow Definition ---

**URL:** llms-txt#----workflow-definition----

blog_generator_workflow = Workflow(
    name="Blog Post Generator",
    description="Advanced blog post generator with research and content creation capabilities",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/blog_generator.db",
    ),
    steps=blog_generation_execution,
    session_state={},  # Initialize empty session state for caching
)

if __name__ == "__main__":
    import random

async def main():
        # Fun example topics to showcase the generator's versatility
        example_topics = [
            "The Rise of Artificial General Intelligence: Latest Breakthroughs",
            "How Quantum Computing is Revolutionizing Cybersecurity",
            "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint",
            "The Future of Work: AI and Human Collaboration",
            "Space Tourism: From Science Fiction to Reality",
            "Mindfulness and Mental Health in the Digital Age",
            "The Evolution of Electric Vehicles: Current State and Future Trends",
            "Why Cats Secretly Run the Internet",
            "The Science Behind Why Pizza Tastes Better at 2 AM",
            "How Rubber Ducks Revolutionized Software Development",
        ]

# Test with a random topic
        topic = random.choice(example_topics)

print("ðŸ§ª Testing Blog Post Generator v2.0")
        print("=" * 60)
        print(f"ðŸ“ Topic: {topic}")
        print()

# Generate the blog post
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

pprint_run_response(resp, markdown=True, show_time=True)

asyncio.run(main())
```

For more examples and advanced patterns, see [here](/basics/workflows/usage). Each file demonstrates a specific pattern with detailed comments and real-world use cases.

---

## Few-shot examples showing good support patterns

**URL:** llms-txt#few-shot-examples-showing-good-support-patterns

**Contents:**
- Usage

support_examples = [
    # Example 1: Simple issue resolution
    Message(role="user", content="I forgot my password and can't log in"),
    Message(
        role="assistant",
        content="""
    I'll help you reset your password right away.
    
    **Transferring to Support Specialist**: Please assist this customer with password reset:
    - Guide them through the password reset process
    - Verify their identity if needed  
    - Ensure they can successfully log in
    - Provide tips for creating a secure password
    
    Expected outcome: Customer successfully logs in with new password.
    """.strip(),
    ),
    # Example 2: Escalation pattern
    Message(
        role="user",
        content="I've been charged twice for the same order and your chat bot couldn't help!",
    ),
    Message(
        role="assistant",
        content="""
        I sincerely apologize for the billing issue and the frustration with our chat system.

**Transferring to Escalation Manager**: Priority billing issue requiring immediate attention:
        - Customer charged twice for same order
        - Previous automated support was unsuccessful
        - Customer is understandably frustrated
        - Needs immediate resolution and possible compensation

Expected outcome: Billing corrected, customer satisfaction restored, and follow-up to prevent recurrence.
        """.strip(),
    ),
    # Example 3: Proactive support
    Message(role="user", content="Your app keeps crashing when I try to upload photos"),
    Message(
        role="assistant",
        content="""
    I'm sorry you're experiencing crashes with photo uploads. Let me get this resolved for you.

**Transferring to Support Specialist**: Technical issue with photo upload feature:
    - App crashes during photo upload process
    - Need to troubleshoot device compatibility and app version
    - Provide workaround solutions if available
    - Escalate to technical team if it's a known bug

Expected outcome: Upload feature working properly or clear timeline for fix provided.
        """.strip(),
    ),
]

if __name__ == "__main__":
    # Support Agent
    support_agent = Agent(
        name="Support Specialist",
        role="Handle customer inquiries",
        model=OpenAIChat(id="gpt-5-mini"),
        instructions=[
            "You are a helpful customer support specialist.",
            "Always be polite, professional, and solution-oriented.",
        ],
    )

# Escalation Agent
    escalation_agent = Agent(
        name="Escalation Manager",
        role="Handle complex issues",
        model=OpenAIChat(id="gpt-5-mini"),
        instructions=[
            "You handle escalated customer issues that require management attention.",
            "Focus on customer satisfaction and finding solutions.",
        ],
    )

# Create team with few-shot learning
    team = Team(
        name="Customer Support Team",
        members=[support_agent, escalation_agent],
        model=OpenAIChat(id="gpt-5-mini"),
        add_name_to_context=True,
        additional_input=support_examples,  # ðŸ†• Teaching examples
        instructions=[
            "You coordinate customer support with excellence and empathy.",
            "Follow established patterns for proper issue resolution.",
            "Always prioritize customer satisfaction and clear communication.",
        ],
        debug_mode=True,
        markdown=True,
    )

scenarios = [
        "I can't find my order confirmation email",
        "The product I received is damaged",
        "I want to cancel my subscription but the website won't let me",
    ]

for i, scenario in enumerate(scenarios, 1):
        print(f"ðŸ“ž Scenario {i}: {scenario}")
        print("-" * 50)
        team.print_response(scenario)
bash  theme={null}
    pip install agno openai
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/basic/few_shot_learning.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Wikipedia

**URL:** llms-txt#wikipedia

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/wikipedia

**WikipediaTools** enable an Agent to search wikipedia a website and add its contents to the knowledge base.

The following example requires the `wikipedia` library.

The following agent will run seach wikipedia for "ai" and print the response.

| Name        | Type        | Default | Description                                                                                                        |
| ----------- | ----------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| `knowledge` | `Knowledge` | -       | The knowledge base associated with Wikipedia, containing various data and resources linked to Wikipedia's content. |
| `all`       | `bool`      | `False` | Enable all functionality.                                                                                          |

| Function Name                                | Description                                                                                            |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| `search_wikipedia_and_update_knowledge_base` | This function searches wikipedia for a topic, adds the results to the knowledge base and returns them. |
| `search_wikipedia`                           | Searches Wikipedia for a query.                                                                        |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/wikipedia.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/wikipedia_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will run seach wikipedia for "ai" and print the response.
```

---

## OpenAI Embedder

**URL:** llms-txt#openai-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/openai/usage/openai-embedder

```python  theme={null}
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = OpenAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## ArXiv Reader Async

**URL:** llms-txt#arxiv-reader-async

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/readers/usage/arxiv-reader-async

The **ArXiv Reader** with asynchronous processing allows you to search and read academic papers from the ArXiv preprint repository with better performance for concurrent operations.

```python examples/basics/knowledge/readers/arxiv_reader_async.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.arxiv_reader import ArxivReader
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge = Knowledge(
    # Table name: ai.arxiv_documents
    vector_db=PgVector(
        table_name="arxiv_documents",
        db_url=db_url,
    ),
)

---

## Initialize the Agent

**URL:** llms-txt#initialize-the-agent

agent = Agent(
    model=OpenAIChat(),
    tools=[zep_tools],
    dependencies={"memory": zep_tools.get_zep_memory(memory_type="context")},
    add_dependencies_to_context=True,
)

---

## Setup your Agent with the database and add the session history to the context

**URL:** llms-txt#setup-your-agent-with-the-database-and-add-the-session-history-to-the-context

**Contents:**
  - Where are sessions stored?

agent = Agent(
    db=db,
    add_history_to_context=True, # Automatically add the persisted session history to the context
    num_history_runs=3, # Specify how many messages to add to the context
)
python  theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

**Examples:**

Example 1 (unknown):
```unknown
### Where are sessions stored?

By default, sessions are stored in the `agno_sessions` table of the database.

If the table or collection doesn't exist, it is created automatically when first storing a session.

You can specify where the sessions are stored exactly using the `session_table` parameter:
```

---

## Agno Infra

**URL:** llms-txt#agno-infra

**Contents:**
- Create a new Agno Infra project
- Start infra resources
- Stop infra resources
- Patch infra resources
- Restart infra
- Command Options
  - Environment (`--env`)
  - Infra (`--infra`)
  - Group (`--group`)
  - Name (`--name`)

Source: https://docs.agno.com/deploy/agno-infra

The Agno Infra SDK applies best practices to take your AgentOS to production. It lets you define, deploy, and manage your entire Agentic System as Python code. This library is used by the [AWS template](https://github.com/agno-agi/agent-infra-aws) to manage your infrastructure with support for more cloud providers coming soon.

## Create a new Agno Infra project

Run `ag infra create` to create a new infra project, the command will ask your for a starter template and infra project name.

<CodeGroup>
  
</CodeGroup>

And select a template from the list of available templates.

## Start infra resources

Run `ag infra up` to start i.e. create infra resources. This will start your AgentOS instance and PostgreSQL database locally using docker.

## Stop infra resources

Run `ag infra down` to stop i.e. delete infra resources

## Patch infra resources

Run `ag infra patch` to patch i.e. update infra resources

<Note>
  The `patch` command in under development for some resources. Use `restart` if needed
</Note>

Run `ag infra restart` to stop resources and start them again

<Note>Run `ag infra up --help` to view all options</Note>

### Environment (`--env`)

Use the `--env` or `-e` flag to filter the environment (dev/prd)

### Infra (`--infra`)

Use the `--infra` or `-i` flag to filter the infra (docker/aws/k8s)

### Group (`--group`)

Use the `--group` or `-g` flag to filter by resource group.

Use the `--name` or `-n` flag to filter by resource name

Use the `--type` or `-t` flag to filter by resource type.

### Dry Run (`--dry-run`)

The `--dry-run` or `-dr` flag can be used to **dry-run** the command. `ag infra up -dr` will only print resources, not create them.

### Show Debug logs (`--debug`)

Use the `--debug` or `-d` flag to show debug logs.

### Force recreate images & containers (`-f`)

Use the `--force` or `-f` flag to force recreate images & containers

**Examples:**

Example 1 (unknown):
```unknown
</CodeGroup>

And select a template from the list of available templates.

## Start infra resources

Run `ag infra up` to start i.e. create infra resources. This will start your AgentOS instance and PostgreSQL database locally using docker.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Add content (ingestion + chunking + embedding handled by Knowledge)

**URL:** llms-txt#add-content-(ingestion-+-chunking-+-embedding-handled-by-knowledge)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
    skip_if_exists=True,
)

---

## Readers

**URL:** llms-txt#readers

**Contents:**
- What are Readers?
- How Readers Work

Source: https://docs.agno.com/basics/knowledge/readers/overview

Learn how to use readers to convert raw data into searchable knowledge for your Agents.

Readers are the first step in the process of creating Knowledge from content.
They transform raw content from various sources into structured `Document` objects that can be embedded, chunked, and stored in vector databases.

A **Reader** is a specialized component that knows how to parse and extract content from specific data sources or file formats. Think of readers as translators that convert different content formats into a standardized format that Agno can work with.

Every piece of content that enters your knowledge base must pass through a reader first. The reader's job is to:

1. **Parse** the raw content from its original format
2. **Extract** the meaningful text and metadata
3. **Structure** the content into `Document` objects
4. **Apply chunking** strategies to break large content into manageable pieces

All readers inherit from the base `Reader` class and follow a consistent pattern:

```python  theme={null}

---

## Run Agent Infra Docker Locally

**URL:** llms-txt#run-agent-infra-docker-locally

Source: https://docs.agno.com/templates/agent-infra-docker/run-local

<Snippet file="run-agent-infra-docker-local.mdx" />

---

## Traceloop

**URL:** llms-txt#traceloop

**Contents:**
- Integrating Agno with Traceloop
- Prerequisites
- Sending Traces to Traceloop

Source: https://docs.agno.com/integrations/observability/traceloop

Integrate Agno with Traceloop to send traces and gain insights into your agent's performance.

## Integrating Agno with Traceloop

[Traceloop](https://www.traceloop.com/) provides an LLM observability platform built on [OpenLLMetry](https://github.com/traceloop/openllmetry), an open-source OpenTelemetry extension. By integrating Agno with Traceloop, you can automatically trace agent execution, team workflows, tool calls, and token usage metrics.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup Traceloop Account**

* Sign up for an account at [Traceloop](https://app.traceloop.com/).
   * Obtain your API key from [Settings > API Keys](https://app.traceloop.com/settings/api-keys).

3. **Set Environment Variables**

Configure your environment with the Traceloop API key:

## Sending Traces to Traceloop

* ### Example: Basic Agent Instrumentation

Initialize Traceloop at the start of your application. The SDK automatically instruments Agno agent execution.

```python  theme={null}
from traceloop.sdk import Traceloop
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup Traceloop Account**

   * Sign up for an account at [Traceloop](https://app.traceloop.com/).
   * Obtain your API key from [Settings > API Keys](https://app.traceloop.com/settings/api-keys).

3. **Set Environment Variables**

   Configure your environment with the Traceloop API key:
```

Example 2 (unknown):
```unknown
## Sending Traces to Traceloop

* ### Example: Basic Agent Instrumentation

Initialize Traceloop at the start of your application. The SDK automatically instruments Agno agent execution.
```

---

## Create a News Reporter Agent with a fun personality

**URL:** llms-txt#create-a-news-reporter-agent-with-a-fun-personality

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! ðŸ—½
        Think of yourself as a mix between a witty comedian and a sharp journalist.

Follow these guidelines for every report:
        1. Start with an attention-grabbing headline using relevant emoji
        2. Use the search tool to find current, accurate information
        3. Present news with authentic NYC enthusiasm and local flavor
        4. Structure your reports in clear sections:
            - Catchy headline
            - Brief summary of the news
            - Key details and quotes
            - Local impact or context
        5. Keep responses concise but informative (2-3 paragraphs max)
        6. Include NYC-style commentary and local references
        7. End with a signature sign-off phrase

Sign-off examples:
        - 'Back to you in the studio, folks!'
        - 'Reporting live from the city that never sleeps!'
        - 'This is [Your Name], live from the heart of Manhattan!'

Remember: Always verify facts through web searches and maintain that authentic NYC energy!\
    """),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## memories = agent.get_user_memories(user_id=john_doe_id)

**URL:** llms-txt#memories-=-agent.get_user_memories(user_id=john_doe_id)

---

## Overview

**URL:** llms-txt#overview

**Contents:**
- Guides

Source: https://docs.agno.com/tutorials/overview

Agno is a platform for building AI agents. It provides a set of tools and libraries to help you build and deploy AI agents.

---

## Step 1: Configure Atla

**URL:** llms-txt#step-1:-configure-atla

configure(token=getenv("ATLA_API_KEY"))

---

## OpenTelemetry

**URL:** llms-txt#opentelemetry

**Contents:**
- Key Benefits
- OpenTelemetry Support
- Developer Resources

Source: https://docs.agno.com/integrations/observability/overview

Agno supports observability through OpenTelemetry, integrating seamlessly with popular tracing and monitoring platforms.

Observability helps us understand, debug, and improve AI agents. Agno supports observability through [OpenTelemetry](https://opentelemetry.io/), integrating seamlessly with popular tracing and monitoring platforms.

* **Trace**: Visualize and analyze agent execution flows.
* **Monitor**: Track performance, errors, and usage.
* **Debug**: Quickly identify and resolve issues.

## OpenTelemetry Support

Agno offers first-class support for OpenTelemetry, the industry standard for distributed tracing and observability.

* **Auto-Instrumentation**: Automatically instrument your agents and tools.
* **Flexible Export**: Send traces to any OpenTelemetry-compatible backend.
* **Custom Tracing**: Extend or customize tracing as needed.

<Note>
  OpenTelemetry-compatible backends including Arize Phoenix, Langfuse, Langsmith, Langtrace, Logfire, Maxim, OpenLIT, Traceloop, and Weave are supported by Agno out of the box.
</Note>

## Developer Resources

* [Cookbooks](https://github.com/agno-agi/agno/tree/main/cookbook/integrations/observability)

---

## Initialize RedshiftTools with connection details

**URL:** llms-txt#initialize-redshifttools-with-connection-details

redshift_tools = RedshiftTools(
    host="your-cluster.abc123.us-east-1.redshift.amazonaws.com",
    database="dev",
    user="your-username",
    password="your-password",
    table_schema="public",
)

---

## Imagen Tool with OpenAI

**URL:** llms-txt#imagen-tool-with-openai

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/google/usage/imagen-tool

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cancel Workflow Run

**URL:** llms-txt#cancel-workflow-run

Source: https://docs.agno.com/reference-api/schema/workflows/cancel-workflow-run

post /workflows/{workflow_id}/runs/{run_id}/cancel
Cancel a currently executing workflow run, stopping all active steps and cleanup.
**Note:** Complex workflows with multiple parallel steps may take time to fully cancel.

---

## Cassandra

**URL:** llms-txt#cassandra

Source: https://docs.agno.com/reference/vector-db/cassandra

<Snippet file="vector-db-cassandra-reference.mdx" />

---

## ChromaDB Vector Database

**URL:** llms-txt#chromadb-vector-database

**Contents:**
- Setup
- Example

Source: https://docs.agno.com/integrations/vectordb/chroma/overview

Learn how to use ChromaDB as a vector database for your Knowledge Base

```python agent_with_knowledge.py theme={null}
import asyncio

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.chroma import ChromaDb

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Switch to budget model within same provider (safe)

**URL:** llms-txt#switch-to-budget-model-within-same-provider-(safe)

budget_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a helpful assistant for technical discussions.",
    db=db,
    add_history_to_context=True,
)

---

## Valyu

**URL:** llms-txt#valyu

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/valyu

ValyuTools provides academic and web search capabilities with advanced filtering and relevance scoring.

The following agent can perform academic and web searches:

| Parameter                | Type                  | Default | Description                                     |
| ------------------------ | --------------------- | ------- | ----------------------------------------------- |
| `api_key`                | `Optional[str]`       | `None`  | Valyu API key. Uses VALYU\_API\_KEY if not set. |
| `enable_academic_search` | `bool`                | `True`  | Enable academic sources search functionality.   |
| `enable_web_search`      | `bool`                | `True`  | Enable web search functionality.                |
| `enable_paper_search`    | `bool`                | `True`  | Enable search within paper functionality.       |
| `text_length`            | `int`                 | `1000`  | Maximum length of text content per result.      |
| `max_results`            | `int`                 | `10`    | Maximum number of results to return.            |
| `relevance_threshold`    | `float`               | `0.5`   | Minimum relevance score for results.            |
| `content_category`       | `Optional[str]`       | `None`  | Content category for filtering.                 |
| `search_start_date`      | `Optional[str]`       | `None`  | Start date for search filtering (YYYY-MM-DD).   |
| `search_end_date`        | `Optional[str]`       | `None`  | End date for search filtering (YYYY-MM-DD).     |
| `search_domains`         | `Optional[List[str]]` | `None`  | List of domains to search within.               |
| `sources`                | `Optional[List[str]]` | `None`  | List of specific sources to search.             |
| `max_price`              | `float`               | `30.0`  | Maximum price for API calls.                    |

| Function          | Description                                                   |
| ----------------- | ------------------------------------------------------------- |
| `academic_search` | Search academic sources for research papers and publications. |
| `web_search`      | Search web sources for general information and content.       |
| `paper_search`    | Search within specific papers for detailed information.       |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/valyu.py)
* [Valyu API Documentation](https://valyu.ai/docs)
* [Academic Search Best Practices](https://valyu.ai/academic-search)

---

## Data Analyst Agent

**URL:** llms-txt#data-analyst-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/gateways/langdb/usage/data-analyst

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## What are Models?

**URL:** llms-txt#what-are-models?

**Contents:**
- Error handling
- Learn more

Source: https://docs.agno.com/basics/models/overview

Language Models are machine-learning programs that are trained to understand natural language and code.

When we discuss Models, we are normally referring to Large Language Models (LLMs).

These models act as the **brain** of your Agents - enabling them to reason, act, and respond to the user. The better the model, the smarter the Agent.

<Tip>
  Use [model strings](/basics/models/model-as-string) (`"provider:model_id"`) for simpler configuration. For advanced use cases requiring custom parameters like `temperature` or `max_tokens`, use the full model class syntax.
</Tip>

You can configure your Model to retry requests if they fail. This is useful to handle temporary failures or rate limiting errors from the model provider.

<Note>
  You can also configure `retries`, `delay_between_retries`, and `exponential_backoff` directly on your Agent or Team, to retry the full runs instead of just the Model requests.
</Note>

<CardGroup cols={2}>
  <Card title="Supported Model Providers" icon="layer-group" href="/integrations/models/model-index">
    See the full list of supported model providers.
  </Card>

<Card title="Model-as-string" icon="code" href="/basics/models/model-as-string">
    Use the convenient provider:model\_id string format to specify models without importing model classes.
  </Card>

<Card title="Compatibility" icon="code-compare" href="/basics/models/compatibility">
    See what features are supported by each model provider.
  </Card>

<Card title="Cache Response" icon="database" href="/basics/models/cache-response">
    Cache the response from the model provider to avoid duplicate API calls.
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Use [model strings](/basics/models/model-as-string) (`"provider:model_id"`) for simpler configuration. For advanced use cases requiring custom parameters like `temperature` or `max_tokens`, use the full model class syntax.
</Tip>

## Error handling

You can configure your Model to retry requests if they fail. This is useful to handle temporary failures or rate limiting errors from the model provider.
```

---

## Setup your Agent with Memory

**URL:** llms-txt#setup-your-agent-with-memory

**Contents:**
- Memory Optimization

agent = Agent(
    db=db,
    enable_user_memories=True, # This enables Memory for the Agent
    add_memories_to_context=False, # This disables adding memories to the context
)
python  theme={null}
from agno.memory.strategies.types import MemoryOptimizationStrategyType

**Examples:**

Example 1 (unknown):
```unknown
## Memory Optimization

As users accumulate memories over time, and these memories are added to your context on each request, token costs can grow significantly.
Memory optimization helps reduce these costs by combining multiple memories into fewer, more efficient memories while preserving all the key information.

**When to optimize:**

* Users with 50+ memories
* Before high-cost operations
* Periodic maintenance for long-running applications
```

---

## OpenAI o4-mini

**URL:** llms-txt#openai-o4-mini

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/o4-mini

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Performance on Agent Instantiation with Tool

**URL:** llms-txt#performance-on-agent-instantiation-with-tool

Source: https://docs.agno.com/basics/evals/performance/usage/performance-instantiation-with-tool

Example showing how to analyze the runtime and memory usage of an Agent that is using tools.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## âœ… Good: Each user has isolated memories

**URL:** llms-txt#âœ…-good:-each-user-has-isolated-memories

**Contents:**
  - The Double-Enable Pitfall

agent.print_response("I love pizza", user_id="user_123")
agent.print_response("I'm allergic to dairy", user_id="user_456")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Best practice:** Always pass `user_id` explicitly, especially in multi-user applications.

### The Double-Enable Pitfall

**The Problem:** Using both `enable_user_memories=True` and `enable_agentic_memory=True` doesn't give you bothâ€”agentic mode overrides automatic mode.
```

---

## Sets the session state for the session with the id "user_1_session_1"

**URL:** llms-txt#sets-the-session-state-for-the-session-with-the-id-"user_1_session_1"

team.print_response("What is my name?", session_id="user_1_session_1", user_id="user_1", session_state={"user_name": "John", "age": 30})

---

## This will raise an InputCheckError

**URL:** llms-txt#this-will-raise-an-inputcheckerror

**Contents:**
- Learn More
- Developer Resources

agent.run("Can you check what's in https://fake.com?")
```

<CardGroup cols={2}>
  <Card title="PII Detection" icon="user-shield" href="/basics/guardrails/included/pii">
    Detect and redact personally identifiable information
  </Card>

<Card title="Prompt Injection Defense" icon="bug-slash" href="/basics/guardrails/included/prompt-injection">
    Stop prompt injection and jailbreak attempts
  </Card>

<Card title="OpenAI Moderation" icon="shield-halved" href="/basics/guardrails/included/openai-moderation">
    Detect content that violates OpenAI's content policy
  </Card>
</CardGroup>

## Developer Resources

* View [Reference](/reference/hooks/base-guardrail)
* View [Agent Examples](/basics/guardrails/usage/agent)
* View [Team Examples](/basics/guardrails/usage/team)
* View [Agent Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/guardrails)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/guardrails)

---

## Hacker News

**URL:** llms-txt#hacker-news

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/hackernews

**HackerNews** enables an Agent to search Hacker News website.

The following agent will write an engaging summary of the users with the top 2 stories on hackernews along with the stories.

| Parameter                 | Type   | Default | Description                    |
| ------------------------- | ------ | ------- | ------------------------------ |
| `enable_get_top_stories`  | `bool` | `True`  | Enables fetching top stories.  |
| `enable_get_user_details` | `bool` | `True`  | Enables fetching user details. |
| `all`                     | `bool` | `False` | Enables all functionality.     |

| Function                     | Description                                                                                                                                                                      |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `get_top_hackernews_stories` | Retrieves the top stories from Hacker News. Parameters include `num_stories` to specify the number of stories to return (default is 10). Returns the top stories in JSON format. |
| `get_user_details`           | Retrieves the details of a Hacker News user by their username. Parameters include `username` to specify the user. Returns the user details in JSON format.                       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/hackernews.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/hackernews_tools.py)

---

## "My name is John Doe and I like to paint.", stream=True, user_id=john_doe_id

**URL:** llms-txt#"my-name-is-john-doe-and-i-like-to-paint.",-stream=true,-user_id=john_doe_id

---

## Generate Video Analysis

**URL:** llms-txt#generate-video-analysis

response = agent.run(query, videos=[Video(content=video_file)])

---

## Setup your Agent using an extra reasoning model

**URL:** llms-txt#setup-your-agent-using-an-extra-reasoning-model

deepseek_plus_claude = Agent(
    model=Claude(id="claude-3-7-sonnet-20250219"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)

---

## Define workflow steps

**URL:** llms-txt#define-workflow-steps

research_step = Step(name="Research", agent=research_agent)
content_step = Step(name="Content", agent=content_agent)

---

## vLLM Embedder

**URL:** llms-txt#vllm-embedder

**Contents:**
- Code
- Usage
- Notes

Source: https://docs.agno.com/basics/knowledge/embedder/vllm/usage/vllm-embedder

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run PgVector">
    
  </Step>

<Step title="Run the example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* This example uses **local mode** where vLLM loads the model directly (no server needed)
* For **remote mode**, the code includes `knowledge_remote` example with `base_url` parameter
* GPU with \~14GB VRAM required for e5-mistral-7b-instruct model
* For CPU-only or lower memory, use smaller models like `BAAI/bge-small-en-v1.5`
* Models are automatically downloaded from HuggingFace on first use

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run PgVector">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Agent can now search this knowledge

**URL:** llms-txt#agent-can-now-search-this-knowledge

**Contents:**
  - Choosing an embedder
  - Supported embedders
  - Best Practices
  - Batch Embeddings

agent = Agent(knowledge=knowledge, search_knowledge=True)
agent.print_response("What color is the sky?")
python  theme={null}
from agno.knowledge.embedder.openai import OpenAIEmbedder

embedder=OpenAIEmbedder(
    id="text-embedding-3-small",
    dimensions=1536,
    enable_batch=True,
    batch_size=100
)
```

The following embedders currently support batching:

* [Azure OpenAI](/basics/knowledge/embedder/azure-openai)
* [Cohere](/basics/knowledge/embedder/cohere)
* [Fireworks](/basics/knowledge/embedder/fireworks)
* [Gemini](/basics/knowledge/embedder/gemini)
* [Jina](/basics/knowledge/embedder/jina)
* [Mistral](/basics/knowledge/embedder/mistral)
* [Nebius](/basics/knowledge/embedder/nebius)
* [OpenAI](/basics/knowledge/embedder/openai)
* [Together](/basics/knowledge/embedder/together)
* [Voyage AI](/basics/knowledge/embedder/voyageai)

**Examples:**

Example 1 (unknown):
```unknown
### Choosing an embedder

Pick based on your constraints:

* **Hosted vs local**: Prefer local (e.g., Ollama, FastEmbed) for offline or strict data residency; hosted (OpenAI, Gemini, Voyage) for best quality and convenience.
* **Latency and cost**: Smaller models are cheaper/faster; larger models often retrieve better.
* **Language support**: Ensure your embedder supports the languages you expect.
* **Dimension compatibility**: Match your vector DB's expected embedding size if it's fixed.

#### Quick Comparison

| Embedder        | Type         | Best For                          | Cost    | Performance |
| --------------- | ------------ | --------------------------------- | ------- | ----------- |
| **OpenAI**      | Hosted       | General use, proven quality       | \$\$    | Excellent   |
| **Ollama**      | Local        | Privacy, offline, no API costs    | Free    | Good        |
| **Voyage AI**   | Hosted       | Specialized retrieval tasks       | \$\$\$  | Excellent   |
| **Gemini**      | Hosted       | Google ecosystem, multilingual    | \$\$    | Excellent   |
| **FastEmbed**   | Local        | Fast local embeddings             | Free    | Good        |
| **HuggingFace** | Local/Hosted | Open source models, customization | Free/\$ | Variable    |

### Supported embedders

The following embedders are supported:

* [OpenAI](/basics/knowledge/embedder/openai)
* [Cohere](/basics/knowledge/embedder/cohere)
* [Gemini](/basics/knowledge/embedder/gemini)
* [AWS Bedrock](/basics/knowledge/embedder/aws-bedrock)
* [Azure OpenAI](/basics/knowledge/embedder/azure-openai)
* [Fireworks](/basics/knowledge/embedder/fireworks)
* [HuggingFace](/basics/knowledge/embedder/huggingface)
* [Jina](/basics/knowledge/embedder/jina)
* [Mistral](/basics/knowledge/embedder/mistral)
* [Nebius](/basics/knowledge/embedder/nebius)
* [Ollama](/basics/knowledge/embedder/ollama)
* [Qdrant FastEmbed](/basics/knowledge/embedder/qdrant-fastembed)
* [Together](/basics/knowledge/embedder/together)
* [Voyage AI](/basics/knowledge/embedder/voyageai)

### Best Practices

<Tip>
  **Chunk your content wisely**: Split long docs into 300â€“1,000 token chunks with 10-20% overlap. This balances context preservation with retrieval precision.
</Tip>

<Tip>
  **Store rich metadata**: Include titles, source URLs, timestamps, and permissions with each chunk. This enables filtering and better context in responses.
</Tip>

<Tip>
  **Test your retrieval quality**: Use a small set of test queries to evaluate if you're finding the right chunks. Adjust chunking strategy and embedder if needed.
</Tip>

<Warning>
  **Re-embed when you change models**: If you switch embedders, you must re-embed all your content. Vectors from different models aren't compatible.
</Warning>

### Batch Embeddings

Many embedding providers support processing multiple texts in a single API call, known as batch embedding. This approach offers several advantages: it reduces the number of API requests,
helps avoid rate limits, and significantly improves performance when processing large amounts of text.

To enable batch processing, set the `enable_batch` flag to `True` when configuring your embedder.
The `batch_size` paramater can be used to control the amount of texts sent per batch.
```

---

## "Remove all existing memories of me.",

**URL:** llms-txt#"remove-all-existing-memories-of-me.",

---

## Agent Session

**URL:** llms-txt#agent-session

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/getting-started/09-agent-session

This example shows how to create an agent with persistent memory stored in a SQLite database. We set the session\_id on the agent when resuming the conversation, this way the previous chat history is preserved.

* Stores conversation history in a SQLite database
* Continues conversations across multiple sessions
* References previous context in responses

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create and use the agent

**URL:** llms-txt#create-and-use-the-agent

agent = Agent(knowledge=knowledge)
agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

---

## DynamoDB for Team

**URL:** llms-txt#dynamodb-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/integrations/database/dynamodb/usage/dynamodb-for-team

Agno supports using DynamoDB as a storage backend for Teams using the `DynamoDb` class.

You need to provide `aws_access_key_id` and `aws_secret_access_key` parameters to the `DynamoDb` class.

```python dynamo_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.dynamo import DynamoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Execute team workflow

**URL:** llms-txt#execute-team-workflow

**Contents:**
- Sample Trace
- Advanced Features
  - LangDB Capabilities
- Notes
- Resources

reasoning_team.print_response("Analyze Apple (AAPL) investment potential")
```

View a complete example trace in the LangDB dashboard: [Finance Reasoning Team Trace](https://app.langdb.ai/sharing/threads/73c91c58-eab7-4c6b-afe1-5ab6324f1ada)

<Frame caption="LangDB Finance Team Thread">
  <img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=177481792ff1b6fcdc03d464b62f7711" style={{ borderRadius: '10px', width: '100%', maxWidth: '800px' }} alt="langdb-agno finance team observability" data-og-width="1241" width="1241" data-og-height="916" height="916" data-path="images/langdb-finance-thread.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=13536f1cb8949e0e540505a137e8c978 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2783572ef55c1d8645c2f9dc34c31809 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=d08349da04673fd7f1f358f0201cfd55 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b13d7216f599fef647e6f1f705198368 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0124f22d2915636b0086f19f628515f3 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0470d1883900d3fc2e0e4d2515417291 2500w" />
</Frame>

### LangDB Capabilities

* **Virtual Models**: Save, share, and reuse model configurationsâ€”combining prompts, parameters, tools, and routing logic into a single named unit for consistent behavior across apps
* **MCP Support**: Enhanced tool capabilities through Model Context Protocol servers
* **Multi-Provider**: Support for OpenAI, Anthropic, Google, xAI, and other providers

* **Initialization Order**: Always call `init()` before creating any Agno agents or teams
* **Environment Variables**: With `LANGDB_API_KEY` and `LANGDB_PROJECT_ID` set, you can create models with just `LangDB(id="model_name")`

* [LangDB Documentation](https://docs.langdb.ai/)
* [Building a Reasoning Finance Team Guide](https://docs.langdb.ai/guides/building-agents/building-a-reasoning-finance-team-with-agno)
* [LangDB GitHub Samples](https://github.com/langdb/langdb-samples/tree/main/examples/agno)
* [LangDB Dashboard](https://app.langdb.ai/)

By following these steps, you can effectively integrate Agno with LangDB, enabling comprehensive observability and monitoring of your AI agents.

---

## MongoDB for Workflow

**URL:** llms-txt#mongodb-for-workflow

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/integrations/database/mongo/usage/mongodb-for-workflow

Agno supports using MongoDB as a storage backend for Workflows using the `MongoDBDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python mongodb_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.mongodb import MongoDBDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "mongodb://localhost:27017"

db = MongoDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Python filter expression

**URL:** llms-txt#python-filter-expression

filter_expr = AND(EQ("status", "published"), GT("views", 1000))

---

## Define the expected output structure

**URL:** llms-txt#define-the-expected-output-structure

**Contents:**
  - 3e. Define Analysis Principles

report_format = dedent("""
    REPORT FORMAT:

### Executive Dashboard
    - **Brand Health Score**: [1-10] with supporting evidence
    - **Net Sentiment**: [%positive - %negative] with trend analysis
    - **Key Drivers**: Top 3 positive and negative factors
    - **Alert Level**: Normal/Monitor/Crisis with threshold reasoning

### Quantitative Metrics
    | Sentiment | Posts | % | Avg Engagement | Influence Score |
    |-----------|-------|---|----------------|-----------------|
    [Detailed breakdown with engagement weighting]

### Strategic Recommendations
    **IMMEDIATE (â‰¤48h)**: Crisis response, high-impact replies
    **SHORT-TERM (1-2 weeks)**: Content strategy, community engagement
    **LONG-TERM (1-3 months)**: Product positioning, market strategy
""")

print("Report format defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3e. Define Analysis Principles
```

---

## DeepSeek Reasoner

**URL:** llms-txt#deepseek-reasoner

Source: https://docs.agno.com/basics/reasoning/usage/models/deepseek/deepseek-reasoner

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your DeepSeek API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your DeepSeek API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Qdrant Vector Database

**URL:** llms-txt#qdrant-vector-database

**Contents:**
- Setup
- Example
- Qdrant Params
- Developer Resources

Source: https://docs.agno.com/integrations/vectordb/qdrant/overview

Learn how to use Qdrant as a vector database for your Knowledge Base

Follow the instructions in the [Qdrant Setup Guide](https://qdrant.tech/documentation/guides/installation/) to install Qdrant locally. Here is a guide to get API keys: [Qdrant API Keys](https://qdrant.tech/documentation/cloud/authentication/).

<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      Qdrant also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>

<Tip className="mt-4">
      Using <code>aload()</code> and <code>aprint\_response()</code> with asyncio provides non-blocking operations, making your application more responsive under load.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_qdrant_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/qdrant_db/qdrant_db.py)
* View [Cookbook (Async)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/qdrant_db/async_qdrant_db.py)

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Async Support âš¡">
  <div className="mt-2">
    <p>
      Qdrant also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Performance on Agent with Storage

**URL:** llms-txt#performance-on-agent-with-storage

Source: https://docs.agno.com/basics/evals/performance/usage/performance-with-storage

Example showing how to analyze the runtime and memory usage of an Agent that is using storage.

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Team with Custom Tools

**URL:** llms-txt#team-with-custom-tools

**Contents:**
- Code

Source: https://docs.agno.com/basics/tools/usage/team-with-custom-tools

This example demonstrates how to create a team with custom tools, using custom tools alongside agent tools to answer questions from a knowledge base and fall back to web search when needed.

```python cookbook/examples/teams/tools/01_team_with_custom_tools.py theme={null}
"""
This example demonstrates how to create a team with custom tools.

The team uses custom tools alongside agent tools to answer questions from a knowledge base
and fall back to web search when needed.
"""

from agno.agent import Agent
from agno.team.team import Team
from agno.tools import tool
from agno.tools.duckduckgo import DuckDuckGoTools

@tool()
def answer_from_known_questions(question: str) -> str:
    """Answer a question from a list of known questions

Args:
        question: The question to answer

Returns:
        The answer to the question
    """

# FAQ knowledge base
    faq = {
        "What is the capital of France?": "Paris",
        "What is the capital of Germany?": "Berlin",
        "What is the capital of Italy?": "Rome",
        "What is the capital of Spain?": "Madrid",
        "What is the capital of Portugal?": "Lisbon",
        "What is the capital of Greece?": "Athens",
        "What is the capital of Turkey?": "Ankara",
    }

# Check if question is in FAQ
    if question in faq:
        return f"From my knowledge base: {faq[question]}"
    else:
        return "I don't have that information in my knowledge base. Try asking the web search agent."

---

## Could Not Connect To Docker

**URL:** llms-txt#could-not-connect-to-docker

**Contents:**
- Quick fix
- Full details
- More info

Source: https://docs.agno.com/faq/could-not-connect-to-docker

If you have Docker up and running and get the following error, please read on:

Create the `/var/run/docker.sock` symlink using:

In 99% of the cases, this should work. If it doesnt, try:

Agno uses [docker-py](https://github.com/docker/docker-py) to run containers, and if the `/var/run/docker.sock` is missing or has incorrect permissions, it cannot connect to docker.

**To fix, please create the `/var/run/docker.sock` file using:**

If that does not work, check the permissions using `ls -l /var/run/docker.sock`.

If the `/var/run/docker.sock` does not exist, check if the `$HOME/.docker/run/docker.sock` file is missing. If its missing, please reinstall Docker.

**If none of this works and the `/var/run/docker.sock` exists:**

* Give your user permissions to the `/var/run/docker.sock` file:

* Give your user permissions to the docker group:

* [Docker-py Issue](https://github.com/docker/docker-py/issues/3059#issuecomment-1294369344)
* [Stackoverflow answer](https://stackoverflow.com/questions/48568172/docker-sock-permission-denied/56592277#56592277)

**Examples:**

Example 1 (unknown):
```unknown
## Quick fix

Create the `/var/run/docker.sock` symlink using:
```

Example 2 (unknown):
```unknown
In 99% of the cases, this should work. If it doesnt, try:
```

Example 3 (unknown):
```unknown
## Full details

Agno uses [docker-py](https://github.com/docker/docker-py) to run containers, and if the `/var/run/docker.sock` is missing or has incorrect permissions, it cannot connect to docker.

**To fix, please create the `/var/run/docker.sock` file using:**
```

Example 4 (unknown):
```unknown
If that does not work, check the permissions using `ls -l /var/run/docker.sock`.

If the `/var/run/docker.sock` does not exist, check if the `$HOME/.docker/run/docker.sock` file is missing. If its missing, please reinstall Docker.

**If none of this works and the `/var/run/docker.sock` exists:**

* Give your user permissions to the `/var/run/docker.sock` file:
```

---

## ZDR Reasoning Agent

**URL:** llms-txt#zdr-reasoning-agent

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/models/native/openai/responses/usage/zdr-reasoning-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get current weather for a location

**URL:** llms-txt#get-current-weather-for-a-location

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("What's the current weather in Tokyo?", markdown=True)
```

| Parameter                | Type   | Default  | Description                                                                  |
| ------------------------ | ------ | -------- | ---------------------------------------------------------------------------- |
| `api_key`                | `str`  | `None`   | OpenWeatherMap API key. If not provided, uses OPENWEATHER\_API\_KEY env var. |
| `units`                  | `str`  | `metric` | Units of measurement. Options: 'standard', 'metric', 'imperial'.             |
| `enable_current_weather` | `bool` | `True`   | Enable current weather function.                                             |
| `enable_forecast`        | `bool` | `True`   | Enable forecast function.                                                    |
| `enable_air_pollution`   | `bool` | `True`   | Enable air pollution function.                                               |
| `enable_geocoding`       | `bool` | `True`   | Enable geocoding function.                                                   |

| Function              | Description                                                                                          |
| --------------------- | ---------------------------------------------------------------------------------------------------- |
| `get_current_weather` | Gets current weather data for a location. Takes a location name (e.g., "London").                    |
| `get_forecast`        | Gets weather forecast for a location. Takes a location name and optional number of days (default 5). |
| `get_air_pollution`   | Gets current air pollution data for a location. Takes a location name.                               |
| `geocode_location`    | Converts a location name to geographic coordinates. Takes a location name and optional result limit. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/openweather.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/openweather_tools.py)

---

## File Upload

**URL:** llms-txt#file-upload

**Contents:**
- Usage
- Working example

Source: https://docs.agno.com/integrations/models/native/anthropic/usage/file-upload

Learn how to use Anthropic's Files API with Agno.

With Anthropic's [Files API](https://docs.anthropic.com/en/docs/build-with-claude/files), you can upload files and later reference them in other API calls.
This is handy when a file is referenced multiple times in the same flow.

<Steps>
  <Step title="Upload a file">
    Initialize the Anthropic client and use `client.beta.files.upload`:

<Step title="Initialize the Claude model">
    When initializing the `Claude` model, pass the necessary beta header:

<Step title="Reference the file">
    You can now reference the uploaded file when interacting with your Agno agent:

Notice there are some storage limits attached to this feature. You can read more about that on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/files#file-storage-and-limits).

```python cookbook/models/anthropic/pdf_input_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file
from anthropic import Anthropic

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Initialize the Claude model">
    When initializing the `Claude` model, pass the necessary beta header:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Reference the file">
    You can now reference the uploaded file when interacting with your Agno agent:
```

Example 3 (unknown):
```unknown
</Step>
</Steps>

Notice there are some storage limits attached to this feature. You can read more about that on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/files#file-storage-and-limits).

## Working example
```

---

## Asynchronous Streaming Agent

**URL:** llms-txt#asynchronous-streaming-agent

**Contents:**
- Code

Source: https://docs.agno.com/integrations/models/native/meta/usage/async-stream

```python cookbook/models/meta/llama/async_basic_stream.py theme={null}
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutput  # noqa
from agno.models.meta import Llama

agent = Agent(model=Llama(id="Llama-4-Maverick-17B-128E-Instruct-FP8"), markdown=True)

---

## Define steps using different executor types

**URL:** llms-txt#define-steps-using-different-executor-types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

streaming_content_workflow = Workflow(
    name="Streaming Content Creation Workflow",
    description="Automated content creation with streaming custom execution functions",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflow.db",
    ),
    steps=[
        research_step,
        content_planning_step,
    ],
)

---

## Use the agent

**URL:** llms-txt#use-the-agent

**Contents:**
- OpenLIT Dashboard Features
- Configuration Options
- CLI-Based Instrumentation
- Notes
- Integration with Other Platforms

agent.print_response("What is currently trending on Twitter?")
python  theme={null}
openlit.init(
    otlp_endpoint="http://127.0.0.1:4318",  # OTLP collector endpoint
    tracer=None,  # Custom OpenTelemetry tracer
    disable_batch=False,  # Disable batch span processing
    environment="production",  # Environment name for filtering
    application_name="my-agent",  # Application identifier
)
bash  theme={null}
openlit-instrument \
  --service-name my-ai-app \
  --environment production \
  --otlp-endpoint http://127.0.0.1:4318 \
  python your_app.py
```

This approach is particularly useful for:

* Adding observability to existing applications without code changes
* CI/CD pipelines where you want to instrument automatically
* Testing observability before committing to code modifications

* **Automatic Instrumentation**: OpenLIT automatically instruments supported LLM providers (OpenAI, Anthropic, etc.) and frameworks
* **Zero Code Changes**: Use either `openlit.init()` in your code or the `openlit-instrument` CLI to trace all LLM calls without modifications
* **OpenTelemetry Native**: OpenLIT uses standard OpenTelemetry protocols, ensuring compatibility with other observability tools
* **Open-Source & Self-Hosted**: OpenLIT is fully open-source and runs on your own infrastructure for complete data privacy and control

## Integration with Other Platforms

[OpenLIT](https://openlit.io/) can export traces to other observability platforms like Grafana Cloud, New Relic and more. See the [Langfuse integration guide](/integrations/observability/langfuse) for an example of using OpenLIT with Langfuse.

**Examples:**

Example 1 (unknown):
```unknown
## OpenLIT Dashboard Features

Once your agents are instrumented, you can access the OpenLIT dashboard to:

* **View Traces**: Visualize complete execution flows including agent runs, tool calls, and LLM requests
* **Monitor Performance**: Track latency, token usage, and throughput metrics
* **Analyze Costs**: Monitor API costs across different models and providers
* **Track Errors**: Identify and debug exceptions with detailed stack traces
* **Compare Models**: Evaluate different LLM providers based on performance and cost

<Frame caption="OpenLIT Trace Details">
  <video autoPlay muted loop controls className="w-full aspect-video" src="https://mintcdn.com/openlit/oP6rqLGiwYvXWG_M/images/trace-details.mp4?fit=max&auto=format&n=oP6rqLGiwYvXWG_M&q=85&s=80a9b4bf54862dd386284f175c71f714" />
</Frame>

## Configuration Options

The `openlit.init()` function accepts several parameters:
```

Example 2 (unknown):
```unknown
## CLI-Based Instrumentation

For true zero-code instrumentation, you can use the `openlit-instrument` CLI command to run your application without modifying any code:
```

---

## Competitor Analysis Agent

**URL:** llms-txt#competitor-analysis-agent

**Contents:**
- What You'll Learn
- Use Cases
- How It Works
- Code
- What to Expect
- Usage
- Next Steps

Source: https://docs.agno.com/examples/use-cases/agents/competitor-analysis-agent

Build an AI agent that performs comprehensive competitive intelligence by combining web search, content scraping, and reasoning tools to analyze competitors and generate strategic reports with actionable insights.

By building this agent, you'll understand:

* How to integrate multiple tools (Firecrawl and ReasoningTools) for complex workflows
* How to structure multi-phase research processes with clear instructions
* How reasoning tools make the agent's analytical thinking transparent
* How to define structured output templates for consistent reports

Build market research tools, competitive intelligence systems, strategic planning assistants, or due diligence platforms for investment decisions.

The agent follows a systematic five-phase research process:

1. **Discovery**: Finds competitors and industry information using web search
2. **Analysis**: Scrapes websites and extracts detailed information
3. **Comparison**: Analyzes features and competitive positioning
4. **Synthesis**: Conducts SWOT analysis with reasoning tools
5. **Reporting**: Compiles findings into actionable recommendations

Reasoning tools provide transparency into the agent's analytical thinking throughout the process.

The agent will conduct a comprehensive competitive analysis in five phases. It uses Firecrawl to search for competitors and scrape their websites, then applies reasoning tools to analyze findings. The analysis streams in real-time so you can follow the agent's research and thinking process.

The final output is a detailed strategic report with competitor profiles, SWOT analyses, comparative tables, and actionable recommendations organized by timeline (immediate, short-term, long-term).

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* Modify the target company in the example query to analyze different markets
* Adjust `search_params` and `limit` in FirecrawlTools for deeper or broader research
* Customize the `expected_output` template to focus on specific competitive aspects
* Explore [Reasoning Tools](/basics/tools/reasoning_tools/reasoning-tools) for advanced analytical capabilities

**Examples:**

Example 1 (unknown):
```unknown
## What to Expect

The agent will conduct a comprehensive competitive analysis in five phases. It uses Firecrawl to search for competitors and scrape their websites, then applies reasoning tools to analyze findings. The analysis streams in real-time so you can follow the agent's research and thinking process.

The final output is a detailed strategic report with competitor profiles, SWOT analyses, comparative tables, and actionable recommendations organized by timeline (immediate, short-term, long-term).

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## from rich.pretty import pprint

**URL:** llms-txt#from-rich.pretty-import-pprint

---

## Create an Agent with the DALL-E tool

**URL:** llms-txt#create-an-agent-with-the-dall-e-tool

agent = Agent(tools=[DalleTools()], name="DALL-E Image Generator")

---

## Add CORS middleware

**URL:** llms-txt#add-cors-middleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

---

## When the model calls check_condition with value > 100,

**URL:** llms-txt#when-the-model-calls-check_condition-with-value->-100,

---

## Define how the agent should gather data

**URL:** llms-txt#define-how-the-agent-should-gather-data

**Contents:**
  - 3b. Define the Analysis Framework

data_collection_strategy = dedent("""
    DATA COLLECTION STRATEGY:
    - Use X Tools to gather direct social media mentions with full engagement metrics
    - Use Exa Tools to find broader web discussions, articles, and forum conversations
    - Cross-reference findings between social and web sources for comprehensive coverage
""")

print("Data collection strategy defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3b. Define the Analysis Framework
```

---

## 5. Extract and cut video segments - Optional

**URL:** llms-txt#5.-extract-and-cut-video-segments---optional

def extract_segments(response_text):
    import re

segments_pattern = r"\|\s*(\d+:\d+)\s*\|\s*(\d+:\d+)\s*\|\s*(.*?)\s*\|\s*(\d+)\s*\|"
    segments: list[dict] = []

for match in re.finditer(segments_pattern, str(response_text)):
        start_time = match.group(1)
        end_time = match.group(2)
        description = match.group(3)
        score = int(match.group(4))

# Convert timestamps to seconds
        start_seconds = sum(x * int(t) for x, t in zip([60, 1], start_time.split(":")))
        end_seconds = sum(x * int(t) for x, t in zip([60, 1], end_time.split(":")))
        duration = end_seconds - start_seconds

# Only process high-scoring segments
        if 15 <= duration <= 60 and score > 7:
            output_path = output_dir / f"short_{len(segments) + 1}.mp4"

# FFmpeg command to cut video
            command = [
                "ffmpeg",
                "-ss",
                str(start_seconds),
                "-i",
                video_path,
                "-t",
                str(duration),
                "-vf",
                "scale=1080:1920,setsar=1:1",
                "-c:v",
                "libx264",
                "-c:a",
                "aac",
                "-y",
                str(output_path),
            ]

try:
                subprocess.run(command, check=True)
                segments.append(
                    {"path": output_path, "description": description, "score": score}
                )
            except subprocess.CalledProcessError:
                print(f"Failed to process segment: {start_time} - {end_time}")

logger.debug(f"{response.content}")

---

## ************* Run AgentOS *************

**URL:** llms-txt#*************-run-agentos-*************

**Contents:**
- AgentOS - Production Runtime for Multi-Agent Systems
- The Complete Agentic Solution
- Get started

if __name__ == "__main__":
    agent_os.serve(app="agno_agent:app", reload=True)
```

## AgentOS - Production Runtime for Multi-Agent Systems

Building Agents is easy, running them as a secure, scalable service is hard. AgentOS solves this by providing a high performance runtime for serving multi-agent systems in production. Key features include:

1. **Pre-built FastAPI app**: AgentOS includes a ready-to-use FastAPI app for running your agents, teams and workflows. This gives you a significant head start when building an AI product.

2. **Integrated Control Plane**: The [AgentOS UI](https://os.agno.com) connects directly to your runtime, so you can test, monitor and manage your system in real time with full operational visibility.

3. **Private by Design**: AgentOS runs entirely in your cloud, ensuring complete data privacy. No data leaves your environment, making it ideal for security conscious enterprises..

When you run the `agno_agent.py` script shared above, you get a FastAPI app that you can connect to the [AgentOS UI](https://os.agno.com). Here's what it looks like in action:

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/aEfJPs-hg36UsUPO/videos/agno-agent-chat.mp4?fit=max&auto=format&n=aEfJPs-hg36UsUPO&q=85&s=b8ac56bfb2e9436799299fcafa746d4a" type="video/mp4" data-path="videos/agno-agent-chat.mp4" />
  </video>
</Frame>

## The Complete Agentic Solution

Agno provides the complete solution for companies building agentic systems:

* The fastest framework for building agents, multi-agent teams and agentic workflows.
* A ready-to-use FastAPI app that gets you building AI products on day one.
* A control plane for testing, monitoring and managing your system.

We bring a novel architecture that no other framework provides, your AgentOS runs securely in your cloud, and the control plane connects directly to it from your browser. You don't need to send data to any external services or pay retention costs, you get complete privacy and control.

If you're new to Agno, follow the [quickstart](/get-started/quickstart) to build your first Agent and run it using the AgentOS.

After that, checkout the [examples gallery](/examples/use-cases/agents/overview) and build real-world applications with Agno.

<Tip>
  If you're looking for Agno 1.0 docs, please visit [docs-v1.agno.com](https://docs-v1.agno.com).

We also have a [migration guide](/how-to/v2-migration) for those coming from Agno 1.0.
</Tip>

---

## External Tool Execution Async

**URL:** llms-txt#external-tool-execution-async

Source: https://docs.agno.com/basics/hitl/usage/external-tool-execution-async

This example demonstrates how to execute tools outside of the agent using external tool execution in an asynchronous environment. This pattern allows you to control tool execution externally while maintaining agent functionality with async operations.

<Steps>
  <Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Install infinity

**URL:** llms-txt#install-infinity

pip install "infinity-emb[all]"

---

## SingleStore

**URL:** llms-txt#singlestore

Source: https://docs.agno.com/reference/vector-db/singlestore

<Snippet file="vector-db-singlestore-reference.mdx" />

---

## Agentic RAG with Reasoning Tools

**URL:** llms-txt#agentic-rag-with-reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/knowledge/search-and-retrieval/usage/agentic-rag-with-reasoning

This example demonstrates how to implement Agentic RAG with Reasoning Tools, combining knowledge base search with structured reasoning capabilities for more sophisticated responses.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your ANTHROPIC API  key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Create a Python file">
    Create a Python file and add the above code.

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/agentic_search" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your ANTHROPIC API  key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Nebius Embedder

**URL:** llms-txt#nebius-embedder

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/embedder/nebius/usage/nebius-embedder

```python  theme={null}
from agno.knowledge.embedder.nebius import NebiusEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = NebiusEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## OpenAI gpt-5-mini with reasoning effort

**URL:** llms-txt#openai-gpt-5-mini-with-reasoning-effort

Source: https://docs.agno.com/basics/reasoning/usage/models/openai/reasoning-effort

<Steps>
  <Step title="Create a Python file">
    
  </Step>

<Step title="Add the following code to your Python file">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Export your OpenAI API key">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Add the following code to your Python file">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Ask questions that benefit from real-time information

**URL:** llms-txt#ask-questions-that-benefit-from-real-time-information

**Contents:**
- Usage

agent.print_response(
    "What are the current market trends in renewable energy?",
    stream=True,
    markdown=True,
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/grounding.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/grounding.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Dedicated tracing database

**URL:** llms-txt#dedicated-tracing-database

tracing_db = SqliteDb(db_file="tmp/traces.db", id="traces_db")

---

## Getting Help

**URL:** llms-txt#getting-help

**Contents:**
- Need help?
- Building with Agno?
- Looking for dedicated support?

Source: https://docs.agno.com/get-started/getting-help

Connect with builders, get support, and explore Agent Engineering.

Head over to our [community forum](https://agno.link/community) for help and insights from the team.

## Building with Agno?

Share what you're building on [X](https://agno.link/x), [LinkedIn](https://www.linkedin.com/company/agno-agi) or join our [Discord](https://agno.link/discord) to connect with other builders.

## Looking for dedicated support?

We've helped many companies turn ideas into products. [Book a call](https://cal.com/team/agno/intro) to get started.

---

## Create a team for collaborative image transformation

**URL:** llms-txt#create-a-team-for-collaborative-image-transformation

**Contents:**
- Usage

transformation_team = Team(
    name="Image Transformation Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[style_advisor, image_transformer],
    instructions=[
        "Transform images with artistic style and precision.",
        "Style Advisor: First analyze transformation requirements and recommend styles.",
        "Image Transformer: Apply transformations using AI tools with style guidance.",
    ],
    markdown=True,
)

transformation_team.print_response(
    "a cat dressed as a wizard with a background of a mystic forest. Make it look like 'https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png'",
    stream=True,
)
bash  theme={null}
    pip install agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    export FAL_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/image_to_image_transformation.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Knowledge base with Infinity reranker for high performance

**URL:** llms-txt#knowledge-base-with-infinity-reranker-for-high-performance

knowledge_primary = Knowledge(
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_primary",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        reranker=InfinityReranker(
            base_url="http://localhost:7997/rerank", model="BAAI/bge-reranker-base"
        ),
    ),
)

---

## BaiduSearch

**URL:** llms-txt#baidusearch

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/integrations/toolkits/search/baidusearch

**BaiduSearch** enables an Agent to search the web for information using the Baidu search engine.

The following example requires the `baidusearch` library. To install BaiduSearch, run the following command:

| Parameter             | Type   | Default | Description                                                                                          |
| --------------------- | ------ | ------- | ---------------------------------------------------------------------------------------------------- |
| `fixed_max_results`   | `int`  | -       | Sets a fixed number of maximum results to return. No default is provided, must be specified if used. |
| `fixed_language`      | `str`  | -       | Set the fixed language for the results.                                                              |
| `headers`             | `Any`  | -       | Headers to be used in the search request.                                                            |
| `proxy`               | `str`  | -       | Specifies a single proxy address as a string to be used for the HTTP requests.                       |
| `timeout`             | `int`  | `10`    | Sets the timeout for HTTP requests, in seconds.                                                      |
| `enable_baidu_search` | `bool` | `True`  | Enable the baidu\_search functionality.                                                              |
| `all`                 | `bool` | `False` | Enable all functionality.                                                                            |

| Function       | Description                                    |
| -------------- | ---------------------------------------------- |
| `baidu_search` | Use this function to search Baidu for a query. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/baidusearch.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/baidusearch_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Agents

**URL:** llms-txt#agents

notion_agent = Agent(
    name="Notion Manager",
    model=OpenAIChat(id="gpt-4o"),
    tools=[
        NotionTools(
            api_key=os.getenv("NOTION_API_KEY", ""),
            database_id=os.getenv("NOTION_DATABASE_ID", ""),
        )
    ],
    instructions=[
        "You are a Notion page manager.",
        "You will receive instructions with a query and a pre-classified tag.",
        "CRITICAL: Use ONLY the exact tag provided in the instructions. Do NOT create new tags or modify the tag name.",
        "The valid tags are: travel, tech, general-blogs, fashion, documents",
        "Workflow:",
        "1. Search for existing pages with the EXACT tag provided",
        "2. If a page exists: Update that page with the new query content",
        "3. If no page exists: Create a new page using the EXACT tag provided",
        "Always preserve the exact tag name as given in the instructions.",
    ],
)

---

## Milvus Async

**URL:** llms-txt#milvus-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/integrations/vectordb/milvus/usage/async-milvus-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## LangWatch

**URL:** llms-txt#langwatch

**Contents:**
- Integrating Agno with LangWatch
- Prerequisites
- Sending Traces to LangWatch

Source: https://docs.agno.com/integrations/observability/langwatch

Integrate Agno with LangWatch to send traces and gain insights into your agent's performance.

## Integrating Agno with LangWatch

LangWatch provides a robust platform for tracing and monitoring AI model calls. By integrating Agno with LangWatch, you can utilize OpenInference instrumentation to send traces and gain insights into your agent's performance.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup LangWatch Account**

* Sign up for an account at [LangWatch](https://langwatch.ai/).
   * Obtain your API key from the LangWatch dashboard.

3. **Set Environment Variables**

Configure your environment with the LangWatch API key:

## Sending Traces to LangWatch

* ### Example: Using LangWatch with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to LangWatch.

```python  theme={null}
import langwatch
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from openinference.instrumentation.agno import AgnoInstrumentor

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup LangWatch Account**

   * Sign up for an account at [LangWatch](https://langwatch.ai/).
   * Obtain your API key from the LangWatch dashboard.

3. **Set Environment Variables**

   Configure your environment with the LangWatch API key:
```

Example 2 (unknown):
```unknown
## Sending Traces to LangWatch

* ### Example: Using LangWatch with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to LangWatch.
```

---

## Team with Agentic Knowledge Filters

**URL:** llms-txt#team-with-agentic-knowledge-filters

**Contents:**
- Code

Source: https://docs.agno.com/basics/knowledge/teams/usage/team-with-agentic-knowledge-filters

This example demonstrates how to use agentic knowledge filters with teams. Unlike predefined filters, agentic knowledge filters allow the AI to dynamically determine which documents to search based on the query context, providing more intelligent and context-aware document retrieval.

```python cookbook/examples/teams/knowledge/03_team_with_agentic_knowledge_filters.py theme={null}
"""
This example demonstrates how to use agentic knowledge filters with teams.

Agentic knowledge filters allow the AI to dynamically determine which documents
to search based on the query context, rather than using predefined filters.
"""

from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.utils.media import (
    SampleDataFileExtension,
    download_knowledge_filters_sample_data,
)
from agno.vectordb.lancedb import LanceDb

---

## Input Validation Pre-Hook

**URL:** llms-txt#input-validation-pre-hook

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/basics/hooks/usage/team/input-validation-pre-hook

This example demonstrates how to use a pre-hook to validate the input of an Team, before it is presented to the LLM.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---
