{
  "title": "Ollama",
  "content": "Source: https://docs.agno.com/reference/models/ollama\n\nThe Ollama model provides access to open source models, both locally-hosted and via **Ollama Cloud**.\n\n**Local Usage**: Run models on your own hardware using the Ollama client. Perfect for development, privacy-sensitive workloads, and when you want full control over your infrastructure.\n\n**Cloud Usage**: Access cloud-hosted models via [Ollama Cloud](https://ollama.com) with an API key for scalable, production-ready deployments. No local setup required - simply set your `OLLAMA_API_KEY` and start using powerful models instantly.\n\n* **Dual Deployment Options**: Choose between local hosting for privacy and control, or cloud hosting for scalability\n* **Seamless Switching**: Easy transition between local and cloud deployments with minimal code changes\n* **Auto-configuration**: When using an API key, the host automatically defaults to Ollama Cloud\n* **Wide Model Support**: Access to extensive library of open-source models including GPT-OSS, Llama, Qwen, DeepSeek, and Phi models\n\n| Parameter               | Type                          | Default                    | Description                                                      |\n| ----------------------- | ----------------------------- | -------------------------- | ---------------------------------------------------------------- |\n| `id`                    | `str`                         | `\"llama3.2\"`               | The name of the Ollama model to use                              |\n| `name`                  | `str`                         | `\"Ollama\"`                 | The name of the model                                            |\n| `provider`              | `str`                         | `\"Ollama\"`                 | The provider of the model                                        |\n| `host`                  | `str`                         | `\"http://localhost:11434\"` | The host URL for the Ollama server                               |\n| `timeout`               | `Optional[int]`               | `None`                     | Request timeout in seconds                                       |\n| `format`                | `Optional[str]`               | `None`                     | The format to return the response in (e.g., \"json\")              |\n| `options`               | `Optional[Dict[str, Any]]`    | `None`                     | Additional model options (temperature, top\\_p, etc.)             |\n| `keep_alive`            | `Optional[Union[float, str]]` | `None`                     | How long to keep the model loaded (e.g., \"5m\", 3600 seconds)     |\n| `template`              | `Optional[str]`               | `None`                     | The prompt template to use                                       |\n| `system`                | `Optional[str]`               | `None`                     | System message to use                                            |\n| `raw`                   | `Optional[bool]`              | `None`                     | Whether to return raw response without formatting                |\n| `stream`                | `bool`                        | `True`                     | Whether to stream the response                                   |\n| `retries`               | `int`                         | `0`                        | Number of retries to attempt before raising a ModelProviderError |\n| `delay_between_retries` | `int`                         | `1`                        | Delay between retries, in seconds                                |\n| `exponential_backoff`   | `bool`                        | `False`                    | If True, the delay between retries is doubled each time          |",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Key Features",
      "id": "key-features"
    },
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    }
  ],
  "url": "llms-txt#ollama",
  "links": []
}