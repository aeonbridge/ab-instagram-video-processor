{
  "title": "Model",
  "content": "Source: https://docs.agno.com/reference/models/model\n\nThe Model class is the base class for all models in Agno. It provides common functionality and parameters that are inherited by specific model implementations like OpenAIChat, Claude, etc.\n\n| Parameter               | Type                              | Default  | Description                                                                             |\n| ----------------------- | --------------------------------- | -------- | --------------------------------------------------------------------------------------- |\n| `id`                    | `str`                             | Required | The id/name of the model to use                                                         |\n| `name`                  | `Optional[str]`                   | `None`   | The display name of the model                                                           |\n| `provider`              | `Optional[str]`                   | `None`   | The provider of the model                                                               |\n| `frequency_penalty`     | `Optional[float]`                 | `None`   | Penalizes new tokens based on their frequency in the text so far                        |\n| `presence_penalty`      | `Optional[float]`                 | `None`   | Penalizes new tokens based on whether they appear in the text so far                    |\n| `response_format`       | `Optional[str]`                   | `None`   | The format of the response                                                              |\n| `seed`                  | `Optional[int]`                   | `None`   | Random seed for deterministic sampling                                                  |\n| `stop`                  | `Optional[Union[str, List[str]]]` | `None`   | Up to 4 sequences where the API will stop generating further tokens                     |\n| `stream`                | `bool`                            | `True`   | Whether to stream the response                                                          |\n| `temperature`           | `Optional[float]`                 | `None`   | Controls randomness in the model's output                                               |\n| `top_p`                 | `Optional[float]`                 | `None`   | Controls diversity via nucleus sampling                                                 |\n| `max_tokens`            | `Optional[int]`                   | `None`   | Maximum number of tokens to generate                                                    |\n| `request_params`        | `Optional[Dict[str, Any]]`        | `None`   | Additional parameters to include in the request                                         |\n| `cache_response`        | `bool`                            | `False`  | Enable caching of model responses to avoid redundant API calls                          |\n| `cache_ttl`             | `Optional[int]`                   | `None`   | Time-to-live for cached model responses, in seconds. If None, cache never expires       |\n| `cache_dir`             | `Optional[str]`                   | `None`   | Directory path for storing cached model responses. If None, uses default cache location |\n| `retries`               | `int`                             | `0`      | Number of retries to attempt before raising a ModelProviderError                        |\n| `delay_between_retries` | `int`                             | `1`      | Delay between retries, in seconds                                                       |\n| `exponential_backoff`   | `bool`                            | `False`  | If True, the delay between retries is doubled each time                                 |",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    }
  ],
  "url": "llms-txt#model",
  "links": []
}