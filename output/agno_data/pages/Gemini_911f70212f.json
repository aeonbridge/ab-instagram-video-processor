{
  "title": "Gemini",
  "content": "Source: https://docs.agno.com/reference/models/gemini\n\nThe Gemini model provides access to Google's Gemini models.\n\n| Parameter               | Type                       | Default              | Description                                                      |\n| ----------------------- | -------------------------- | -------------------- | ---------------------------------------------------------------- |\n| `id`                    | `str`                      | `\"gemini-1.5-flash\"` | The id of the Gemini model to use                                |\n| `name`                  | `str`                      | `\"Gemini\"`           | The name of the model                                            |\n| `provider`              | `str`                      | `\"Google\"`           | The provider of the model                                        |\n| `api_key`               | `Optional[str]`            | `None`               | The API key for Google AI (defaults to GOOGLE\\_API\\_KEY env var) |\n| `generation_config`     | `Optional[Dict[str, Any]]` | `None`               | Generation configuration parameters for the model                |\n| `safety_settings`       | `Optional[List[Dict]]`     | `None`               | Safety settings to filter content                                |\n| `tools`                 | `Optional[List[Dict]]`     | `None`               | Tools available to the model                                     |\n| `tool_config`           | `Optional[Dict[str, Any]]` | `None`               | Configuration for tool use                                       |\n| `system_instruction`    | `Optional[str]`            | `None`               | System instruction for the model                                 |\n| `cached_content`        | `Optional[str]`            | `None`               | Cached content identifier for context caching                    |\n| `request_params`        | `Optional[Dict[str, Any]]` | `None`               | Additional parameters to include in the request                  |\n| `client_params`         | `Optional[Dict[str, Any]]` | `None`               | Additional parameters for client configuration                   |\n| `thinking_enabled`      | `Optional[bool]`           | `None`               | Whether to enable thinking mode for supported models             |\n| `retries`               | `int`                      | `0`                  | Number of retries to attempt before raising a ModelProviderError |\n| `delay_between_retries` | `int`                      | `1`                  | Delay between retries, in seconds                                |\n| `exponential_backoff`   | `bool`                     | `False`              | If True, the delay between retries is doubled each time          |",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    }
  ],
  "url": "llms-txt#gemini",
  "links": []
}