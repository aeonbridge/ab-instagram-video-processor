{
  "title": "Evaluate the accuracy of the Team's responses",
  "content": "evaluation = AccuracyEval(\n    name=\"Multi Language Team\",\n    model=OpenAIChat(id=\"o4-mini\"),\n    team=multi_language_team,\n    input=\"Comment allez-vous?\",\n    expected_output=\"I can only answer in the following languages: English and Spanish.\",\n    num_iterations=1,\n)\n\nresult: Optional[AccuracyResult] = evaluation.run(print_results=True)\nassert result is not None and result.avg_score >= 8\n\npython accuracy_comparison.py theme={null}\nfrom typing import Optional\n\nfrom agno.agent import Agent\nfrom agno.eval.accuracy import AccuracyEval, AccuracyResult\nfrom agno.models.openai import OpenAIChat\nfrom agno.tools.calculator import CalculatorTools\n\nevaluation = AccuracyEval(\n    name=\"Number Comparison Evaluation\",\n    model=OpenAIChat(id=\"o4-mini\"),\n    agent=Agent(\n        model=OpenAIChat(id=\"gpt-5-mini\"),\n        tools=[CalculatorTools()],\n        instructions=\"You must use the calculator tools for comparisons.\",\n    ),\n    input=\"9.11 and 9.9 -- which is bigger?\",\n    expected_output=\"9.9\",\n    additional_guidelines=\"Its ok for the output to include additional text or information relevant to the comparison.\",\n)\n\nresult: Optional[AccuracyResult] = evaluation.run(print_results=True)\nassert result is not None and result.avg_score >= 8\n\nbash  theme={null}\n    pip install -U agno\n    bash Mac theme={null}\n      python accuracy.py\n      bash Windows theme={null}\n      python accuracy.py\n      python evals_demo.py theme={null}\n\n\"\"\"Simple example creating a evals and using the AgentOS.\"\"\"\n\nfrom agno.agent import Agent\nfrom agno.db.postgres.postgres import PostgresDb\nfrom agno.eval.accuracy import AccuracyEval\nfrom agno.models.openai import OpenAIChat\nfrom agno.os import AgentOS\nfrom agno.tools.calculator import CalculatorTools",
  "code_samples": [
    {
      "code": "## Accuracy with Number Comparison\n\nThis example demonstrates evaluating an agent's ability to make correct numerical comparisons, which can be tricky for LLMs when dealing with decimal numbers:",
      "language": "unknown"
    },
    {
      "code": "## Usage\n\n<Steps>\n  <Snippet file=\"create-venv-step.mdx\" />\n\n  <Step title=\"Install libraries\">",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Run your Accuracy Eval Example\">\n    <CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n  </Step>\n</Steps>\n\n## Track Evals in your AgentOS\n\nThe best way to track your Agno Evals is with the AgentOS platform.\n\n<video autoPlay muted controls className=\"w-full aspect-video\" src=\"https://mintcdn.com/agno-v2/hzelS2cST9lEqMuM/videos/eval_platform.mp4?fit=max&auto=format&n=hzelS2cST9lEqMuM&q=85&s=9329eaac5cd0f551081e51656cc0227c\" data-path=\"videos/eval_platform.mp4\" />",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Accuracy with Number Comparison",
      "id": "accuracy-with-number-comparison"
    },
    {
      "level": "h2",
      "text": "Usage",
      "id": "usage"
    },
    {
      "level": "h2",
      "text": "Track Evals in your AgentOS",
      "id": "track-evals-in-your-agentos"
    }
  ],
  "url": "llms-txt#evaluate-the-accuracy-of-the-team's-responses",
  "links": []
}