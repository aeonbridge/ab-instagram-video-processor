{
  "title": "--- Helper Functions ---",
  "content": "def get_cached_blog_post(session_state, topic: str) -> Optional[str]:\n    \"\"\"Get cached blog post from workflow session state\"\"\"\n    logger.info(\"Checking if cached blog post exists\")\n    return session_state.get(\"blog_posts\", {}).get(topic)\n\ndef cache_blog_post(session_state, topic: str, blog_post: str):\n    \"\"\"Cache blog post in workflow session state\"\"\"\n    logger.info(f\"Saving blog post for topic: {topic}\")\n    if \"blog_posts\" not in session_state:\n        session_state[\"blog_posts\"] = {}\n    session_state[\"blog_posts\"][topic] = blog_post\n\ndef get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:\n    \"\"\"Get cached search results from workflow session state\"\"\"\n    logger.info(\"Checking if cached search results exist\")\n    search_results = session_state.get(\"search_results\", {}).get(topic)\n    if search_results and isinstance(search_results, dict):\n        try:\n            return SearchResults.model_validate(search_results)\n        except Exception as e:\n            logger.warning(f\"Could not validate cached search results: {e}\")\n    return search_results if isinstance(search_results, SearchResults) else None\n\ndef cache_search_results(session_state, topic: str, search_results: SearchResults):\n    \"\"\"Cache search results in workflow session state\"\"\"\n    logger.info(f\"Saving search results for topic: {topic}\")\n    if \"search_results\" not in session_state:\n        session_state[\"search_results\"] = {}\n    session_state[\"search_results\"][topic] = search_results.model_dump()\n\ndef get_cached_scraped_articles(\n    session_state, topic: str\n) -> Optional[Dict[str, ScrapedArticle]]:\n    \"\"\"Get cached scraped articles from workflow session state\"\"\"\n    logger.info(\"Checking if cached scraped articles exist\")\n    scraped_articles = session_state.get(\"scraped_articles\", {}).get(topic)\n    if scraped_articles and isinstance(scraped_articles, dict):\n        try:\n            return {\n                url: ScrapedArticle.model_validate(article)\n                for url, article in scraped_articles.items()\n            }\n        except Exception as e:\n            logger.warning(f\"Could not validate cached scraped articles: {e}\")\n    return scraped_articles if isinstance(scraped_articles, dict) else None\n\ndef cache_scraped_articles(\n    session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]\n):\n    \"\"\"Cache scraped articles in workflow session state\"\"\"\n    logger.info(f\"Saving scraped articles for topic: {topic}\")\n    if \"scraped_articles\" not in session_state:\n        session_state[\"scraped_articles\"] = {}\n    session_state[\"scraped_articles\"][topic] = {\n        url: article.model_dump() for url, article in scraped_articles.items()\n    }\n\nasync def get_search_results(\n    session_state, topic: str, use_cache: bool = True, num_attempts: int = 3\n) -> Optional[SearchResults]:\n    \"\"\"Get search results with caching support\"\"\"\n\n# Check cache first\n    if use_cache:\n        cached_results = get_cached_search_results(session_state, topic)\n        if cached_results:\n            logger.info(f\"Found {len(cached_results.articles)} articles in cache.\")\n            return cached_results\n\n# Search for new results\n    for attempt in range(num_attempts):\n        try:\n            print(\n                f\"üîç Searching for articles about: {topic} (attempt {attempt + 1}/{num_attempts})\"\n            )\n            response = await research_agent.arun(topic)\n\nif (\n                response\n                and response.content\n                and isinstance(response.content, SearchResults)\n            ):\n                article_count = len(response.content.articles)\n                logger.info(f\"Found {article_count} articles on attempt {attempt + 1}\")\n                print(f\"‚úÖ Found {article_count} relevant articles\")\n\n# Cache the results\n                cache_search_results(session_state, topic, response.content)\n                return response.content\n            else:\n                logger.warning(\n                    f\"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type\"\n                )\n\nexcept Exception as e:\n            logger.warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n\nlogger.error(f\"Failed to get search results after {num_attempts} attempts\")\n    return None\n\nasync def scrape_articles(\n    session_state,\n    topic: str,\n    search_results: SearchResults,\n    use_cache: bool = True,\n) -> Dict[str, ScrapedArticle]:\n    \"\"\"Scrape articles with caching support\"\"\"\n\n# Check cache first\n    if use_cache:\n        cached_articles = get_cached_scraped_articles(session_state, topic)\n        if cached_articles:\n            logger.info(f\"Found {len(cached_articles)} scraped articles in cache.\")\n            return cached_articles\n\nscraped_articles: Dict[str, ScrapedArticle] = {}\n\nprint(f\"üìÑ Scraping {len(search_results.articles)} articles...\")\n\nfor i, article in enumerate(search_results.articles, 1):\n        try:\n            print(\n                f\"üìñ Scraping article {i}/{len(search_results.articles)}: {article.title[:50]}...\"\n            )\n            response = await content_scraper_agent.arun(article.url)\n\nif (\n                response\n                and response.content\n                and isinstance(response.content, ScrapedArticle)\n            ):\n                scraped_articles[response.content.url] = response.content\n                logger.info(f\"Scraped article: {response.content.url}\")\n                print(f\"‚úÖ Successfully scraped: {response.content.title[:50]}...\")\n            else:\n                print(f\"‚ùå Failed to scrape: {article.title[:50]}...\")\n\nexcept Exception as e:\n            logger.warning(f\"Failed to scrape {article.url}: {str(e)}\")\n            print(f\"‚ùå Error scraping: {article.title[:50]}...\")\n\n# Cache the scraped articles\n    cache_scraped_articles(session_state, topic, scraped_articles)\n    return scraped_articles",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#----helper-functions----",
  "links": []
}