{
  "title": "--- Main Execution Function ---",
  "content": "async def blog_generation_execution(\n    session_state,\n    topic: str = None,\n    use_search_cache: bool = True,\n    use_scrape_cache: bool = True,\n    use_blog_cache: bool = True,\n) -> str:\n    \"\"\"\n    Blog post generation workflow execution function.\n\nArgs:\n        session_state: The shared session state\n        topic: Blog post topic (if not provided, uses execution_input.input)\n        use_search_cache: Whether to use cached search results\n        use_scrape_cache: Whether to use cached scraped articles\n        use_blog_cache: Whether to use cached blog posts\n    \"\"\"\n\nif not blog_topic:\n        return \"âŒ No blog topic provided. Please specify a topic.\"\n\nprint(f\"ğŸ¨ Generating blog post about: {blog_topic}\")\n    print(\"=\" * 60)\n\n# Check for cached blog post first\n    if use_blog_cache:\n        cached_blog = get_cached_blog_post(session_state, blog_topic)\n        if cached_blog:\n            print(\"ğŸ“‹ Found cached blog post!\")\n            return cached_blog\n\n# Phase 1: Research and gather sources\n    print(\"\\nğŸ” PHASE 1: RESEARCH & SOURCE GATHERING\")\n    print(\"=\" * 50)\n\nsearch_results = await get_search_results(\n        session_state, blog_topic, use_search_cache\n    )\n\nif not search_results or len(search_results.articles) == 0:\n        return f\"âŒ Sorry, could not find any articles on the topic: {blog_topic}\"\n\nprint(f\"ğŸ“Š Found {len(search_results.articles)} relevant sources:\")\n    for i, article in enumerate(search_results.articles, 1):\n        print(f\"   {i}. {article.title[:60]}...\")\n\n# Phase 2: Content extraction\n    print(\"\\nğŸ“„ PHASE 2: CONTENT EXTRACTION\")\n    print(\"=\" * 50)\n\nscraped_articles = await scrape_articles(\n        session_state, blog_topic, search_results, use_scrape_cache\n    )\n\nif not scraped_articles:\n        return f\"âŒ Could not extract content from any articles for topic: {blog_topic}\"\n\nprint(f\"ğŸ“– Successfully extracted content from {len(scraped_articles)} articles\")\n\n# Phase 3: Blog post writing\n    print(\"\\nâœï¸ PHASE 3: BLOG POST CREATION\")\n    print(\"=\" * 50)\n\n# Prepare input for the writer\n    writer_input = {\n        \"topic\": blog_topic,\n        \"articles\": [article.model_dump() for article in scraped_articles.values()],\n    }\n\nprint(\"ğŸ¤– AI is crafting your blog post...\")\n    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))\n\nif not writer_response or not writer_response.content:\n        return f\"âŒ Failed to generate blog post for topic: {blog_topic}\"\n\nblog_post = writer_response.content\n\n# Cache the blog post\n    cache_blog_post(session_state, blog_topic, blog_post)\n\nprint(\"âœ… Blog post generated successfully!\")\n    print(f\"ğŸ“ Length: {len(blog_post)} characters\")\n    print(f\"ğŸ“š Sources: {len(scraped_articles)} articles\")",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#----main-execution-function----",
  "links": []
}