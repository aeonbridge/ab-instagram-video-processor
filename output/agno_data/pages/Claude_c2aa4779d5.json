{
  "title": "Claude",
  "content": "Source: https://docs.agno.com/reference/models/anthropic\n\nThe Claude model provides access to Anthropic's Claude models.\n\n| Parameter               | Type                                     | Default                        | Description                                                      |\n| ----------------------- | ---------------------------------------- | ------------------------------ | ---------------------------------------------------------------- |\n| `id`                    | `str`                                    | `\"claude-3-5-sonnet-20241022\"` | The id of the Anthropic Claude model to use                      |\n| `name`                  | `str`                                    | `\"Claude\"`                     | The name of the model                                            |\n| `provider`              | `str`                                    | `\"Anthropic\"`                  | The provider of the model                                        |\n| `max_tokens`            | `Optional[int]`                          | `4096`                         | Maximum number of tokens to generate in the chat completion      |\n| `thinking`              | `Optional[Dict[str, Any]]`               | `None`                         | Configuration for the thinking (reasoning) process               |\n| `temperature`           | `Optional[float]`                        | `None`                         | Controls randomness in the model's output                        |\n| `stop_sequences`        | `Optional[List[str]]`                    | `None`                         | A list of strings that the model should stop generating text at  |\n| `top_p`                 | `Optional[float]`                        | `None`                         | Controls diversity via nucleus sampling                          |\n| `top_k`                 | `Optional[int]`                          | `None`                         | Controls diversity via top-k sampling                            |\n| `cache_system_prompt`   | `Optional[bool]`                         | `False`                        | Whether to cache the system prompt for improved performance      |\n| `extended_cache_time`   | `Optional[bool]`                         | `False`                        | Whether to use extended cache time (1 hour instead of default)   |\n| `request_params`        | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters to include in the request                  |\n| `mcp_servers`           | `Optional[List[MCPServerConfiguration]]` | `None`                         | List of MCP (Model Context Protocol) server configurations       |\n| `api_key`               | `Optional[str]`                          | `None`                         | The API key for authenticating with Anthropic                    |\n| `default_headers`       | `Optional[Dict[str, Any]]`               | `None`                         | Default headers to include in all requests                       |\n| `client_params`         | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters for client configuration                   |\n| `client`                | `Optional[AnthropicClient]`              | `None`                         | A pre-configured instance of the Anthropic client                |\n| `async_client`          | `Optional[AsyncAnthropicClient]`         | `None`                         | A pre-configured instance of the async Anthropic client          |\n| `retries`               | `int`                                    | `0`                            | Number of retries to attempt before raising a ModelProviderError |\n| `delay_between_retries` | `int`                                    | `1`                            | Delay between retries, in seconds                                |\n| `exponential_backoff`   | `bool`                                   | `False`                        | If True, the delay between retries is doubled each time          |",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    }
  ],
  "url": "llms-txt#claude",
  "links": []
}