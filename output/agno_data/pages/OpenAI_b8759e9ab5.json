{
  "title": "OpenAI",
  "content": "Source: https://docs.agno.com/reference/models/openai\n\nThe OpenAIChat model provides access to OpenAI models like GPT-4o.\n\n| Parameter               | Type                                               | Default        | Description                                                                        |\n| ----------------------- | -------------------------------------------------- | -------------- | ---------------------------------------------------------------------------------- |\n| `id`                    | `str`                                              | `\"gpt-4o\"`     | The id of the OpenAI model to use                                                  |\n| `name`                  | `str`                                              | `\"OpenAIChat\"` | The name of the model                                                              |\n| `provider`              | `str`                                              | `\"OpenAI\"`     | The provider of the model                                                          |\n| `store`                 | `Optional[bool]`                                   | `None`         | Whether to store the conversation for training purposes                            |\n| `reasoning_effort`      | `Optional[str]`                                    | `None`         | The reasoning effort level for o1 models (\"low\", \"medium\", \"high\")                 |\n| `verbosity`             | `Optional[Literal[\"low\", \"medium\", \"high\"]]`       | `None`         | Controls verbosity level of reasoning models                                       |\n| `metadata`              | `Optional[Dict[str, Any]]`                         | `None`         | Developer-defined metadata to associate with the completion                        |\n| `frequency_penalty`     | `Optional[float]`                                  | `None`         | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |\n| `logit_bias`            | `Optional[Any]`                                    | `None`         | Modifies the likelihood of specified tokens appearing in the completion            |\n| `logprobs`              | `Optional[bool]`                                   | `None`         | Whether to return log probabilities of the output tokens                           |\n| `top_logprobs`          | `Optional[int]`                                    | `None`         | Number of most likely tokens to return log probabilities for (0 to 20)             |\n| `max_tokens`            | `Optional[int]`                                    | `None`         | Maximum number of tokens to generate (deprecated, use max\\_completion\\_tokens)     |\n| `max_completion_tokens` | `Optional[int]`                                    | `None`         | Maximum number of completion tokens to generate                                    |\n| `modalities`            | `Optional[List[str]]`                              | `None`         | List of modalities to use (\"text\" and/or \"audio\")                                  |\n| `audio`                 | `Optional[Dict[str, Any]]`                         | `None`         | Audio configuration (e.g., `{\"voice\": \"alloy\", \"format\": \"wav\"}`)                  |\n| `presence_penalty`      | `Optional[float]`                                  | `None`         | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |\n| `seed`                  | `Optional[int]`                                    | `None`         | Random seed for deterministic sampling                                             |\n| `stop`                  | `Optional[Union[str, List[str]]]`                  | `None`         | Up to 4 sequences where the API will stop generating further tokens                |\n| `temperature`           | `Optional[float]`                                  | `None`         | Controls randomness in the model's output (0.0 to 2.0)                             |\n| `user`                  | `Optional[str]`                                    | `None`         | A unique identifier representing your end-user                                     |\n| `top_p`                 | `Optional[float]`                                  | `None`         | Controls diversity via nucleus sampling (0.0 to 1.0)                               |\n| `service_tier`          | `Optional[str]`                                    | `None`         | Service tier to use (\"auto\", \"default\", \"flex\", \"priority\")                        |\n| `strict_output`         | `bool`                                             | `True`         | Controls schema adherence for structured outputs                                   |\n| `extra_headers`         | `Optional[Any]`                                    | `None`         | Additional headers to include in requests                                          |\n| `extra_query`           | `Optional[Any]`                                    | `None`         | Additional query parameters to include in requests                                 |\n| `extra_body`            | `Optional[Any]`                                    | `None`         | Additional body parameters to include in requests                                  |\n| `request_params`        | `Optional[Dict[str, Any]]`                         | `None`         | Additional parameters to include in the request                                    |\n| `role_map`              | `Optional[Dict[str, str]]`                         | `None`         | Mapping of message roles to OpenAI roles                                           |\n| `api_key`               | `Optional[str]`                                    | `None`         | The API key for authenticating with OpenAI (defaults to OPENAI\\_API\\_KEY env var)  |\n| `organization`          | `Optional[str]`                                    | `None`         | The organization ID to use for requests                                            |\n| `base_url`              | `Optional[Union[str, httpx.URL]]`                  | `None`         | The base URL for the OpenAI API                                                    |\n| `timeout`               | `Optional[float]`                                  | `None`         | Request timeout in seconds                                                         |\n| `max_retries`           | `Optional[int]`                                    | `None`         | Maximum number of retries for failed requests                                      |\n| `default_headers`       | `Optional[Any]`                                    | `None`         | Default headers to include in all requests                                         |\n| `default_query`         | `Optional[Any]`                                    | `None`         | Default query parameters to include in all requests                                |\n| `http_client`           | `Optional[Union[httpx.Client, httpx.AsyncClient]]` | `None`         | HTTP client instance for making requests                                           |\n| `client_params`         | `Optional[Dict[str, Any]]`                         | `None`         | Additional parameters for client configuration                                     |\n| `retries`               | `int`                                              | `0`            | Number of retries to attempt before raising a ModelProviderError                   |\n| `delay_between_retries` | `int`                                              | `1`            | Delay between retries, in seconds                                                  |\n| `exponential_backoff`   | `bool`                                             | `False`        | If True, the delay between retries is doubled each time                            |",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    }
  ],
  "url": "llms-txt#openai",
  "links": []
}